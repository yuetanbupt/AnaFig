{
    "0": {
        "figure1": "2505.02672v1_Figure1",
        "label1": "fig:1",
        "caption1": " (Color online) (a) Scheme of the model: the vision cone of the black particle is the blue region defined by the angle $2\\beta$. (b) Phase diagram varying the angle $\\beta$ and the noise intensity $D_{\\theta}$. The circle encloses a region of coexistence of different collective patterns. The inset displays the probability, starting from random initial conditions, of observing such patterns. Color code: worms (red), 3-twist (blue), 2-twist (green), ring (violet), and cloud (yellow); for more details see \\cite{SuppInf}. }  \\end{center",
        "text": "{\\it Model.--} We consider a system of $N$ active particles that move at speed $v_0$  and interact with all particles within their field of view via a long-range attractive force.  The equation of motion of the $i$-th particle is given by:  where, in Eq.~(\\ref{eq:x}), ${\\bf x}_i$ is the particle position,  $v_0$ corresponds to the particle active speed,  and $\\theta_i$ encodes the velocity direction using $\\hat{\\bf{e}}[\\cdot] \\equiv \\cos(\\cdot)\\hat{x} + \\sin(\\cdot)\\hat{y}$.   In Eq.~(\\ref{eq:theta}), $\\gamma$ is a relaxation constant,  $\\Omega_i$ denotes the set of particles present in the field of view of particle $i$ [see Fig. \\ref{fig:1}(a)],  and $n_i$ is the cardinal number of $\\Omega_i$, i.e. the number of particles in the field of view.   Finally, $\\alpha_{ij}$ is the polar angle associated with the vector  $\\mathbf{\\Delta}_{ij}=({\\bf x}_j- {\\bf x}_i)/|| {\\bf x}_j- {\\bf x}_i|| = \\hat{\\bf{e}}[\\alpha_{ij}]$,  and $\\xi_i(t)$ is a white noise with  $\\langle \\xi_i(t) \\rangle = 0$ and  $\\langle \\xi_i(t)  \\xi_j(t') \\rangle = \\delta_{ij}\\delta(t-t')$  with $D_{\\theta}$ an angular diffusion coefficient.  The set $\\Omega_i$ is defined by the following condition: any particle $j$ such that  $\\hat{\\bf{e}}[\\alpha_{ij}]\\cdot\\hat{\\bf{e}}[\\theta_i]>\\cos(\\beta)$  belongs to the field of view of the particle $i$. Note that $\\beta$ %is then  controls the size of the vision cone. % [see Fig.\\ref{fig:1}].   In the following, without loss of generality, we fix $v_0=1$ and $\\gamma=5$.\nAs in the Game of Life ~\\cite{adamatzky2010game}, the initial configuration of the particles determines the collective pattern we will observe [see Fig.\\ref{fig:1}(b)].  This suggests the coexistence of several attraction basins for fixed parameter sets.  Importantly, the transport properties of the center of mass (CM) of these structures differ from one structure to another: the CM can diffuse, move ballistically (for a very long characteristic time), or rotate.  Interestingly, there is a clear connection between the topology of the structure and its transport properties.",
        "summarize_figure": "2505.02672v1_Figure1",
        "summarization": "The image illustrates a model of active particles and their emergent phase behavior. The left panel shows that a particle moves with fixed speed and interacts only with others within a vision cone defined by angle 2β, aligned with its velocity direction. The right panel presents a phase diagram mapping the collective behaviors as functions of β and the angular noise intensity Dθ. Different colors represent distinct dynamic patterns: worms (red), 3-twist (blue), 2-twist (green), ring (violet), and cloud (yellow). A marked region indicates coexistence of multiple patterns, and the inset quantifies their respective probabilities from random initial conditions. The diagram highlights the sensitivity of the system to both visual range and noise, revealing the coexistence of attractor basins in parameter space."
    },
    "1": {
        "figure1": "2505.02675v1_nodes_ratio_SD",
        "label1": "fig: emp vs. theo. SD",
        "caption1": "This is a plot of the number of nodes vs. the ratio of empirical and theoretical SD for each component of $\\beta$. The color code is identical to that of Figure \\ref{fig: decreasing bias}.",
        "text": "In Figure~\\ref{fig: emp vs. theo. SD}, we are checking the efficiency of our estimate by comparing the standard deviation of our estimates to the theoretical standard deviation predicted by the GLM theory. With oracle latent positions, this ratio is very close to $1$ for all parameters, thus supporting our theory that our estimate is normal. With the other two methods, the ratios for $\\beta_1, \\beta_2$ are close to $1$. For $\\beta_3$, it is not as close, but trends toward $1$. As for $\\beta_4$, it is the least accurate, but it also trends toward $1$.",
        "summarize_figure": "2505.02675v1_nodes_ratio_SD",
        "summarization": "The image plots the ratio of empirical to theoretical standard deviation for parameters β1 to β4 across varying node counts, comparing three methods: NO, OA, and OL. For β1 and β2, all methods yield ratios close to 1, indicating accurate variance estimation. For β3, NO and OA show overestimated variance, while OL remains near 1. For β4, NO and OA yield significantly higher ratios, especially with fewer nodes, but all methods trend toward 1 as node count increases. The plot highlights how estimation accuracy improves with more data and varies across methods and parameters."
    },
    "2": {
        "figure1": "2505.02675v1_Polarization_Through_Time",
        "label1": "fig: Polarization Exp",
        "caption1": "This is an example of latent position polarizing over time. For this simulation, we used $\\beta = [1,1,-4,5]$, initialized at $\\Dir(\\begin{bmatrix} 1&1&1 \\end{bmatrix})$. We can see that the latent positions are well-mixed for a while in the beginning, then the groups start to distance themselves from other groups. By $t = 12$, the latent positions are almost completely separated by group.",
        "text": "Since the flocking/polarizing behavior happens at the groups level, the size of $\\beta_2$ determines the rate of flocking, i.e. how fast each group is contracting, and the sign of $\\beta_3$ will determine the type of behavior that the model will display. Large value of $\\beta_2$ corresponds to a fast rate of flocking within each group. $\\beta_3 > 0$ means that each node will be attracted to the latent position of all its neigbors with different group membership, i.e., all latent positions will get closer, and the model will display flocking behavior. In contrast, when $\\beta_3 <0$, every node will be repelled from the latent positions of all its neighbors with different group membership, so the latent positions of nodes with different group membership will grow further apart, thus resulting in a polarized model. Figure \\ref{fig: Polarization Exp} shows an example of the evolution in the latent space for a polarizing model.",
        "summarize_figure": "2505.02675v1_Polarization_Through_Time",
        "summarization": "The image illustrates group polarization in latent space over time at T = 6, 8, 10, and 12. Initially, the three groups (colored red, green, and blue) are spatially mixed, but as time progresses, they become increasingly separated. By T = 12, the groups are clearly polarized in the 2D latent space. The contour plots reveal increasing intra-group cohesion and inter-group separation. This reflects the influence of model parameters where repulsion between different group members leads to spatial divergence of latent positions."
    },
    "3": {
        "figure1": "2505.02675v1_AW_eigenplt",
        "label1": "fig: Eigen Away",
        "caption1": "This is the plot of Eigenvalues vs. rank for the Away graph at period $0$ and $1$. We used this to determine the dimension to embed the adjacency matrices. We can note that eigenvalues with absolute values higer than 20 are all positive. That corresponds to top $10$-ish eigenvalues.",
        "text": "In Figure \\ref{fig: Eigen Away}, we show the scree plots, for the adjacency matrices of the Away group as mentioned in Section \\ref{sec: the away group}. We can see that eigenvalues with absolute values greater than $20$ are all positive and there are about $10$ of them.",
        "summarize_figure": "2505.02675v1_AW_eigenplt",
        "summarization": "The image shows the eigenvalues of the adjacency matrix for the Away group at time t = 0 and t = 1, ranked in descending order. The horizontal axis indicates the rank, and the vertical axis shows the eigenvalue magnitudes, with color distinguishing positive (blue) and negative (red) values. At both time points, the top roughly 10 eigenvalues are positive and exceed 20, indicating that the dominant structural information is captured by these leading components. This justifies using the top positive eigenvalues for dimensional embedding."
    },
    "4": {
        "figure1": "2505.02680v1_WRactivity_generation_example_Europe",
        "label1": "fig:WRactivitygeneration",
        "caption1": "Visualisation of $WRact_{mean}$ and $WRact_{agg}$ by using the weather regime index. The upper panel shows the ensemble forecast (green lines) and the perfect member (ERA5, turquoise line) of the daily IWR. Further, the dark blue horizontal lines indicate the mean weekly IWR in ERA5 and the turquoise dots the daily IWR in ERA5 for forecast week 3. The latter is used for the computation of the $WRact_{agg}$ by comparing them against the activity threshold (black dashed line). In the lower panel, for each forecast week, the $WRact_{mean}$ (dark blue bars) and $WRact_{agg}$ (turquoise bars) in ERA5 and in forecasts (purple and green bars respectively) are shown. By definition $WRact_{mean}$ (dark blue bars) for ERA5 takes values of either 0.0 or 1.0. ",
        "text": "} Figure \\ref{fig:WRactivitygeneration} illustrates the computation process. Seven consecutive daily IWR values from ERA5 reanalysis, shown as turquoise circles in the upper panel, are averaged to compute the weekly mean IWR (blue circles). This value (here 2.1 in week 3) is then compared to the activity threshold IWR$_{min}$ (black dashed line, here 1.0), determining whether the regime is considered active (1.0) or inactive (0.0). The corresponding $WRact_{mean}$ is represented as blue bar in the lower panel. The $WRact_{agg}$ follows a similar approach, but instead assesses each daily IWR value individually against IWR$_{min}$. The fraction of days exceeding IWR$_{min}$ defines the $WRact_{agg}$, visualised by the turquoise bar in the lower panel.\nThe same procedure applies to ensemble forecasts, where the IWR forecast for each ensemble member, initialised at the beginning of the shown period, is processed in the same manner (green lines in the upper panel of Figure \\ref{fig:WRactivitygeneration}). The resulting $WRact_{mean}$ for the ensemble is displayed as purple bars in the lower panel, while the $WRact_{agg}$ is shown in green. This methodology enables a comprehensive assessment of weather regime activity across both, reanalysis and forecasts. Since this process is applied individually to each weather regime, multiple regimes can be considered active simultaneously, reflecting the inherent variability in large-scale atmospheric patterns.",
        "summarize_figure": "2505.02680v1_WRactivity_generation_example_Europe",
        "summarization": "This figure shows how weather regime activity is calculated using the WR index. The upper panel has the NWP ensemble daily WR index (green lines) and ERA5 daily values (turquoise line). Dark blue horizontal lines are weekly means, and turquoise dots are ERA5 daily values for week 3. Comparing these to the black dashed activity threshold helps compute WRact_agg. The lower panel displays WRact_mean (from weekly means) and WRact_agg (from daily exceedances) for ERA5 and forecasts over different weeks. ERA5's WRact_mean is either 0 or 1, and WRact_agg offers more detail. Forecasts identify the regime correctly but underestimate peak activity in week 3."
    },
    "5": {
        "figure1": "2505.02680v1_phase_diagram_OMI_2018-02-10_ABCvisualisation",
        "label1": "fig:MJOphasediagram",
        "caption1": "Phase diagram of the outgoing long-wave radiation Madden-Julian Oscillation index. The first and second principal components derived from empirical orthogonal function analysis of filtered OLR data are given on the x- and y-axis. Further the phases of the MJO are indicated with dashed lines and labelling. For visualisation how the atmospheric-conditioned climatological forecast works, one initial date, February 10, 2018, is marked with a red diamond and all values in the 45\\textdegree arc used for the MJO-conditioned climatological forecast of that initialisation date (blue dots). Values in the $\\pm45$ days window around the initial date (orange dots) are excluded from the data used for the computation. With grey dots, all other data points of the 91 day running climatology are indicated. For a visualisation of the OMI phase space similar to the real-time multivariate MJO index (RMM1/2), OMI1 is multiplied by -1 and OMI1 and OMI2 are exchanged with each other. ",
        "text": "The sliding atmospheric-conditioned climatological forecast for the MJO is generated as follows (illustrated in Figure \\ref{fig:MJOphasediagram}): First, all historical OMI1 and OMI2 pairs (the first two principal components of the OLR anomaly empirical orthogonal function) within a 91-day sliding climatological window (grey dots) centred on the day of year of the current initialisation date (red diamond) are collected. Rather than selecting only the single MJO phase for the current date, we include all historical dates whose OMI1/OMI2 pairs fall within a 45\\textdegree arc (blue dots) around the current OMI pair in the phase space. Dates with an OMI amplitude less than 1 (inner circle), representing the MJO inactive phase, are excluded. Additionally, to ensure independence from recent events all events in a windows of $\\pm$ 45 days (orange dots) are removed from consideration. Finally, the mean $WRact_{agg}$ is computed from the remaining historical dates and used as the MJO-conditioned climatological forecast for the current initialisation.",
        "summarize_figure": "2505.02680v1_phase_diagram_OMI_2018-02-10_ABCvisualisation",
        "summarization": "The figure shows the OMI phase diagram for February 10, 2018, mapping MJO activity in the space of its first two principal components derived from OLR anomalies. The red diamond marks the initial date, blue dots represent past events within a 45-degree slice around the current phase, orange dots are excluded events within ±45 days of the initial date, and grey dots denote the full 91-day climatology. Dashed lines divide the eight MJO phases, and the inner circle indicates inactive phases with amplitude less than 1. This visualization demonstrates how MJO-conditioned climatological forecasts are constructed using phase-space proximity while ensuring temporal independence."
    },
    "6": {
        "figure1": "2505.02680v1_baserates_GL_18_variablephasespecific_Europe",
        "label1": "fig:baserates_GLactivity",
        "caption1": "Base rate (blue dots) and forecast rate (orange dots) for weekly mean Greenland Blocking activity ($WRact_{mean, GL}$) in the extended winter period (November--March). These rates are split into subsets based on phases of the MJO or SPV with a lead time of three weeks. The size of the subset is indicated at the bottom of the respective phase-column. ",
        "text": "As a preliminary step, we compare the base rate and forecast rate of weekly Greenland Blocking activity three weeks after specific states of the MJO and SPV (Figure \\ref{fig:baserates_GLactivity}). For an analysis across all forecasting weeks see Figure \\ref{fig:rates_GLactivity_allleads} in the Supplementary Material. On average, throughout the analysis period, the base rate and the forecast rate of $WRact_{mean, GL}$ are approximately 15\\%, with the forecast rate slightly lower than the base rate, indicating a slight negative bias in $WRact_{mean, GL}$ and thus in GL frequency as found earlier \\citep[e.g.,][]{Osman2023}. An analysis conditioned on the MJO phases reveals that during the inactive MJO phase, the base rate and forecast rate align closely, both at approximately 14\\%. The base rate is generally higher for MJO phases 5--8 and 1, while it is lower for phases 2--4. Therefore, the phases 5--8 and 1 can be considered as climatological windows of opportunities (phases 7, 8 and 1 identify as significant for GL, according to the bootstrap test, see Section \\ref{chap:methodsWoO}). Notably, forecast rates are always closer to the climatological base rate and the inactive phase than the respective base rates. This discrepancy may be attributed to the three-week forecast lead time, as extended-range forecasts tend to regress toward climatological values.  A similar behaviour between the base rate and forecast rate can be observed for strong and weak SPV states, although the differences are negligible. The difference between the base rate and forecast rate for the neutral SPV phase is larger, which could be an indicator that the model is able to represent the occurrence of $WRact_{mean, GL}$ better when an anomalous SPV state (either strong or weak) is present at initialisation time. The positive base rate of $WRact_{mean, GL}$ after phases of weak SPV, with a significant increase compared to the neutral SPV phase, makes the weak SPV state a climatological window of opportunity. One can also consider the strong SPV state as a climatological window of opportunity but for a reduced occurrence of Greenland Blocking, which is also captured by the forecast rate.\nOverall, Figure \\ref{fig:baserates_GLactivity} suggests that while the forecasted frequency of weather regime activity aligns with observed occurrences, discrepancies remain in capturing the observed $WRact_{mean, GL}$ three weeks after a specific atmospheric state. Large differences exist particularly when separating by MJO phases at initialisation, with the mean frequency of $WRact_{mean, GL}$ ranging from 6\\% following MJO phase 3 to 28\\% following MJO phase 8. The absolute values of the base rates and forecast rates should be treated with care. The sample size for individual MJO phases, where an NWP reforecast is initialised, is small (1720 reforecasts considered across 21 winter seasons, sample size per phase is indicated at bottom of Figure \\ref{fig:baserates_GLactivity}). MJO phases 7,8 and 1 as well as the weak SPV can be considered as climatological window of opportunity with significantly different base rates compared to their neutral phases, marking them as promising phases for further analysis to identify whether these climatological signals also translate into model windows of opportunities.      }\nThe HR' and PSS' values closely align for almost all MJO and SPV states, suggesting that the false alarm rates (FAR) in these sub-selections are similar to those of the neutral phases (not shown). The only exception is the weak SPV state during which PSS' is smaller than HR', indicating that the FAR is greater than for the neutral phase. As shown in Figure \\ref{fig:baserates_GLactivity}, the BR' values are positive for MJO phases 5--8 and 1. Furthermore, for phases 8 and 1, both HR' and PSS' are positive, indicating that these phases act as a type 2 model window of opportunity, where BR', HR', and PSS' are all positive. Note that only for MJO phase 1 all three anomalies are significantly different to the neutral phase on a 90\\% confidence interval. MJO phase 4 is classified as a type 3 model window of opportunity for $WRact_{mean, GL}$ with a three-week lead time. In this case, HR' and PSS' is distinctly positive (significant difference to neutral phase), despite the BR' being negative. This suggests that, although $WRact_{mean, GL}$ is significantly less frequent following phase 4, the forecast accurately predicts these fewer occurrences, with an even lower FAR than in the neutral phase.  The signals associated with SPV phases are less distinct, likely due to the broad classification into only three categories (strong, neutral and weak SPV) which encompasses a wide range of individual cases. Further, with the SPV index being a scalar, no information about the further temporal evolution of the index is provided. Three weeks after a weak SPV, $WRact_{mean, GL}$ is increased (positive BR'), and the forecast also predicts more frequent activity, leading to a higher HR. However, the FAR also increases, which reduces PSS'. In fact, PSS' is negative, indicating the HR/FAR ratio is worse than for the neutral SPV phase, categorising this as a type 1 model window of opportunity. In contrast, strong SPV phases are followed by less frequent $WRact_{mean, GL}$, along with a decrease in HR, PSS, and FAR, ultimately resulting in no identifiable window of opportunity type. The significant positive and negative BR' for the weak and strong SPV state, respectively, indicate that there is a strong link between the SPV and the occurrence of a weather regime activity. The negative PSS' (and therefore the higher FAR) following the weak SPV state indicate that the model has trouble predicting GL activities correctly following an extreme state of the SPV. The model shows a GL activity response too often following a weak SPV state, even when it does not realise.",
        "summarize_figure": "2505.02680v1_baserates_GL_18_variablephasespecific_Europe",
        "summarization": "The plot shows base rates (blue) and forecast rates (orange) of Greenland blocking activity during November - March, categorized by MJO phases and SPV strength. Sample sizes are indicated below each category. Activity peaks after MJO phases 8 (28%) and 1 (25%), and is lowest after phase 3 (6%). Weak SPV states increase activity rates, but forecasts overpredict, reducing accuracy. Strong SPV states have less activity with little forecast signal. Forecasts often approach climatological values, yet MJO phase 1 and weak SPV states show distinct responses, hinting at model windows of opportunity."
    },
    "7": {
        "figure1": "2505.02681v1_network",
        "label1": "fig:network",
        "caption1": "Network diagram for the $N = 74$ student drawings. Each node represents a drawing element, with diameter proportional to element frequency and color representing the element's category from the Communities of Practice framework. Elements that were depicted in fewer than 5 drawings are omitted. Each edge represents two elements occurring in the same drawing, with thickness representing the number of co-occurrences.",
        "text": "As depicted in Figure \\ref{fig:network}, we represent each drawing element as a node with a diameter proportional to the number of drawings this element was found in. Each node is color-coded based on its category within the COP framework (goal, member, practice). For each pair of elements that occur in the same drawing, we connect their nodes with an edge. An edge's weight is proportional to the number of drawings these two elements were found in together. Visually depicting this network immediately gives a general idea of which nodes are more central, and therefore more important to the network.\nFigure \\ref{fig:network} shows the network of drawing elements created by all $N = 74$ students. Again, each node represents a single element with size corresponding to the element's frequency $f^{\\alpha}_{i}$ across the set of drawings and color indicating the node's category in the COP framework. Nodes are connected by edges if they appear in the same drawing, with edge thickness corresponding to the number of co-occurrences $a^{\\alpha}_{ij}$.  %%% Bring this back up in limitations with an example.\nWithin the full network and each subnetwork, we found different relative frequencies of goal-, member-, and practice-related elements. Visually examining Figure \\ref{fig:network} and Tables \\ref{tab:centralities} through \\ref{tab:subgroups}, we can see that practice-related elements dominate the network. This is perhaps to be expected, as the IPLS studio is a hands-on learning environment with many items of equipment, technical tasks, and ways of collaborating that are new to these students and important for their course outcomes. This prominence of practices illustrates Irving, McPadden, and Caballero's argument that an introductory lab that functions as a COP ``facilitates a focus on practice development'' by engaging students in a shared repertoire of authentic practices \\cite{irving2020communities}.\nA primary assumption of this study is that students responding to the drawing-based lab survey will choose to depict elements that are important to them. This assumption is supported in the survey's validation study \\cite{lane2024using}, but it is always possible that different students respond to a survey differently. To address this concern, we point to the broad diversity of elements and their combinations depicted in Figure \\ref{fig:network}. Whether the students' reasoning was conscious or subconscious, they have chosen different elements to depict, and the survey picks up these differences without needing to identify their origin. %In another manuscript \\cite{lane2025using} we demonstrate how students in the same lab group created very different depictions, which our element cataloging procedure can detect.\nOur survey is designed under the assumption that a student's experience in an introductory lab is appropriately modeled as a COP. While this assumption is supported by the literature \\cite{irving2014conditions,irving2020communities}, and likely true for many students' experiences, it is not guaranteed that every studio section in our study similarly functions as a COP, or that each lab group or each student similarly experiences the lab as a COP. However, our whole-class network diagram in Figure \\ref{fig:network} shows a consistent central experience for our students, with 20 elements being depicted in at least 25\\% of the students' drawings, and the remaining elements revealing a diversity of perspectives across this common experience. The fact that the center of the network features so few goals speaks to potential avenues for reinforcing the shared purpose of the course, which we discuss below.",
        "summarize_figure": "2505.02681v1_network",
        "summarization": "The image shows a network graph constructed from 74 student drawings, where each node represents a drawing element, sized by its frequency across drawings and colored by its category in the Communities of Practice framework (red: practice, green: member, blue: goal). Edges connect elements co-occurring in the same drawing, with thickness indicating co-occurrence frequency. Practice-related elements dominate the network center, reflecting students’ focus on hands-on activities and technical collaboration. The diversity of peripheral nodes suggests varied individual perspectives. The sparse presence of goal-related elements in the center highlights a possible need to reinforce shared objectives within the course structure."
    },
    "8": {
        "figure1": "2505.02688v1_controlled_diff_sgd",
        "label1": "fig:1dcompare",
        "caption1": "Top two figures: Demonstration of numerical solutions using the Batch projection algorithm. Bottom two figures: demonstration of error decay by using different number of iterations for $K$.}  % Add a label for referencing",
        "text": "To design parameters so that an analytic formula of the solution exists, we pick the function $r^i, x^{i,*}$ to be the following, and $u^i_t$ can be found accordingly:  \t& r^1_t := \\frac{-t^2/2}{\\beta_t}, \\ x^{1,*}=t+(\\frac{T^2}{2 \\sigma^2} - \\frac{X^1_T}{\\sigma^2}  ) \\alpha_t , \\  u^1_t= \\frac{-t^2/2+T^2/2-X_T^1}{\\beta_t}, \\nonumber \\\\  \t&r^2_t :=\\frac{-\\sin(t)}{\\beta_t}, \\ x^{2,*}=\\cos(t) +(\\frac{\\sin(T)}{\\sigma^2}- \\frac{X^2_T}{\\sigma^2} )\\alpha_t , \\ u^2_t :=\\frac{-\\sin(t)+\\sin(T)-X^2_T}{\\beta_t}.  where $$\\alpha_t=\\ln\\frac{(1+\\sigma^2)+\\sigma^2 T}{(\\sigma^2+1)+\\sigma^2(T-t)} \\ , \\ \\beta_t=(1+\\sigma^2)+\\sigma^2(T-t) $$ and with $D=\\ln(1+\\frac{\\sigma^2 T}{1+\\sigma^2} )/ \\big(\\sigma^2+\\ln(1+\\frac{\\sigma^2 T}{1+\\sigma^2} )  \\big) $ The parameters are set to be $x_0=1.0, \\sigma=0.5$, $T=1.0$.  We solve the problem using projection algorithm with batch samples. In this example (Figure \\ref{fig:1dcompare}), to obtain the numerical solutions for the top two figures, we take the total of temporal discretization $N=60$ and use batch size $M=N$. The total number of iterations is taken to be $N^2=3600$. The red curves are the exact solutions. To see that the predicted decay rate of $\\sim \\mathcal{O}(\\sqrt{\\frac{1}{K}+ \\frac{1}{N^2}})$, we note that:",
        "summarize_figure": "2505.02688v1_controlled_diff_sgd",
        "summarization": "This figure presents four panels illustrating the numerical behavior of a Batch SGD algorithm for controlled differential equations. The top panels show the computed solutions u1 and u2 with N=60 time steps, where numerical results (blue dots) closely match the analytic solutions (red lines), validating solution accuracy. The bottom panels depict the logarithmic decay of error as a function of N for two scenarios: when iteration count K scales with N (left), the slope is -0.5; when K scales with N² (right), the slope is -1.0. These trends align with the expected theoretical decay rate of O(sqrt(1/K + 1/N²)), confirming the algorithm's convergence characteristics under different iteration regimes."
    },
    "9": {
        "figure1": "2505.02688v1_controlled_diff_contraction",
        "label1": "fig:2dcompare",
        "caption1": "Top two figures: Demonstration of numerical solutions using the damped contraction algorithm. Bottom two figures: demonstration of error decay by using different number of batch size $M$. $x-$axis is on log scale.}  % Add a label for referencing",
        "text": "We now solve the problem using damped contraction algorithm with batch samples. In this example (Figure \\ref{fig:2dcompare}), to obtain the numerical solutions for the top two figures, we take the total of temporal discretization $N=55$ and use batch size $M=N^2$. The total number of contraction steps is taken to be $800$.",
        "summarize_figure": "2505.02688v1_controlled_diff_contraction",
        "summarization": "This figure demonstrates the performance of the damped contraction algorithm applied to controlled differential equations. The top panels show numerical solutions for control variables u1 and u2 at N=55 time steps, with blue dots closely following the red analytic curves, confirming numerical accuracy. The bottom panels depict error decay on a logarithmic N-axis under two batch size settings. When M equals N squared (left), the error decays with a slope of -1.0, indicating fast convergence. In contrast, with M equal to N (right), the slope is -0.5, reflecting slower decay. These results highlight the algorithm’s improved efficiency with larger batch sizes."
    },
    "10": {
        "figure1": "2505.02688v1_uncontrolled_diff_sgd",
        "label1": "fig:3dcompare",
        "caption1": "Top two figures: Demonstration of numerical solutions using the Batch projection algorithm. Bottom two figures: demonstration of error decay by using different number of iterations for $K$. $x-$axis in log scale.}  % Add a label for referencing",
        "text": "We solve the problem using projection algorithm with batch samples with high order scheme for the forward SDE. In this example (Figure \\ref{fig:3dcompare}), to obtain the numerical solutions for the top two figures, we take the total temporal discretization $N=60$ and use batch size $M=N$. The total number of iterations is taken to be $N^2=3600$. To see that the predicted decay rate of $\\sim \\mathcal{O}(\\sqrt{\\frac{1}{K}+ \\frac{1}{N^2}})$, we note that:",
        "summarize_figure": "2505.02688v1_uncontrolled_diff_sgd",
        "summarization": "This figure presents numerical results and error decay analysis for solving an uncontrolled stochastic differential equation using the Batch projection algorithm. The top panels show the numerical solutions for state variables u1 and u2 at N=60 time steps, where the blue dots closely follow the red analytic curves, indicating high accuracy. The bottom panels demonstrate error decay with respect to increasing N on a logarithmic scale. Whether the total iteration count K scales linearly with N (left) or quadratically (right), the observed error decay shows a slope of -1.0, supporting the predicted convergence rate of order sqrt(1/K + 1/N^2). This validates the algorithm’s efficiency and consistency under a high-order forward SDE scheme."
    },
    "11": {
        "figure1": "2505.02689v1_figure1",
        "label1": "fig1",
        "caption1": "\\textbf{Multi-scale microfluidic experimental set-up for filtration}. $a$) The porous structure is comprised of more than 65,000 grains: the longitudinal distance between inlet (top left) and outlet (bottom right) is $L = 970$~mm. $b$) A close-up view of the designed structure where solid grains are shown as black disks and pore throats are the space among closest neighbors. $c$) Statistical distribution (PDF) of grain radii $r$ and $d$) of pore throat size, $\\xi$. ",
        "text": "We design a porous medium with a broad pore size distribution resembling the structure of natural soils or rocks. The medium structure (shown in Fig.~\\ref{fig1}~$a$) consists of non-overlapping disks representing solid and impermeable grains (Fig.~\\ref{fig1}~$b$) having random radius $r \\in [0.06~\\textrm{mm} < r < 0.12 \\textrm{mm}]$ distributed as $P_r(r) \\sim r^{-3}$, also shown in Fig.~\\ref{fig1}~$c$. The distance between neighboring grains $\\xi$ is power law distributed with an exponential cutoff,  \tp_\\xi(\\xi) \\sim \\xi^{-\\beta} e^{-\\xi / \\xi_0}, where $\\beta = 0.6$, $\\xi_0 = 0.063$~mm, shown in Fig.~\\ref{fig1}~$d$. The set of nearest neighbor grains is defined via a Delaunay triangulation~\\cite{deAnnaPRF2017}. These grains of random size and location are placed within a folded strip of width $w = 3$~mm, equivalent to 48 times the characteristic pore size $\\xi_0$ and length $L = 990$~mm, equivalent to $15,720 \\, \\xi_0$ (see Fig.~\\ref{fig1}~$a$). We use soft lithography (see Methods) to print this geometry into a microfluidics chip (Fig.~\\ref{fig1}~$a$).",
        "summarize_figure": "2505.02689v1_figure1",
        "summarization": "This figure illustrates a multi-scale microfluidic design used for filtration experiments. Panel a shows a serpentine channel 970 mm long containing over 65,000 non-overlapping grains. Panel b magnifies a section, revealing black disk-shaped grains with pore throats between them. Panel c presents the grain radius distribution Pr(r), following a power-law decay Pr(r) ∼ r⁻³, ranging from 0.06 mm to about 0.25 mm. Panel d shows the pore throat size distribution Pξ(ξ), fitted by a truncated power law Pξ(ξ) ∼ ξ⁻ᵝ e⁻ξ/ξ̄ with mean ξ̄ = 63 microns. Neighboring grains are defined via Delaunay triangulation, and the structure is fabricated via soft lithography."
    },
    "12": {
        "figure1": "2505.02689v1_figure2",
        "label1": "fig2",
        "caption1": "\\textbf{Experimental colloidal transport and filtration by the porous structure.} Deposition Profile (DP) in panel $a.$) as the measured concentration of colloids attached to the medium solid surfaces along the distance from the inlet after $25h$ of continuous injection, $b$) Hyper-exponential fit $H_2(p,k_1,k_2)$ with $p=0.6$ and rates $k_1=0.03$ and $k_2=0.0005$.c.) Measured suspended colloids concentration at the medium outlet, Breakthrough Curve (BTC). Time is rescaled by the time needed to flush 1 pore volume $t_{PV}$. After $1t_{PV}$, a typical heavy tailing of the BTC signal with slope $t^{-0.67}$ is a signature of the anomalous transport, not captured by continuum approaches as prescribed by CFT theory.",
        "text": "We first saturate the microfluidic chip with a 1:1 milliQ water and D$_2$O mixture. The flow is imposed and controlled with a Harvard Apparatus PHD-ULTRA syringe pump, which is set at a constant flow rate $Q = 0.72 \\, \\mu$L/min. It results in an average (Darcy) velocity of $q^{\\ast} = Q/A \\phi = 0.16$~mm/s, where $A = h w = 0.15$~mm$^2$ is the chip cross-sectional surface and $\\phi = 0.46$ is the medium porosity. Once the chip is saturated, we continuously inject a colloidal suspension for about 30 hours (corresponding to the elution of 20~pore volumes). The colloidal suspension is composed of polystyrene micro-spheres (diameter 1.1~$\\mu$m and density $\\rho = 1.05$~g/mL Thermofischer Fluoromax B100) dispersed in a density-matched 1:1 milliQ water and D$_2$O mixture, avoiding particle sedimentation. The relative importance of advection and diffusion processes is quantified by the corresponding Péclet number, $Pe = \\overline{\\xi} q^{\\ast} / (\\phi D) \\sim 10^5$ based on the mean fluid velocity $q^{\\ast}$ and the colloidal diffusion coefficient $D = 1.4 × 10^{-7}$~mm$^2$/s~\\cite{bordoloi_structure_2022}. In such conditions, the flow is laminar since $Re = \\xi q^{\\ast} / \\nu \\sim 0.01$, where $\\nu = 1$~mm$^2$/s is the water kinematic viscosity. In the following, time is normalized by the characteristic time necessary to elute a pore volume with the average Darcy velocity $q^{\\ast}$, $t_{PV} = L/q^{\\ast} = 103$~minutes.\\\\ By comparing consecutive pictures collected with time-lapse video-microscopy of the fluorescent colloids within the pore space, we count the number of deposited and suspended colloids across the microfluidic reactor (as described in the Methods section). The deposition profile of colloids retained by the porous medium along the longitudinal position $x$, $a(x)$ after eluting 20 pore volumes is shown in Fig.~\\ref{fig2}~$a$. It exhibits a non-exponential behavior, as shown in Fig.~\\ref{fig2}~$b$ as blue dots. An hyper-exponential fit of the form of $H_2(x) = pe^{-k_1x} + (1-p)e^{-k_2x}/A$ (red solid line), where $A$ is a normalization factor, characterized by two characteristic scales fits relatively well the data that, thus, deviate from the CFT prediction. Fig.~\\ref{fig2}~$c$ shows the complementary BTC, $1-c(t)/c_0$: as expected, after $t/t_{PV} = 1$ it quickly decays when colloids reach the outlet. However, it deviates from the exponential decay predicted by the classical models based on dispersion~\\cite{Dentz2011} since at times larger than $t/t_{PV}= 1$, a power-law like heavy tail emerges, as characteristic of anomalous transport~\\cite{Berkowitz2006}. \\\\\nThe BTC is measured as the distribution of colloid arrival times at the outlet given by the sum of all travel times of flights and dives crossed ($BTC(t)=\\langle \\sum_n t_{F_n} + t_{D_n}\\rangle$, $n$ being the stochastic number of steps needed to reach the outlet for a trajectory). The tailing observed in Fig.~\\ref{fig2} (from experiments) is a signature of the distribution of pore sizes combined with a Poiseuille flow profile within each pore. Thus, pore size variability controls the distribution of low velocities which enhances the dispersion of an injected plume of colloids, resulting in heavy tails in the arrival time distributions. The analytical description of the DP emerges from the interaction between transport and deposition~\\cite{miele_stochastic_2019}. \\\\",
        "summarize_figure": "2505.02689v1_figure2",
        "summarization": "This figure shows experimental results on colloidal transport and deposition in a porous medium. Panel a presents the deposition profile a(x) along the longitudinal direction, showing a non-exponential decay on a log-log scale. Panel b magnifies the data and fits a hyper-exponential function H₂(x) = p·exp(-k₁x) + (1-p)·exp(-k₂x)/A to the measurements, indicating multiple characteristic deposition lengths. Panel c displays the breakthrough curve BTC as 1 - c/c₀ versus normalized time t/t_PV, with a heavy power-law tail after t/t_PV > 1, revealing anomalous transport behavior. This arises from the heterogeneous pore size distribution and Poiseuille flow effects, which produce slow-flow regions and enhance tailing, diverging from classical dispersion predictions."
    },
    "13": {
        "figure1": "2505.02710v1_143069_073-075_77_79_full_042525",
        "label1": "fig:discharges_differentPEC_121224",
        "caption1": "Time histories of key parameters in density limit discharges with varied ECRH power. Here $I_p$: plasma current; Zeff: averaged effective charge number; $P_\\text{EC}$: injected ECRH power; $V_\\text{gas}$: voltage applied to control gas puffing, which is positively correlated with the  puffed gas amount; $n_{e}$: line-averaged electron density measured using the central chord of HCN interferometer; Total Rad.: relative intensity of total radiation measured using bolometer. ",
        "text": "There are two main series of discharges in our experiments {(Table~\\ref{tab1})}, {namely, the} one with varied pre-filled neutral gas pressure at a fixed ECRH power and the other with varied ECRH heating powers at a fixed pre-filled pressure. We selected a reference discharge \\#143069 to serve as a point of comparison, {which {is based on an} ECRH-assisted Ohmic start-up scenario {similar to} those reported in references \\cite{runze2024,wenbin2024}}. The time history of its key parameters, including total radiation levels, plasma current $I_p$, ECRH power and line-available electron density $n_e$, are shown in Fig.~\\ref{fig:discharges_differentPEC_121224}. The line-averaged density, $n_e$, used in this article is measured by the central chord of the HCN interferometer unless otherwise specified. This discharge starts at 0.0 s, with a toroidal magnetic field of 2.5 T, an initial gas puffing voltage of about 3 V with the corresponding number of injected deuterium (D$_2$) of $6.6\\times10^{19}$, an ECRH power of about 600 kW with the pulse width [0.0 s, 5.0 s], and a plasma current of 250 kA. Hydrogen gas is injected as the working gas, and no other kind of gas or impurity is injected throughout the discharge. During the current plateau period, the gas injection continues until the plasma density limit of about 1.5~$n_{\\text{\\tiny G}}$ is reached. At this point, the plasma density starts to decrease and then rapidly drops to zero, without any prior manifestation of MHD activities. During the whole discharge, the feedback control technique is used to make sure the time evolution of line-averaged electron density follow the designed path~\\cite{Yuan2010}.\n{Compared with W7-X \\cite{Klinger2019,escande2022}, {which has a plasma volume $V = 30~\\mathrm{m}^3$, a magnetic field $B= 3~\\mathrm{T}$, and a minor radius $a = 0.53~\\mathrm{m}$}, EAST has {$V=9 ~\\mathrm{m}^3$, $B= 2.5$~T and $a=0.45~\\mathrm{m}$}. The magnetic field $B$ and minor radius $a$ are close for the two machines. Then, it makes sense to compare the injected power per unit volume $P/V$ for EAST and W7-X experiments. One finds respectively {$P/V = 0.6~\\mathrm{MW}/9~\\mathrm{m}^3$} for EAST, and {$2~\\mathrm{MW}/30~\\mathrm{m}^3$} for W7-X, {which are exactly the same ratio $P/V=1/15~\\mathrm{MW/m^{-3}}$}.  In addition, in W7-X experiments, a density of $2 \\times 10^{19} \\mathrm{m}^{-3}$ (an order of magnitude higher than in tokamak breakdown) is reached with a single gas puff, right before injecting waves (Section 7 of \\cite{escande2022}). In EAST, almost the same density is reached at the end of the ECRH pulse (Fig.~\\ref{fig:discharges_differentPEC_121224}), which further increases to about $5\\times10^{19} \\mathrm{m}^{-3}$. These results demonstrate tokamaks can operate in a way similar to stellarator and reach likewise a higher density limit. The big difference is in the duration of the pulse: for W7-X, the start-up takes about 100 ms, while it is more than an order of magnitude longer in EAST to reach the same density with ECRH assisted start-up. This is reasonable because a tokamak needs a finite poloidal magnetic field to confine, while the confinement is available in a stellarator from the outset.}",
        "summarize_figure": "2505.02710v1_143069_073-075_77_79_full_042525",
        "summarization": "The image shows the time - histories of key parameters in density limit discharges with varied ECRH power. Plasma current Ip​ rises and then drops sharply around 6.5 s. Effective charge number Zeff​ decreases and stabilizes. Injected ECRH power PEC​ is constant until 5 s when it shuts off, related to gas injection voltage Vgas​. Line - averaged electron density ne​ increases continuously to about 5×1019m−3 before collapsing. Total radiation intensity fluctuates and spikes before density collapse. Experiments compared discharges with varied pre - filled gas pressure at fixed ECRH power and varied ECRH power at fixed pre - filled pressure. Reference discharge #143069 has a toroidal magnetic field of 2.5 T, initial gas puffing voltage about 3 V, ECRH power about 600 kW with pulse width [0.0 s, 5.0 s], and plasma current 250 kA. Hydrogen is the working gas. Comparing EAST and W7 - X, their power per unit volume ratios are the same, but EAST takes longer to reach high density as tokamaks need poloidal magnetic field for confinement."
    },
    "14": {
        "figure1": "2505.02714v1_linear_reg_example",
        "label1": "fig:lr",
        "caption1": "Illustration of the weight forecasting procedure. Weight sequences are generated for a two-dimensional linear regression problem from Equation~\\ref{eqn:lsm}. Black points are weights from SGD, red points are predictions from forecasting. The red arrow \\textcolor{red}{$\\rightarrow$} shows farcasting from a partial weight sequence. ",
        "text": "Analogous to long-term time series forecasting (LTSF), we are given a sequence of weights from (stochastic) gradient descent or one of its variants of $n+1$ steps, denoted as $ \\mathbf{X} = (\\mathbf{w}_0, \\mathbf{w}_1,\\mathbf{w}_2,...,\\mathbf{w}_n)$ and our goal is to predict future weights $m$ steps ahead, denoted as $ \\mathbf{Y} = (\\mathbf{w}_{n+1},\\mathbf{w}_{n+2},...,\\mathbf{w}_{n+m})$, where $m \\gg 10$. For simplification, our prediction can be expressed as $ \\mathbf{X} \\in \\mathbb{R}^{d \\times (n+1)} \\rightarrow \\mathbf{Y} \\in \\mathbb{R}^{d \\times m}$.  Figure \\ref{fig:lr} illustrates our problem using 2 dimensional linear regression. Black points are weights from SGD, red points are predictions from forecasting. The red arrow \\textcolor{red}{$\\rightarrow$} shows farcasting from a partial weight sequence. }",
        "summarize_figure": "2505.02714v1_linear_reg_example",
        "summarization": "The figure illustrates a two-dimensional linear regression scenario, where black points represent weights updated via stochastic gradient descent, and red points with arrows indicate predicted future weights using a forecasting method. The horizontal and vertical axes denote the two weight components. The trajectory of SGD exhibits a curved progression, while the red arrow projects a forecasted path from a partial sequence. This visualization highlights the feasibility and challenges of farcasting weight updates based on limited historical observations, particularly for long-term forecasting beyond typical short-step predictions."
    },
    "15": {
        "figure1": "2505.02717v1_figure2_data_lupus",
        "label1": "fig:data_lupus",
        "caption1": "\\textbf{Lupus murine spleen cell centroid data}. A) Cell centroid spatial distribution of the 25 cell types in lupus murine spleen CODEX data set. The data set of consists of 3 healthy samples (BALBc-1, BALBc-2, BALBc-3), 3 samples in an early stage of the disease (MRL-4, MRL-5, MRL-6), 2 samples in an intermediate stage of the disease (MRL-7, MRL-8) and 1 sample in a late stage of the disease (MRL-9). B) Cell counts of the two types of pulp for each sample (entire slide) in tens of thousands of cells. Solid lines are guides for the eye. C) Point clouds corresponding to the main parts of the spleen, the red pulp and the white pulp, for each sample, classified by disease stage. The white pulp forms compartments surrounded by the red pulp. ",
        "text": "The first data set, obtained with co-detection by indexing (CODEX), consists of multiplexed images of normal and lupus murine spleen and it was taken from \\cite{lupus-mouse-spleen}. Nine samples were obtained from 9 mice and classified by the authors according to the state of disease progression. The authors identified 25 cell types (see \\autoref{fig:data_lupus}A) and used a Delaunay neighbourhood graph to quantify changes in contacts between pairs of cell types and in the environments immediately surrounding cells.\nThe spleen can be divided into two major anatomical regions: the red pulp, which is primarily responsible for the mechanical filtration of red blood cells, and the white pulp, which carries out an active immune response. In \\cite{lupus-mouse-spleen}, 'the major splenic anatomic compartments were reflected in two, large, mutually exclusive clusters of positive associations [between cell types], which appeared to correspond to red pulp and the white pulp'. We use these findings to classify cell types into red and white pulp (see lists on \\autoref{fig:data_lupus}A). Throughout the article, we use the terms 'red pulp cells' and 'white pulp cells' to refer to these broader categories. In \\autoref{fig:data_lupus}C we see that the white pulp roughly consists of various disconnected compartments, surrounded by the red pulp. The compartments are not closed: some red pulp cells are present in the white pulp and vice versa.",
        "summarize_figure": "2505.02717v1_figure2_data_lupus",
        "summarization": "The figure illustrates the spatial distribution and cell count dynamics of spleen cell centroids in murine lupus progression. Panel A shows the positions of 25 identified cell types across healthy, early, intermediate, and late disease stages, highlighting a progressive disruption of white pulp compartmentalization and increased red pulp infiltration. Panel B quantifies the total number of red and white pulp cells per sample, revealing a trend of increasing red pulp and decreasing white pulp cells with disease advancement. Panel C displays red and white pulp distributions across samples, showing that white pulp compartments are gradually reduced and dispersed as lupus progresses, indicating structural breakdown in splenic organization."
    },
    "16": {
        "figure1": "2505.02717v1_figure1_pipeline",
        "label1": "fig:PH_examples",
        "caption1": "\\textbf{Examples of persistent homology.} A) Example of the alpha filtration for a point cloud forming a loop. Corresponding persistence diagram with one persistent feature in degree 1 corresponding to the loop. B) Example of persistent homology for a point cloud consisting of four loops of various sizes and densities. For each loop, we indicate the corresponding feature in the persistence diagram. Less dense loops have larger birth values, bigger loops have larger death values. C) Steps of the computation of a persistence image with weight on persistence $w_1(b,p) = p$. ",
        "text": "We define the ``shape\" of data at a scale $\\epsilon$ by taking the unions of balls of radius $\\epsilon$ with centre at each data point. We illustrate this with an example in \\autoref{fig:PH_examples}A, where data points are distributed forming a loop. For small values of the scale parameter $\\epsilon$, the shape we obtain is topologically equivalent to the original points, and for large values, it consists of a large blob. For intermediate values of $\\epsilon$, the shape captures geometrical features of the data. In the example, the light-blue balls expand around each point, initially forming a set of disconnected balls, then merging to create a loop, which eventually fills in. Since relevant geometric features can appear at multiple scales, we avoid choosing an ``optimal\" value of $\\epsilon$, and instead, study the family of shapes indexed by $\\epsilon$.\nTo make the analysis computationally tractable, we replace the unions of balls by simpler combinatorial objects known as simplicial complexes, while preserving topological features. A simplicial complex $K$ is a collection of vertices (0-simplices), edges (1-simplices), triangles (2-simplices) and their generalisations in higher dimensions ($p$-simplices). Starting from the collection of data points as our simplicial complex for $\\epsilon = 0$, we progressively add simplices as $\\epsilon$ grows, following a proximity rule. The collection of simplicial complexes we obtain is known as a filtration (see \\autoref{fig:PH_examples}A). We use two constructions: the alpha filtration $A_\\bullet$ \\cite{Edelsbrunner1983-alpha,Edelsbrunner1995-alpha} and the witness filtration $W_\\bullet$ \\cite{witness1, witness2}.\nTo build the alpha filtration $A_\\bullet$, we begin by constructing the Voronoi cells of the data points. A set of data points are considered Voronoi-neighbours if their Voronoi cells intersect. For a fixed scale parameter $\\epsilon$, we construct the complex $A_\\epsilon$ by first placing a ball of radius $\\epsilon$ around each data point, as described above.  Next, we examine the intersections of these balls for Voronoi-neighbours. If the balls of two Voronoi-neighbours intersect, we add an edge (or 1-simplex) between the corresponding points, representing their connectivity at scale $\\epsilon$. To capture higher-order features beyond connectivity, we continue by adding a triangle (or 2-simplex) whenever the balls of three Voronoi-neighbours intersect. In general, we add a $p$-simplex if the balls of $p+1$ Voronoi-neighbours have a common intersection. An example of the alpha filtration can be seen in \\autoref{fig:PH_examples}A.\nThe inclusion of simplicial complexes in a filtration, $K_\\delta \\subset K_\\epsilon$ for $\\delta \\leq \\epsilon$, induces a map between the corresponding homology groups at each degree: $H_n(K_\\delta) \\to H_n(K_\\epsilon)$, which tracks the topological features across the filtration. A feature is born at parameter value $b$ if it first appears in $K_b$, and it dies at parameter value $d$ if it first stops being present in $K_d$. For example, the loop (1-hole) in \\autoref{fig:PH_examples}A appears in the filtration at $\\epsilon = b$ and is filled in at $\\epsilon = d$. The difference $d-b$ is called persistence, and it measures the prominence of the feature. This interpretation finds its formal algebraic foundation in the Structure Theorem of persistence modules, which states that all the homological information of the filtration can be summarised in a combinatorial way, as follows. The output of PH is a multiset of intervals $[b,d)$, each corresponding to the birth and death values of a feature. This information can be presented in a persistence diagram, where points with coordinates $(b,d)$ describe each feature in the data.\nWe plot the persistence diagram for the example data set in \\autoref{fig:PH_examples}A. We circle the most prominent feature in the diagram of degree 1, with coordinates $(b,d)$, corresponding to the large loop. Smaller loops appear between neighbouring points, as can be seen when $\\epsilon = b$ in the filtration, but they have low persistence (i.e. they are rapidly filled in after they appear). If the data points represented cell centroids, the large loop would correspond to a region where cells are absent, surrounded by a wall of cells (e.g. immune cell exclusion from a tumour nest, or a structural feature like a section of a lung airway). The smaller loops would correspond to small voids formed between neighbouring cells.\nTo better understand the role of birth and death values in degree 1, we consider a second data set consisting of four loops of varying sizes and densities in \\autoref{fig:PH_examples}B. We indicate which point in the persistence diagram of degree 1 corresponds to each loop. The denser the points forming the loop are, the earlier in the filtration the loop is formed, so loops (3) and (4) have lower birth values than loops (1) and (2). The larger the loop is, the later in the filtration the loop gets filled in, so loops (2) and (4) have higher death values than loops (1) and (3). Persistence, which is the difference between death and birth value, is larger for loop (3) than for loop (1), even though they have a similar size, and similarly when comparing (4) to (2). We say that (3) and (4) are more prominent than (1) and (2) respectively.\nPersistence images are a stable vectorisation of persistent diagrams that were introduced in \\cite{Adams2017}. Intuitively, persistence images are a way of ``smoothing out\" persistence diagrams. First, we change the persistence diagram from birth-death coordinates $(b,d)$ to birth-persistence coordinates $(b,d-b)$. We then place a Gaussian of a certain variance centred at each point of the persistence diagram. We obtain a surface by summing all the Gaussians, weighted by a function that depends on the point where they are centred. To ensure stability, this weighting function must be non-negative, zero along the horizontal axis, continuous and piecewise differentiable. Finally, we fix a grid of a certain resolution, and we integrate the surface over each box. The coordinates of our vector will be the values of these integrals. The steps of this pipeline are exemplified in \\autoref{fig:PH_examples}C.",
        "summarize_figure": "2505.02717v1_figure1_pipeline",
        "summarization": "The figure illustrates fundamental concepts of persistent homology and the steps of computing persistence images. Panel A shows how a circular point cloud evolves under increasing scale via alpha filtration, capturing a prominent 1-dimensional loop, which is reflected as a distant point from the diagonal in the persistence diagram. Panel B presents a dataset of four loops with varying size and density, explaining how birth and death values relate to geometric properties: denser loops have earlier births, larger loops have later deaths. Panel C outlines the conversion of a persistence diagram into a persistence image by transforming coordinates, applying Gaussian weights, and integrating over a grid, yielding a stable, vectorized representation."
    },
    "17": {
        "figure1": "2505.02725v1_mfcc_4_classes_graphs",
        "label1": "fig:mfcc_4_classes_graphs",
        "caption1": "Accuracy and loss graph for classifying four movement categories using Processing Record script.",
        "text": "While this model may appear complex, the experiment shows~no signs of  over-fitting~\\cite{ying2019overview}, as confirmed by the loss functions in  Figure~\\ref{fig:mfcc_4_classes_graphs}. We also verified lack over-fitting with a  second validation dataset split before any computations. After several checks, we deem the  results of this experiment as reliable and representative. This model demonstrates~a high level of confidence, achieving a \\num{98}\\%   classification accuracy rate, with F1-scores ranging between \\num{97}\\% and \\num{99}\\% for each category. These performance metrics are reflected in Table \\ref{tbl:mfcc_4_classes_scores}.",
        "summarize_figure": "2505.02725v1_mfcc_4_classes_graphs",
        "summarization": "The image presents training and validation metrics for a four-class movement classification model. The left graph shows a sharp rise in training accuracy approaching 100% by epoch 10, with validation accuracy consistently around 97–98%. The right graph illustrates a rapid decline in training loss and a stable, low validation loss. These curves indicate no significant overfitting and suggest strong generalization performance of the model."
    },
    "18": {
        "figure1": "2505.02725v1_background_record_class_graphs",
        "label1": "fig:background_record_class_graphs",
        "caption1": "Accuracy and loss graph for four area classification using background recording.",
        "text": "Computation of \\acp{mfcc}~and their normalization are the only pre-processing steps required   and are computed using the same methods as in the previous experiment. Using ten epochs with the same \\ac{cnn}~model, we observe a plateau of the validation loss,  which leads us to assume that the \\ac{ml} model stops improving. Based on the batch size of 32 data points and the \\textit{``sum\\_over\\_batch\\_size\"} reduction  method during loss calculation, we obtain a loss value of 0.8, which is higher than  the average loss for each data point. With this setting, we achieved the overall accuracy of 74\\%.  The classification report can be seen in Table \\ref{tbl:background_record_class_report}, along with the accuracy and loss function graph in Figure \\ref{fig:background_record_class_graphs}.",
        "summarize_figure": "2505.02725v1_background_record_class_graphs",
        "summarization": "The image presents training and validation performance for a four - area classification with background recordings over 10 epochs. In the left subplot, training accuracy steadily rises to around 90% by the last epoch, while validation accuracy plateaus at 74% after epoch 5. In the right subplot, training loss consistently drops, but validation loss starts to oscillate and slightly increase after epoch 5, suggesting potential overfitting. With 10 epochs, a CNN model, batch size of 32, and “sum_over_batch_size” loss reduction, an overall accuracy of 74% and a loss value of 0.8 (higher than per - data - point average loss) were obtained. Further training may yield limited gains."
    },
    "19": {
        "figure1": "2505.02727v1_squareness_KBM_stab_boundary",
        "label1": "fig:schematic_squareness",
        "caption1": "Schematic effect of squareness on PBM (solid) and KBM (dashed) stability boundary in pedestal width ($\\Delta_{\\mathrm{ped} }$) and height ($\\beta_{\\theta,\\mathrm{ped} }$) space. The PBM boundary is relatively invariant to changes in squareness in MAST-U 48339.",
        "text": "In this paper, we demonstrate that shaping could be a major lever for power-plant–relevant H-mode operation. We show that at low aspect ratio, increasing plasma squareness moves the pedestal away from the ELMy regime by modifying the KBM stability boundary while leaving the PBM boundary nearly unchanged. This idea is an extension of those presented in \\cite{Nelson2022,Parisi_2024b}, and has a different result to \\cite{Snyder2015,Merle2017} where plasma triangularity had a strong effect on the PBM boundary. \\Cref{fig:schematic_squareness} illustrates how higher squareness lowers the KBM stability threshold without substantially shifting the PBM boundary. Readers seeking a quick summary can refer to \\Cref{fig:squareness_first_second_stab_MASTU}, where we substantiate the schematic in \\Cref{fig:schematic_squareness} with simulation data, demonstrating how shaping can lead to inherently ELM-free H-mode operation in low-aspect-ratio tokamaks. This idea has already been explored experimentally for triangularity on MAST-U \\cite{Nelson2024c}. In this work, we focus on squareness.",
        "summarize_figure": "2505.02727v1_squareness_KBM_stab_boundary",
        "summarization": "The image illustrates how increasing plasma squareness affects the stability boundaries of KBM and PBM in the Δped–βθ,ped parameter space. The PBM boundary (black curve) remains largely unchanged, while the KBM stability threshold (colored lines) decreases as squareness increases, moving downward in the plot. The shaded region above the PBM boundary represents the ELMy regime. The arrow indicates that increasing squareness shifts the KBM boundary, effectively moving the pedestal operating point away from the ELMy region. This suggests that squareness can be used as a control parameter to achieve ELM-free H-mode operation in low-aspect-ratio tokamaks."
    },
    "20": {
        "figure1": "2505.02728v1_Figures",
        "label1": "fig:SPTvsTPT",
        "caption1": " Spacetime diagram showing the impact of FSL effects in SPTs (left) and TPTs (right). Neglecting FSL effects (gray, $c=\\infty$), both schemes diffract the atoms instantaneously at $T_\\ell$, changing the trajectory (solid green). The upwards pointing laser (frequency $\\omega_+$, wave vector $k_+$, blue, positioned at $z=-L$) is pulsed and controls the time of interaction \\wrt{} the reference height $z=0$. The light cones accounting for FSL (light blue, $c<\\infty$) indicate that the switching of the lasers must be advanced to $T_\\ell$. Furthermore, the time of interaction shifts by $\\Delta T_\\ell$, resulting in perturbed atomic trajectories (dashed green). For TPTs, a second laser (frequency $\\omega_-$, wave vector $k_-$, red, positioned at $z=+L$) is pointed downwards in addition. ",
        "text": "For SPTs, shown in the spacetime diagram of Fig.~\\ref{fig:SPTvsTPT} on the left, a single, upwards-pointing laser of frequency  $\\omega_+$ displayed in blue and positioned at $z=-L$ interacts with moving atoms by imprinting its phase and transferring its momentum $\\hbar k_+$, where $k_+$ is the laser's wave vector. When neglecting FSL ($c=\\infty$, gray), this momentum transfer of the $\\ell$th light pulse at time $T_\\ell$ modifies the idealized atomic trajectory (solid green). However, the exact time of this interaction including FSL ($c<\\infty$, blue light cone) is determined by the intersection of the light cone with the atomic trajectory and leads to a position-dependent time delay $\\Delta T_\\ell$. We assume that the laser switching is timed such that the pulse passes $z=0$ at time $T_\\ell$. The delay $\\Delta T_\\ell$ will induce an atomic trajectory (green dashed) slightly perturbed from the idealized trajectory.\nIn the situation displayed on the right of Fig.~\\ref{fig:SPTvsTPT}, an additional second downwards-pointing laser (red) with frequency $\\omega_-$ and wave vector $k_-$, positioned at $z=L$ is used to drive TPTs and transfer the total momentum $\\hbar K=\\hbar(k_++k_-)$. Since the time of interaction is determined by the blue control laser, the passive red laser does not influence the time of interaction, even though both lasers might be switched on at different times~\\cite{Tan2016, Dimopoulos2008, Wang2021}.\nFor optimal transitions, the frequencies of the light fields have to be matched to the transferred energy. Because atoms are subject to gravity and they are therefore accelerated in the lab frame, their time-dependent Doppler shift~\\cite{Marzlin1996} has to be compensated by the light fields to remain on resonance. Thus, their frequencies $\\omega_\\pm$ are chirped by rates $\\omega_\\pm\\sigma_\\pm/c$, where $\\sigma_\\pm$ are given in units of acceleration. For the light phase, we make therefore the ansatz which we separate into a spatial $\\kappa_\\pm(z,t)$ and purely time-dependent part. We use the initial conditions $\\kappa_\\pm(\\mp L, t)=0$ such that the pulses are initiated at positions $z=\\mp L$ at time $t=t_i$ with initial phases $\\phi_\\pm=\\Phi_\\pm(0,t_i)$ in agreement with Fig.~\\ref{fig:SPTvsTPT}. Equation~\\eqref{eq:Eikonal} is solved by the spatial part up to order $\\mathcal O(c^{-2})$. Since the frequency chirp implies also a change of the associated wave vector, we observe a modification that increases linearly in time. In addition, chirping causes an accelerational redshift~\\cite{Okolow2020} of the light phase scaling quadratically with position, which adds to the gravitational redshift caused by general-relativistic light propagation in the Rindler metric.\nAtomic diffraction relying on TPTs like Raman and Bragg diffraction is induced by subsequent absorption and emission of photons from both counterpropagating light beams. These transitions lead to an effective light phase which can be divided into a dominant light phase $\\Phi_\\text L$ and relativistic corrections $\\delta\\Phi$. Note that the limit of SPTs can be recovered by setting $\\Phi_-=0$, while recoilless E1-M1 TPTs that rely on the absorption of photons from both light beams are in principle also included by choosing opposite signs for $\\omega_-$, $k_-$ and $\\phi_-$ compared to the laser traveling upwards. The dominant light phase is given by with initial phase $\\Phi_\\text{off}=\\Phi_\\text{eff}(0, t_i^\\prime)$, effective two-photon wave vector $K=k_+ + k_-$ corresponding to the transferred momentum, frequency difference $\\Delta \\omega = \\omega_+-\\omega_-=c\\Delta k$ corresponding to the transferred energy, and modified initial time $t_i^\\prime=t_i+L/c$. Thus, all laser pulses are giving a lead in time by $L/c$ to account for the propagation from the laser sources at $z=\\pm L$ to the center of the experimental setup at $z=0$, as already anticipated in Fig.~\\ref{fig:SPTvsTPT}. While it is possible to keep two different chirping parameters~\\cite{Tan2016, Tan2017, Tan2017relativistic} we have chosen opposite chirping, \\ie{} $\\sigma_\\pm=\\mp\\sigma$, which has been shown to suppress FSL effects~\\cite{Peters2001, Cheng2015}. The last contribution of $\\PL$ is used to lock the frequency chirp $\\sigma$ to gravity in gravimetric applications, see Sec.~\\ref{sec:MZI}.",
        "summarize_figure": "2505.02728v1_Figures",
        "summarization": "The figure shows spacetime diagrams for SPTs and TPTs. In SPTs (left), an upward - pointing laser (ω+, k+) at z = -L transfers momentum to atoms at Tℓ. In TPTs (right), an additional downward - pointing laser (ω-, k-) at z = L is added. Ignoring FSL (c = ∞, gray), atoms diffract instantaneously at Tℓ (solid green trajectory). Considering FSL (c < ∞, blue light cone), the laser switching advances to Tℓ, with a time delay ΔTℓ causing a perturbed atomic trajectory (dashed green). In TPTs, the blue laser controls the interaction time, and the red laser contributes to total momentum transfer without changing it."
    },
    "21": {
        "figure1": "2505.02733v1_rho_TD",
        "label1": "fig:rho-TD",
        "caption1": "Time-distance map of density at vertical (along $y$) cut at $x=0$. The solid lines for estimating the slopes at the selected locations of the time-distance map are marked in black. The vertical extent of the upper and lower plasmoids is marked with braces and pointed by the blue arrows, the interface between the plasmoids is marked with a cyan arrow. ",
        "text": "Space-time information on the formation of the various plasmoids and their associated dynamics is shown in Figure \\ref{fig:rho-TD}. Here we compute a time-distance (TD) map of the plasma density along the vertical line at $x=0$. We estimate the speed of the plasmoids by tracking the enhanced density region in the TD map corresponding to the plasmoid motions, and calculating their respective slopes. Most of the identified plasmoids have speeds within the range $\\approx 35-290$~km~s$^{-1}$. The primary upper and lower plasmoids that appear at around $t=80$~s are approximately $3$~Mm in size along the vertical direction (marked by the black braces); the downward speed of the upper plasmoid is around $35$~km~s$^{-1}$, whereas the lower plasmoid moves much more slowly and its global velocity cannot be determined with the same level of accuracy as for the others. In fact, when the upper plasmoid starts to merge with the lower one at around $t=120$~s, the interface between the upper and lower plasmoids is pushed downward at a speed of around $60$ km s$^{-1}$. Around $t=137$ s, when the merging is complete, the size of the merged plasmoid starts to grow vertically and ends up reaching a vertical size of $\\approx 8$ Mm, and interacting with another (nearly stationary) secondary plasmoid (forms at $y\\approx 20$ Mm) at the end of the simulation. The downward moving plasmoid that appears at $y\\approx 20$ Mm at around $t=64$ s gains a high speed of around $210$ km s$^{-1}$ as marked by the white arrow in Figure~\\ref{fig:rho-TD}, due to the imbalance in the vertical component of the Lorentz force, as explained above.",
        "summarize_figure": "2505.02733v1_rho_TD",
        "summarization": "The image is a time - distance map of plasma density along the vertical cut at x = 0, showing log₁₀(ρ) in g/cm³. It depicts the formation and motion of plasmoids. Around t = 80 s, upper and lower merging plasmoids appear; the upper one moves downward at about 34.7 km/s. At t = 120 s, they start merging, with the interface moving down at about 60.9 km/s. After merging, the plasmoid grows vertically to around 8 Mm and interacts with a near - stationary secondary plasmoid near y = 20 Mm by the end of the simulation. Some plasmoids move rapidly downward, with speeds up to 289.9 km/s, due to imbalanced vertical Lorentz forces."
    },
    "22": {
        "figure1": "2505.02734v1_Timeline",
        "label1": "fig:measurements",
        "caption1": "Illustration of $T_{TFM}$ and $T_{Com}$ measurements. Our $T_{TFM}$ measurement begins with the synchronized start of the communicating applications, while our $T_{Com}$ measurement starts in steady state and the publish (or equalivalent) call.",
        "text": "We measure middleware performance using the following three metrics, illustrated in \\cref{fig:measurements}:     S = \\frac{T_{\\text{Com}}}{S_{\\text{Mes}}} and provides insight into how the communication performance of a middleware scales with increasing data volumes. A lower value of $S$ indicates better scaling behavior and more efficient handling of high data rates.",
        "summarize_figure": "2505.02734v1_Timeline",
        "summarization": "The image illustrates the communication timeline between two electronic control units (ECU A and ECU B), highlighting the processes across application, middleware, and network layers. Two latency metrics are defined: T_TFM measures the time from synchronized start to the first callback, encompassing discovery, publishing, transmission, receiving, and callback; T_Com captures the steady-state communication delay from message publishing to callback. The diagram visualizes the message flow and layer interactions, enabling clear identification of delays and aiding performance evaluation of middleware under varying data loads."
    },
    "23": {
        "figure1": "2505.02736v1_example-tw-clique-covering",
        "label1": "fig:example-BFS-layering",
        "caption1": "A BFS-layering~$L_1,L_2,L_3$ of a $2$-tree~$G$. The labels of the vertices correspond to their insertion order in the construction sequence of~$G$.",
        "text": "Let~$Q$ be the initial $k$-clique in the construction sequence of a $k$-tree~$G$.  Let~$G'$ be the graph obtained from~$G$ by adding a new vertex~$r$ that is connected to every vertex of~$Q$. Consider the partition of the vertices of~$G$ into layers~$L_1, \\dots, L_t$ where the $i$-th layer contains all vertices with distance~$i$ to the vertex~$r$. We call such a partition a \\emph{BFS-layering}, see \\cref{fig:example-BFS-layering} for an example. We remark that Dujmovi\\'{c}, Morin and Wood~\\cite[Theorem 6.1]{DujmovicMW05} considered a similar concept for a generalization of~$k$-trees.",
        "summarize_figure": "2505.02736v1_example-tw-clique-covering",
        "summarization": "The image shows a BFS - layering of a 2 - tree G with three layers L₁, L₂, L₃. Vertex labels represent their insertion order in G's construction sequence. Based on distance from a new vertex r (not in the figure), vertices are partitioned into layers. It demonstrates the hierarchical structure of the 2 - tree, with layer - to - layer connections following the topological insertion order."
    },
    "24": {
        "figure1": "2505.02736v1_outerplanar-claim",
        "label1": "fig:outerplanar-claim",
        "caption1": "The partially precolored outerplanar graph $A$ in \\cref{claim:outerplanar}.",
        "text": "Let $A$ be an outerplanar graph with a partial coloring $\\psi \\colon V(A) \\to [8]$ as depicted in \\cref{fig:outerplanar-claim}, consisting of a triangle colored $2,3,5$ and two paths $P_u,P_w$.         Assume that $\\psi$ is proper (i.e., $i \\neq 1,2$ and $j \\neq 3,4$) and good on the path $[u_1,u_2,v,w_2,w_1]$ (i.e., $i,j \\neq 5$).         Let $G_A \\subseteq A$ be any subgraph of $A$.         Then $\\psi$ can be extended to a proper $8$-coloring of $A$ that is good on $P_u$ and $P_w$, and strong odd for $G_A$, and satisfies $\\psi(u_3) \\neq 2$ and $\\psi(w_3) \\neq 3$.         We may assume (by renaming colors if needed) that $i \\neq 6$ and $j \\neq 8$.         Let $P'_u = P_u - \\{u_1,u_2\\}$ and $P'_w = P_w - \\{w_1,w_2\\}$ be the two paths of uncolored vertices.         As a preliminary coloring, let $\\phi \\colon P'_u \\cup P'_w \\to \\{6,7,8\\}$ be the (unique) proper coloring that is good on $P'_u$ and $P'_w$ with $\\phi(u_3) = 6$ and $\\phi(u_4)= \\phi(w_4) = 7$ and $\\phi(w_3) = 8$.         Extending $\\psi$ by $\\phi$ yields a proper coloring of $A$ that is good on $P_u$ and $P_w$.         If this extension is strong odd for $G_A$, we are done.          Otherwise, vertex $v$ has in $G_A$ an even non-zero number of neighbors colored $6$ (or $7$, or $8$).",
        "summarize_figure": "2505.02736v1_outerplanar-claim",
        "summarization": "The image illustrates a partially precolored outerplanar graph composed of a triangle colored with values 2, 3, and 5, and two extension paths Pu and Pw leading to nodes i and j. Vertex 5 connects both paths and is also part of the triangle. Below the paths are sequences of uncolored vertices (e.g., u3, u4, w3, w4), marked in light yellow, which are part of the extension. The figure demonstrates how these uncolored vertices can be extended into a proper 8-coloring under specific rules that maintain local path-goodness and global strong-odd properties within any subgraph GA. The graph structure is designed to show the conditions under which such coloring extensions are feasible in outerplanar graphs."
    },
    "25": {
        "figure1": "2505.02736v1_outerplanar-step",
        "label1": "fig:outerplanar-step",
        "caption1": "Illustrating the proof of \\cref{prop:outerplanar-at-most-8}.",
        "text": "Next, we extend the coloring $\\psi$ to the neighbors of $w_2$ (the case of $u_2$ is symmetric).     Consider the path $P'$ obtained by gluing $P_w$ and a subpath of $P$ at $w_1$, see \\cref{fig:outerplanar-step}, and observe that coloring $\\psi$ is good on $P'$.     Moreover, $w_2$ is the unique vertex on $P'$ with two colored neighbors ($v$ and $y$) outside $P'$.     In other words, we are in the same situation as before, with $w_2$ taking the role of $v$ and $P'$ taking the role of $P$.     By \\cref{claim:outerplanar} we can extend the coloring $\\psi$ to the neighbors of $w_2$ on $L_{i+1}$, and afterwards proceed with $w_1$ and so forth, to eventually color all neighbors of $P$.",
        "summarize_figure": "2505.02736v1_outerplanar-step",
        "summarization": "The figure illustrates a step in the proof of the outerplanar-at-most-8 proposition, showing how to extend a partial coloring to the neighbors of vertex w2. The path P is highlighted in blue, representing a portion with a good coloring, while the green region marks the subpath Pw, and the yellow path P' is formed by connecting Pw with a subpath of P at vertex w1. Vertex w2 is the only node on P' with two colored neighbors outside the path, namely v and y. This situation mirrors the earlier case with v and P, allowing the same coloring extension strategy to be applied. According to the prior claim, the coloring can be extended to all neighbors of w2 in the next layer, and the process continues with w1 and subsequent vertices to eventually cover all neighbors of P."
    },
    "26": {
        "figure1": "2505.02738v1_phases",
        "label1": "fig:workflow",
        "caption1": "\\textbf{Phase 1}: Thin Application Wrapper} \\end{subfigure}\\hfill \\begin{subfigure}[b]{0.49\\textwidth} \\centering \\includegraphics[width=17.5pc,page=2,clip,trim={0in 1in 0in 0in}]{Arxiv/figures/phases} \\caption{\\textbf{Phase 2}: Sub-Program Integration} \\end{subfigure}% \\vspace{1em} \\begin{subfigure}[b]{0.49\\textwidth} \\centering \\includegraphics[width=17.5pc,page=3,clip,trim={0in 1in 0in 0in}]{Arxiv/figures/phases} \\caption{\\textbf{Phase 3}: Retargetable Building Blocks} \\end{subfigure}\\hfill \\begin{subfigure}[b]{0.49\\textwidth} \\centering \\includegraphics[width=17.5pc,page=4,clip,trim={0in 1in 0in 0in}]{Arxiv/figures/phases} \\caption{\\textbf{Phase 4}: Algorithmic Description} \\end{subfigure}% \\vspace{.8em} \\caption{Workflow of the proposed HPC framework. Phases enable gradual application integration --- the more information provided, the more capabilities can be unlocked to reduce the overhead of utilizing modern hardware.",
        "text": "We propose beginning with a non-intrusive framework that enables bridging of models through interfaces without any attempts to add new abstractions. This should allow applications to plug into the framework and be able to use features that they may not already have, as shown in Phase 1 of Figure~\\ref{fig:workflow}. The applications may be able to use limited AI/ML interfaces. As we progress towards later phases, we begin to introduce abstractions where, in exchange for handing some of the control and details over to the  framework, the applications gain greater performance and portability while simplifying their own code base. For example, in Phase 2 the framework may become the primary custodian of data containers, with applications indicating what their needs are, allowing them to hand over the communication and data movement to the framework. In the next phase, we can further abstract the knowledge of hardware and control of data from the application so that the framework can internally apply transformations and optimizations as needed. Our final goal is to reach a state where the algorithm writer can express their arithmetic and data storage requirements without binding them to one another. Instead, they can leave the management to the framework.",
        "summarize_figure": "2505.02738v1_phases",
        "summarization": "The image shows the workflow of a proposed HPC framework with four phases. Phase 1 uses a non - intrusive approach to bridge unmodified programs via interfaces, enabling access to features like limited AI/ML interfaces. In Phase 2, the framework takes over data container management. Phase 3 further abstracts hardware knowledge and data control from applications for internal optimizations. The final Phase 4 aims for algorithm writers to express requirements independently of hardware binding, with the framework handling management for efficient multi - core hardware execution."
    },
    "27": {
        "figure1": "2505.02741v2_intro_new",
        "label1": "fig:intro",
        "caption1": "The proposed dynamic spectral sparsification method (\\textit{dyGRASS}) and its applications in (non)linear circuit network modeling and optimization tasks. ",
        "text": "To address this gap, we propose a practically efficient algorithmic framework, \\textit{dyGRASS}, for dynamic spectral sparsification of large undirected graphs. By leveraging localized random walks, \\textit{dyGRASS} allows updating the spectral sparsifier in constant time for each edge insertion/deletion,  as shown in Fig. \\ref{fig:intro}. In each edge update, \\textit{dyGRASS} estimates the spectral distortion \\cite{feng2016spectral,feng2020grass,zhang2020sf}  due to edge insertion/deletion by leveraging a localized random walk algorithm: for edge insertions \\textit{dyGRASS} determines whether to retain or remove edges bounded by a resistance diameter, whereas for edge deletions \\textit{dyGRASS} recovers edges along a selected path with minimum resistance to retain structural properties in the sparsifier.",
        "summarize_figure": "2505.02741v2_intro_new",
        "summarization": "The figure illustrates the dyGRASS framework for dynamic spectral sparsification in nonlinear circuit network modeling and optimization. The left side shows traditional modeling and simulation steps for circuit networks. On the right, the graph G(0) undergoes edge insertions and deletions to form G(1), G(2), etc. For each update, dyGRASS incrementally or decrementally modifies the sparsifier H using localized random walks. During edge insertion, dyGRASS evaluates edge importance via resistance diameter to decide retention; for deletions, it recovers edges along minimum resistance paths to preserve structural integrity. This enables constant-time spectral update, ensuring efficient and scalable analysis of evolving graph models."
    },
    "28": {
        "figure1": "2505.02741v2_inc_update",
        "label1": "figure:inc_up",
        "caption1": "The schematic of incremental graph update algorithm showcasing the process for inserting the edges $e_{6,17}$ and $e_{19,25}$.",
        "text": "Fig. \\ref{figure:inc_up} shows the incremental graph update algorithm in response to introducing two edges $e_{6, 17}$ and $e_{19,25}$ to the original graph $G^{(0)}$. Fig.  \\ref{figure:inc_up} (a) illustrates the decomposition of the graph sparsifier $H^{(0)}$ through a $2$-level low-resistance-diameter (LRD) decomposition. The number of low-resistance-diameter (LRD) decomposition level is determined according to the desired condition number. This process generates a sparse data structure for each node, which includes node cluster indices at each level, referred as node resistance embeddings. In Fig. \\ref{figure:inc_up} (b) the edge filtering process of inGRASS algorithm is shown utilizing the node resistance embeddings which results in addition of edge $e_{6,17}$, and  exclusion of  edge $e_{19,25}$ from graph sparsifier (updating   weight of $e_{19,20}$).",
        "summarize_figure": "2505.02741v2_inc_update",
        "summarization": "The image illustrates the incremental update process of a graph sparsifier under dyGRASS. In the setup phase, the original graph's sparsifier H undergoes a multilevel low-resistance-diameter decomposition to generate resistance-based node clusters. This forms resistance embeddings used for edge evaluation. In the update phase, edge e(6,17) is inserted into the sparsifier due to its structural significance, while edge e(19,25) is rejected and its influence is absorbed by adjusting the weight of edge e(19,20). This shows how dyGRASS leverages node resistance vectors to selectively preserve edges for efficient and accurate sparsification during dynamic updates."
    },
    "29": {
        "figure1": "2505.02741v2_dygrass_overview",
        "label1": "figure:dygrass_overview",
        "caption1": "Key steps in the proposed dyGRASS framework.",
        "text": "Fig. \\ref{figure:dygrass_overview} illustrates the overview of our proposed dynamic spectral sparsification algorithm for graphs (dyGRASS): (a) the original graph $G^{(0)}$ and its spectral sparsifier $H^{(0)}$, computed using GRASS \\cite{feng2020grass}, serve as input to the dyGRASS algorithm; (b) the dyGRASS \\textbf{setup phase} is executed only once, applying low-resistance-diameter (LRD) decomposition on $G^{(0)}$ and $H^{(0)}$ to facilitate decremental and incremental dynamic updates, respectively; (c) the spectral sparsifier is updated using localized random walk and resistance embedding upon receiving streams of edge deletion and edge insertion, respectively.",
        "summarize_figure": "2505.02741v2_dygrass_overview",
        "summarization": "The figure outlines the core workflow of the dyGRASS framework for dynamic spectral sparsification. It begins with the input phase, where the original graph G and its spectral sparsifier H are provided. In the setup phase, dyGRASS applies low-resistance-diameter decomposition to generate node clusters from G and resistance vectors from H. These structures support efficient updates. Finally, in the update phase, dyGRASS performs incremental and decremental updates using localized random walks and resistance embeddings to adapt the sparsifier dynamically as edges are inserted or deleted. This structured pipeline enables real-time adaptation of sparsifiers while preserving spectral properties."
    },
    "30": {
        "figure1": "2505.02749v1_FAccT24_scrutiny_ranges",
        "label1": "scrutiny",
        "caption1": "Legal sources classify demographic features into three scrutiny ranges. For RRA, only strict scrutiny features are above the ``exclusion from model inputs'' (yellow) scrutiny threshold, i.e., expected by legal sources to be excluded from the AI models' input space. There are conflicting legal sources about whether intermediate scrutiny or rational basis features (such as sex/gender or age) should be excluded.",
        "text": "Contextualizing our fairness thresholds in the judicial scrutiny-based framework, we only know that the procedural fairness threshold must be below the strict scrutiny range because all features in this range (race, national origin) are excluded from model inputs. As legal sources disagree on whether sex/gender and age should be excluded from model inputs, we do not know where our procedural fairness threshold should go with respect to the intermediate scrutiny and rational basis ranges. We illustrate this open question in Figure \\ref{scrutiny}. %in \\autoref{appendix-results}. %(Appendix A). %Once we know where our procedural fairness threshold should be relative to the three scrutiny ranges, follow-up human study and/or legal research can be conducted to determine where our group fairness and the individual fairness thresholds should be on the scrutiny spectrum.",
        "summarize_figure": "2505.02749v1_FAccT24_scrutiny_ranges",
        "summarization": "The figure illustrates the uncertainty in defining the exclusion threshold for model inputs within a judicial scrutiny framework. The scrutiny spectrum is divided into strict scrutiny (e.g., race, national origin), intermediate scrutiny (e.g., gender), and rational basis (e.g., age). The yellow threshold line represents the exclusion boundary for model inputs. Currently, it is established that the yellow threshold must lie below the strict scrutiny range as race and national origin are excluded from model inputs. However, legal sources disagree on the exclusion of intermediate scrutiny and rational basis features such as gender and age, leaving the precise position of the yellow threshold within these ranges undetermined. The illustration underscores the ongoing debate regarding the appropriate exclusion criteria for AI model inputs based on varying levels of judicial scrutiny."
    },
    "31": {
        "figure1": "2505.02750v1_Workflow",
        "label1": "fig1",
        "caption1": "\\textbf{A Sloppy Workflow for QSP.} After building detailed, literature-derived models, parameter values are calibrated by fitting to system data. Carefully selecting ``model queries'' based on the target application, a reduced-order model is built that removes sloppy parameters and models the ``active manifold'' that efficiently propagates information from available data to predictions of interest. Calculations performed on the reduced model are mechanistically interpretable while having improved numerical conditioning, stability, and computational cost. ",
        "text": "The workflow we propose is designed specifically to address issues related to model complexity in the broader QSP framework. As such, our proposed procedure complements most other workflows, and key elements can be incorporated into other operational paradigms. Key elements of our procedure are illustrated in Fig~\\ref{fig1} and briefly summarized here.   A more detailed discussion is given in section~\\ref{sec:SloppyWorkflow}. Successful QSP flows from well-formulated questions; therefore, based on a specific QSP objective, e.g., target discovery, toxicology, etc., the modeling steps flow from this primary goal. First, a detailed, mechanistic QSP model is constructed by uncritically combining mechanisms found in the relevant literature, and its (many) parameters are estimated from available data. Next, specific modeling scenarios are designed, referred to as queries in our nomenclature (again, see section~\\ref{sec:SloppyWorkflow} for longer discussion). For example, one might be interested in predicting the system response under a novel drug regimen. These queries are typically scenarios for which data is unavailable but for which one would like to use the model to make predictions. Conceptually, the mathematical model is a machine that extracts information from the training data and propagates it forward to queries of interest. However, because of sloppiness, the relationship between information in fitting data and future predictions is obscured by the complexity of the literature-derived model, and many computational techniques are ill-conditioned. The next step, therefore, is to restrict the model to the parameters relevant to the queries. We construct a reduced order model based on these queries using a sloppy modeling algorithm called the Manifold Boundary Approximation Method (MBAM)\\cite{transtrum2014model}. By design, MBAM retains those parameters that are relevant to both the available data and the predictions of interest, clarifying the relationship between the two. The reduced order model determines what information was learned from the data and how it propagates forward to inform the QSP objectives. Critically, the model may not be identifiable from the data, but unidentifiable parameters in the reduced model reflect a fundamental limitation in the information available in the data. Finally, subsequent QSP calculations--such as hypothesis testing, virtual population generation, or experimental design--are performed using the reduced model, and are informed by which modeling elements were retained by the reduction process. In our experience, nearly all practical modeling tasks are improved, accelerated, and clarified by working with the parsimonious reduced model.\nA primary outcome of this work is the formalization of a QSP workflow based on principles and techniques of ``Sloppy modeling.'' As summarized briefly in the introduction, our proposed workflow is illustrated graphically in Fig~\\ref{fig1}. Here we elaborate on this process before presenting the results of the validation studies.\nOnce realized in code, the literature-derived model can be thought of as a generic computational model since the parameters have not been calibrated to any specific data, and it can be computationally evaluated for a range of physically plausible parameter values corresponding to diverse model systems or even specific patients. Therefore, the next step is to calibrate the parameters using experimental data. In the flow diagram, the data to which the model is calibrated is labeled ``System data.'' This data is qualitatively different from the structural, literature-derived information used above. The literature-derived model uses discrete knowledge about mechanisms to inform the mathematical and computational form of the model. The experimental system data is continuous data used in regression analysis to calibrate the tunable parameters. The result of this step is the ``calibrated model'' in Fig~\\ref{fig1}. Although not shown, one could have multiple calibrated models, for example, if data are taken for several distinct model systems, patients, etc. In our SHIV study, we have multiple calibrated models for responsive, non-responsive, and untreated patients.\nThe model queries are usually of two types: those for which experimental data is available and those for which it is not. We represent these graphically by the different colored curves in Fig~\\ref{fig1}. The red and blue curves correspond to the model predictions for the red and blue experimental data points. However, the green curve is meant to reflect a model prediction for a novel experimental condition for which experimental data are unavailable.",
        "summarize_figure": "2505.02750v1_Workflow",
        "summarization": "The figure illustrates a workflow for Quantitative Systems Pharmacology (QSP). A detailed QSP model is initially constructed from mechanisms derived from literature, and its parameters are calibrated using system data. Targeted model queries are then selected, and a reduced-order model is built by removing irrelevant parameters, improving numerical stability and computational efficiency. This reduced model can retain mechanistic interpretability while being more suitable for hypothesis testing, virtual population generation, and experimental design in subsequent QSP calculations."
    },
    "32": {
        "figure1": "2505.02750v1_Nayak2015_combined",
        "label1": "fig2",
        "caption1": " \\textbf{Coagulation model.} Network diagram of the full (background) and reduced (foreground) coagulation cascade. The reduced model has 5 parameters and highlights the feed-forward loop, including the activation of Factor V by Factor IIa as the relevant mechanism. The thrombin response is adaptive, as shown in the inset for a typical profile. ",
        "text": "In this study, we use the model proposed by Nayak et al.\\cite{nayak2015using} as our base, literature-derived model whose primary reactions are summarized in the background of Fig~\\ref{fig2}. The model integrates the classical pathway elements into a detailed mechanistic model with an eye toward human-derived parameters for pharmacological applications.  The model was validated using thrombin generation profiles (Factor IIa) and the system has been used in benchmarks for parameter identification, sensitivity analysis, and model reduction\\cite{gadkar2016six,arumugam2017novel,hansen2019automated,neves‐zaph2024quantitativ,chen2025multistep}\nThe parameter values reported by Nayak et al. give us a calibrated model; however, sensitivity analysis reveals that the model is sloppy, and most of these parameters are unidentifiable from these data. We wish to explore the ability of a reduced-order model to make predictions for QSP-specific queries. The queries we chose are thrombin time series (Factor IIa) in response to various doses of Factor VIIa and Factor Xa added to both normal and Factor VII-deficient human plasma, for a total of 28 time series. These are the same time series used in model calibration. The thrombin response in each of these time series is adaptive\\cite{ma2009defining,transtrum2016bridging}, \\textit{i.e.}, after an initial response to the dosage, it returns to baseline levels after long times. Our final coarse model based on these queries has five parameters, as is represented graphically in the foreground of Fig~\\ref{fig2}, along with a typical Thrombin profile for both the full and reduced models.",
        "summarize_figure": "2505.02750v1_Nayak2015_combined",
        "summarization": "The figure depicts a coagulation model with a network diagram showing both the full model (background) and the reduced model (foreground). The reduced model, with five parameters, highlights the feed-forward loop, specifically the activation of Factor V by Factor IIa. The inset shows a typical thrombin profile, illustrating the adaptive nature of the thrombin response, where after an initial peak, levels return to baseline over time. The comparison between the full and reduced models shows that the reduced model, despite having fewer parameters, still effectively predicts thrombin response."
    },
    "33": {
        "figure1": "2505.02754v1_kernel_density_estimation_tessarine",
        "label1": "fig:kde",
        "caption1": " Empirical mean integrated square errors of four density estimators when $n=50$ (top left panel) and $n=75$ (top middle panel): $\\hat p_X(x)$ (dotted lines), $\\hat p_{\\text{naive}}(x)$ (dashed lines), $\\hat p_{\\text{dk}}(x)$ (dot-dashed lines), and $\\hat p_{\\text{tc}}(x)$ (solid lines), along with boxplots of estimates from each method when $n=50$ and $\\lambda=0.9$ superimposed on the true density (solid lines).",
        "text": "Figure \\ref{fig:kde} presents the empirical mean integrated error for the three considered estimators compared with the reference estimator $\\hat p_X(x)$.  Even though the deconvoluting kernel density estimator $\\hat p_{\\text{dk}}(x)$ outperforms the debiased estimator $\\hat p_{\\text{tc}}(x)$ in terms of the mean integrated squared error, $\\hat p_{\\text{tc}}(x)$ significantly improves upon the naive estimator $\\hat p_{\\text{naive}}(x)$ and is less variable than $\\hat p_{\\text{dk}}(x)$ despite the lack of real-valued solutions to  \\eqref{eq:findbcd}. Moreover, the computation time of  $\\hat p_{\\text{tc}}(x)$ is dramatically shorter than that of  $\\hat p_{\\text{dk}}(x)$. At a sample size of $n=50$,  it takes around 9 seconds to compute $\\hat p_{\\text{dk}}(x)$, whereas $\\hat p_{\\text{tc}}(x)$ takes 0.11 seconds to compute on a computer with an Intel Core i5 12600KF running at 4500 Mhz.",
        "summarize_figure": "2505.02754v1_kernel_density_estimation_tessarine",
        "summarization": "The graph presents the empirical mean integrated square errors (MISE) of four density estimators for sample sizes of 50 (top left) and 75 (top middle). These estimators are: p_X(x) (dotted lines), p_naive(x) (dashed lines), p_dk(x) (dot-dashed lines), and p_tc(x) (solid lines). At n=50, the deconvoluting kernel estimator p_dk(x) performs best in terms of mean squared error. However, the tessarine correction estimator p_tc(x) significantly improves upon the naive estimator p_naive(x) and is less variable than p_dk(x), even though p_tc(x) lacks real-valued solutions to the equation. Additionally, the computation time of p_tc(x) is much shorter than p_dk(x). While computing p_dk(x) takes around 9 seconds at n=50, p_tc(x) only takes 0.11 seconds. The boxplots superimposed on the true density (solid line) show the estimates for each method at n=50."
    },
    "34": {
        "figure1": "2505.02768v1_Nadara",
        "label1": "fig:nadara",
        "caption1": "A graph $G$ with $\\clin(G)=4$ (left) and a topological minor $H$ of $G$ (right) with $\\clin(H)>4$.",
        "text": "Let $G$ be the graph from \\cref{fig:nadara} (left) and $\\psi$ be the coloring of $G$ specified by the numbers on its vertices in the aforementioned figure. Let $H$ be the graph from \\cref{fig:nadara} (right) and observe that $H$ is a topological minor of $G$ that can be obtained by contracting the edge $kc$. We show that $G$ and $H$ satisfy the statement of the proposition. First, we verify that $\\psi$ is a linear coloring by checking all the paths in $G$.  Towards a contradiction suppose there is a path $P$ that does not have a center.  Then, $P$ must see one of the colors 1 and 2. Suppose it sees 1 but not 2. Then, in order to see color 1 twice, $P$ contains vertices $g$, $f$, and $a$ in order. It follows that $P$ contains a center of color 0 or 3, a contradiction. By symmetry, it remains to analyze the case when $P$ sees both 1 and 2. In order to see colors 1 and 2 twice, $P$ starts in $g$ and ends in $h$. It cannot use $e$  (otherwise $P=gfedh$, admitting $g$ as a center) so it starts with $gfa$ and ends with $cdh$.   In the rest of the graph (i.e., $G[\\{i,b,j,k\\}]$) no component contains twice the color $0$, so any way to complete the path contains a center of color 0, a contradiction.",
        "summarize_figure": "2505.02768v1_Nadara",
        "summarization": "The image shows graph G (left) and its topological minor H (right). In G, vertices are colored 1, 2, 3, 0. By contracting edge kc in G, H is obtained. The coloring ψ in G is a linear coloring where each path has a center vertex. Analyzing paths in G shows this property holds. The graph transformation from G to H illustrates how edge contraction affects coloring properties, with H having a higher linear - coloring number than G."
    },
    "35": {
        "figure1": "2505.02768v1_obstructionList",
        "label1": "fig:listObs",
        "caption1": "The list $\\mathcal{F}$ of subgraph obstructions to $\\clin(G)\\leq 3$.",
        "text": "Let $\\mathcal{F}$ be the set of graphs depicted in \\Cref{fig:listObs} and let $G$ be a graph. Then, $\\clin(G)\\le 3$ if and only if $G$ contains none of the graphs in $\\mathcal{F}$ as a subgraph.\nLet $F\\in \\mathcal{F}$ be one of the graphs depicted in \\Cref{fig:listObs}.  Then $\\clin(F) \\ge 4$.\nIn the following we handle the remaining graphs of \\Cref{fig:listObs}, namely $F_2$, $F_4$, $F_5$, $F_7$, and $F_9$ in order, by assuming there exists a linear coloring $\\varphi$ with colors $\\{0,1,2\\}$ and reaching a contradiction.",
        "summarize_figure": "2505.02768v1_obstructionList",
        "summarization": "The image presents a collection of graphs labeled F1 to F9 and additional standard graphs K4, P8, C5, C6, C7. These graphs form the complete list of subgraph obstructions to clin(G) less than or equal to 3. Each graph shows distinct structural features such as triangles, diamonds, paths, cycles, and combinations of these. The image specifies the node counts and connections clearly. It illustrates that the presence of any of these subgraphs implies clin(G) must be at least 4, offering a visual criterion to determine the linear coloring threshold for a graph."
    },
    "36": {
        "figure1": "2505.02788v1_spline_nonlinear_comparison",
        "label1": "fig:spline_nonlinear",
        "caption1": "% \\textbf{Spline-Based Approximation of Non-Linear Functions.} The green curves illustrate the iterative refinement of the regression fit obtained using our quantum-assisted least squares approach with \\textit{linear splines} (20 knots), one qubit per parameter, and up to 10 iterations. The lighter green lines correspond to intermediate fits across iterations, while the darker green curve represents the final approximation after 10 iterations.",
        "text": "We applied \\algoName to approximate five standard non-linear functions using linear splines with 20 knots. Figure~\\ref{fig:spline_nonlinear} shows the results. The blue scatter points denote ground-truth values; the green curves illustrate the iterative spline fits. Intermediate fits (light green) gradually approach the final estimate (dark green) over 10 iterations. The piecewise linear nature of the spline basis leads to a segmented, angular approximation pattern.",
        "summarize_figure": "2505.02788v1_spline_nonlinear_comparison",
        "summarization": "The figure shows the results of approximating five standard non-linear functions using linear splines with 20 knots through a quantum-assisted least squares method. The green curves illustrate the iterative refinement of the spline fit, with lighter green representing intermediate iterations and darker green showing the final approximation after 10 iterations. The blue scatter points indicate the true data values. The piecewise linear nature of the spline leads to a segmented, angular approximation pattern. This demonstrates how the algorithm progressively refines its fit towards the final regression result over multiple iterations."
    },
    "37": {
        "figure1": "2505.02789v1_transitions",
        "label1": "fig:transitions",
        "caption1": "Lifecycle of node colors in algorithm $\\asix$",
        "text": "In this section, we present an algorithm $\\asix=(\\csix,\\cinit,\\xisix)$ that enables the agent to explore a graph using six colors, namely $\\cinit$, $\\cpath$, $\\cfin$, $\\cheadone$, $\\cheadtwo$, and $\\cneigh$, by simulating the movement of semi-DFS. The definition of $\\asix$ is given in Algorithm~\\ref{al:asix}. Some readers might be concerned about the absence of a definition for $\\xisix(\\cfin,M)$ in Algorithm~\\ref{al:asix}; however, this poses no issue since, as we shall see later, the agent is located at a node colored $\\cfin$ only after the algorithm terminates. \\figcap\\ref{fig:transitions}, which describes how a node changes its color from $\\cinit$ to $\\cfin$, may help the reader quickly grasp the algorithm.",
        "summarize_figure": "2505.02789v1_transitions",
        "summarization": "This figure illustrates a node state transition flow for a graph exploration algorithm. The node starts at the \"init\" state and progresses through various states (such as head1, path, and head2) until it reaches the \"fin\" state. The arrows in the diagram indicate the flow between nodes, showing how the algorithm gradually changes the node's state (or color), similar to a depth-first search (DFS). The use of different color states helps in the comprehensive exploration of the graph."
    },
    "38": {
        "figure1": "2505.02796v1_experiment_1",
        "label1": "fig:1",
        "caption1": "Relationship between Average Relative Regret and Time Horizon",
        "text": "In \\Cref{fig:1}, the performances of online algorithms do approximate that of benchmark (offline optimum). As shown in the figure, the relative errors in both cases converge close to 0 as the time scale gets larger, which is the results of the sublinear regret. It shows the remarkable performance of our online algorithms compared to the benchmark. What's more, \\Cref{fig:1} shows that when $V_T=0$, our informative algorithm performs better than the uninformative one, which illustrates the benefits of the predictions.",
        "summarize_figure": "2505.02796v1_experiment_1",
        "summarization": "The figure illustrates the relationship between the average relative regret of online algorithms and the time horizon. As shown, the relative errors for both cases approach zero as the time scale increases, demonstrating sublinear regret and indicating that the online algorithms perform similarly to the benchmark (offline optimum). Notably, when the time horizon $V_T=0$, the informative algorithm outperforms the uninformative one, highlighting the benefits of predictions in improving performance."
    },
    "39": {
        "figure1": "2505.02800v1_two_spheres",
        "label1": "fig:two_spheres",
        "caption1": "\\footnotesize Uniform sampling, called {\\em Fibonacci sampling}, of two spheres (left) has a one-dimensional associated persistence landscape (right) with trivial Chen signature.",
        "text": "Consider the singleton barcode $B = \\{ (b,d) \\}$. Then $I(B)=\\left(0,\\frac{ d-b}{{2}},0\\right)$ is a one-dimensional time-series of length $3$ whose discrete signature up to weight $3$ is         &\\langle \\Phi_I(B), Z \\rangle = 0\\\\                 & \\langle \\Phi_I(B), Z^2 \\rangle = \\frac{(d-b)^2}{2}\\\\                         &\\langle \\Phi_I(B), Z \\otimes Z \\rangle  = - \\frac{(d-b)^2}{2^2}\\\\                                 &\\langle\\Phi_I(B), Z^3\\rangle = 0\\\\                                         &\\langle\\Phi_I(B), Z^2 \\otimes Z\\rangle = - \\frac{(d-b)^3}{2^{3}}\\\\         &\\langle\\Phi_I(B), Z\\otimes Z^2\\rangle = \\frac{(d-b)^3}{2^{3}}\\\\                 &\\langle\\Phi_I(B), Z\\otimes Z \\otimes Z\\rangle = 0 \\ .     Now consider a barcode $C = \\{ (b,d),(b',d') \\}$ where $b' > d$ (similar to Figure \\ref{fig:two_spheres}). Then, up to $twt$--equivalence,      is a one-dimensional time-series of length $5$ whose signature up to weight 3 is         &\\langle\\Phi_I(C), Z\\rangle = 0\\\\         &\\langle\\Phi_I(C), Z^2\\rangle = \\frac{1}{2}\\left((d-b)^2 + (d'-b')^2\\right) \\\\         &\\langle\\Phi_I(C), Z \\otimes Z\\rangle = -\\frac{1}{2^2}\\left( (d-b)^2 + (d'-b')^2 \\right)\\\\         &\\langle\\Phi_I(C), Z^3\\rangle = 0\\\\         &\\langle\\Phi_I(C), Z^2 \\otimes Z\\rangle = -\\frac{1}{2^3}\\left( (d-b)^3 + (d'-b')^3 \\right)\\\\         &\\langle\\Phi_I(C), Z\\otimes Z^2\\rangle =  \\frac{1}{2^3}\\left( (d-b)^3 + (d'-b')^3 \\right)\\\\         &\\langle\\Phi_I(C), Z\\otimes Z \\otimes Z\\rangle = 0 \\ . In slightly more generality, whenever a barcode $B$ has a landscape $\\lambda^B$ with a single level,  the discrete signature captures persistence, but does not capture additional information beyond it.     The signature of a barcode $B$ defining a single-level landscape $\\lambda^B$ is completely determined by the persistence of the bars in $B$.     If $\\lambda^B$ has a single level then there exists a total ordering $\\prec$ of $B$ so that $d \\leq b'$ whenever $(d,b) \\prec (d',b')$, since otherwise there would be a second level. Label the barcodes in such a way that $(b_1,d_1)<(b_2,d_2) < ... < (b_{|B|},d_{|B|}))$, with $d_i<b_j$ if $i<j$. Then the time-series of differences has the form      {\\small \\[     Since the signature is obtained by evaluation of polynomials on the time-series of differences, the lemma follows.\n0 & \\text{if } n \\equiv 1 \\ (mod \\ 2) \\\\ The latter equation can be interpreted as a list of even moments of the time-series, so that standard statistical descriptors of time-series are naturally captured by the discrete signature. In particular, going back to the example in Figure \\ref{fig:two_spheres}, the above observation shows that the discrete signature encodes information about the persistence of the spheres, which is determined by their radii, whereas the Chen signature is trivial.",
        "summarize_figure": "2505.02800v1_two_spheres",
        "summarization": "The figure illustrates the uniform sampling of two spheres (left) using the Fibonacci sampling method and the associated one-dimensional persistence landscape (right) with a trivial Chen signature. The left diagram shows the distribution of uniformly sampled points on the spheres, while the right diagram represents the persistence landscape derived from these samples, which captures only the persistence of the points without any additional information. The persistence signature and landscape construction reflect the application of Fibonacci sampling in morphological data."
    },
    "40": {
        "figure1": "2505.02800v1_table_ARI_NMI",
        "label1": "fig:ARI_NMI_table",
        "caption1": "\\footnotesize Correlations (via ARI and NMI) between the signature-based clusters and the labeling determined by 9 structural class representative. The first row highlights a significantly high correlation after truncating the landscape at the first $15$ levels, the discrete signature at weight $3$ and imposing $9$ clusters.",
        "text": "The metrics in Figure \\ref{fig:ARI_NMI_table} show high similarity between the clustering based on the $9$ structural class representatives, and the k-means clustering using the first $15$ levels of landscapes, discrete signatures up to weight $3$, and $9$ clusters, with ARI of $0.958$ and NMI of $0.895$. We find it remarkable that such a high accuracy is already reached by truncating the discrete signature up to weight $3$. We will work with these parameters in the remainder of the section. \\\\",
        "summarize_figure": "2505.02800v1_table_ARI_NMI",
        "summarization": "The table illustrates the correlations between signature-based clustering and labeling determined by 9 structural class representatives, evaluated through ARI and NMI. The first row highlights a notably high correlation after truncating the landscape to the first 15 levels, using a discrete signature with a weight of 3, and imposing 9 clusters. The metrics show high similarity between clustering based on these parameters and the labeling, with an ARI of 0.958 and an NMI of 0.895, indicating that a high accuracy is achieved even with just the first three weights of the discrete signature."
    },
    "41": {
        "figure1": "2505.02800v1_class_separability_sigVreps",
        "label1": "fig:separation_ratio",
        "caption1": "\\footnotesize Centroid-based permutation test using separation ratio with average signatures.",
        "text": "A large ratio means that different classes have different discrete signatures; equivalently, signatures separate structural classes. Figure \\ref{fig:separation_ratio} shows that signatures do not separate random classes, for which the ratio is $< 0.5$ (dashed blue line), in contrast to the ratio of $2.49$ (dashed red line) obtained with the protein dataset. Again, this test validates a strong correlation between discrete signatures and clustering of structural classes.",
        "summarize_figure": "2505.02800v1_class_separability_sigVreps",
        "summarization": "The figure illustrates a centroid-based permutation test using the separation ratio with average signatures. A large separation ratio indicates that different classes have distinct discrete signatures, meaning the signatures can separate structural classes. As shown, the ratio for random classes is below 0.5 (blue dashed line), while the ratio for the protein dataset is 2.49 (red dashed line). This test validates a strong correlation between discrete signatures and the clustering of structural classes, confirming the effectiveness of signatures in distinguishing structural classes."
    },
    "42": {
        "figure1": "2505.02802v1_validity_perc",
        "label1": "fig:heatmap-comp",
        "caption1": "Heatmap of the \\emph{JSON validity} percentage for each LLM, prompt and temperature}  \\Description{Heatmap of the \\emph{JSON validity} percentage for each LLM, prompt and temperature",
        "text": "Analyzing the results obtained for the \\emph{JSON validity} metric, it appears that GPT4 is the best model to generate acceptable routines by HomeAssistant for each prompt and temperature tested. Its predecessor, GPT3.5, also maintained a good \\emph{JSON validity} (in the tested cases), while MISTRAL and codeLLAMA have very low scores. Finally, it is important to highlight that the two tested LLAMA2 models (7b and 70B) obtained the lowest \\emph{JSON validity} and did not generate any valid HomeAssistant response. Additionally, we found that such a difference was statistically significant while considering the LLM, F(5)=209.2066, p$<$.001. There were no significant differences for all the other factors. Figure \\ref{fig:heatmap-comp} shows the heatmap of the \\emph{JSON validity} percentage.",
        "summarize_figure": "2505.02802v1_validity_perc",
        "summarization": "The figure shows a heatmap of the JSON validity percentage for different LLMs, prompts, and temperatures. Analysis of the results reveals that GPT4 is the best model for generating acceptable routines for HomeAssistant across all tested prompts and temperatures, with its predecessor GPT3.5 also performing well. MISTRAL and codeLLAMA have very low scores, while both LLAMA2 models (7b and 70B) failed to generate valid HomeAssistant responses, resulting in the lowest JSON validity. Statistical analysis indicates a significant difference in JSON validity between the LLMs (F(5)=209.2066, p<0.001), but no significant differences for other factors."
    },
    "43": {
        "figure1": "2505.02806v1_Fig1",
        "label1": "fig:systemmodel",
        "caption1": "The proposed  CF-mMIMO SWIPT system.} \\vspace{0em",
        "text": "We consider a CF-mMIMO system  under  time-division duplex operation, where $M$ APs serve (in DL)  $K_d$ IUs and $L$ EU nodes with EH capability. Each AP is connected to the CPU via a high-capacity fronthaul link.\\footnote{ In general, the system should consist of multiple CPUs connected via fronthaul links. Depending on the specific processing requirements, these CPUs can exchange information such as channel estimates, data, and power control coefficients \\cite{Ngo:PROC:2024}.} Each IU and EU is equipped with one single antenna, while each AP is equipped with $N$ antennas. All APs, IUs and EUs are half-duplex devices. For notational simplicity, we define the sets $\\Kd\\triangleq \\{1,\\dots,K_d\\}$ and  $\\mathcal{L}\\triangleq\\{1,\\ldots,L\\}$ to collect the indices of the IUs and EUs, respectively. Moreover, the set of all APs is denoted by $\\MM \\triangleq \\{1, \\dots, M\\}$. As shown in Fig.~\\ref{fig:systemmodel},  information and energy transmissions are performed simultaneously and in the same frequency band via AP mode selection approaches, i.e., DL information or energy transmission mode at the APs. IUs receive information from a group of the APs (called I-APs) and, at the same time, EUs harvest energy from the remaining APs (called E-APs). EUs consume the average energy they have collected to transmit pilots and data.  Each coherence block includes two phases: 1) UL training for channel estimation; 2) DL WIT and WPT. We assume a quasi-static channel model, with each channel coherence interval spanning $\\tau_c$ time slots. The duration of the training is denoted as $\\tau$, while the duration of the DL WIT and WPT is $(\\tau_c-\\tau)$.",
        "summarize_figure": "2505.02806v1_Fig1",
        "summarization": "The figure illustrates the proposed CF-mMIMO SWIPT system. In this system, time-division duplex operation is employed, where $M$ APs serve $K_d$ information users (IUs) and $L$ energy users (EUs) with energy harvesting capability. Each AP is connected to the CPU via a high-capacity fronthaul link. All APs, IUs, and EUs are half-duplex devices. The figure shows the simultaneous information and energy transmission through AP mode selection strategies, i.e., either information transmission mode (DL) or energy transmission mode (WPT) at the APs. IUs receive information from a set of APs (called I-APs) while EUs harvest energy from the remaining APs (called E-APs). Each coherence block consists of two phases: 1) UL training for channel estimation; 2) DL WIT and WPT. A quasi-static channel model is assumed, with each channel coherence interval spanning $\\tau_c$ time slots."
    },
    "44": {
        "figure1": "2505.02806v1_Fig2",
        "label1": "figconv",
        "caption1": " Convergence behavior of the optimization problem (\\textbf{P3.2}) ($M=60$, $N=12$, $\\Gamma_{\\ell}=250$ $\\mu$Joule).} \\vspace{-1em",
        "text": "In Fig.~\\ref{figconv}, we compare the convergence rate of the optimization problem (\\textbf{P3.2}) for different number of IUs and EUs in the network and for a convergence percision of $\\epsilon=10^{-5}$. To solve~\\eqref{P:SHE:max3}, we use the convex conic solver MOSEK and set $c_2=1500$.   By increasing the number of IUs from $5$ to $10$, we reduced $\\SEQoS$ from $18$ [bit/s/Hz] to $10$ [bit/s/Hz], to ensure the feasibility of the optimization problem.  When the optimization problem begins to converge, i.e., $a_m\\approx a_m^{(n)}\\in\\{0,1\\}$, then the value of the objective function converges to the maximum level of sum-HE, denoted by $L\\phi$. We can see that with a small number of iterations (less than $30$ iterations), problem (\\textbf{P3.2}) returns the optimized solution.  Furthermore, it is worth mentioning that the resulting values of the parameters $a_m$ converge to $1$ and $0$ with high accuracy. The other optimization problem discussed in the paper also exhibits similar convergence behavior, but it is not shown here due to space constraints and to avoid repetition.",
        "summarize_figure": "2505.02806v1_Fig2",
        "summarization": "The figure illustrates the convergence behavior of the optimization problem (P3.2) with varying numbers of IUs and EUs. The objective function value gradually converges to its maximum, indicating successful convergence of the optimization problem. Increasing the number of IUs from 5 to 10 reduces $\\SEQoS$ from 18 [bit/s/Hz] to 10 [bit/s/Hz] to ensure the feasibility of the optimization. The figure shows that within 30 iterations, the optimization problem converges to the optimal solution. Moreover, the parameter values $a_m$ converge to 0 or 1 with high accuracy, demonstrating the stability of the algorithm."
    },
    "45": {
        "figure1": "2505.02806v1_Fig3",
        "label1": "figsumSEsdl",
        "caption1": "Average sum-SE versus the per-IU SE requirement. The solid lines depict results for $\\tilde{\\rho}_t = 0.25$~W, while the dashed lines show results for $\\tilde{\\rho}_t = 0.05$~W ($M=40$, $N=12$, $K_d=L=5$, $\\Gamma_{\\ell}=250$ $\\mu$Joule).} \\vspace{0em",
        "text": "Figure~\\ref{figsumSEsdl} shows the average sum-SE versus the per-IU SE requirement and for two different levels of  pilot power. This figure provides insights into the maximum achievable $\\SEQoS$ levels for different schemes and specific network setups. Specifically, we observe that SWIPT through orthogonal multiple access (Benchmark $3$) fails to support predefined SE requirements greater than $12$ \\bsHz~ for IUs and the predefined average HE of $\\Gamma_{\\ell}=250$ $\\mu$Joule for EUs. Moreover, with random AP mode selection and equal power allocation (Benchmark $1$), neither the IUs nor the EUs' predefined requirements are met even for small values of $\\SEQoS$. However, by applying power control design, we can extend the coverage for IUs and EUs to $\\SEQoS = 15$ \\bsHz. Furthermore, with optimal AP mode selection and power control, this coverage is extended to $\\SEQoS = 21$ \\bsHz, and the overall sum-SE shows a significant improvement over Benchmark $2$, especially in the high $\\SEQoS$ regime.  Finally, we observe that reducing the pilot power from $0.25$ W to $0.05$ W leads to a significant decrease in the achievable sum-SE of the system. This decline is attributed to the reduced channel estimation accuracy and the increased channel estimation error.",
        "summarize_figure": "2505.02806v1_Fig3",
        "summarization": "The figure shows the relationship between the average total SE and the per-IU SE requirement for two different pilot power levels (0.25 W and 0.05 W). It is observed that SWIPT through orthogonal multiple access (Benchmark 3) fails to support predefined SE requirements greater than 12 [bit/s/Hz] for IUs and the predefined average HE (250 microjoules) for EUs. Additionally, with random AP mode selection and equal power allocation (Benchmark 1), neither the IUs nor the EUs' predefined requirements are met even for small SEQoS values. By applying power control design, the coverage for IUs and EUs extends to SEQoS = 15 [bit/s/Hz]. Furthermore, with optimal AP mode selection and power control, the coverage extends to SEQoS = 21 [bit/s/Hz], and the overall sum-SE shows a significant improvement over Benchmark 2, especially in the high SEQoS regime. Finally, reducing the pilot power from 0.25 W to 0.05 W leads to a significant decrease in the achievable sum-SE due to reduced channel estimation accuracy and increased channel estimation error."
    },
    "46": {
        "figure1": "2505.02809v1_ce_cifar100_new",
        "label1": "fig:hessian",
        "caption1": "\\small {\\bf(a, b):}  The Hessian matrix of a  1-hidden-layer network with $8$ hidden neurons at random initialization on CIFAR-100 dataset (\\# hidden neuron $m = 8$ and \\# classes or output neuron $C = 100$). For clearer visualization, we report the absolute value of each Hessian entry, and this applies to all Hessian matrices reported in this work. We observe near-block-diagonal structures under both MSE and CE loss with $m + C = 108$ blocks in total.}  \\vspace{-0.3cm",
        "text": "The Hessian matrix of neural networks (NNs) is crucial for understanding training dynamics, as well as motivating better algorithm designs. A classical work \\citep{collobert2004large} first empirically reported that the Hessian of NNs is highly structured: the Hessian is observed to be {\\it near-block-diagonal}. We reproduce this result in Figure \\ref{fig:hessian}.  Unfortunately, no rigorous theory has been established in the past two decades to explain this phenomenon.\nWe emphasize that these experiments can reveal more Hessian properties not shown in the CIFAR-100 experiments in Figure \\ref{fig:hessian}. We highlight the new changes as follows.",
        "summarize_figure": "2505.02809v1_ce_cifar100_new",
        "summarization": "The figure presents the Hessian matrix of a 1-hidden-layer network with 8 hidden neurons randomly initialized on the CIFAR-100 dataset. For clearer visualization, the absolute value of each Hessian entry is shown, which applies to all Hessian matrices reported in this work. The figure reveals that, under both MSE and CE loss, the Hessian matrix exhibits a near-block-diagonal structure with a total of 108 blocks. This result reproduces the classical finding that the Hessian of neural networks is highly structured and near-block-diagonal. However, despite empirical observations, no rigorous theory has been established in the past two decades to explain this phenomenon. These experiments also reveal additional Hessian properties not shown in the CIFAR-100 experiments."
    },
    "47": {
        "figure1": "2505.02809v1_0315-synthetic-gaussian-nn-dim-500-width-8-class-500-sample_per_class-10-adam-MSEvisiondegree100T500optimizer-adamlr-0.0001_fullhessian_T_500",
        "label1": "fig:closer_look_mse",
        "caption1": "\\small  {\\bf(a-f):}  The Hessian of a 1-hidden-layer network on Gaussian synthetic data under MSE loss. We notice the near-block-diagonal patterns in  $H_{ww}$ and $H_{vv}$ with $m + C = 508$ blocks in total, and they maintain along training.}  \\vspace{-0.3cm",
        "text": "The results are shown in Figure  \\ref{fig:closer_look_mse} and \\ref{fig:closer_look_ce}. We summarize two findings.",
        "summarize_figure": "2505.02809v1_0315-synthetic-gaussian-nn-dim-500-width-8-class-500-sample_per_class-10-adam-MSEvisiondegree100T500optimizer-adamlr-0.0001_fullhessian_T_500",
        "summarization": "The figure shows the Hessian matrix of a 1-hidden-layer network on Gaussian synthetic data under MSE loss. It is observed that during training, both $H_{ww}$ and $H_{vv}$ exhibit near-block-diagonal structures with a total of 508 blocks. This near-block-diagonal pattern is preserved throughout the training process, providing valuable insights into the dynamics of neural network training."
    },
    "48": {
        "figure1": "2505.02809v1_0315-synthetic-gaussian-nn-dim-500-width-8-class-500-sample_per_class-10-adam-CEvisiondegree60T50000optimizer-adamlr-0.0001_fullhessian_T_50000",
        "label1": "fig:closer_look_ce",
        "caption1": "\\small  {\\bf(a-f):}  The Hessian of a 1-hidden-layer network on Gaussian synthetic data under CE loss. At initialization, we observe the “block-circulant” pattern in $H_{wv}$, and the near-block-diagonal structure in $H_{ww}$ and $H_{vv}$ (with $m + C = 508$ blocks in total). We refer to it as {\\it “block-circulant-block-diagonal matrix”}.  We notice that the “block-circulant” pattern in $H_{wv}$ vanishes along training, while the near-block-diagonal patterns in  $H_{ww}$ and $H_{vv}$ are preserved. }  \\vspace{-0.3cm",
        "text": "The results are shown in Figure  \\ref{fig:closer_look_mse} and \\ref{fig:closer_look_ce}. We summarize two findings.",
        "summarize_figure": "2505.02809v1_0315-synthetic-gaussian-nn-dim-500-width-8-class-500-sample_per_class-10-adam-CEvisiondegree60T50000optimizer-adamlr-0.0001_fullhessian_T_50000",
        "summarization": "The figure shows the Hessian matrix of a 1-hidden-layer network on Gaussian synthetic data under CE loss. At initialization, the Hwv matrix exhibits a \"block-circulant\" pattern, while Hww and Hvv show a near-block-diagonal structure, with a total of 508 blocks. This is referred to as a \"block-circulant-block-diagonal matrix.\" As training progresses, the \"block-circulant\" pattern in Hwv vanishes, while the near-block-diagonal patterns in Hww and Hvv are preserved. These results highlight the structural properties of the neural network Hessian matrix and its changes during training."
    },
    "49": {
        "figure1": "2505.02812v1_sub_it2",
        "label1": "img::iteration2",
        "caption1": "Base case of Claim \\ref{claimseqofpaths}. ",
        "text": "We now show property II for $p=1$. Let $Z'$ be any element of $\\z_1$. Then $Z'$ is an $\\{i,j\\}$-potential in $(G,c)$. Let $a,b$ be the corresponding twin vertices. Then we claim that exactly one of the vertices among $a$ and $b$ is in $\\C_1$. Suppose for a contradiction that either both $a$ and $b$ are in $\\C_1$ or none of them. Given that $\\{c_1(a),c_1(b)\\} = \\{i,j\\}$ in both of these cases we obtain that $Z'$ is a maximum zigzag in $(G,c)$ if and only if $Z'$ (up to a permutation) is a maximum zigzag in $(G,c_1)$. This contradicts the assumption that $Z'$ is an element of $\\z_1$. Thus, each element in $\\z_1$ has exactly one twin vertex in $\\C_1$. This verifies II for $p=1$, and thus the base case is ready.  This base case is illustrated in Figure \\ref{img::iteration2}.",
        "summarize_figure": "2505.02812v1_sub_it2",
        "summarization": "The figure illustrates the base case of Claim 2. It shows a tree structure with nodes zi and zi'. The red and blue lines represent potential zigzag paths in different sets, C1 and C2, respectively. Each zigzag path has the corresponding i,j-potential, and the tree structure clearly shows the relationships between nodes and how they satisfy the conditions of Claim 2. This figure is used to verify Property II for p=1, demonstrating that each element in z1 has a unique twin vertex in set C1."
    },
    "50": {
        "figure1": "2505.02812v1_sub_gen_proc",
        "label1": "fig:Claim16",
        "caption1": "Paths $P_1,P_2,\\dots, P_m$",
        "text": "We now construct a sequence $P_1, P_2,\\dots , P_{m}$ of pairwise (vertex-)disjoint paths in $G_{ij}$ and pick a sequence of $\\{i,j\\}$-potential zigzags $Z_1, Z_2, \\dots , Z_{m-1}$ of $(G,c)$ with twin vertices $\\{a_1, b_1\\}, \\,$ $ \\{a_2, b_2\\},\\dots ,\\{a_{m-1}, b_{m-1}\\}$, respectively,  such that See Figure~\\ref{fig:Claim16}. By Claim~\\ref{claimseqofpaths}, for every $1 \\leq q \\leq m-1$, each zigzag in $\\z_q$ has one twin in $\\C_q$ and the other twin  not in $\\C_1 \\cup \\C_2 \\cup \\dots \\cup \\C_q$. As in the proof of that claim, let $A_q$ and $B_q$ denote $\\{a \\mid a \\in V(\\C_q) \\text{ is a twin vertex of a zigzag in } \\z_q\\}$ and $\\{b \\mid b \\not\\in V(\\C_1 \\cup \\C_2 \\ldots \\cup \\C_q)$ is a twin vertex of a zigzag in $\\z_q\\}$, respectively, for all $1 \\leq q \\leq m-1$. Further let $b_{m-1}\\in B_{m-1}$ be such that $z_j\\in C_{ij}^{b_{m-1}}$. Choose $P_{m}$ as any $(b_{m-1},z_j)$-path in $C_{ij}^{b_{m-1}}$. Let $Z_{m-1}$ be a potential zigzag in $\\mathcal{Z}_{m-1}$ with twin vertices $\\{{a}_{m-1}, b_{m-1}\\}$. Note that $a_{m-1}$ is a vertex of $A_{m-1}$, and that there exists a sequence of $\\{i,j\\}$-potential zigzags $Z_{m-2}, Z_{m-3}, \\dots ,Z_{1}$ in $\\z_{m-2}, \\z_{m-3}, \\ldots \\z_1$, respectively, with twin vertices $\\{a_{m-2},b_{m-2}\\}, \\{a_{m-3},b_{m-3}\\}, \\dots ,\\{a_1,b_1\\}$, respectively, where $a_{q} \\in A_q$ and $b_q \\in B_q$, for all $1 \\leq q \\leq m-2$. For $2\\le q\\le m-1 $ choose for $P_{q}$ some $(a_{q+1},b_{q})$-path in the subgraph $\\C_{q}$ Finally,  as $a_{1} \\in \\C_1 = C_{ij}^{z_i}$, we can choose a path $P_{1}$ in $\\C_1$ joining $z_i$ and $a_{1}$. As $\\C_1, \\C_2, \\ldots \\C_{p+1}$ are pairwise disjoint subgraphs of $G_{ij}$, these paths are (vertex-)disjoint.",
        "summarize_figure": "2505.02812v1_sub_gen_proc",
        "summarization": "The figure illustrates the construction of a sequence of pairwise vertex-disjoint paths P1, P2, ..., Pm in graph Gij, and a sequence of i,j-potential zigzag paths Z1, Z2, ..., Zm-1, with twin vertices {a1, b1}, {a2, b2}, ..., {am-1, bm-1}. According to Claim 2, for each 1 ≤ q ≤ m-1, each zigzag in Zq has one twin vertex in Cq and the other twin vertex not in C1 ∪ C2 ∪ ... ∪ Cq. The figure demonstrates how these paths and zigzag structures maintain pairwise disjoint relationships by selecting appropriate subgraphs, ultimately forming a sequence of pairwise disjoint paths."
    },
    "51": {
        "figure1": "2505.02813v1_sys_model",
        "label1": "fig:sys_model",
        "caption1": "Query-based (pull-based) sampling of a Markovian source.",
        "text": "In this work, we consider the problem of query-based sampling of a continuous time Markov chain (CTMC), where a remote monitor sends queries at exponential intervals to the Markov source, which instantaneously responds back with the system state to the remote monitor (see Fig.~\\ref{fig:sys_model}). Our contributions can be summarized as follows: We introduce three new estimators, named exponential, Erlang and $\\tau$-MAP estimators, and we provide closed-form analytical expressions for the binary freshness metric under each of them. Through theoretical and numerical results, we show that through a slight modification to the martingale estimators from which $\\tau$-MAP estimators are derived, a significant gain in the binary freshness metric can be achieved.",
        "summarize_figure": "2505.02813v1_sys_model",
        "summarization": "The figure shows a query - based (pull - based) sampling model of a Markovian source. A remote monitor sends queries at exponential intervals to the Markov source (a drone with state X(t) having states like Land, Charge, Flying, Hover), which instantaneously sends the system state X^(t) back. The model introduces exponential, Erlang, and τ - MAP estimators, with closed - form expressions for the binary freshness metric. Slight modifications to martingale estimators for τ - MAP estimators can improve the binary freshness metric."
    },
    "52": {
        "figure1": "2505.02813v1_Erlang_chain",
        "label1": "fig:erlang_chain",
        "caption1": "State transition diagram for $Y(t)$.",
        "text": "Let $Y(t)$ be a CTMC independent of $X(t)$, with $\\Gamma$ states where the state transitions occur only from the $k$th state to the $(k+1)$th state with rate $\\lambda \\Gamma$ and from the $k$th state to state $1$ with rate $\\mu$. State $\\Gamma$ can only transition to state $1$ with rate $\\mu$; see Fig.~\\ref{fig:erlang_chain}. Starting with $Y(0)=1$, the time to reach state $\\Gamma$ when  $\\mu=0$ is governed by an Erlang distribution whose rate and shape parameters are $\\lambda \\Gamma$ and $\\Gamma-1$. Denote by $\\hat{X}_{\\Gamma,\\lambda}(t)$, the Erlang estimator. Then, $\\hat{X}_{\\Gamma,\\lambda}(t)$ is defined as follows,         X(G(t)),& \\text{if}~Y(t)\\neq \\Gamma,\\\\         i^*,&\\text{if}~Y(t)=\\Gamma.",
        "summarize_figure": "2505.02813v1_Erlang_chain",
        "summarization": "The figure illustrates the state transition diagram of a continuous-time Markov chain (CTMC) with Γ states. The state transitions occur only from the k-th state to the (k+1)-th state with a rate of λΓ and from the k-th state to state 1 with a rate of μ. State Γ can only transition to state 1 with a rate of μ. Starting with Y(0)=1, the time to reach state Γ when μ=0 follows an Erlang distribution with rate λΓ and shape parameter Γ-1. The figure shows the state transition process of the Markov chain and defines the Erlang estimator, where the estimate is i* when Y(t)=Γ."
    },
    "53": {
        "figure1": "2505.02813v1_var_mu",
        "label1": "fig:var_mu",
        "caption1": "Variation of binary freshness with the sampling rate for $\\Gamma=10$ and $\\lambda=\\frac{1}{\\tau^*}$ for a generic 4-state CTMC.",
        "text": "In this section, we compare and evaluate the performance of the structured estimators as we change their respective parameters. First, we will compare how the binary freshness varies under the martingale, exponential and Erlang estimators, as we change the sampling rate for a generic CTMC of four states. In all numerical experiments we only consider CTMCs with a unique stationary maximum. As illustrated in Fig.~\\ref{fig:var_mu}, the exponential estimator (red) outperforms the martingale estimator (green) at low sampling rates while the opposite behavior is observed at high sampling rates. This is due to the fact that, for high sampling rates, when the exponential timer runs out before we take the next sample, it would most probably run out at smaller time intervals (smaller than $\\tau^*$). Thus, our estimator will shift to $i^*$ prematurely decreasing the binary freshness metric. At all sampling rates, we can see that the Erlang estimator outperforms the other two estimators. For higher sampling rates the performance of the martingale estimator approaches that of the Erlang estimator, while for lower sampling rates the exponential estimator exhibits comparable performance to the Erlang estimator.",
        "summarize_figure": "2505.02813v1_var_mu",
        "summarization": "The figure shows the variation of binary freshness with the sampling rate for a generic 4-state continuous-time Markov chain (CTMC) with Γ=10 and λ=1/τ*. It compares the performance of martingale, exponential, and Erlang estimators as the sampling rate changes. At low sampling rates, the exponential estimator (red) outperforms the martingale estimator (green), while at high sampling rates, the opposite behavior is observed. This is due to the fact that at high sampling rates, the exponential timer runs out before the next sample is taken, leading to premature switching to i*, which reduces the binary freshness metric. Overall, the Erlang estimator outperforms the other two estimators at all sampling rates. At higher sampling rates, the performance of the martingale estimator approaches that of the Erlang estimator, while at lower sampling rates, the exponential estimator exhibits comparable performance to the Erlang estimator."
    },
    "54": {
        "figure1": "2505.02822v1_arborexample",
        "label1": "fig:arborexample",
        "caption1": "On the left, a graph decomposed into $3$ forests.  On the right, the same graph decomposed into $2$ forests.",
        "text": "In Figure~\\ref{fig:arborexample}, two different decompositions of the same graph $G$ are shown.  On the left, the graph is decomposed into $3$ forests, thus demonstrating that the arboricity is at most three, $a(G) \\leq 3$.  On the right, $G$ is decomposed into $2$ forests, thus demonstrating that $a(G) \\leq 2$.  Since $G$ contains cycles, the arboricity is also at least two, therefore $a(G) = 2$ for this example.   \\end{example}",
        "summarize_figure": "2505.02822v1_arborexample",
        "summarization": "The figure shows two different decompositions of the same graph G. On the left, the graph is decomposed into three forests, indicating that the arboricity of the graph is at most three, a(G) ≤ 3. On the right, the graph is decomposed into two forests, indicating that the arboricity of the graph is at most two, a(G) ≤ 2. Since the graph contains cycles, its arboricity is at least two, so a(G) = 2 for this example."
    },
    "55": {
        "figure1": "2505.02826v1_rho",
        "label1": "fig:density",
        "caption1": "\\textbf{Liquid water's density vs. temperature and pressure, predictions and experiments.}  \\textbf{a)} Density vs. temperature at atmospheric pressure: model (solid black line); experiments\\cite{Mishima2010, Pallares2016} (orange open triangles); TIP4P (open green circles); MB-pol (open blue circles). \\textbf{b)} The computed populations of the three model microstates. The dotted line in \\textbf{a, b} indicates the temperature of maximum density, 4 $^\\circ$C. \\textbf{c)} Density isobars for non-ambient pressures. Theory predictions for $p>0$ are shown in red and for $p<0$ in blue. Circles are the experimental data points. \\textbf{d)} Maximum density prediction in comparison with experiments (orange triangles), TIP4P (green circles), and DNN@MBpol \\cite{Sciortino2025} (blue circles). Pressures are in MPa.",
        "text": "A purpose of this model is to provide a microscopic interpretation of water's liquid across temperature and pressure, based on the calculated populations ($f_{cage}, f_{pHB}, \\And f_{vdW}$) of the microstates, figure \\ref{fig:density}b.  Here we compare the model predictions: (i) to experiments and (ii) to the TIP4P/2005 \\cite{Abascal2005, Gonzalez2016} and to MB-pol \\cite{Palos2024} explicit water models, which are often regarded as the state-of-the-art atomistic models for pure liquid water at least at ambient pressure.\nWe compute water's density from the molar volume, $v_{mol}$, with typical density relation $\\rho = M_w/v_{mol}N_A$, where $M_w = 18\\ g/mol$ is molecular weight of water per mole and $N_A = 6.022 \\times 10^{23}\\ molecules/mol$ is the Avogadro number. Figure \\ref{fig:density} shows that this model quantitatively predicts the density for the entire range $pT$ ($-100\\ MPa\\ to\\ 400\\ MPa \\And -75^\\circ C\\ to\\ 150^\\circ C $) explored experimentally \\cite{Mishima2010, Pallares2016}. At atmospheric pressure, the prediction is as accurate as that of TIP4P/2005 and MB-pol. It predicts a density maximum around 4$^\\circ$C, shown with a vertical dotted line in figure \\ref{fig:density}a, and also the boiling at 100$^\\circ$C (see figure \\ref{fig:vle}).\\\\\nFigure \\ref{fig:density}c shows predicted density-temperature curves for different pressures. First, as expected, water's density increases with pressure. Figure S2 shows the model prediction that increasing pressure crunches cages into pHB pieces. Figure S3 shows that the two crossover points, from $f_{cage}$ to $f_{pHB}$ and from $f_{cage}$ to $f_{vdW}$, shift to lower temperatures with increased pressure, resulting in the temperature of maximum density (TMD) shifting towards a lower temperature with pressure.  Figure \\ref{fig:density}d experimental predictions that are comparable to, or better than, those of TIP4P/2005 and the deep neural network surrogate of MB-pol (DNN@MB-pol) \\cite{Sciortino2025}. Under stretchable pressures, $p<0$, TMD undergoes a turnaround point, resulting from the nonmonotonic behavior of $f_{vdW}$. It increases with pressure in cold water, $T<\\approx 10^\\circ$C until it plateaus at $\\approx9\\%$, figure S2c. \\\\",
        "summarize_figure": "2505.02826v1_rho",
        "summarization": "This figure shows the density of liquid water as a function of temperature and pressure, comparing model predictions to experimental data. Panel (a) shows the density-temperature relationship at atmospheric pressure, with the model (solid black line) compared to experimental data (orange open triangles), TIP4P (green open circles), and MB-pol (blue open circles). The temperature of maximum density (around 4°C) is marked with a vertical dotted line. Panel (b) shows the computed populations of three microstates of the model: Cage, pHB, and vdW. Panel (c) presents density isobars for different pressures, with red showing predictions for positive pressures and blue for negative pressures, with experimental data points as circles. Panel (d) compares the maximum density predictions with experimental data (orange triangles), TIP4P (green circles), and DNN@MBpol (blue circles), with the theory prediction shown as a solid black line. Overall, the model performs well in predicting the density of liquid water across a wide range of temperatures and pressures, with predictions at atmospheric pressure comparable to TIP4P and MB-pol models."
    },
    "56": {
        "figure1": "2505.02826v1_Cp",
        "label1": "fig:cp",
        "caption1": "\\textbf{Heat capacity, $C_p$, and its components vs temperature.}  \\textbf{a)} $C_p$ at atmospheric pressure. \\textbf{b)} Model component contributions to $C_p$. \\textbf{c)} $C_p$ isobars. Pressures are in MPa. Legends are the same as in figure \\ref{fig:density}. Experimental data is taken from ref. \\cite{Angell1982, Mallamace2014, Lin2012, pathak2021}.",
        "text": "We calculate the heat capacity from the enthalpy using the thermodynamic relationship      C_p = \\frac{\\partial  \\langle h \\rangle}{\\partial T} =\\sum_j \\frac{\\partial  (f_j \\langle h_{j} \\rangle)}{\\partial T} \\equiv \\sum_j C_{p,j}, here $C_{p,j}$ is the contribution from the $j^{th}$ state. Figure \\ref{fig:cp} shows agreement with experiments no worse than TIP4P/2005 or MB-pol at ambient temperatures. It also captures the anomalous increase in the supercooled region. The pronounced minimum (compared to a subtle minimum observed in experiments \\cite{Debenedetti2003}) is around $35^\\circ$C, which results from the dominant contribution switch from cage to pHB, see figure \\ref{fig:cp}b. The maximum deviation from the experiment is near this minimum, $\\approx 0.6\\ J/gK$, which is reasonable. At lower temperatures, the contribution from breaking of cages dominates which causes $C_p$ to increase until it peaks, $C_p^{max}$, around $-44 ^\\circ$C, figures \\ref{fig:cp}b. Confined water experiments \\cite{Mallamace2008, Mallamace2014, Oguni2008} as well as theories (MD simulations \\cite{Biddle2017, Gonzalez2016}, equation of state models \\cite{Biddle2017, Holten2012, Holten2014b}, and recent statmech based model \\cite{coronas2024p}) exhibit such peaks. At high pressure, figure \\ref{fig:cp}c shows that predicted $C_p$, similar to experiment, decreases with pressure at ambient temperature and $C_p^{max}$ shifts to a lower temperature (see figure S4) similar to TMD. This is a consequence of the shift in the crossover point between the cage and pHB water to a lower temperature with pressure, figure S3.",
        "summarize_figure": "2505.02826v1_Cp",
        "summarization": "This figure shows the heat capacity (Cp) of liquid water as a function of temperature, comparing model predictions to experimental data. Panel (a) shows Cp at atmospheric pressure, with the model (solid black line) compared to experimental data (orange open triangles), TIP4P (green open circles), and MB-pol (blue open circles). Panel (b) shows the contributions of different microstates to Cp, including Cage, pHB, and vdW states. Panel (c) presents Cp isobars for different pressures, with red lines showing Cp at higher pressures and lighter lines for lower pressures. The figure reveals that the model's predictions for Cp at ambient temperatures are comparable to TIP4P and MB-pol, and it captures the anomalous increase in Cp in the supercooled region. A pronounced minimum occurs around 35°C, driven by the shift in contributions from Cage to pHB. The maximum deviation from experiment is around 0.6 J/gK, near this minimum. At lower temperatures, the contribution from breaking of cages dominates, causing Cp to peak around -44°C. Panel (c) shows that at high pressure, Cp decreases with pressure, and Cp's maximum shifts to lower temperatures, similar to TMD behavior."
    },
    "57": {
        "figure1": "2505.02897v1_fig_phipm",
        "label1": "fig:phipm",
        "caption1": "An example of compactly supported smooth functions $\\varphi^+$ (red) and $\\varphi^-$ (blue). They are constructed from bump functions of the form $\\varphi^+(x)=\\exp\\left(-\\frac{b_+}{a_+^2 - x^2} + \\frac{b_+}{a_+^2 - 1}\\right)$ (for $\\abs{x}\\leqslant a_+$) and $\\varphi^-(x)=\\exp\\left(-\\frac{b_-}{a_-^2 - x^2} + \\frac{b_-}{a_-^2}\\right)$ (for $\\abs{x}\\leqslant a_-$), with appropriately chosen parameters.",
        "text": "The first consequence of theorem \\ref{theorem:error} is that the Cardy formula is valid when the size of the twist interval decays to zero as long as the process is slow enough: \tLet $\\delta>0$ and $\\gamma\\in[0,1/4)$ be fixed, let $\\mathcal{N}(h_*,\\delta,\\gamma,J)$ be the number of spin-$J$ Virasoro primary states with $h\\in(h_*-\\delta J^{-\\gamma},h_*+\\delta J^{-\\gamma})$. Then we have \tConsider two types of compactly supported smooth test functions $\\varphi^\\pm$ satisfying \twhere $\\theta_{(-1,1)}(x)$ is the indicator function of $(-1,1)$. See figure \\ref{fig:phipm} for an example. \tDefine \taccording to \\eqref{def:phirescale}. \tSince the spectral density is positive, we have \t\t\t2\\delta J^{-\\gamma}\\int dh\\,\\rho_J(J+2h)\\,\\varphi_{h_*,\\delta J^{-\\gamma}}^-(h)\\leqslant\\mathcal{N}(h_*,\\delta,\\gamma,J)\\leqslant 2\\delta J^{-\\gamma}\\int dh\\,\\rho_J(J+2h)\\,\\varphi_{h_*,\\delta J^{-\\gamma}}^+(h). \tLet us first focus on the upper bound. By theorem \\ref{theorem:error}  and proposition \\ref{prop:rhovaclimituniform}, we obtain \tfor any $\\varphi^+$ satisfying the above conditions. Notice that proposition \\ref{prop:rhovaclimituniform} does not apply for $\\gamma<0$ because the support of the test function stops being bounded, so we lose uniformity. \tThe left-hand side of \\eqref{limsupN} does not depend on $\\varphi^+$, so we can optimize the upper bound by taking the infimum over the allowed choices of $\\varphi^+$. Recall that $\\varphi^+$ is required to be greater than the indicator function $\\theta_{(-1,1)}$, but may be taken arbitrarily close to it. Hence, we have \t\t\t&\\inf\\limits_{\\varphi^+ > \\theta_{(-1,1)}} \\int_{h_* - \\delta}^{h_* + \\delta} dh\\, \\rho_{\\rm c}(h)\\, \\varphi^+_{h_*, \\delta}(h) = \\delta^{-1} \\int_{h_* - \\delta}^{h_* + \\delta} dh\\, \\rho_{\\rm c}(h), \\\\ \t\t\t&\\inf\\limits_{\\varphi^+ > \\theta_{(-1,1)}} \\int dx\\, \\varphi^+(x) = 2. \tTherefore, taking the infimum on the right-hand side of \\eqref{limsupN} yields",
        "summarize_figure": "2505.02897v1_fig_phipm",
        "summarization": "This figure shows examples of two compactly supported smooth functions: ϕ^+ (in red) and ϕ^- (in blue). These functions are constructed from bump functions of the form ϕ^+(x) = exp(-b+/(a+^2 - x^2) + b+/(a+^2 - 1)) for |x| ≤ a+ and ϕ^-(x) = exp(-b-/(a-^2 - x^2) + b-/(a-^2)) for |x| ≤ a-. The figure displays the shapes of these functions in red and blue, with the rectangular regions indicating the support intervals of each function. The example illustrates the shape characteristics of these functions and their application in mathematical models."
    },
    "58": {
        "figure1": "2505.02898v1_Fig1",
        "label1": "fig:1",
        "caption1": " A schematic illustration depicting the sequential phases and steps of the GrAviPaSt method in identifying a macro-filament composed of three micro-filaments. Four galaxy groups are presented, with the method’s phases applied sequentially from bottom to top. ",
        "text": "GrAviPaSt method specifically uses Prim's MST algorithm (\\cite{prim1957shortest}) instead of other MST algorithms such as Kruskal's algorithm (e.g., as mentioned in \\cite{santiago2020identification}). This choice is due to the optimal implementation of Prim's MST. In GrAviPaSt method, the spatial coordinates of each galaxy group's BCG (or BGG) serve as the node coordinates for the input of the MST algorithm. The Prim's MST algorithm begins by initializing the process with an arbitrary node (vertex) from the input dataset, designating it as part of the growing minimum spanning tree (MST), while assigning an initial cost of infinity to all other nodes except the starting node, which has a cost of zero. Next, it iteratively selects the edge with the smallest weight that connects any node in the MST to a node outside it. For each newly added node, the algorithm updates the costs of its neighboring nodes, ensuring that any smaller edge weight replaces the current cost. This process is repeated until all nodes are included in the MST, thereby creating a structure with minimum-cost connections. Following the implementation of Prim's MST, the pairs of galaxy groups between which the GrAviPaSt method will search for micro-filaments are identified. An example of the implementation of Prim's MST is shown in Figure \\ref{fig:1}.\nWith the truncated pyramid and cropped box in place, the GrAviPaSt method moves to the next step of this phase. In this step, the truncated pyramid is filled with grid points, each separated by a distance of $d$ Mpc from its horizontal and vertical neighbors. The value of d is determined by: d = h\\times(\\frac{N}{V})^{\\frac{1}{3}} where $h$ ($H_0/100$) is the Hubble constant, $N$ is the total number of galaxies in the input dataset (not the cropped box), and $V$ is the total volume of the input dataset. These grid points are shown between the galaxy group pairs in Figure \\ref{fig:1}.\nAfter determining this minimum path (or deepest gravitational well), the GrAviPaSt method considers this path as the main skeleton of the micro-filament connecting the two galaxy groups. These skeletons are illustrated between galaxy groups in Figure \\ref{fig:1}.\nOnce the final structure of the micro-filaments is determined, the GrAviPaSt method evaluates all the micro-filaments and eliminates those containing only a single galaxy. As a result, all of the final micro-filaments, as well as the macro-filaments (Section \\ref{sec:3.4}), contain at least two galaxies within their cylindrical structure. This cylindrical structure is depicted in Figure \\ref{fig:1}.",
        "summarize_figure": "2505.02898v1_Fig1",
        "summarization": "This figure shows the sequential phases and steps of the GrAviPaSt method in identifying a macro-filament composed of three micro-filaments. Four galaxy groups are shown, and the phases of the GrAviPaSt method are applied sequentially from bottom to top. The method uses Prim's MST algorithm, with the BCG (or BGG) of each galaxy group as the node coordinates for the MST input. Prim’s MST begins with an arbitrary node and iteratively adds the edge with the smallest weight, eventually creating a minimum-cost spanning tree. The truncated pyramid and cropped box are used for spatial partitioning, and grid points are filled within the truncated pyramid. The method identifies the connection between micro-filaments and evaluates them to ensure each contains at least two galaxies in its structure."
    },
    "59": {
        "figure1": "2505.02898v1_Fig2",
        "label1": "fig:2",
        "caption1": " The median trend of micro-filament thickness, along with its associated error, is presented for their length distribution in co-moving (\\textit{Top}) and proper (\\textit{Middle}) coordinates. At the \\textit{Bottom}, the median trend of the velocity of galaxies along the micro-filaments' skeleton is shown. Note that, according to the implementation of the GrAviPaSt method, the skeleton of micro-filaments originates at the smaller group and terminates at the larger group. ",
        "text": "The distribution of micro-filament lengths is illustrated in Figure \\ref{fig:2}, with the top panel showing the co-moving lengths, and middle panel the proper lengths. As shown in this figure, the co-moving lengths of micro-filaments remain within a similar range across different redshifts, exhibiting slow evolution. By factoring out the Hubble flow, the specific influences of gravity and DE on filament length evolution become more apparent (\\cite{galarraga2024evolution}).\nIn contrast, the distribution of proper filament lengths varies significantly across different redshifts. As illustrated in the middle panel of Figure \\ref{fig:2}, the range of micro-filament lengths expands with decreasing redshift. This global growth, evident in proper coordinates, is a direct consequence of the Universe’s expansion. Driven by the Hubble flow, the cosmic web undergoes continuous stretching, leading to an overall increase in micro-filament lengths.\nThe statistical analysis, performed using two-sided Kolmogorov–Smirnov (K-S) tests, further supports these findings. Comparisons of the co-moving length distributions of micro-filaments at redshifts 0 and 0.5, 0 and 1, as well as 0.5 and 1, yield p-values of 0.92, 0.68, and 0.52, respectively. These values indicate no statistically significant differences among the distributions. In contrast, comparisons of the proper length distributions result in p-values of $1.8 \\times 10^{-5}$, $9.34 \\times 10^{-15}$, and $1.21 \\times 10^{-4}$ respectively, indicating statistically significant differences among these distributions. Additionally, Figure \\ref{fig:2} illustrates that the micro-filament population, in both proper and co-moving coordinates, is consistently dominated by shorter filaments across all redshifts.\nThe thickness distribution of micro-filaments across different redshifts, in both co-moving and proper coordinates, represents another key characteristic examined in this study. As illustrated in the panels of Figure \\ref{fig:2}, the range of thickness distributions in co-moving coordinates remains largely consistent across redshifts, similar to the length distributions. However, in proper coordinates, as redshift decreases, the range of thickness distributions expands. This thickening of micro-filaments over cosmic time, akin to the trend observed in proper length distributions, is driven by the Hubble flow and the Universe’s continuous expansion.\nIt is important to note that the micro-filament skeletons, as identified by the GrAviPaSt method, begin at the smaller galaxy group and extend to the larger one (Section \\ref{sec:3.2}). Smaller normalized positions correspond to galaxies near the smaller group, while larger normalized positions indicate galaxies closer to the larger group. In the bottom panel of Figure \\ref{fig:2}, negative velocities represent galaxies moving toward the micro-filament skeleton, while positive velocities indicate galaxies moving away from it. As shown in this figure, galaxies with smaller normalized positions, and therefore closer to the smaller group, tend to exhibit more negative velocities and approaching the filament skeleton. Moving further along the micro-filament skeleton, galaxy velocities gradually become more positive. Within the normalized range of 0 to 0.3, the median velocities are negative across different redshifts, transitioning to values close to zero around 0.3. Beyond this point, within the range of 0.3 to 1, the median velocities remain approximately zero.",
        "summarize_figure": "2505.02898v1_Fig2",
        "summarization": "This figure shows the median trend of micro-filament thickness along with its associated error for their length distribution in co-moving (top) and proper (middle) coordinates. At the bottom, the median trend of galaxy velocities along the micro-filament skeleton is shown. According to the implementation of the GrAviPaSt method, the skeleton of micro-filaments originates at the smaller group and terminates at the larger group. The top panel shows the co-moving length distribution, which remains within a similar range across different redshifts, exhibiting slow evolution. The middle panel shows the proper length distribution, which significantly expands with decreasing redshift, reflecting the expansion of the Universe. Statistical tests reveal no significant differences in co-moving lengths but significant differences in proper lengths. The thickness distribution of micro-filaments follows a similar trend, expanding in proper coordinates as redshift decreases. The bottom panel displays the velocity trends of galaxies along the skeleton, showing that galaxies closer to the smaller group have negative velocities, indicating motion toward the filament, while galaxies further along the filament exhibit velocities approaching zero."
    },
    "60": {
        "figure1": "2505.02898v1_Fig3",
        "label1": "fig:3",
        "caption1": " The radial profiles of mass density contrast are presented for total (\\textit{top}), CC (\\textit{middle}) and GG micro-filaments (\\textit{bottom}), each displayed across three redshifts using different colors. The corresponding fit curves, based on the Plummer profile, are represented by dashed lines. ",
        "text": "We selected this profile due to its effective representation of the total mass density contrast of micro-filaments. The radial mass density contrast profile of micro-filaments across three different redshifts is illustrated in the top panel of Figure \\ref{fig:3}, with the corresponding fit-curves shown as dashed lines. The fitted parameters and Mean Squared Errors (MSEs) are summarized in Table \\ref{tab:1}. As depicted in this figure, the mass density contrast declines as the distance from the micro-filament skeleton increases. Additionally, as redshift decreases, micro-filaments exhibit a gradual reduction in mass density contrast, reflecting a slow evolutionary trend.\nBeyond the total population of micro-filaments, we also examined two specific types: those connecting galaxy clusters (CCs) and those linking groups of galaxies (GGs). CC micro-filaments were identified as structures bridging galaxy groups with at least 100 members, while GG micro-filaments connected groups containing 10 to 20 members. The middle and bottom panels of Figure \\ref{fig:3} illustrate the profiles and corresponding fit-curves for CC and GG micro-filaments, modeled using the Plummer profile. Their fitted parameters and MSEs are detailed in Table \\ref{tab:1}.\nAs shown in Figure \\ref{fig:3}, the radial mass density profiles of CC micro-filaments across all three redshifts exhibit a general shape similar to that of the overall micro-filament population. In contrast, the profiles of GG micro-filaments differ significantly from those of both total filaments and CC micro-filaments.",
        "summarize_figure": "2505.02898v1_Fig3",
        "summarization": "This figure shows the radial profiles of mass density contrast for total (top), CC (middle), and GG micro-filaments (bottom), each displayed across three redshifts with different colors. The corresponding fit curves, based on the Plummer profile, are shown as dashed lines. The figure reveals that the mass density contrast decreases with the distance from the micro-filament skeleton. As redshift decreases, micro-filaments exhibit a gradual reduction in mass density contrast, indicating slow evolutionary trends. Beyond the total micro-filament population, we also examined two specific types: CC micro-filaments connecting galaxy clusters and GG micro-filaments linking groups of galaxies. The profiles and fit curves for CC and GG micro-filaments are shown in the middle and bottom panels. The radial mass density profiles of CC micro-filaments are similar to those of the total population, while GG micro-filaments exhibit a distinct profile from both the total and CC micro-filaments."
    },
    "61": {
        "figure1": "2505.02899v1_QUADRATIC",
        "label1": "fig1",
        "caption1": "Figure describing the celestial description of conformal correlator (right) for $n\\to m$ scattering amplitude in asymptotically flat space (left). ",
        "text": "Traditionally, the external states of scattering amplitudes are parametrized by the energy eigenvalues. Alternatively, the conformal primary basis is a particularly interesting alternative, which renders the resulting scattering amplitude in a form that transforms it into a conformal correlator on the celestial sphere schematically shown in Fig.~\\eqref{fig1}. As such, scattering amplitudes expressed in the conformal primary basis are usually referred to as celestial amplitudes. For the massless field scattering, external states in the four-momentum basis can be parametrized in terms of frequency $\\omega$ and a point on celestial sphere $(z,\\bar z)$ as  the external momenta can be parametrized as \\cite{Arkani-Hamed:2020gyp,Pasterski:2016qvg},     p^i_\\mu=\\frac{\\omega}{\\sqrt{2}}\\left(1+|z_i|^2,z_i+\\bar z_i,-i(z_i-\\bar z_i),1-|z_i|^2\\right)\\,.\\label{2.1e} In celestial conformal field theory (CCFT), the massless four-momenta can be written as \\eqref{2.1e}. In $(-+++)$ convention $z$ is the cross-ratio and $\\bar z$ is its complex conjugate z=\\frac{z_{12}z_{34}}{z_{13}z_{24}},\\,\\,\\,\\,\\,\\,\\bar z=\\frac{\\bar z_{12}\\bar z_{34}}{\\bar z_{13}\\bar z_{24}}.",
        "summarize_figure": "2505.02899v1_QUADRATIC",
        "summarization": "This figure describes the celestial representation of the conformal correlator (right) for $n \\to m$ scattering amplitude in asymptotically flat space (left). Traditionally, external states of scattering amplitudes are parametrized by energy eigenvalues. Alternatively, the conformal primary basis is a particularly interesting alternative, which transforms the scattering amplitude into a conformal correlator on the celestial sphere, as shown schematically. Scattering amplitudes expressed in the conformal primary basis are referred to as celestial amplitudes. For massless field scattering, external states in the four-momentum basis are parametrized in terms of frequency $\\omega$ and a point on the celestial sphere $(z, \\bar{z})$. The external momenta can be written in this form. In celestial conformal field theory (CCFT), the massless four-momenta are parametrized as shown. In $(-+++) $ convention, $z$ is the cross-ratio, and $\\bar{z}$ is its complex conjugate."
    },
    "62": {
        "figure1": "2505.02899v1_CIRCLE",
        "label1": "fig:3",
        "caption1": "Figure depicting the regime of validity for Mellin transformed amplitude(s) in different kinematical regime.",
        "text": "As discussed previously there are three distinct kinematic regions: $\\mathbf{12 \\to 34}$, $\\mathbf{13 \\to 24}$, and $\\mathbf{14 \\to 23}\\,.$ These regions differ significantly from one another and are each valid only within specific ranges of the cross-ratios. To go from one kinematical region to another, one typically employs a systematic procedure of analytic continuation. This process is illustrated schematically in Fig.~\\eqref{fig:3}. Now, the celestial amplitude (for external scalar operators) in three different kinematical regions is given by, &{\\mathbfcal{A}}^{12\\to 34}(\\mathbf \\Delta,z)=2^{3-\\gamma}z^2\\rmint_0^\\infty d\\omega\\,\\omega^{\\gamma-1}\\,\\mathbb{M}_{\\textrm{Born}}^{\\textrm{EFT}}\\left(s=\\omega^2,t=-\\frac{z-1}{z}\\omega^2\\right),\\,\\,\\,\\,\\,(z>1)\\\\ &\\hspace{2.3 cm}    =\\frac{i \\pi  4^{2-3 \\gamma } z^2 }{3 \\left(-1+e^{2 i \\pi  \\gamma }\\right)}\\Bigg(\\kappa^{2 \\gamma -2} \\left(\\beta ^{1-2 \\gamma }-2^{2 \\gamma +1} \\alpha ^{1-2 \\gamma }\\right)\\Bigg)-\\underbrace{\\frac{2^{3-\\gamma}\\, \\pi\\,  z^3 \\delta (i (\\gamma +2))}{(z-1)\\kappa}}_{\\text{Einstein gravity}}\\,,\\\\ & {\\mathbfcal{A}}^{13\\to 24}(\\mathbf \\Delta,z)=2^{3-\\gamma}z^{2+\\frac{\\gamma}{2}}\\rmint_0^\\infty d\\omega\\,\\omega^{\\gamma-1}\\,\\mathbb{M}_{\\textrm{Born}}^{\\textrm{EFT}}\\left(s=-z\\omega^2,t=-(1-z)\\omega^2\\right),\\,\\,\\,\\,\\,(0<z<1)\\\\ & \\hspace{2.3 cm}\\to {\\mathbfcal{A}}^{12\\to 34}(\\mathbf \\Delta,z)\\,, {\\mathbfcal{A}}^{14\\to 23}(\\mathbf \\Delta,z)=2^{3-\\gamma}(-z)^{2+\\frac{\\gamma}{2}}(1-z)^{-\\frac{\\gamma}{2}}\\rmint_0^\\infty d\\omega\\,\\omega^{\\gamma-1}\\,\\mathbb{M}_{\\textrm{Born}}^{\\textrm{EFT}}\\left(s=\\frac{z}{1-z}\\omega^2,t=\\omega^2\\right),\\,\\,\\,\\,\\,(z<0) Collecting the result of the integrals, we can write the full amplitude in the conformal basis as,",
        "summarize_figure": "2505.02899v1_CIRCLE",
        "summarization": "The figure illustrates the regime of validity for Mellin transformed amplitudes in three distinct kinematical regions: from 12 to 34, from 13 to 24, and from 14 to 23. Each region is valid within specific ranges of cross-ratios. To transition from one region to another, a systematic procedure of analytic continuation is typically employed. The figure shows that each region corresponds to a different physical model, with each kinematical amplitude related to external scalar operators. The amplitudes in different regions have unique mathematical representations and relationships."
    },
    "63": {
        "figure1": "2505.02899v1_CONT",
        "label1": "fig:o",
        "caption1": "Figure depicting the chosen contour for dispersion relation in the complex-$\\omega$ plane. The cross signs depict the location of the poles.",
        "text": "The integral can also be written after a change of variable change in the following way, This has poles at,     G\\omega'=e^{-i\\pi/2}\\left(n+\\frac{1}{2}\\right),\\,\\,\\, \\mbox{and}\\,\\, \\omega'=\\frac{\\kappa }{\\alpha},-\\frac{\\kappa }{\\alpha},\\frac{\\kappa }{2\\beta},-\\frac{\\kappa }{2\\beta}\\,,\\,\\,\\,\\,\\, n \\in \\mathbb{Z}_{\\geq0}\\,. Note that we also have extra poles at $\\omega'$ entirely due to the higher curvature contributions. We also have a branch cut, which is running from $0\\rightarrow i\\infty$. However, as we have poles on positive and negative real axis, we need to change the argument about the branch cut. The integral in $\\omega'$  can now be analytically continued to the complex plane, and we use a contour in the clockwise orientation in the complex plane as depicted in Fig.~\\ref {fig:o}. The integral can now be written as a sum of four separate pieces as,     I_{C}      &\\sim z^2\\oint_C d\\omega\\,\\omega^{\\gamma/2}\\Gamma\\left(\\frac{1}{2}-iG\\omega\\right)\\Bigg(\\frac{1}{\\mu \\epsilon}\\Bigg)^{2iG\\omega}\\frac{\\widetilde{\\mathbb{M}}_{\\textrm{Born}}^{\\textrm{EFT}}(\\omega)}{\\omega}\\,,\\\\     &= I_1+I_2 +I_{\\epsilon}+I_R\nwhere the decomposition of the contour $\\Gamma=\\Gamma_1\\cup \\Gamma_R\\cup \\Gamma_2\\cup \\Gamma_\\epsilon$ corresponds to each of the four integrals in the second line, with $\\Gamma_R$ the semi-circle part of the contour of radius $R$ and $\\Gamma_\\epsilon$ the smaller semi-circle part of radius $\\epsilon$ as shown in Fig.~\\eqref{fig:o}. It can be shown that in the limits where $\\epsilon\\to0$ and $R\\to\\infty$, the contributions from $\\Gamma_\\epsilon$ and $\\Gamma_R$ vanishes. Therefore, we have,     &I_1+I_2=2\\pi i \\sum_{n\\geq1} \\textbf{Res}\\Bigg[z^2\\,\\omega^{\\gamma/2}\\Gamma\\left(\\frac{1}{2}-iG\\omega\\right)\\Bigg(\\frac{1}{\\mu \\epsilon}\\Bigg)^{2iG\\omega}\\frac{\\widetilde{\\mathbb{M}}_{\\textrm{Born}}^{\\textrm{EFT}}(\\omega)}{\\omega}\\Bigg]\\,\\nonumber\\\\&\\hspace{6 cm}\\text{at} \\,\\,\\omega=G^{-1}e^{-i\\pi/2}(n+1/2) where the integrals ($I_1,\\,I_2$) are given by,       I_1=-\\mathbfcal{A}^{\\textrm{EFT}}_{eik}(\\gamma,z),\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,       I_2=e^{-i\\pi \\gamma/2}\\,\\overline{\\mathbfcal{A}^{\\textrm{EFT}}_{eik}}(\\bar\\gamma,-z) \\,. $\\overline{\\mathbfcal{A}^{\\textrm{EFT}}_{eik}}(\\bar \\gamma) $ denotes the complex conjugate of the eikonal amplitude $\\mathbfcal{A}^{\\textrm{EFT}}_{eik}(\\gamma)$ . Therefore we can write the following \\textit{dispersion relation} including the GR contribution as,",
        "summarize_figure": "2505.02899v1_CONT",
        "summarization": "The figure shows the chosen contour for the dispersion relation in the complex - ω plane. Cross signs mark pole locations at ω' = e^(-iπ/2)(n + 1/2), ω' = κ/α, -κ/α, κ/(2β), -κ/(2β) (n ∈ ℤ≥0), with extra poles from higher curvature. There's a branch cut from 0 to i∞. The integral is analytically continued using a clockwise contour Γ = Γ1 ∪ ΓR ∪ Γ2 ∪ Γε. In the limits ε→0 and R→∞, Γε and ΓR contributions vanish. The integral I_C = I1 + I2 + I_ε + I_R, and I1 + I2 equals 2πi times the sum of residues at ω = G^(-1)e^(-iπ/2)(n + 1/2), leading to the dispersion relation with GR contribution."
    },
    "64": {
        "figure1": "2505.02901v1_hilbertspaces_nosym",
        "label1": "fig:hilbertspaces_nosym",
        "caption1": "Enumeration of computational basis states of different Hilbert space blocks. (a) \\lstinline{Spinhalf} block on $N=4$ sites with $n_{\\uparrow}=2$ $\\uparrow$-spins. (b) \\lstinline{tJ} block on $N=4$ sites with $n_{\\uparrow}=2$ $\\uparrow$-spins and $n_{\\downarrow}=1$ $\\downarrow$-spins. (c) \\lstinline{Electron} block on $N=4$ sites with $n_{\\uparrow}=2$ $\\uparrow$-spins and $n_{\\downarrow}=1$ $\\downarrow$-spins. The index of a given basis configuration (represented as an object of type \\lstinline{ProductState}) can be obtained using the \\lstinline{index} function.",
        "text": "ndn = 1 b2 = tJ(N, nup, ndn) b3 = Electron(N, nup, ndn) The result of printing out the configurations of specific blocks is shown in Fig.~\\ref{fig:hilbertspaces_nosym}. This enumeration is important to interpret the coefficients of wave functions. By printing out the basis states of spin configurations, the user can also assess how the computational basis states are ordered internally in XDiag.",
        "summarize_figure": "2505.02901v1_hilbertspaces_nosym",
        "summarization": "The figure shows the enumeration of computational basis states for different Hilbert space blocks. Panel (a) displays a Spinhalf block on 4 sites with 2 up-spin (↑) particles. Panel (b) presents a tJ block on 4 sites with 2 up-spin (↑) particles and 1 down-spin (↓) particle. Panel (c) depicts an Electron block on 4 sites with 2 up-spin (↑) particles and 1 down-spin (↓) particle. The index of each basis configuration can be obtained using the index function. This enumeration of spin configuration basis states is crucial for interpreting wave function coefficients. By printing the basis states, users can also evaluate how these states are internally ordered in XDiag."
    },
    "65": {
        "figure1": "2505.02901v1_fig_benchmark_lanczos2",
        "label1": "fig:bench_1dheisen",
        "caption1": "Scaling of the computation time for a single Lanczos iteration as a function of the total number of threads used. The calculations were done for the Heisenberg $a)$, Hubbard $b)$ and t-J $c)$ models on a linear chain using different total numbers of spins/sites. The solid black line in each panel represents the slope corresponding to ideal linear scaling. In the calculation, we used both the U(1) and lattice symmetries.",
        "text": "In Fig.~\\ref{fig:bench_1dheisen}, we present the scaling of the computational time of a single Lanczos step with the total number of threads used in the calculation. As shown in the three panels, the computation time practically follows a linear scaling with the number of OpenMP threads, decreasing with the inverse of the total number of threads. In table~\\ref{tab:threads_computational_time}, we present the computational time for the calculation using a given type of Hilbert space block and the symmetries used with $n_{\\textrm{threads}}=64$ threads. We conclude that the multithreading parallelization of XDiag exhibits close to linear scaling up to $n_{\\textrm{threads}}=64$ threads.",
        "summarize_figure": "2505.02901v1_fig_benchmark_lanczos2",
        "summarization": "The figure shows the scaling of the computation time for a single Lanczos iteration as a function of the total number of threads used in the calculation, for the Heisenberg (panel a), Hubbard (panel b), and t-J (panel c) models on a linear chain with different numbers of spins/sites. The solid black line in each panel represents the slope corresponding to ideal linear scaling. As shown, the computation time scales nearly linearly with the number of threads, decreasing with the inverse of the total number of threads. The calculations also employed U(1) and lattice symmetries. In conclusion, the multithreaded parallelization of XDiag exhibits nearly linear scaling up to 64 threads."
    },
    "66": {
        "figure1": "2505.02901v1_fig_benchmark_lanczos_mpi",
        "label1": "fig:bench_mpi",
        "caption1": "Scaling of the computation time for a single Lanczos iteration as a function of the total number of MPI processes used. The calculations were done for the Heisenberg $a)$, Hubbard $b)$, and t-J $c)$ models using different total numbers of spins/sites. The solid black line in each panel represents the slope corresponding to ideal linear scaling. In $a)$, the calculation was performed in the zero magnetization sector, in $b)$ it was done at half-filling for $N =20$, at $3/11$ for $N=22$ and $1/3$ for $N=24$.",
        "text": "XDiag also supports distributed-memory parallelization via MPI for the \\lstinline{SpinhalfDistributed}, \\lstinline{ElectronDistributed}, and \\lstinline{tJDistributed} blocks. We benchmark the computation time of a single Lanczos Iteration as a function of the number of MPI processes used. The calculations were performed on the Max-Planck-Gesellschaft Raven supercomputer using nodes also equipped with Intel Xeon(R) Platinum $8360\\text{Y}$ (Ice Lake) 2.4 GHz processors, each featuring 72 cores. These nodes are interconnected with a 100 $\\text{Gb}/\\text{s}$ HDR100 network. In these benchmarks, lattice symmetries are not used, only U(1) number conservation symmetries. We consider the case of $N=36$ and $N=40$ for the Heisenberg model at zero magnetization. For the Hubbard model, we consider $N=20$ at half-filling, and $N=22$ and $N=24$ with $n_{\\uparrow}=n_{\\downarrow}=8$ electrons. The $t$-$J$ model is studied on $N=28$ and $N=32$ sites with $2$ holes at zero magnetization. We observe that for all systems considered, the computation time scales almost inversely proportional to the total number of MPI processes, as shown in Fig,~\\ref{fig:bench_mpi}. Moreover, we provide, in the table~\\ref{tab:threads_computational_time_mpi}, the actual computational time required for each Lanczos iteration across the different models considered. We conclude that the MPI parallelization exhibits almost linear strong scaling up to several thousand processes.",
        "summarize_figure": "2505.02901v1_fig_benchmark_lanczos_mpi",
        "summarization": "The figure shows the scaling of computation time for a single Lanczos iteration as a function of MPI processes used, covering Heisenberg (a), Hubbard (b), and t-J (c) models. Each subplot’s solid black line represents the ideal linear scaling. The data suggest that computation time decreases almost inversely with the number of MPI processes, demonstrating the effectiveness of MPI parallelization. For the Heisenberg model in the zero magnetization sector, the computation time scales linearly with the increase in MPI processes. In the Hubbard model, for different electron fillings (N=20, 22, and 24), similar trends are observed. The t-J model shows a significant reduction in computation time as MPI processes increase, also following a near-linear scaling trend. These results suggest that MPI parallelization maintains near-linear strong scaling up to several thousand processes."
    },
    "67": {
        "figure1": "2505.02915v1_gcs_simulation",
        "label1": "fig:gcs_sim",
        "caption1": "Visualization of GCS rectified tactile readings in simulation, when the peg is not contact with the environment. Left shows the tactile reading of the left finger sensor in real world. Right 4 shows the GCS readings with randomization sampled from \\Tableref{tab:domain_randomization}.",
        "text": "To this end, we first identify key aspects of the sim-to-real gap: non-uniform contact, contact Poisson Effect, and force scaling difference. We then develop simple yet efficient techniques to mitigate these gaps (\\Figref{fig:gcs_sim}): surface with \\textbf{G}aussian bumps, \\textbf{C}onvolution for Poisson Effect and domain randomization for Force \\textbf{S}caling. In our experiment, we show that our approach \\textbf{\\method~} enables \\textit{zero-shot} sim-to-real transfer of RL policy on \\textit{blind} peg-in-hole insertion tasks, i.e., the robot does not have accurate peg poses. Results suggest that our \\method~ method outperforms all previous sim-to-real techniques and improves the success rate by \\textbf{50\\%}. For some tasks, our RL policy succeeds on 9 out of 10 real-world trials. To the best of our knowledge, we contribute the first method that bridges the sim-to-real gap on insertion tasks of dense, distributed, 3-axis tactile readings between the simulation and real world.",
        "summarize_figure": "2505.02915v1_gcs_simulation",
        "summarization": "The figure visualizes GCS rectified tactile readings when the peg isn't in contact with the environment. The left image shows real - world tactile readings from the left finger sensor, and the four right images display GCS readings with randomization from domain randomization. By using Gaussian bumps, convolution for Poisson Effect, and domain randomization for force scaling, the sim - to - real gap is mitigated. In blind peg - in - hole insertion tasks with no accurate peg pose, the method enables zero - shot sim - to - real transfer of RL policy, outperforming previous techniques, boosting the success rate by 50%, and achieving 9/10 success in some real - world trials."
    },
    "68": {
        "figure1": "2505.02922v1_distribution",
        "label1": "fig:aivi_dist",
        "caption1": " {\\textbf{Dynamic Sparsity in Attention.}} The distribution of top-100 attention weights in a 128K context varies across decoding steps.",
        "text": "Identifying important tokens is challenging for three reasons. First, as shown in Figure~\\ref{fig:aivi_dist}, important tokens (i.e., those with high attention weights) are scattered across the context, making their positions unpredictable.  Second, tokens that are important at one decoding step (i.e., one query vector) may not remain so in subsequent steps. As shown in Figure~\\ref{fig:aivi_dist}, the overlap of the top-100 KV vectors across three decoding steps is only 31\\%, highlighting token importance dynamics. Third, the sparsity exhibits high variability from two sources: the architecture of the model itself and the nature of the decoding query. As shown in Figure~\\ref{fig:sparsity}, attention distributions differ significantly across model layers and attention heads (Figure~\\ref{fig:sparsity} (a)), potentially reflecting the varying semantic roles of heads and layers~\\cite{retrieval_head,li2024snapkv}.  Different queries within the same context or across different tasks exhibit substantially different sparsity ratios on the same attention head (Figure~\\ref{fig:sparsity} (b)).",
        "summarize_figure": "2505.02922v1_distribution",
        "summarization": "This figure shows the distribution of the top-100 attention weights in a 128K context, with changes across decoding steps. The blue, green, and orange lines represent Step 1, Step 20, and Step 40, respectively. It is observed that the distribution of attention weights changes significantly as the decoding steps progress. Specifically, at Step 1, the attention weights are concentrated on fewer token positions, while as the decoding steps increase, the distribution becomes more spread out and dispersed. The inset zooming into the range between 94K and 95K further highlights the changes in attention weights across different decoding steps. Overall, the figure emphasizes the dynamic nature of the attention weight distribution, indicating that the model's focus on different tokens evolves as decoding progresses."
    },
    "69": {
        "figure1": "2505.02927v1_solverjclabel",
        "label1": "fig:solver_diagram",
        "caption1": "Computational setup for determining the relative spin orientation $\\sigma_{i_{0}}\\sigma_{j_{0}}$ across the central bond (shown in red). We set the lattice constant to unity. The correct value of $\\sigma_{i_{0}}\\sigma_{j_{0}}$ is that within the GS of the entire system (comprised of black, light blue, and red bonds). The LSBS computes $\\sigma_{i_{0}}\\sigma_{j_{0}}$ within the GS of the local subsystem (light blue and red bonds).} %(b)-(d) %Hardness profile of an %$L=128$ square lattice spin-glass system examined with an open $L_{\\rm sub}=40$ %subsystem whose central bond is solved with the LSBS (near the boundaries, the % subsystem becomes smaller). Red bonds- ``hard'' bonds satisfying % $|\\JCsub-J_{ij}|<\\vartheta$; blue (yellow)- ``easy'' bonds satisfying %  $|\\JCsub-J_{ij}|\\geq \\vartheta$ correctly (incorrectly) solved by the LSBS (see %   text). Here, $\\JCsub$ denotes the critical threshold of bond $(ij)$ and %   $\\vartheta=0.1,0.4,0.8$ for (b)-(d). Note the relative sparsity of LSBS %    errors. When errors arise, they are typically associated with small %     $|\\JCsub-J_{ij}|$.",
        "text": "Previous related studies attempted to combine multiple local solvers %from multiple subsystems to address global optimization problems \\cite{TSM}, fuse local GSs for the construction of global spin-glass instances with planted solutions for benchmarking learning \\cite{RL_local,RL_local1}. Here, we take on a different perspective and directly investigate to which extent one can predict (parts of) the global GSs based on optimization of local subsystems. To reduce this problem to its bare essentials and minimize boundary effects, we study pairs of neighboring spins (bonds) of the model and introduce a ``local single bond solver'' (LSBS) that examines these neighboring spins within a local subsystem. We first focus on the GS products $\\SISJNOT$ for bonds $\\langle i_0 j_0 \\rangle$ which, in the simplest case, are located at the center of the system, cf.\\ Fig.~\\ref{fig:solver_diagram}.\nTo systematically study the relation of local and global solutions in spin systems, we rely on exact (combinatorial) GS algorithms, using a minimum-weight perfect matching (MWPM) method \\cite{fractal2d} based on Blossom V Gurobi software \\cite{gurobi_optimization_llc_gurobi_2022} for 3D systems and the SK model. Figure~\\ref{fig:solver_diagram} schematically depicts the setup: we computed GSs for the entire ($N$-spin) systems as well as for subsystems with $N_\\mathrm{sub}$ lattice sites centered on the considered bond $\\langle i,j\\rangle$ (corresponding to the LSBS), comparing the predicted values of the product $\\SISJNOT$ (open boundary conditions are applied in both cases). If the results agree, the bond product was predicted correctly (with zero error). For $L \\times L$ square lattices of sizes $L=256$, $512$, $1024$, we examined $L_{\\rm sub} \\times L_{\\rm sub}$ subsystems with $4 \\le L_{\\rm sub} \\le 512$, using $\\sim 60,000$ disorder realizations. For cubic lattices of side lengths $L=10$ and $12$, the $L_{\\rm sub} \\times L_{\\rm sub} \\times L_{\\rm sub}$ subsystems were of size $4 \\le L_{\\rm sub} \\le 10$, and we used $\\sim 2,000$ realizations. From these calculations we estimate the LSBS error rate $\\mathcal{E}$. Figure size for several cases.  For the  $\\overline{J} = 0$ lattice systems we find that, although the 2D and 3D spin-glass GS problems belong to different complexity classes (P and NP, respectively), in both cases the error rate is well described by a power-law decay in the subsystem size \\footnote{While we will   largely focus on the central nearest neighbor bond (with $i=i_{0}$ and $j=j_{0}$),   in the End Matter we discuss more general situations in which the sites   $i$ and $j$ are a distance ${\\sf r}_{ij} >1$ apart. To that end and in order to   avoid cumbersome notation, we will often dispense with ``0'' subscripts.}   righthand side of Eq. (\\ref{eq:solver}) constitutes an upper bound on the    error $\\mathcal{E}_{ij}$. Clearly, when $L_{\\rm sub} = L$ (when the subsystem is   the entire system), the error $\\mathcal{E}_{ij}=0$ by definition.}, viz.\\ Here, $\\left[\\sigma_i\\sigma_j\\sigma^{\\rm sub}_i\\sigma^{\\rm sub}_j\\right]$ denotes the pair-overlap correlation function, where $\\left[ ...\\right]$ represents the disorder average, and $\\ell_\\mathcal{E}$ is an effective length scale.  Importantly, this algebraic decay is {\\it independent of the linear system size} $L$, cf.\\ the data collapse in Figs.~\\ref{fig:solver_performance}(a) and (b). Comparing the respective exponents $\\kappa$ that are collected in Table \\ref{tab:fit_params}, we see that errors in the 2D systems drop more rapidly than in the (harder) 3D systems. When expressed in terms of the total volume $N_{\\rm sub} = L_{\\rm sub}^d$, where $d$ is the lattice dimension, this sharp change in the algebraic decays of errors between the 2D and 3D cases becomes yet more acute.",
        "summarize_figure": "2505.02927v1_solverjclabel",
        "summarization": "The image shows a computational setup for determining relative spin orientation across a central red - highlighted bond. It uses a Local Single Bond Solver (LSBS) in a local subsystem to compute spin configurations compared to the global ground state. The setup aims to predict global ground state parts via local subsystem optimization. Red bonds are “hard” bonds with values based on a critical threshold, helping understand bond behavior under various conditions and offering insights into spin - glass system's local and global properties."
    },
    "70": {
        "figure1": "2505.02938v1_spider_gmm_lausanne",
        "label1": "fig:spider_gmm_lausanne",
        "caption1": "Spider diagram for the GMM with 5 clusters (Lausanne).",
        "text": "To further explore the nature of the identified clusters, we employ histogram representations of the feature distributions. Figures~\\ref{fig:spider_gmm_lausanne} and \\ref{fig:spider_gmm_phily} present stacked histograms depicting the relative importance of the various urban form-related features within each cluster. The normalized feature values signify each variable's contribution to assigning a BSU to a specific cluster. These visual representations facilitate a comparative analysis of cluster characteristics, highlighting which features exert the strongest influence on the clustering outcomes and provides a more comprehensive understanding of the underlying patterns distinguishing one cluster from another. A detailed interpretation of the clustering results for both cities is given next.",
        "summarize_figure": "2505.02938v1_spider_gmm_lausanne",
        "summarization": "The chart illustrates the GMM clustering results for Lausanne city data, using five clusters, represented in a spider chart to show the distribution of various features across clusters. The different shapes and widths of the areas corresponding to each cluster indicate the relative importance of each feature. For instance, features like \"building_residential\" and \"highway_pedestrian\" have higher values in some clusters, reflecting their significance in those clusters. Additionally, features like \"degree_centrality\" and \"public_transport\" show noticeable variation across clusters, suggesting differences in public transportation and urban connectivity across the regions. This comparison provides valuable insights into the urban form characteristics behind each cluster, aiding future urban planning and development."
    },
    "71": {
        "figure1": "2505.02938v1_spider_gmm_philadelphia",
        "label1": "fig:spider_gmm_phily",
        "caption1": "Spider diagram for the GMM with 8 clusters (Philadelphia).",
        "text": "To further explore the nature of the identified clusters, we employ histogram representations of the feature distributions. Figures~\\ref{fig:spider_gmm_lausanne} and \\ref{fig:spider_gmm_phily} present stacked histograms depicting the relative importance of the various urban form-related features within each cluster. The normalized feature values signify each variable's contribution to assigning a BSU to a specific cluster. These visual representations facilitate a comparative analysis of cluster characteristics, highlighting which features exert the strongest influence on the clustering outcomes and provides a more comprehensive understanding of the underlying patterns distinguishing one cluster from another. A detailed interpretation of the clustering results for both cities is given next.",
        "summarize_figure": "2505.02938v1_spider_gmm_philadelphia",
        "summarization": "The spider charts show the distribution of features across different clusters for both Philadelphia (with eight clusters) and Lausanne (with five clusters). The charts reveal distinct patterns in the urban features like roads, buildings, and public amenities across these clusters. For example, Cluster 1 in Philadelphia has high values for natural features, while Cluster 7 stands out for its strong focus on transportation infrastructure such as public transport and road services. These visualizations provide a deeper insight into the characteristics of each cluster and how these features vary between the two cities."
    },
    "72": {
        "figure1": "2505.02938v1_lausanne_phily",
        "label1": "fig:lausanne_phily",
        "caption1": "Combined clustering result for Lausanne and Philadelphia with a uniform grid size.",
        "text": "Figure~\\ref{fig:lausanne_phily} demonstrates that some clusters are shared between Lausanne and Philadelphia when a uniform grid resolution is applied, suggesting the presence of comparable urban typologies across both cities. However, certain clusters—particularly Clusters 1 and 2, which are prominent in central Lausanne—are absent from the Philadelphia portion of the map. This discrepancy indicates that a single grid size may be insufficient to capture the varying spatial scales and urban structures of cities with distinct physical layouts and demographic profiles.",
        "summarize_figure": "2505.02938v1_lausanne_phily",
        "summarization": "The figure shows clustering results for Lausanne and Philadelphia at a uniform grid size, revealing some shared clusters between the two cities, indicating comparable urban typologies. However, certain clusters, especially Clusters 1 and 2, which are prominent in central Lausanne, are absent in Philadelphia. This discrepancy suggests that a single grid size may not fully capture the varying spatial scales and urban structures of cities with distinct physical layouts and demographic profiles. This emphasizes the need for tailored analytical methods or grid sizes based on each city's unique characteristics."
    },
    "73": {
        "figure1": "2505.02951v1_draw",
        "label1": "fig:the_same_stream_and_separate_stream",
        "caption1": "Illustration of the same stream and the separate stream transmission. Different colors illustrate different data streams. The figure illustrates three streams being received by users as an example, though the number of streams could vary.} \\vspace{-3mm",
        "text": "In this section, we provide system models for downlink data transmission. We will describe two different ways to transmit signals in the downlink. The essence of cell-free massive MIMO with single-antenna users is that the cooperating APs send the same data stream using coherent joint beamforming. When the users have multiple antennas, multiple spatially multiplexed data streams can be sent to each user. Joint transmission then implies that a user is served by multiple APs, but each individual stream can either be transmitted by one or multiple APs. To study these different scenarios, we considered two categories: \\textit{same stream transmission} and \\textit{separate stream transmission}. The respective system models and notations are detailed in the following subsections. \\textcolor{blue}{The concepts of same stream transmission and separate stream transmission are illustrated with an example coloring in Fig. \\ref{fig:the_same_stream_and_separate_stream}. Transmitted streams from APs are assigned distinct color codes, enabling a visual differentiation between the same stream and separate transmissions.}",
        "summarize_figure": "2505.02951v1_draw",
        "summarization": "This figure illustrates the difference between same stream transmission and separate stream transmission. On the left side, the same stream transmission scenario is shown, where multiple access points (APs) send the same data stream to the user. On the right, the separate stream transmission scenario is displayed, where each AP transmits independent data streams to different users. Distinct colors are used to represent different data streams, making it easy to visually distinguish between the two transmission methods. In the same stream transmission, all APs collaboratively transmit the same data stream using coherent beamforming, whereas in the separate stream transmission, multiple data streams are transmitted simultaneously to the users through different APs, allowing for higher spatial multiplexing. The example shows three streams being received, but the number of streams can vary."
    },
    "74": {
        "figure1": "2505.02951v1_FigureB",
        "label1": "fig:figureB",
        "caption1": "Comparison of the average per-user SEs achieved with Methods 1-3 as a function of the number of AP antennas and with $M=2$ user antennas.} \\vspace{-3mm",
        "text": "We first investigate the impact of the choice of transmission methods on the SEs. Fig.~\\ref{fig:figureB} shows the average SE of a randomly located user as a function of the number of antennas per AP \\textcolor{blue}{with the MMSE precoders}. From this figure, we can see that Method 1 performs better than the other methods. This outcome is anticipated as Method 1 builds on sharing CSI among all APs so that the centralized MMSE precoder can be utilized to suppress interference in a coordinated manner. Method 3 provides slightly lower SEs than Method 1, as the lack of data sharing forces the precoding matrix to be block diagonal. However, Method 3 can still suppress inter-AP interference since CSI is shared to enable that. One clear benefit of Method 3 is that there is no need for phase synchronization between the APs, while for Method 1, synchronization is crucial for the APs to jointly beamforming signals that add constructively at the intended user and destructively at other users. Finally, Method 2 performs worse than Method 3 as it lacks CSI sharing between APs, but it can only suppress inter-AP interference based on its statistics.",
        "summarize_figure": "2505.02951v1_FigureB",
        "summarization": "The figure shows the average per-user spectral efficiency (SE) as a function of the number of antennas per access point (AP) for three transmission methods. Method 1 outperforms the others, with SE continuously improving as the number of antennas increases. This is due to the sharing of channel state information (CSI) among all APs, allowing for coordinated interference suppression through a centralized MMSE precoder. Method 3 performs slightly worse than Method 1 because the lack of data sharing limits the precoding matrix to a block diagonal form. However, it can still suppress inter-AP interference with CSI sharing. One advantage of Method 3 is that it does not require phase synchronization between APs, unlike Method 1, which relies on synchronization for coordinated beamforming. Method 2 performs the worst as it lacks CSI sharing and can only suppress interference based on statistical data."
    },
    "75": {
        "figure1": "2505.02951v1_Figureaa",
        "label1": "fig:figureaa",
        "caption1": "Comparison of the average per-user SEs achieved with Methods 1-3 as a function of the angular standard deviation and with $N=4$ and $M=2$.} \\vspace{-2mm",
        "text": "Next, we investigate the impact spatial correlation has on the SEs. Spatial correlation is modeled similarly to that in \\cite[Sec. 5.2]{demir2021foundations} assuming that both AP and users are equipped with ULAs. Moreover, the spatial correlation matrix elements are computed between each AP and user antennas as explained there. The ASD variation changes the correlation from the AP's viewpoint, thereby modeling how widely the scattering objects are distributed around the user. \\textcolor{blue}{Fig.~\\ref{fig:figureaa}} shows the average SE as a function of the ASD, where a small value means high spatial correlation and vice versa. The figure shows that when the ASD is low, the same stream transmission and separate stream transmission with CSI sharing work particularly well. The performance then reduces as the ASD increases. However, the separate stream transmission without CSI sharing (Method 2) performs better when the ASD is higher.  Hence, spatial correlation can either enhance or degrade the system's performance depending on the method used.",
        "summarize_figure": "2505.02951v1_Figureaa",
        "summarization": "The figure illustrates how the average per-user spectral efficiency (SE) varies with angular standard deviation (ASD) for Methods 1, 2, and 3. As ASD increases, all methods show a decline in performance, but with noticeable differences. When ASD is low, indicating high spatial correlation, Methods 1 and 3 (same-stream and separate-stream transmission with CSI sharing) perform better with higher average SE. However, as ASD increases, the performance of these methods deteriorates. On the other hand, Method 2 (separate-stream transmission without CSI sharing) shows improved performance at higher ASD. This indicates that spatial correlation has varying effects on system performance depending on the transmission method used."
    },
    "76": {
        "figure1": "2505.02955v1_intro",
        "label1": "fig:intro",
        "caption1": "Synchronization behavior for the deterministic Kuramoto system \\eqref{eq:intro_kuramoto} with $\\omega=2$ and $\\tau = 0.5$. \\textbf{Bottom row:} time series with $\\kappa=0.09$ (left) and a histogram of bin counts as a probability distribution (right). The system is not synchronized and trajectories (starting from $t=10$) fill the entire torus. \\textbf{Top row:} time series with $\\kappa=0.26$ (left) and a histogram of bins counts (right). The system is asymptotically synchronized and trajectories (starting from $t=10$) do not fill the entire torus. \\textbf{Right column:} Arnold tongue diagram depicting the parameter regime for which the system synchronizes (purple) and for which it does not synchronize (black).",
        "text": "To illustrate these ideas in the deterministic setting, consider a system of coupled oscillators which has already been reduced to a phase description: Kuramoto oscillators of the form         x' &= \\omega + \\tau + \\kappa \\sin(y-x)         y' &= \\omega + \\kappa \\sin(x-y) where $x$ and $y$ are phase-like variables evolving on the torus: $x,y\\in [0,2\\pi)$. The oscillators become synchronized when their phase difference is asymptotically constant. Therefore, we consider the evolution of the phase difference, $\\psi = y-x$, where $\\tau$ is the frequency difference of the oscillators. It is straightforward to show that when $\\kappa > {|\\tau|}/{2}$, the oscillators synchronize with a constant phase difference. Note that the synchronization boundary, $\\kappa={|\\tau|}/{2}$, is specified in terms of two parameters: coupling strength $\\kappa$ and frequency difference (detuning) $\\tau$. Figure \\ref{fig:intro} displays the synchronization region or Arnold tongue for equation \\eqref{eq:intro_kuramoto}, along with sample trajectories in phase space.",
        "summarize_figure": "2505.02955v1_intro",
        "summarization": "This figure displays the synchronization behavior of the deterministic Kuramoto system. It includes two time series graphs and an Arnold tongue diagram. The first row shows a time series with a coupling strength kappa of 0.26, where the system asymptotically synchronizes, and the trajectories from t=10 onward do not fill the entire torus. The second row shows a time series with a kappa of 0.09, where the system does not synchronize, and the trajectories fill the entire torus. Each row also includes a histogram on the right showing the probability distribution under each condition. The Arnold tongue diagram on the right depicts the synchronization and non-synchronization regions. The purple region indicates synchronization, and the black region indicates non-synchronization. The synchronization boundary is determined by the coupling strength kappa and the frequency difference tau. When kappa is greater than half the absolute value of tau, the system synchronizes."
    },
    "77": {
        "figure1": "2505.02955v1_discrete_eval_bif_nonid",
        "label1": "fig:id_eval_bif",
        "caption1": "Eigenvalue bifurcations in the case of identical oscillators ($\\tau=0$, top block) and in the case of non-identical oscillators ($\\tau=0.01$, bottom block). Black lines denote ground truth, while colored lines denote the leading order analytic approximation. The top row of each block shows the imaginary parts of the eigenvalues, while the bottom row of each block shows the real parts. Vertical dotted lines denote the leading order approximation to the KT point. \\textbf{Left:} 4D linear system with $\\omega=2$ and $D=0.1$. \\textbf{Middle:} 2D ring model with $\\omega=2$ and $D=0.1$. \\textbf{Right:} 9D discrete-state model with $\\omega=2$.",
        "text": "We now analyze the leading order behavior of the repeated, unperturbed eigenvalue, $\\lambda_1=-\\eta+i\\omega$, as a function of both $\\kappa$ and $\\tau$. We directly compute the matrix $\\mathcal{M}$ in \\eqref{eq:M matrix} (see Appendix \\ref{appendix:M compuation} for a more detailed derivation), which yields         -\\kappa+i\\tau & \\kappa  with eigenvalues  Consequently, our analysis predicts that the repeated eigenvalue, $\\lambda_1$, generically splits into distinct eigenvalues     &+ o(\\kappa) + o(\\tau) which recovers the eigenvalues of the exact solution \\eqref{eq:4D linear eigenvalues}; see Figure \\ref{fig:id_eval_bif}. A similar analysis may be employed to study the complex conjugate eigenvalue $\\overline{\\lambda_1}$.\nOur theory provides a leading order approximation in $\\kappa$ and $\\tau$ of the eigenvalues. One may show by direct computation that the matrix $\\mathcal{M}$ for the repeated, unperturbed eigenvalue, $\\lambda_1=-D+i\\omega$, is given by         i\\tau & \\kappa/2  with eigenvalues  Consequently, our analysis predicts that the unperturbed, repeated $Q$-function eigenvalue, $\\lambda_1$, of \\eqref{eq:2D Kuramoto model system} generically splits into distinct eigenvalues      &+ o(\\kappa) + o(\\tau) Figure \\ref{fig:id_eval_bif} displays the results.\nWe now analyze the leading order behavior of the repeated, unperturbed eigenvalue, $\\lambda_1= \\left(-\\frac{3}{2} + \\frac{\\sqrt{3}}{2}i\\right)\\omega$, as a function of both $\\kappa$ and $\\tau$. Directly computing the matrix $\\mathcal{M}$ in \\eqref{eq:M matrix} yields     $} with eigenvalues      &\\quad\\pm \\frac{\\sqrt{2}}{4}\\sqrt{-4\\kappa^2\\left(1-\\sqrt{3}i\\right) + 3\\tau^2\\left(1-\\sqrt{3}i\\right)} Consequently, our analysis predicts that the repeated eigenvalue, $\\lambda_1$, generically splits into distinct eigenvalues which recovers the two eigenvalues of the exact solution \\eqref{eq:discrete_eval_exact}; see Figure \\ref{fig:id_eval_bif}. A similar analysis may be employed for the complex conjugate eigenvalue $\\overline{\\lambda_1}$.\nNote that in this case, we include the multiplicative prefactor $\\frac{\\sqrt{3}i+1}{4}$ in \\eqref{eq:discreteD} to emphasize that the discriminant in this case is neither purely real nor purely imaginary as in the previous two cases. This fact is reflected in the right column of Figure \\ref{fig:id_eval_bif}, which shows splitting of both the real and imaginary parts of the eigenvalues.",
        "summarize_figure": "2505.02955v1_discrete_eval_bif_nonid",
        "summarization": "This figure shows eigenvalue bifurcations as κ varies. For identical oscillators (τ = 0, top) and non - identical oscillators (τ = 0.01, bottom), the first row depicts the imaginary parts and the second row the real parts of eigenvalues. Black lines are ground truth, colored lines are leading - order analytic approximations. Vertical dotted lines mark the approximate KT point. As κ increases, eigenvalue behavior changes, with more pronounced bifurcation for non - identical oscillators."
    },
    "78": {
        "figure1": "2505.02964v1_vpa_target",
        "label1": "fig:normal_vpa",
        "caption1": "Memory consumption pattern of all applications listed in Section \\ref{sec:apps}. The recommendation given by the Vertical Pod Autoscaler is also shown. The data has a sampling time of 5 seconds. ",
        "text": "In this work, we analyze nine different applications and their respective workloads. These applications are representative of several scientific domains and exhibit distinct consumption patterns, both in terms of quantity and variation. For each listed application, we built a containerized version of them. The typical memory usage patterns of these applications are shown in Figure \\ref{fig:normal_vpa}, and a numerical summary of their execution times and memory footprint is presented in Table \\ref{tab:numerical-results-raw}.",
        "summarize_figure": "2505.02964v1_vpa_target",
        "summarization": "The chart shows the memory consumption patterns of nine different applications, with data sampled every 5 seconds. It also includes recommendations provided by the Vertical Pod Autoscaler. These applications, representing various scientific domains, exhibit distinct memory usage patterns, both in terms of quantity and variation. Containerized versions of these applications were created, and their typical memory usage patterns are displayed in the figure. The chart reveals that memory consumption fluctuates over time, with each application exhibiting unique characteristics. For example, some applications show a rapid increase in memory usage during startup, while others maintain relatively stable usage. The recommendations provided by the Vertical Pod Autoscaler are based on these consumption patterns, offering dynamic adjustments to optimize resource allocation and efficiency."
    },
    "79": {
        "figure1": "2505.02964v1_arcv_design",
        "label1": "fig:arcv-design",
        "caption1": "A depiction of the high-level design of the ARC-V autoscaler.",
        "text": "To establish a vertical autoscaling policy that gives suitable recommendations for HPC workloads on containerized environments, we design the Adaptive Resource Controller - Vertical (ARC-V). This policy, together with its implementation, is established as a state machine that requires specific signals (named ``memory alerts\") to move between its different states whereas a depiction may be seen in Figure \\ref{fig:arcv-design}. There are two major guiding remarks when designing ARC-V:",
        "summarize_figure": "2505.02964v1_arcv_design",
        "summarization": "The chart depicts the high-level design of the ARC-V autoscaler. The state machine design moves between three main states—Growing, Stable, and Dynamic—based on specific signals, namely \"memory alerts.\" The transitions between these states depend on memory usage thresholds. For instance, when memory usage reaches the first alert level, the state shifts from Stable to Growing. When memory usage remains within the alert range, the state stays in Dynamic. The arrows and labels in the chart clearly illustrate the relationships and how resource allocation is dynamically adjusted for containerized workloads."
    },
    "80": {
        "figure1": "2505.02964v1_arcv_vpa",
        "label1": "fig:arcv_vs_vpa",
        "caption1": "On left, the ratios between the values of memory footprint and execution time of VPA and the ARC-V policy. On right, a depiction of how the VPA simulator works: every time the recommendation is lower than the actual usage, the application needs to restart with 20\\% more memory. ",
        "text": "Figure~\\ref{fig:arcv_vs_vpa} displays all the results obtained with ARC-V compared to the simulated VPA policy that is established for Kubernetes and cloud applications. For MiniFE, that uses swap memory just at the end of its execution, we do not count the swap as provisioned memory in ARC-V since the resource being used in this case is disk.",
        "summarize_figure": "2505.02964v1_arcv_vpa",
        "summarization": "The chart presents the ratios of memory footprint and execution time between VPA and ARC-V policies. On the left, blue bars represent the execution time ratio, while green bars represent the memory footprint ratio. On the right, the diagram illustrates how the VPA simulator operates: whenever the recommendation is lower than the actual memory usage, the application needs to restart with 20% more memory. This mechanism helps optimize resource allocation and manage memory fluctuations. By comparing the performance of different applications under VPA and ARC-V, it is evident that some applications show significant differences in execution time while maintaining relatively stable memory usage."
    },
    "81": {
        "figure1": "2505.02972v1_hpc305",
        "label1": "fig:hpc305",
        "caption1": "Maximum error across varying heterogeneity parameter \\( h \\), evaluated under two outlier proportion settings: \\( \\epsilon = 0 \\) (left) and \\( \\epsilon = 0.1 \\) (right). Simulations are conducted with \\( n = 100 \\), \\( T = 50 \\), \\textbf{\\( p = 30 \\)}, and \\( r = 5 \\). Evaluation metrics and computational settings are described in Sections~\\ref{evalmetrics}.",
        "text": "in Sections~\\ref{evalmetrics}.} Figure~\\ref{fig:hpc305} shows the maximum error in the regular dataset \\( S \\) as we vary the heterogeneity level \\( h \\). As \\( h \\) increases and tasks become more diverse, GeoERM consistently achieves the lowest error, demonstrating robustness to task heterogeneity. In the left panel (\\(\\epsilon = 0\\)), pERM and AdaptRep exhibit stable performance across varying \\( h \\), whereas GeoERM shows a slight increase in error but maintains a clear advantage over all other methods at each \\( h \\). Spectral methods give moderate results, and pooled LR and single-task LR struggle with heterogeneous tasks. With outliers (\\(\\epsilon = 0.1\\), right panel), GeoERM maintains minimal error growth as \\( h \\) increases, outperforming all other methods. MoM and spectral methods resist outliers to some extent but are less robust than GeoERM, while pooled LR suffers severely. Additional experimental results are provided in Appendix \\ref{Supplementary Experiments}.",
        "summarize_figure": "2505.02972v1_hpc305",
        "summarization": "In the figure, the maximum error across varying heterogeneity parameter h is displayed for two different outlier proportion settings, ε = 0 (left) and ε = 0.1 (right). As the heterogeneity level h increases, GeoERM consistently performs the best with minimal error growth, demonstrating strong robustness to task heterogeneity. In the left panel, both pERM and AdaptRep show stable performance, while GeoERM retains a clear advantage over other methods. Spectral methods offer moderate performance, and pooled LR and single-task LR perform poorly under heterogeneous tasks. In the right panel, with outliers (ε = 0.1), GeoERM still exhibits minimal error growth, outperforming all other methods. MoM and spectral methods show some resistance to outliers but are less robust than GeoERM, while pooled LR suffers the most."
    },
    "82": {
        "figure1": "2505.02972v1_hpc505",
        "label1": "fig:hpc505",
        "caption1": "Maximum error across varying \\( h \\), under \\(\\epsilon = 0\\) (left) and \\(\\epsilon = 0.1\\) (right). Simulations: \\( n = 100 \\), \\( T = 50 \\), \\textbf{\\( p = 50 \\)}, \\( r = 5 \\). Evaluation metrics and computational settings are in Sections~\\ref{evalmetrics}.",
        "text": "Figure~\\ref{fig:hpc505} repeats the experiment with \\( p = 50 \\), increasing feature dimensionality. As expected, the task becomes more challenging, and performance differences between methods become more pronounced. GeoERM again achieves the lowest maximum error, demonstrating strong adaptability under higher dimensionality. Notably, pERM’s performance is closer to GeoERM’s with outliers, indicating stable behavior under these scenarios. Spectral methods follow closely but have slightly higher errors.",
        "summarize_figure": "2505.02972v1_hpc505",
        "summarization": "The figure shows the maximum error across varying heterogeneity parameter h, under both no outliers (ε = 0) on the left and with outliers (ε = 0.1) on the right. The simulation settings were n = 100, T = 50, p = 50, r = 5. In the left panel, without outliers, GeoERM consistently performs best with minimal error growth as h increases. pERM and AdaptRep show relatively stable performance but are still less effective than GeoERM. Other methods like ERM, spectral methods, and linear regression show increasing error with higher h, with single-task LR performing the worst. In the right panel, with outliers, GeoERM remains the most robust method, with minimal error increase compared to the other methods. MoM and spectral methods show some robustness to outliers, but they do not perform as well as GeoERM. Linear regression methods, especially pooled LR, experience a significant increase in error with outliers."
    },
    "83": {
        "figure1": "2505.02972v1_hpc805",
        "label1": "fig:hpc805",
        "caption1": "Maximum error across varying \\( h \\), under \\(\\epsilon = 0\\) (left) and \\(\\epsilon = 0.1\\) (right). Simulations: \\( n = 100 \\), \\( T = 50 \\), \\textbf{\\( p = 80 \\)}, \\( r = 5 \\).",
        "text": "Figure~\\ref{fig:hpc805} increases dimensionality further to \\(p = 80\\). GeoERM continues to perform best under \\(\\epsilon = 0\\). However, for outlier tasks (\\(\\epsilon = 0.1\\)), GeoERM’s performance declines. This drop matches the theoretical condition from \\citet{tian2023learning} that \\( n \\gtrsim p + \\log T \\) is needed. Nevertheless, GeoERM remains the best performer on non-outlier tasks due to the weaker requirement \\( n \\gtrsim r + \\log T \\). The sharper decline in performance with increasing \\( h \\) reflects the difficulty of learning under high-dimensional heterogeneity.",
        "summarize_figure": "2505.02972v1_hpc805",
        "summarization": "The figure illustrates the variation in maximum error across different values of h, for two scenarios: ε = 0 (left) and ε = 0.1 (right). The simulation parameters are n = 100, T = 50, p = 80, and r = 5. From the graph, it is evident that GeoERM performs best when ε = 0. However, with the introduction of outlier tasks (i.e., ε = 0.1), GeoERM's performance declines, which aligns with the theoretical condition that n ≳ p + log T. Nevertheless, GeoERM remains the best performer on non-outlier tasks, as it performs well under the weaker condition n ≳ r + log T. The sharp drop in performance as h increases highlights the difficulty of learning under high-dimensional heterogeneity."
    },
    "84": {
        "figure1": "2505.02974v1_plaid_files",
        "label1": "fig:plaid_format",
        "caption1": "PLAID files structure.",
        "text": "PLAID datasets are provided either as human-readable data storage, or stored using Hugging Face datasets tools~\\cite{hf_datasets}. In the former case, YAML and CSV files can be opened with any text editor, while CGNS files containing physical configurations can be visualized using tools such as ParaView, see Figure~\\ref{fig:plaid_format}. In the latter, we benefit directly from powerful data management such as caching and online streaming.",
        "summarize_figure": "2505.02974v1_plaid_files",
        "summarization": "This figure illustrates the structure of PLAID files. The folder contains various components of the dataset, starting with a collection of individual samples. Each sample includes mesh files in the CGNS format and related scalar data in CSV format. The dataset also contains metadata describing the dataset itself and metadata related to the specific problem or task, including problem definitions, problem-related information, and dataset partitioning for training, validation, or testing purposes. The mesh and scalar data for each sample are stored in dedicated files, with corresponding descriptive files (such as YAML and CSV formats). This file structure facilitates efficient data management and access while providing clear task and problem definitions."
    },
    "85": {
        "figure1": "2505.02974v1_ViT",
        "label1": "fig:ViT",
        "caption1": "Vi-Transformer architecture. Input meshes are partitioned using the Metis domain decomposition algorithm \\cite{metis}. Each such sub-domain is then tokenized before passing through the transformer encoder. In the end, each token is decoded into its domain's corresponding fields. Input scalars are embedded during the tokenization procedure while output scalars are estimated as uniform fields.",
        "text": "The chosen approach relies on a transformer encoder architecture and is analogous to Vision Transformers (Vi-Transformer). Rather than considering each node of the mesh as a token by its own, the encoder takes as input tokenized point-cloud patches. Local information is kept within the patches while long-range information is retrieved through the transformer's mapping, which compares all token pairs together. The general architecture of the Vi-Transformer is depicted in Figure \\ref{fig:ViT}.",
        "summarize_figure": "2505.02974v1_ViT",
        "summarization": "The figure illustrates the Vi-Transformer architecture. In this architecture, input meshes are partitioned using the Metis domain decomposition algorithm, and each sub-domain is tokenized before passing through the transformer encoder. During the encoding process, each token is decoded into its corresponding field for the domain. Input scalars are embedded during tokenization, while output scalars are estimated as uniform fields. This approach draws from Vision Transformers (Vi-Transformer), but instead of considering each mesh node as a token, it tokenizes point-cloud patches. Local information is retained within patches, while long-range information is retrieved through the transformer's mapping, which compares all token pairs. The general architecture of the Vi-Transformer is shown in the figure."
    },
    "86": {
        "figure1": "2505.02977v1_gpu-ordering-horizontal",
        "label1": "fig:depth",
        "caption1": "Top figure shows e-tree depth using the classical e-tree computation vs. actual e-tree height vs. triangular solve critical path length. Bottom figure shows the corresponding time usage by each ordering on GPU, and the ratio of fill-in in the resulting lower triangular factor. The ratio is defined as $\\frac{2 * \\texttt{nnz($G$)}}{\\texttt{nnz($L$)}}$, where $G$ is the resulting factor and $L$ is the input.} \\centering \\includegraphics[width=0.6\\linewidth]{fig/depth.png}  \\includegraphics[width=1\\linewidth]{fig/gpu-ordering-horizontal.png",
        "text": "In classical factorization, eliminating a vertex creates a full clique among its neighbors, and the e-tree is built by linking each vertex to the first nonzero element in its column of the Cholesky factor—effectively capturing all serial dependencies (see \\cite{george1981computer}, \\cite{davis2006direct}). This means that a vertex can only be processed after all its e-tree descendants have been eliminated. However, when clique subsampling is used, the full set of fill-in edges is replaced by a spanning tree that connects the neighbors. Many serial dependencies that exist in the classical e-tree are eliminated because the sampling “cuts” away edges. This relaxed dependency graph allows more vertices to be processed concurrently, enhancing parallelism (see \\cite{sachdeva2023simple}, \\cite{rchol}). At the same time, the essential connectivity needed for a good preconditioner is maintained. However, it introduces the problem that the classical e-tree is different from the actual e-tree; \\cref{fig:depth} shows examples of this. Therefore, the key question is how do we design ParAC so that it can simultaneously expose the large degree of parallelism not labeled by the classical elimination tree while maintaining ordering integrity?\nMany of the performance indications on CPU no longer apply to GPU. For example, the AMD ordering is faster on CPU due to locality, but is slower than the other two orderings on GPU.  Figure~\\ref{fig:depth} provides an explanation for this. For each ordering and all matrices, we report the classical e-tree height (the one obtained by doing the classical restrictive e-tree calculation), the actual e-tree height, and the longest  path. We see that all orderings benefits from the reduction in the e-tree height thanks to the sparsity of the preconditioner. However, the AMD ordering benefits much less than the other two orderings. Unlike on CPU, ParAC on GPU heavily depends on coarse level parallelism since each thread block has weak computation power compared to a CPU thread. Similarly, the performance of triangular solve on GPU also relies on exploiting structural parallelism~\\cite{sync-free-triangular, li2020efficient}.  In particular, if we view the triangular matrix as a directed acyclic graph (DAG), then the longest path/critical path in that graph (corresponding to max path in \\cref{fig:depth}),  will have a significant impact on the performance. Indeed, \\cref{fig:depth} shows that AMD ordering leads to longer crtical paths and is the slowest on GPU. Another reason for the CPU-GPU performance gap can be attributed to bandwidth. using the NERSC documentation\\footnote{https://docs.nersc.gov/systems/perlmutter/architecture/}, we see that A100's bandwidth is nearly 8 times the bandwidth of an EPYC 7763 CPU, which is helpful since ParAC is bandwidth bound, and so is triangular solve.\nLastly, we also make the observation that unlike classical Cholesky, the resulting nonzero count of the computed triangular factors is not that sensitive to elimination ordering, as shown in \\Cref{fig:depth}. All orderings produced similar number of nonzeros, and this also applies to the CPU case since the statistical property is the same. This further strengthen the case that random sorting or nnz-sort is preferable on GPU. Furthermore, those two orderings generally runs faster than AMD, which is much more sequential in nature.\nA theoretical analysis on the degree of parallelism ParAC achieves would also be interesting. One way to interpret this is by drawing some inspiration from the parallel maximal independent set (MIS) problem \\cite{luby}. A random elimination ordering corresponds to assigning the vertices a set of random numbers. Based on \\cref{lemma:reachability}, a node only executes if it's smaller than it's neighbors, which is similar to some variants of parallel MIS. However, unlike MIS, when a vertex $v$ is eliminated in ParAC, only its incident edges are removed, but not its neighbors. In addition, each elimination step also creates new fill-in edges. MIS terminates in $O(\\log n)$ rounds with high probability, and it would be interesting to explore if some parallel theory can be established for ParAC.  Finally, as \\cref{fig:depth} demonstrates, ordering has a huge impact on the critical path length and tree height. It is still unclear why AMD ordering does not benefit as much from parallelism as nnz-sort and random.  From HPC's perspective, we are interested in extending this algorithm to a distributed setting. However, since the algorithm is bandwidth bound with only $O(1)$ arithmetic intensity, it's difficult to justify the communication cost.",
        "summarize_figure": "2505.02977v1_gpu-ordering-horizontal",
        "summarization": "The first plot illustrates factorization time across various matrix names, where AMD ordering shows significantly higher factorization times for some matrices, particularly for G3-circuit. On the other hand, NNZ Sort and Random methods tend to result in lower factorization times. The second plot shows solver time, with AMD again being slower in comparison to the other strategies, especially for matrices like ecology1 and G3-circuit. Lastly, the third plot highlights the nonzero count summary, where all methods generally produce similar results in terms of nonzero counts, although NNZ Sort and Random ordering methods show some slight variations."
    },
    "87": {
        "figure1": "2505.02978v1_plot_pm_pe",
        "label1": "fig:p_m_interval",
        "caption1": "Realistic variations in mechanical power input",
        "text": "The mechanical power input of a generator usually changes in response to the power system's economic dispatch.  Furthermore, even if the economic dispatch commands coming from the independent system operator have a longer interval (say, $5$ minutes), a utility may want to change their generation at shorter intervals (say, $\\mathrm{tens}$ of seconds),  particularly if the system has a large number of CIGs whose inputs are a function of weather conditions. Thus, a typical variation in the $p_m$ of a generator over an economic dispatch period ($T$) may look similar to the plot shown in Fig. \\ref{fig:p_m_interval}.  In the figure, $p_m$ ramps considerably in the intervals $d_1$, $d_2$, and $d_3$, and is relatively flat in other time intervals. However, even though it may be flat, its value is not the same. Additionally, one cannot measure $p_m$ in real-time using a PMU. Therefore, even if $p_m$ remains constant, one cannot know its value by direct measurement.\nIt is also clear from Fig. \\ref{fig:p_m_interval} that $p_m$ can change significantly between the start and end of an economic dispatch period.  Lastly, because $p_m$ can change within the economic dispatch period, the durations for which $p_m$ is constant (relatively flat in Fig. \\ref{fig:p_m_interval}) might not be known in advance. These practical considerations explain why \\eqref{eq: linear model} is unsuitable for inertia estimation when $p_m$ changes. A strategy to circumvent this problem is presented below.\nSince PMUs produce outputs at a much faster rate than the frequency with which $p_m$ changes, one can create rolling windows of duration much smaller than the economic dispatch period ($T$ in Fig. \\ref{fig:p_m_interval}), and solve \\eqref{eq: linear model} for each window. The outcome will be that the estimates over consecutive windows will become \\textit{inconsistent} whenever $d_1$, $d_2$, $d_3$ appear inside the windows. However, the inconsistency of the estimates will reveal the durations over which $p_m$ is varying.  This information is leveraged below to create a partitioned formulation of the swing equation for inertia estimation.",
        "summarize_figure": "2505.02978v1_plot_pm_pe",
        "summarization": "The figure depicts the variation in mechanical power input (pm) over an economic dispatch period (T). In the graph, pm shows significant ramps in intervals d1, d2, and d3, and remains relatively flat during other periods. However, even when flat, the value of pm is not constant. As pm changes due to weather conditions, it is impossible to measure its exact value in real-time using a PMU. The graph illustrates that pm can vary significantly between the start and end of the economic dispatch period, and that intervals where pm remains constant may not be predictable. This creates challenges for inertia estimation using traditional linear models."
    },
    "88": {
        "figure1": "2505.02979v1_Backprop",
        "label1": "fig:inverse",
        "caption1": "Flow diagram of the inverse model} .",
        "text": "The optimisation process is applied as described in Figure~\\ref{fig:inverse}. It involves solving the forward model, minimising the mismatch between the model output and the observations using the stochastic gradient descend method \\eqref{eq:gradientdescent} to give an improved estimation of parameter values that are consistent with the observations. The optimisation process ends when the maximum number of iterations (defined as 150) is reached, or the solution has converged. Solution is considered converged at iteration $\\bm{n}$ if both the relative change in loss $J$ is less than $0.01\\%$  and the relative change in all parameters $\\bm{p}$ are less than $0.1\\%$.",
        "summarize_figure": "2505.02979v1_Backprop",
        "summarization": "The flowchart depicts the inverse model optimization process. It begins with random initialization of parameters, followed by solving the forward model and minimizing the mismatch between model output and observations using the stochastic gradient descent (SGD) method. The process involves calculating the mismatch function J, computing the gradients, and updating the parameters iteratively. The optimization stops when either the maximum iteration count (150) is reached, or the solution converges. The solution is considered converged if both the relative change in loss function J is less than 0.01% and the relative change in all parameters is less than 0.1%."
    },
    "89": {
        "figure1": "2505.02979v1_cov",
        "label1": "fig:cov",
        "caption1": "Scatter plots of initial vs final values for the synthetic data set of a) surface albedo $\\alpha$, b) heat transfer coefficient $h$, c) Bowen ratio $\\beta$, d) $h(1+\\beta^{-1})$, e) Initial/interior temperature $T_b$, f) volumetric heat capacity $C$, g) thermal conductivity $\\lambda$ and h) $\\mu$ compared with their true values (black dotted line) ",
        "text": "The results from the 50 trials are shown in Figure~\\ref{fig:cov} in the blue circles.  Scatter plots are shown for each parameter, with its initial value on the $x$-axis and its final value on the $y$-axis. Clearly, the ideal outcome is that the same final value is reached regardless of the initial parameter value, but this is not always the case. The results indicate that the algorithm is able to determine $T_b$ and $\\alpha$. However, the value of $h$ is strongly dependent on its initial value value. The parameters $\\beta$, $\\lambda$ and $C$ have a substantial spread in their optimal values, in a seemingly uncorrelated manner with their initial value. Hence, the conclusion from the `naive' parameter calibration is that it is not possible to obtain a reliable estimation of the parameter values by assuming the parameters are independent and using a single observation of the soil temperature profile.\nThe reason why this happened is because an optimiser converges to a global minimum only when the optimisation problem is convex. This condition will be violated when two parameters are dependent on each other.  This is the case for the heat transfer coefficient $h$ and the Bowen ratio $\\beta$, which can be seen to only occur as the product $h(1+\\beta^{-1})$ in \\eqref{eq:bc1}. This makes the optimisation problem non-unique. Figure~\\ref{fig:hbeta_Clambda_cov}(a) shows the final values for $h$ and $\\beta$ plotted against each other, showing a clear relation between the two quantities. Figure~\\ref{fig:cov}(d) shows that the product $h(1+\\beta^{-1})$ has much less spread than $h$ and $\\beta$ individually.  Thus, the optimisation should use the parameter $h(1+\\beta^{-1})$ instead of $h$ and $\\beta$ separately.\nFurthermore, it is well known that thermal inertia is typically characterised using the thermal admittance $\\mu=\\lambda C$ \\citep{Oke2017urban}, which characterises a material's ability to absorb and release heat.  This suggests that there might be a correlation between $C$ and $\\lambda$ as well. Figure~\\ref{fig:hbeta_Clambda_cov}(b) shows the final values for $C$ and $\\lambda$ plotted against each other, once more showing a clear relation between the two quantities. However, Figure~\\ref{fig:cov}(h) shows that even though $C$ and $\\lambda$ are clearly correlated, there is considerable uncertainty in $\\mu$, and it is not possible to replace $C$ and $\\lambda$ for $\\mu$.\nFigure~\\ref{fig:cov} shows that using observations at two depths, it becomes possible to reliably estimate parameter values.  The parameter values obtained from the inverse model are  $\\alpha=0.199 \\pm 0.010$, $T_b=292.99 \\pm \\SI{0.05}{K}$, $\\lambda=0.81 \\pm \\SI{0.11}{W/m/K}$, $C=2.24 \\pm \\SI{0.32}{MJ/m^3/K}$ and $h(1+\\beta^{-1})=24.90 \\pm \\SI{0.78}{W/m^2/K}$.  Obtained from 50 trials, the mean values of the parameters are within $2$\\% of their real value (see Table \\ref{tab:parameters}). Note that $C$ and $\\lambda$ are the only quantities that have outliers up to 15\\% larger than the mean value.",
        "summarize_figure": "2505.02979v1_cov",
        "summarization": "This figure shows scatter plots of initial vs final values for parameters (surface albedo α, heat transfer coefficient h, Bowen ratio β, etc.) from a synthetic dataset. Blue circles are from 50 trials. Each subplot has initial value on x - axis and final value on y - axis. Ideal is constant final value, but not always so. Algorithm can determine some parameters well (e.g., α, Tb), while h depends on initial value. β, λ, C have large spread in final values, showing single soil temperature observation may not reliably estimate parameter values."
    },
    "90": {
        "figure1": "2505.02979v1_hbeta_Clambda_cov",
        "label1": "fig:hbeta_Clambda_cov",
        "caption1": "Covariance plots of final parameter values for the synthetic dataset of a) $\\beta$ against $h$; and b) $\\lambda$ against $C$",
        "text": "The reason why this happened is because an optimiser converges to a global minimum only when the optimisation problem is convex. This condition will be violated when two parameters are dependent on each other.  This is the case for the heat transfer coefficient $h$ and the Bowen ratio $\\beta$, which can be seen to only occur as the product $h(1+\\beta^{-1})$ in \\eqref{eq:bc1}. This makes the optimisation problem non-unique. Figure~\\ref{fig:hbeta_Clambda_cov}(a) shows the final values for $h$ and $\\beta$ plotted against each other, showing a clear relation between the two quantities. Figure~\\ref{fig:cov}(d) shows that the product $h(1+\\beta^{-1})$ has much less spread than $h$ and $\\beta$ individually.  Thus, the optimisation should use the parameter $h(1+\\beta^{-1})$ instead of $h$ and $\\beta$ separately.\nFurthermore, it is well known that thermal inertia is typically characterised using the thermal admittance $\\mu=\\lambda C$ \\citep{Oke2017urban}, which characterises a material's ability to absorb and release heat.  This suggests that there might be a correlation between $C$ and $\\lambda$ as well. Figure~\\ref{fig:hbeta_Clambda_cov}(b) shows the final values for $C$ and $\\lambda$ plotted against each other, once more showing a clear relation between the two quantities. However, Figure~\\ref{fig:cov}(h) shows that even though $C$ and $\\lambda$ are clearly correlated, there is considerable uncertainty in $\\mu$, and it is not possible to replace $C$ and $\\lambda$ for $\\mu$.",
        "summarize_figure": "2505.02979v1_hbeta_Clambda_cov",
        "summarization": "The image presents covariance plots of final parameter values in a synthetic dataset. In panel a, the heat transfer coefficient h and Bowen ratio β show a strong nonlinear positive correlation across both assimilation methods, suggesting that their coupling in boundary conditions causes non-uniqueness in optimization. This dependency supports the argument to use the compound parameter h(1 + β⁻¹) instead. In panel b, the volumetric heat capacity C and thermal conductivity λ also exhibit a clear positive correlation, especially at higher values, indicating their joint role in determining thermal admittance μ = λC. However, despite this correlation, μ remains uncertain and cannot reliably replace the individual parameters C and λ."
    },
    "91": {
        "figure1": "2505.02984v1_H6_STO6G_2_0_ADAPT_GSD_vs_saGSD",
        "label1": "GSD_vs_asGSD",
        "caption1": "Errors relative to FCI characterizing the ADAPT-VQE-GSD and ADAPT-VQE-saGSD simulations of the H$_6$/STO-6G linear chain with $R_\\text{H--H} = 2.0\\, \\text{\\AA}$.",
        "text": "Similar to classical electronic structure approaches, using a symmetry-adapted operator pool has various advantages. To begin with, the enforcement of symmetries substantially shrinks the size of the operator pool and the targeted Hilbert space of the system of interest. For example, in the linear H$_6$ chain as described by the STO-6G basis \\cite{Hehre.1969.10.1063/1.1672392}, the GSD pool contains 1551, 870, 420, and 312 operators when the ($N$), ($N$, $S_z$), ($N$, $S_z$, point group), and ($N$, $S_z$, point group, $S^2$) symmetries are enforced, respectively. Furthermore, the dimensions of the symmetry-adapted Hilbert spaces are 924 ($N = 6$), 400 ($N = 6$, $S_z = 0$), 200 ($N = 6$, $S_z = 0$, $A_g$ [$D_{2h}$]), and 92 ($N = 6$, $S_z = 0$, $A_g$ [$D_{2h}$], $S^2 = 0$). In \\cref{GSD_vs_asGSD}, we compare the convergence to the exact, FCI ground-state energy of H$_6$/STO-6G, $R_\\text{H--H} = 2.0\\, \\text{\\AA}$, for ADAPT-VQE numerical simulations using the standard GSD pool and its singlet spin-adapted variant (saGSD). Although both pools respect the $N$, $S_z$, and point group symmetries, only the saGSD pool is singlet spin-adapted, whereas the GSD pool is not. As shown in \\cref{GSD_vs_asGSD}, both ADAPT-VQE-GSD and ADAPT-VQE-saGSD ultimately reproduce the FCI result within a few picohartree. However, ADAPT-VQE-saGSD requires only 91 parameters, compared to 199 for its GSD counterpart, mirroring the differences in the dimensions of their respective symmetry-adapted Hilbert spaces. As might have been anticipated, ADAPT-VQE-saGSD reaches chemical accuracy much faster, requiring less than half the number of parameters of ADAPT-VQE-GSD. \t\tH$_6$/STO-6G linear chain with $R_\\text{H--H} = 2.0\\, \\text{\\AA}$.}",
        "summarize_figure": "2505.02984v1_H6_STO6G_2_0_ADAPT_GSD_vs_saGSD",
        "summarization": "The image compares energy errors of ADAPT-VQE-GSD and ADAPT-VQE-saGSD methods for an H₆/STO-6G linear chain with R_H–H = 2.0 Å as a function of the number of parameters. Both methods converge to near-exact FCI energy, but ADAPT-VQE-saGSD, shown in red, achieves chemical accuracy using significantly fewer parameters (around 91) compared to the GSD variant (about 199, in blue). The saGSD method reaches high precision much faster due to spin-singlet symmetry adaptation, which reduces the operator pool and the dimension of the Hilbert space, leading to faster and more efficient convergence."
    },
    "92": {
        "figure1": "2505.02984v1_spin_cross",
        "label1": "spin_cross",
        "caption1": "Illustration of crossing between singlet (blue) and triplet (red) eigenvectors (solid lines). A quantum simulation that breaks $S^2$ symmetry and targets the singlet state will produce an unphysical double minimum (dashed line).",
        "text": "Beyond resource reductions, symmetry-adaptation is also useful for disentangling states with different  symmetry properties, allowing one to track only those states with desired good quantum numbers. To demonstrate the usefulness of this aspect, in \\cref{spin_cross} we illustrate a case in which the lowest-energy singlet ($\\text{S}_0$) and triplet ($\\text{T}_0$) potential energy surfaces intersect. A quantum simulation that does not conserve $S^2$ and targets the $\\text{S}_0$ state may erroneously collapse to the $\\text{T}_0$ state in the region where the latter is lower in energy, producing a seemingly unphysical potential energy surface. Symmetry adaptation avoids such issues and allows for the simulation of the lowest-energy state in each symmetry sector, at no additional cost, using standard ground-state quantum algorithms. \t\tquantum simulation that breaks $S^2$ symmetry and targets the singlet state will produce an unphysical double minimum (dashed line).}",
        "summarize_figure": "2505.02984v1_spin_cross",
        "summarization": "The image illustrates the crossing of singlet (blue) and triplet (red) electronic states on potential energy surfaces. The solid curves represent true singlet S₀ and triplet T₀ states intersecting along nuclear coordinates. A quantum simulation that breaks spin symmetry S² and targets the singlet may collapse into the lower-energy triplet region, yielding an unphysical double-well potential shown by the dashed line. The point labeled \"variational solution\" marks this erroneous minimum. This highlights the importance of symmetry adaptation in quantum algorithms to accurately resolve the lowest-energy state in each spin sector."
    },
    "93": {
        "figure1": "2505.02986v1_PCC_Diff",
        "label1": "fig:PCC",
        "caption1": "Comparison of community detection methods in terms of $\\text{PCC\\_Diff}$, which is the PCC of each method minus the average PCC across all methods. The subfigures show the performances for different sample sizes ($n$) and signal strength ($k$) across three different cases. A higher $\\text{PCC\\_Diff}$ indicates better performance. In weak signal and well-specified covariate settings, CALSM performs the best, and SVD performs the second best with using accurate information about non-zero covariate variable sets of node features. In strong signal or misspecified covariate settings, CALSM also performs best while the latent space model using only network information performs the second best.",
        "text": "Roughly speaking, Case 1 represents well-specified covariate settings where additional node information uniformly contributes to generating the networks. Case 2 corresponds to scenarios with partial node-level mismatches, where covariate information is not useful for some nodes. Case 3 exemplifies completely misspecified covariate settings where all additional node information lacks utility. To systematically vary the separation between latent positions, we generate $\\+X^* = \\tilde{\\+X} /(max_{i,j}|\\tilde{X}_{i,j}|)\\times k$ with $k \\in \\{1,1.5,2,3\\}$. The value of $k$ plays a crucial role in determining the relative contribution of network signals versus covariate information. When $k$ is small, network signals remain weak, making accurate incorporation of covariate information essential for optimal performance. Conversely, with large $k$ values, strong network signals dominate, minimizing the influence of additional covariates and elevating correct model specification as the primary determinant of performance quality. We assign the values of $\\beta^*= -2$ to control the overall sparsity level of the networks (the mean connecting probability is around $1/(1+\\exp(2)) \\approx 0.1$), then generate the networks by equation~\\eqref{eq:likelihood} for each of the above cases.  We use the CAVI algorithm with a stopping criterion of either 200 cycles or a mean difference of less than $10^{-4}$ between the estimated denoised connecting probabilities in two consecutive cycles. We use  Pearson's correlation coefficient (PCC) to measure the discrepancy between true and estimated probabilities. To perform a fair comparison for different approaches, we report $\\text{PCC\\_Diff}$, which is the PCC of each method minus the average PCC across all methods for the specific generated data. The simulation of each case is repeated $25$ times. The simulation results in Figure~\\ref{fig:PCC} demonstrate our theoretical results on the adaptivity and robustness of the proposed method.",
        "summarize_figure": "2505.02986v1_PCC_Diff",
        "summarization": "The image compares five community detection methods under varying sample sizes (n=200, 1000), signal strengths (k=1 to 3), and covariate settings (Case 1 to 3), using PCC_Diff as the performance metric. CALSM consistently shows superior performance, particularly when the signal is weak or covariate specifications are accurate (Case 1, low k). SVD-based methods perform well only under accurate covariates, while LSM becomes more competitive when covariates are partially or fully misspecified (Cases 2 and 3) or when signal strength is high. The results highlight CALSM's adaptivity and robustness across diverse conditions."
    },
    "94": {
        "figure1": "2505.02986v1_PCC_mismatch",
        "label1": "fig:mismatch",
        "caption1": " Comparison of median PCC across $50$ Monte Carlo simulations for different approaches, mismatch ratios, and signal strengths. As the mismatch ratio increases to $0.5$, the performance of  CALSM  will still be better than LSM, while when the mismatch ratio is close to zero, CALSM can benefit a lot from absorbing the additional covariates information similar to joint SVD with Oracle index information of the additional covariates.",
        "text": "We also conducted a simulation to compare the performance of CALSM, LSM and SVDyzO for various mismatch ratios from $0$ to $0.5$, the number of mismatched nodes divided by the number of nodes $n=200$, and different signal strengths $k$. In the simulation, we followed Case 2 settings and changed the process of selecting rows from $\\tilde{\\+X}$: Instead of selecting 5 random rows, we selected $n$ multiplied by the mismatching ratio rows and replaced them with random samples from uniform intervals. The rest of the settings remain the same, and the simulation is repeated $50$ times. The simulation results for signal strengths of $k=1,2,3$ are shown in Figure~\\ref{fig:mismatch}. From the figure, we observed that as the mismatch ratio increased from $0$ to $0.5$, CALSM's performance remained superior to LSM's. However, when the mismatch ratio approached zero, CALSM showed significant improvement by incorporating additional covariate information, similar to joint SVD with oracle subset information of the additional covariates.",
        "summarize_figure": "2505.02986v1_PCC_mismatch",
        "summarization": "The image shows how CALSM, LSM, and SVDyzO perform under varying mismatch ratios (0 to 0.5) and signal strengths (k=1, 2, 3), using PCC as the metric. CALSM consistently outperforms the other methods across all settings. Its advantage is especially strong at zero mismatch, reflecting its effective use of covariate information. Although performance declines as mismatch increases, CALSM remains superior, demonstrating robustness. LSM and SVDyzO perform similarly at low signal strength, while SVDyzO slightly surpasses LSM at higher strengths. Overall, CALSM maintains a clear advantage in both accuracy and stability."
    },
    "95": {
        "figure1": "2505.02987v1_side-stretch-moves",
        "label1": "fig:stretch-side-moves",
        "caption1": "Demonstration of stretch move and side move. Left: stretch move to the four-pointed star; middle: side move to the five-pointed star; right: both moves when points are on a circle",
        "text": "At each time step, our ensemble \\textit{side move} sampler randomly selects one particle $\\mathbf{x}_i(m)$ and two distinct particles $\\mathbf{x}_j(m)$ and $\\mathbf{x}_k(m)$ from the ensemble which are different from $\\mathbf{x}_i(m)$. The sampler proposes the following \\textit{side move} (see an illustration in Figure \\ref{fig:stretch-side-moves}) for the $i$-th particle: where $\\sigma$ is a user-specified scalar parameter, and $\\xi \\sim \\mathcal{N}(0,1)$ is drawn from a normal distribution. We will discuss connections to walk move \\cite{goodman2010ensemble} and differential evolution \\cite{storn1997differential,braak2006markov, vrugt2009accelerating} in Sections  \\ref{sec-Comparison to other affine invariant moves} and \\ref{sec-Connection to differential evolution}. In Section \\ref{sec-Formal analysis of high dimensional behaviors}, we study the high dimensional scaling behavior, which suggests a choice of $\\sigma = 1.687d^{-1/2}$.\nWe illustrate the stretch move and side move concepts in Figure~\\ref{fig:stretch-side-moves}. For the stretch move, the new point is positioned along the line formed by $\\mathbf{x}_i$ and $\\mathbf{x}_j$. In the side move, two distinct particles $\\mathbf{x}_j$ and $\\mathbf{x}_k$ form a reference side, and $\\mathbf{x}_i$ moves in a direction parallel to this side. The effectiveness of each approach depends on the underlying distribution. In the right panel of Figure~\\ref{fig:stretch-side-moves}, we illustrate a scenario where points lie on a circle. Here, for the given selection of points, the side move approximately follows the tangential direction, while the stretch move proposes points far from the circle.\nIn Section \\ref{sec-Formal analysis of high dimensional behaviors}, we provide quantitative high-dimensional scaling analysis for an isotropic Gaussian target. It shows that the side move leads to improved jumped distance compared to the stretch move. The insight is that high-dimensional isotropic Gaussians concentrate on a thin shell, and the inner product between the side direction $\\mathbf{x}_j(m) - \\mathbf{x}_k(m)$ and $\\mathbf{x}_i(m)$ is small with high probability, so the side move proposal typically goes in the favorable tangential direction, in a similar spirit as the right panel of Figure~\\ref{fig:stretch-side-moves}.",
        "summarize_figure": "2505.02987v1_side-stretch-moves",
        "summarization": "The image illustrates the geometric intuition of stretch and side moves in ensemble sampling. The left panel shows a stretch move along the line through xi and xj, while the middle panel depicts a side move where xi shifts in a direction parallel to the segment formed by xj and xk. In the right panel, with points lying on a circle, side moves follow tangents and remain near the high-density region, whereas stretch moves may leave the circular manifold. This highlights side move’s effectiveness in high-dimensional Gaussian settings due to its tangential, shell-preserving nature."
    },
    "96": {
        "figure1": "2505.02987v1_compare-side-and-stretch",
        "label1": "fig: max expected length",
        "caption1": "Side move versus stretch move: expected acceptance rate and squared expected jumped distance. Optimal $\\alpha$ and $\\beta$ in terms of the squared expected jumped distance are marked.",
        "text": "With Proposition \\ref{prop-gaussian-acceptance}, we can perform simulations to find the optimal $\\alpha$ and $\\beta$ that lead to the largest expected squared jumped distance. The results are shown in Figure \\ref{fig: max expected length}. In three decimal numbers, we found that the optimal $\\alpha$ is approximately $1.687$ with a squared jumped distance of $0.744$, and the optimal $\\beta$ is approximately $2.151$ with a squared jumped distance of $0.584$. The corresponding acceptance probabilities are $0.443$ and $0.416$, respectively. With these parameters, the side move can achieve approximately $0.744/0.584 \\approx 1.27$ times the squared jumped distance compared to the stretch move.",
        "summarize_figure": "2505.02987v1_compare-side-and-stretch",
        "summarization": "The image compares side and stretch move strategies using expected squared jumped distance and acceptance probability under a Gaussian model. Both moves exhibit peak efficiency at optimal parameters: α ≈ 1.687 for side move and β ≈ 2.151 for stretch move. Side move achieves a higher maximum jump distance (0.744 vs. 0.584) and a slightly better acceptance rate (0.443 vs. 0.416), making it roughly 1.27 times more efficient than stretch move. While increased α or β reduces acceptance, side move balances exploration and acceptance more effectively."
    },
    "97": {
        "figure1": "2505.02987v1_side_move_scaled_autocorrelation",
        "label1": "fig:stretch-side-acf",
        "caption1": "Scaled autocorrelation functions for sampling anisotropic Gaussian targets with condition number $\\kappa = 1000$. Left: stretch move; right: side move. Scaled lag $= $ original lag$/ $dim$\\times 4$.",
        "text": "In Figure \\ref{fig:stretch-side-acf}, we show a scaled version of the autocorrelation functions for the stretch and side moves. We scale the lag by dividing the original lag by the dimension and then multiplying by $4$. With this scaling, the curve represents the unscaled autocorrelation function for $d=4$. We observe that this rescaling produces similar curves for the scaled autocorrelation function, which confirms the high-dimensional linear scaling behavior of both samplers. Notably, the side move leads to faster decay of autocorrelation.",
        "summarize_figure": "2505.02987v1_side_move_scaled_autocorrelation",
        "summarization": "The image shows scaled autocorrelation functions for side move sampling of an anisotropic Gaussian with condition number 1000 across dimensions 4 to 128. After lag rescaling, all curves closely overlap, indicating consistent decay behavior and strong linear scaling across dimensions. The autocorrelation rapidly decreases below 0.2 by a scaled lag of 20 and approaches zero by lag 100, demonstrating efficient mixing and reduced correlation in high-dimensional settings."
    },
    "98": {
        "figure1": "2505.02992v1_sub_ws",
        "label1": "fig:sub",
        "caption1": "Illustration of the subcritical region $\\ctsub$ (shaded area)",
        "text": "Figure \\ref{fig:sub} illustrates the construction. The curves $\\{C_i\\}_{i=1}^4$ are trajectories of the localized systems in the corresponding regions $\\{R_i\\}_{i=1}^4$. More explicit expressions for these curves and the subcritical region $\\ctsub$ will be provided in Section \\ref{sec:Lyapunov}.",
        "summarize_figure": "2505.02992v1_sub_ws",
        "summarization": "The image depicts the construction of the subcritical region, shaded between the lines w−νs and w−ν−s. Four distinct regions R1 to R4 are defined in the ws-plane, each with a corresponding trajectory curve C1 to C4, representing solutions of localized systems. These curves converge near a central point, with the subcritical region formed by their enclosure. The colored zones illustrate dynamics such as (e1, v1−) and (e2, v2+), highlighting how different phase space behaviors define the geometry of the subcritical area."
    },
    "99": {
        "figure1": "2505.02992v1_sup_ws",
        "label1": "fig:sup",
        "caption1": "Illustration of the supercritical region $\\ctsup$ (shaded area)",
        "text": "Scenario IV is referred to as \\emph{weak alignment}. As in the construction of the subcritical region $\\Sigma_\\flat$, the construction of the supercritical region $\\Sigma_\\sharp$ involves admissible conditions analogous to \\eqref{AC1}-\\eqref{AC3}, which take the form: We will show in Propositions~\\ref{prop:superAC1}, ~\\ref{prop:superAC2} and \\ref{prop:superAC3} that the conditions in \\eqref{eq:ACsup} are satisfied automatically. Therefore, no admissible conditions are required to construct $\\Sigma_\\sharp$. See Figure~\\ref{fig:sup} for an illustration of the supercritical region $\\ctsup$.",
        "summarize_figure": "2505.02992v1_sup_ws",
        "summarization": "The image illustrates the construction of the supercritical region ctsup, shown as the shaded wedge-shaped area between two lines w−νs and w−ν−s in the ws-plane. Four subregions R1 to R4, associated with distinct dynamical behaviors (e1, v1−), (e2, v2−), (e2, v2+), and (e1, v1+), are bounded by trajectories C1 to C4. These trajectories evolve within a semicircular domain and converge toward the arc boundary. Under the weak alignment condition (Scenario IV), no admissibility constraints are required, as the system dynamics inherently satisfy construction criteria. The diagram provides a geometric representation of how these flows define the supercritical region."
    },
    "100": {
        "figure1": "2505.02992v1_blowup",
        "label1": "fig:blowup",
        "caption1": "Illustration of the flow in the supercritical region $\\ctsup$",
        "text": "Consider the dynamics \\eqref{S} with initial data $(w_0,s_0)\\in \\ctsup$. Then, there exists a finite time $T_*$, such that  We divide the supercritical region $\\ctsup$ into four areas $\\{A_i\\}_{i=1}^4$, defined as follows:  \tA_1 & = \\ctsup~\\cap~\\big\\{(w,s): w>0,~ 0<s<\\st_*\\big\\},\\quad \\st_*=\\tfrac12(\\st_1+\\st_2),\\\\  \tA_2 & = \\ctsup~\\cap~\\big\\{(w,s): w>0,~ s\\geq\\st_*\\big\\},\\\\ \tA_3 & = \\ctsup~\\cap~\\big\\{(w,s): w\\leq 0,~ s>\\st_{**}\\big\\},\\quad \\st_{**}=\\tfrac12\\st_3=\\tfrac{1}{2\\cM},\\\\ \tA_4 & = \\ctsup~\\cap~\\big\\{(w,s): w\\leq 0,~ 0<s\\leq\\st_{**}\\big\\}. Refer to Figure \\ref{fig:blowup} for an illustration of these four areas. We will demonstrate that a trajectory progresses sequentially through $A_1$, $A_2$, $A_3$, and $A_4$, ultimately exiting along the negative $w$-axis. Moreover, the solution remains in each area for a finite amount of time.\nTo prove \\eqref{eq:A1}, we apply \\eqref{S} and obtain The positive lower bound corresponds to the fact that $A_1$ is distance away from the line $w=\\nvM s$, as illustrated in Figure \\ref{fig:blowup}. To check this analytically, we split into two cases.",
        "summarize_figure": "2505.02992v1_blowup",
        "summarization": "The image illustrates the dynamical structure of the supercritical region, defined as a semicircular domain bounded by arc C and two sloped lines w = νs and w = ν−s. The space is divided into four colored regions A1 to A4, each indicating a distinct phase of the trajectory. The solution progresses through A1 to A4 in sequence and exits along the negative w-axis. Representative trajectories C1 to C4 highlight directional flows in each area. Boundaries in s are defined via st* and st**, and vector directions indicate phase evolution. The geometry conveys the finite-time transition through all zones, depicting the blow-up dynamics."
    },
    "101": {
        "figure1": "2505.02994v1_TPCH_results",
        "label1": "fig:tpch_results",
        "caption1": " Latencies for TPC-H queries are shown normalized to each query without any Bloom filters applied (No BF, shown as dashed line). Adding Bloom filters during plan post-processing (BF-Post, shown in blue) reduces query latency by 28.8\\%. Including Bloom filters during bottom-up cost-based optimization (BF-CBO, shown in orange) improves query latency by a further 32.8\\% relative to BF-Post. TPC-H queries that did not apply any Bloom filters are omitted. }  \\Description{A bar graph compares 3 methods for each tpch query.",
        "text": "The latencies of the TPC-H queries are shown in Figure~\\ref{fig:tpch_results}  with more details in Table~\\ref{tab:tpch_results}. Single table queries (Q1 and Q6), as well as queries that did not produce Bloom filters in any scenario (Q13-15,22),  are omitted from the analysis. The latencies are normalized to the latency of running the query without Bloom filters enabled (\\textbf{No BF}). Table~\\ref{tab:tpch_results}  also shows the percent reduction in query latency (\\% $\\downarrow$) of BF-CBO compared  to BF-Post as well as the absolute latencies of plan optimization for both BF-CBO and BF-Post. Note that planner runtime  is included in the measurement of absolute query latency before  normalization. Query numbers (Q\\#) where BF-CBO  selected a different plan than BF-Post are shown italicized in \\textcolor{red}{\\textit{\\textbf{red}}}.",
        "summarize_figure": "2505.02994v1_TPCH_results",
        "summarization": "The image shows normalized latencies for various TPC-H queries with and without Bloom filters. Latencies are normalized to the no-Bloom-filter baseline, indicated by the dashed line at 1.0. Blue bars represent adding Bloom filters during post-processing (BF-POST), while orange bars represent inclusion during cost-based bottom-up optimization (BF-CBO). Both strategies reduce latency, but BF-CBO consistently achieves lower values across all shown queries. Notable improvements are observed in queries 7, 16, and 19, highlighting BF-CBO's superior optimization effectiveness. Overall, incorporating Bloom filters—especially during early optimization—significantly enhances query performance."
    },
    "102": {
        "figure1": "2505.02996v1_B_real_exa_appr2",
        "label1": "Figure:real_exa_cum_baseline",
        "caption1": "Estimated cumulative baseline intensity functions with 95\\% pointwise confidence intervals for the MHED study in Approach 2.\\\\ \\footnotesize{\\noindent Note: Model (SSV) in the plots is Model (\\ref{eq:model}).",
        "text": "Figure \\ref{Figure:real_exa_cum_baseline} shows the estimated cumulative baseline intensity functions under different models with 95\\% pointwise confidence intervals in Approach 2. In stratum 1, the estimated function for an arbitrary baseline is close to that for a constant baseline, while they diverge in stratum 2. This difference is also reflected in Table \\ref{Tab:real_S}, where the estimated coefficients in stratum 2 have relatively large differences between different models.",
        "summarize_figure": "2505.02996v1_B_real_exa_appr2",
        "summarization": "The image presents estimated cumulative baseline intensity functions with 95% pointwise confidence intervals for the MHED study under Approach 2. The x-axis represents age and the y-axis shows the estimated cumulative baseline intensity. Six models are compared: NNC, SSC (s=1 and s=2), NNV, and SSV (s=1 and s=2). In stratum 1, model estimates are similar, particularly NNC and SSC (s=1), while in stratum 2, models diverge notably. SSC (s=2) yields the highest estimates, especially after age 10. SSV (s=2) also increases but less sharply, whereas NNV and SSV (s=1) remain low. The figure illustrates model sensitivity to complexity and stratification."
    },
    "103": {
        "figure1": "2505.03008v1_Fig1",
        "label1": "fig:1",
        "caption1": "setup{width=1.49\\linewidth} \\caption{(a) Schematic view of the experimental vacuum chamber (OM: Optical molasses; ZS: Zeeman slower; MOT: Magneto-optical trap). The inset displays a top view of a high numerical aperture (NA) tweezer setup; the 3D MOT side beams are mirrored with respect to the radial axis perpendicular to the objectives axis at a $25\\,^{\\circ}$ angle. Note that the permanent magnets and the coils producing, respectively, the 2D and 3D MOT magnetic fields are not shown. (b) The corresponding numerical setup, including the 2D OM, ZS, 2D and 3D MOTs. The blue dots are superparticles. A video version of this figure is available as online Supplemental Material \\cite{2:0}.",
        "text": "The experimental setup is seen in Fig. \\ref{fig:1}(a). Its main stages include an effusive oven, a 2D optical molasses (OM), a zeeman slower (ZS), a 2D MOT, and, lastly, a 3D MOT. The atoms originating from the effusive oven are first collimated to form a jet that is cooled transversely by the 2D OM and longitudinally by the ZS. The longitudinal cooling slows the atoms into a velocity class within the capture range of the 2D MOT, with the transverse cooling providing additional collimation for an improved loading efficiency. The 3D MOT is loaded from the 2D MOT by using the axial beams only (discussed in Sec. \\hyperref[sec:MainResults]{III}), granting the optical access for the high NA tweezer objectives. We use a 240 l/s ion-getter pump (SAES NEXTorr Z 200) that we attach above the 2D MOT chamber. Without the getter activated, it maintains an estimated $10^{-9}$ Torr pressure at the main science region. With this pressure, the mean lifetime due to background collisions is expected to be on the order of 1 s \\cite{2:4}, which we deem satisfactory for our investigations. We determine the atom numbers at various chamber locations (2D OM, 2D MOT, and 3D MOT) from the voltage readings of an avalanche photodiode (Hamamatsu C12703-01) together with calculated scattering rates and detection solid angles \\cite{2:5,2:6}.\nThe numerical setup is seen in\\;\\;Fig.\\;\\;\\ref{fig:1}(b).\\;\\;It\\;\\;mimics the experimental one starting from the 2D OM stage, with its initial numerical conditions dictated by the collimation geometry after the oven exit and the measured flowrate (Sec. \\hyperref[sec:Experiment_i]{II.A}). We base our model on the $F=0\\rightarrow F'=1$ transition, faithful to blue trapping descriptions of $^{88}$Sr and allowing for a proper treatment of the features related to the magnetic field and light polarization. We build upon the model in Ref. \\cite{2:7} by considering arbitrary beam orientations, although we currently exclude its multiple-scattering effects. The theoretical aspects are detailed in Apps. \\hyperref[sec:Appendix_A1]{A1} and \\hyperref[sec:Appendix_A2]{A2}, where the radiation pressure and the atom lifetime are respectively treated.\\;\\;The\nThe ZS beam enters through a heated sapphire window (at 200$\\,^{\\circ}$C, to prevent deposition of strontium), as seen at the right of Fig. \\ref{fig:1}(a). It has a power of 70 mW (taking into account a non-negligible loss through the window), a $1/e^2$ radius of $4$ mm, and a detuning of $-370$ MHz. The slower design is based on using permanent magnets that are placed inside of cups surrounding opposite sides of the nipple connecting the 2D OM and 2D MOT chambers (50 cm apart) \\cite{41}. It produces a transverse magnetic field ($\\textsf{xy}$ plane) that increases non-monotonically from $-400$ G to $0$ G over the total length of 27 cm. We note that an iron plate attached at the slower exit provides a magnetic shielding of the sensitive end field (mainly against the 2D MOT field).\nTo realize the 2D MOT, we use permanent magnets creating a $67$ G/cm field gradient along the axes of two retroreflected beams with an individual power of 20 mW, a $1/e^2$ radius of $3.2$ mm, and a detuning of $-32$ MHz. The magnets are placed inside of holders [not shown in Fig. \\ref{fig:1}(a)] above and below ($\\textsf{z}$ axis) the jet entrance and exit nipples of the 2D MOT chamber, with the magnetization directions being opposite ($\\textsf{y}$ axis) on the respective nipples. The resulting magnetic field is zero along the axis perpendicular to the main 2D MOT plane ($\\textsf{xy}$ plane) as well as non-cylindrically-symmetric around this axis (unlike in a 3D MOT). The atoms are transferred into the 3D MOT along this axis (Sec. \\hyperref[sec:Experiment_iv]{II.D}).\nThe 3D MOT field is realized with a pair of anti-Helmholtz coils surrounding the rectangular glass cell of the main science region [coils not shown in Fig. \\ref{fig:1}(a); $2\\,\\text{cm}\\,\\times\\,2.5\\,\\text{cm}$ radial ($\\textsf{x}\\times\\textsf{y}$) cell-dimensions], resulting in a cylindrically-symmetric field around the transfer axis ($\\textsf{z}$ axis) at the MOT center ($31.5$ cm below the 2D MOT center). Along this axis, we have 2 independent beams, each with a power of 3.5 mW and a $1/e^2$ radius of $3.2$ mm; one stems from below the 3D MOT with a fixed detuning of $-46$ MHz and the other counter-propagates from above the 2D MOT with a detuning that we vary for testing our loading technique (Sec. \\hyperref[sec:MainResults]{III}). We note that we also consider a special configuration where the bottom beam is slightly misaligned, as discussed later. In the radial plane ($\\textsf{xy}$ plane), there are 2 retroreflected beams with an individual power of 2 mW, a $1/e^2$ radius of $2.4$ mm, and\nFinally, we verify for the 3D MOT side beams their optimal angles with respect to the radial axis perpendicular to the objectives axis. In Fig. \\hyperref[sec:Conclusions]{7}, we display the numerical results for the 3D MOT atom numbers versus the beams angle (mirrored). As can be seen, the $25\\,^{\\circ}$ angle yields the highest number given our experimental constrains (dashed line limit). While larger angles can result in even higher numbers (see right of the dashed line), the beams may be clipped by our objectives [refer to the inset in Fig. \\ref{fig:1}(a)]. The optimum angle is found to be greater, at $30\\,^{\\circ}$, which can be explained to be a combination of a large repumping light volume in the science region (thus bringing many shelved atoms back into the cooling cycle) and the radial trapping forces being closer to perpendicular (thus resulting in more balanced forces). The resulting increase in the atom number is, however, marginal (by ${\\sim}25\\,\\%$), and it is relatively robust against changes within a wide range of angles (from $20\\,^{\\circ}$ to $45\\,^{\\circ}$). Note that the findings here are not universal due to their dependence on the cell geometry. Implicitly, however, we are made aware that the optimum loading is in general not achieved at the standard $90\\,^{\\circ}$ angle between beam a-\nSee Supplemental Material for a video corresponding to the simulation shown in Fig. \\ref{fig:1}(b).\nWe take into account arbitrary beam directions when describing the following two physicals effects: (i) the mean radiation pressure force stemming from a beam, hereafter referred for brevity as the radiation pressure force, (ii) the diffusion resulting from its fluctuations. We discuss these effects in case of a 2D optical molasses (2D OM), a Zeeman slower (ZS), a 2D magneto-optical trap (2D MOT), and a 3D MOT. The corresponding numerical setup is seen in Fig. \\ref{fig:1}(b), for reference.",
        "summarize_figure": "2505.03008v1_Fig1",
        "summarization": "The image shows experimental and numerical setups for strontium atom cooling and trapping. Panel (a) depicts the vacuum chamber with a strontium oven, 2D OM, ZS, 2D MOT, and 3D MOT. Atoms are transversely cooled by 2D OM, longitudinally slowed by ZS, captured by 2D MOT, and transferred to 3D MOT via axial beams. An inset shows the 25 - degree angle for optimal 3D MOT side - beam performance. Panel (b) is the corresponding 3D numerical model with superparticle blue dots and spatial arrangements of trapping stages."
    },
    "104": {
        "figure1": "2505.03008v1_Fig2",
        "label1": "fig:2",
        "caption1": "setup{width=1.49\\linewidth} \\caption{Color online. Diagrams showing the 3D MOT atom numbers (normalized; see text for absolute) using our loading technique connecting the 2D and 3D MOTs via moving molasses, obtained analytically, experimentally, and numerically. The experimental and numerical data points are obtained at the values seen on the axes, with the contours enhancing the features. The vertical dashed line marks the fixed bottom beam detuning ($-46$ MHz), and the slanted dashed line delineates approximately the sloped analytical feature. The cross and the dotted circle highlight respectively the location with the greatest analytical atom number ($44.8$ G/cm gradient at $-7.3$ MHz top beam detuning) and the greatest experimental as well as numerical atom number ($48$ G/cm gradient at $-10$ MHz top beam detuning).",
        "text": "In Fig. \\ref{fig:2}, we compare the 3D MOT loading diagrams obtained analytically, experimentally, and numerically. The parameter space spans over different field gradients and top beam detunings. The shapes enveloping significant atom numbers are observed to take up roughly similar areas in all three cases, with the highest atom numbers (absolute values discussed below) obtained at the same data point locations in the experiment and simulation (dotted circle center, $48$ G/cm gradient at $-10$ MHz top beam detuning), while the analytical model prediction for the highest number is within the point's neighborhood (cross, $44.8$ G/cm gradient at $-7.2$ MHz top beam detuning). The analytical model (Sec. \\hyperref[sec:MainResults_i]{III.A}) can be used to explain the different features in these diagrams as follows.\nRegarding the bright isle with the highest atom numbers, its location depends on the MOT confinement and the dark-state loss. The former expresses itself in the capture probability (see the error functions in Eq. \\hyperref[eq:PO]{1a}) and the relative positioning of the radial beams (see the Gaussian in Eq. \\hyperref[eq:PX]{1b}), while the latter is encapsulated by the lifetime in the trap (see $\\tau_\\ominus$ Eq. \\hyperref[eq:N]{1}). We note that removing the lifetime restriction results in no clear localization of the highest atom numbers, as shown in Fig. \\hyperref[fig:AppB]{B1} with an extended parameter space (compared to Fig. \\ref{fig:2}). The localization must otherwise be present as the trap lifetime becomes shorter for larger axial beam detuning imbalances and higher gradients in this considered space. We are therefore provided evidence that the dark-state loss is of fundamental importance for our observations.\nWe next compare the atom numbers. In the experiment, the highest atom number obtained is ${\\sim}2.3\\times10^5$ atoms with the loading rate of $1.5\\times10^6$ atoms/s, which is expectedly bounded by that of the 2D MOT ($1.5\\times10^9$ atoms/s). A same-order-of-magnitude value for the 3D MOT loading has been reported in literature for a setup employing a push beam for comparable experimental parameters \\cite{42}; nonetheless, further optimizations of our loading rate can be made as discussed later (Secs. \\hyperref[sec:MainResults_iii]{III.D} and \\hyperref[sec:Improvements]{IV}). By using the experimental flowrate in the analytical model's Eq. \\hyperref[eq:N]{1} ($\\mathcal{F}_0\\times\\mathcal{P}_\\oplus\\times\\mathcal{P}_\\ominus=1.5\\times10^6$ atoms/s), we obtain ${\\sim}1.6\\times10^5$ atoms at the peak location (cross in Fig. \\ref{fig:2}), being in close agreement with the experiment (by $30\\,\\%$) and telling that our lifetime estimate reflects well the reality. Regarding the simulation, it predicts the maximum of ${\\sim}3\\times10^4$ atoms, which\\;\\;is\\;\\;roughly\\;\\;8\\;\\;times\nAs implied by our analytical model (Sec. \\hyperref[sec:MainResults_i]{III.A}), the 3D MOT loading efficiency critically depends on the dispersion the atoms accumulate (due to diffusion) during their transfer from the 2D MOT. This dispersion is statistically diminished by the metastable-state shelving (refer to the definition of $\\sigma_r$ in Sec. \\hyperref[sec:MainResults_i]{III.A}) but cannot be completely eliminated. It is thus natural to seek a configuration where the transfer speed is increased and the dispersion is kept low. We experimentally demonstrate this by misaligning the bottom beam, leading to a radiation-pressure imbalance without enhanced scattering. Using such a configuration, we report observing an increase in the experimental atom number (at the encircled point in Fig. \\ref{fig:2}) by roughly twice (to ${\\sim}4.6\\times10^5$ atoms) with an approximately three-fold loading rate improvement (to ${\\sim}4.5\\times10^6$ atoms/s). For this, the bottom beam misalignment of ${\\sim}50$ mrad at the MOT center is used, with higher offset values resulting in an empty MOT. Moreover, we report that the radiation-pressure imbalance achieved using this special configuration makes it possible to trap with equally detuned axial beams, as our simulations also",
        "summarize_figure": "2505.03008v1_Fig2",
        "summarization": "The image compares 3D MOT atom number distributions derived analytically, experimentally, and numerically as functions of top beam detuning (MHz) and MOT magnetic field gradient (G/cm). Color indicates normalized atom number. All three plots reveal similar trapping regions, with peak atom densities forming a bright island at stronger gradients and negative detuning. Experimental and numerical maxima align at approximately 48 G/cm and −10 MHz (circled), while the analytical peak appears near 44.8 G/cm and −7.3 MHz (marked by a cross). Dashed lines indicate fixed bottom beam detuning and the slope predicted analytically. The results show strong agreement across methods, with experiments and simulations closely matching and analytical estimates capturing the general trend."
    },
    "105": {
        "figure1": "2505.03027v1_prelimtask_1",
        "label1": "fig:prelimtask",
        "caption1": "The selection task used in the preliminary study.",
        "text": "This preliminary experiment was conducted in order to test past 3D distal pointing models using modern HWDs that feature higher fidelity displays and tracking systems compared to the systems used in prior distal pointing work. In addition, because our prior experience led us to believe that the $ID_{\\text{DP}}$ model \\cite{kopper_human_2010} had poor performance on very easy distal pointing tasks, such as those found in many AR/VR menu interfaces, we wanted to gather data on such tasks. To accomplish this, the selection task utilized in the study recreated the ISO 9241-411 standard selection task as shown in Figure \\ref{fig:prelimtask}. By utilizing the ISO standard selection task, the results from our study can be compared to other previous distal pointing work that also utilizes the ISO standard selection task.",
        "summarize_figure": "2505.03027v1_prelimtask_1",
        "summarization": "The image shows the interface used for the selection task in the preliminary study, based on the ISO 9241-411 standard. Twelve circular targets are arranged evenly in a ring, with one highlighted in red and outlined in yellow to indicate the current target. The rest are blue, representing non-targets. A thin white line extends from the center toward the target, simulating user pointing. The setup allows performance evaluation of distal pointing using modern HWDs under simple conditions."
    },
    "106": {
        "figure1": "2505.03027v1_plot_dp",
        "label1": "fig:IDDPPrelimStudy",
        "caption1": "Linear regression plot using the data from the preliminary study for the $ID_{\\text{DP}}$ model. ",
        "text": "We used data gleaned from our preliminary study and attempted to fit the data to existing distal pointing models. We began by attempting to fit our data to the preferred $ID_{\\text{DP}}$ model by Kopper et al. \\cite{kopper_human_2010}. We found that our data was modeled fairly well with $R^2$ = 84.79. However, we noticed an interesting pattern when looking at the linear regression plot, shown in figure \\ref{fig:IDDPPrelimStudy}. It appeared that the distal pointing tasks with low ID values at the left of the plot were not modeled particularly well at all. We hypothesized that selection tasks with a low ID value should be modeled separately from those tasks with a higher ID value, as we believed that participants could have been completing tasks with low ID values in a purely ballistic manner. The idea of such a two-part model is not unprecedented, with Shoemaker et al. initially proposing a similar approach as a replacement for Fitts' law in modeling user selection performance on 2D displays where they found that two-part models more accurately modeled distal pointing performance when compared to one-part models.\\cite{shoemaker}. Two-part models have been tested in other works and have been shown to increase model fit when compared to one-part models\\cite{JanzenTwoPart, Sindhupathiraja_2024}. Similarly, Gan \\& Hoffman found that Fitts' law could be less accurate when describing movements with ID smaller than 3 \\cite{gan_geometrical_1988-2}. Schuetz found that when ID is smaller than 1.4, target selection time using gaze becomes close to a constant value \\cite{schuetz_explanation_2019}. We hypothesize that the same could apply to distal pointing scenarios with HWDs.",
        "summarize_figure": "2505.03027v1_plot_dp",
        "summarization": "The image shows a linear regression plot based on preliminary study data using the ID_DP model for distal pointing. The x-axis represents the index of difficulty (ID), and the y-axis shows the average selection time. Blue dots indicate empirical data, and the blue line is the regression fit. With an R² of 80.38%, the model fits well overall, especially for higher ID values. However, data points with lower IDs deviate from the line, suggesting reduced model accuracy in low-difficulty conditions. This supports the hypothesis that ballistic-like behavior dominates low-ID tasks, implying the potential advantage of using a two-part model for improved accuracy."
    },
    "107": {
        "figure1": "2505.03027v1_0.32",
        "label1": "fig:splitregressionplot",
        "caption1": "Linear regression plot that shows the left (blue) and right (red) regression models for ID = 0.32",
        "text": "The idea of modeling distal pointing tasks with a low ID value separately was explored further by creating a two-part model in which we found a ``breakpoint'' in the ID values where two separate left and right models could be created for modeling performance in distal pointing tasks. To do this, the data from the preliminary user study was used to calculate three different values of ID using the $ID_{\\text{ANG}}$, $ID_{\\text{ANG}^3}$, and $ID_{\\text{DP}}$ models. We then ordered each of the ID values from least to greatest. We then explored two-part regression models by breaking the data into two pools at a ``breakpoint'' (i.e. an ID breakpoint of 0.16 would mean that the data associated with all IDs less than 0.16 inclusive would be one data pool while the data for all other ID values would be in another pool). The leftmost breakpoint was the third smallest ID value, while the rightmost breakpoint was the third largest ID value, in order to have enough data to perform the linear regression. Table \\ref{BreakpointTableIDDP} shows the results of these models using $ID_{\\text{DP}}$. The L-R column represents the progression through the distinct ID values, the BreakPoint column is the selected breakpoint for that particular analysis, and the Model-L and Model-R columns show the results for the linear regression calculations. As Table \\ref{BreakpointTableIDDP} shows, the 3-15 and 9-9 splits of the data gave us the best results, though neither was particularly good at fitting both parts of the data. Figure \\ref{fig:splitregressionplot} shows the linear regression plot for the 9-9 row in Table \\ref{BreakpointTableIDDP}.",
        "summarize_figure": "2505.03027v1_0.32",
        "summarization": "The image shows a split linear regression analysis using the ID_DP model, correlating average selection time with task difficulty. The x-axis represents ID values from the ID_DP formula, and the y-axis indicates average selection time. Data are split at ID ≈ 0.32, with blue points and a blue regression line for lower IDs (R² = 72.28%), and red points and a red regression line for higher IDs (R² = 91.62%). The higher fit on the right suggests the model better captures user behavior at higher difficulty, while variability in the low-ID region indicates potential non-linearity. This supports modeling distal pointing performance with a two-part approach."
    },
    "108": {
        "figure1": "2505.03028v1_diffe_A_k_075",
        "label1": "fig:diffe_t_A",
        "caption1": "Representation of $\\overline{A}(\\sigma)$ for different orbital eccentricities and for one orbit (at $\\sigma=0$ the satellite is at the periapsis). The values of the inertia ratio used are $\\kappa = 0.25$ (top panel), $\\kappa = 0.5$ (central panel), and $\\kappa = 0.75$ (bottom panel).  ",
        "text": "Having the goal of performing an approximate analysis of the coupling effects of spin-orbit dynamics, by considering Table \\ref{tb:rate_Deltat} and Fig. \\ref{fig:diffe_t_A},  we notice that for HEOs ($e \\geq 0.8$), $\\overline{A}(f)$ can be approximated by a pulse. For the mathematical description, we decided to model the gravity-gradient through  Dirac deltas, whose maximum amplitude impulse is given by the parameter $K$: i.e., having a zero value everywhere, except than at the periapsis, where the true anomaly and the adimensional time have values f_p=f_r \\triangleq \\pm 2 r \\pi, \\qquad \\sigma_p=\\sigma_r \\triangleq \\pm 2 r \\pi \\equiv f_r, where the $p$ at the subindex indicates `at periapsis', and $r \\in \\mathbb{N}_0$ (a natural number, including $0$) is used as an index enumerating each orbit; it has been taken into account that the orbital period in adimensional time $\\sigma$ is equal to $2 \\pi$, and therefore it yields $\\sigma_r\\equiv f_r$.",
        "summarize_figure": "2505.03028v1_diffe_A_k_075",
        "summarization": "The image illustrates how the generalized gravity-gradient function Ā varies with dimensionless time σ for inertia ratio k = 0.75 and varying orbital eccentricities. As eccentricity increases from 0.2 to 0.95, the curves show sharper peaks centered at σ = 0, corresponding to periapsis. High-eccentricity orbits (e ≥ 0.8) exhibit narrow and intense spikes, while values elsewhere approach zero. This indicates that Ā behaves like a pulse at periapsis in highly elliptical orbits, justifying its approximation with Dirac delta functions to model strong spin-orbit coupling effects near periapsis."
    },
    "109": {
        "figure1": "2505.03028v1_R_2_ecc_09_k_075",
        "label1": "fig:SOP_DiscMap_R_2",
        "caption1": "Set of initial conditions ($\\alpha_0$, $\\alpha_0'$), represented in white color, for which the in-phase impulse condition (with $\\sin(2\\alpha_{0}) > 0$) is satisfied for two successive periapsis passages ($r=1,2$) for $e= 0.9$ and $\\kappa = 0.25$ (top panel), $\\kappa = 0.5$ (central panel), $\\kappa = 0.75$ (bottom panel). ",
        "text": "Fig. \\ref{fig:SOP_DiscMap_R_2} reports the initial conditions $\\alpha_0, \\alpha_0 '$ fulfilling the in-phase condition for the first two periapsis passages (represented with the white color) that we obtain via a numerical implementation of the DM. For the definition of $K = \\frac{3}{2} \\frac{\\kappa}{(1-e)^3} \\Delta \\sigma$, we select $e=0.9$, and the three values of the inertia ratios $\\kappa = 0.25,~0.5,~0.75$. Since the in-phase condition is periodic of period $\\pi$ in $\\alpha_{r}$, we restrict the numerical investigation for $\\alpha_0 \\in [0,\\pi/2)$. As initial angular velocity, we consider a grid in the interval $\\alpha_0 ' \\in [-1,1]$.  From the figure, we notice that for higher values of the inertia ratios $\\kappa$, the regions of the initial conditions fulfilling the in-phase condition tend to expand. Moreover, for all of the three inertia-ratio values, the regions of the initial conditions not fulfilling the in-phase condition accumulate nearby $\\alpha_0 = \\pi/4$. We emphasize that this behavior is periodic in $\\alpha_0'$.",
        "summarize_figure": "2505.03028v1_R_2_ecc_09_k_075",
        "summarization": "The image shows initial conditions (α₀, α₀') for which the in - phase impulse condition (sin(2α₀)>0) is met for two successive periapsis passages with e = 0.9, R = 2, κ = 0.75. White areas represent satisfying conditions, forming symmetric, periodic bands around α₀≈π/4. Higher κ makes these regions expand, and there's periodicity in α₀', aligning with model predictions."
    },
    "110": {
        "figure1": "2505.03029v1_figure1",
        "label1": "fig:density_variance",
        "caption1": "Global budget of the density fluctuations. Different symbols (colors) correspond to the terms in Eq.~(\\ref{density_variance}). ",
        "text": "We aim to mimic the conditions of a typical discharge, where turbulence emerges at the sharp edge gradients of the SOL. Eq.s~(\\ref{continuity_eq})-(\\ref{vorticity_eq}) are solved in a decaying turbulence regime, with an initial imposition of sharp gradients in both density and temperature. The equilibrium is then perturbed with minimal random fluctuations. Following an initial transient phase, a quasi-steady-state turbulence is established, which persists for an extended period. During this phase, energy is injected from the large-scale shear of density and temperature and subsequently dissipated at very small scales through the viscous terms in the equations. Furthermore, turbulence generated by large-scale gradients propagates outward in the $x$-direction, forming blob-like structures that continuously leave the domain, resembling the bursty behavior of edge turbulence observed in laboratory plasmas \\cite{HidalgoEA03, DevynckEA06, GarciaEA07, DudsonEA08, IonitaEA09, KallmanEA10, SechrestEA11, FasoliEA2010, BoedoEA14}. Within this quasi-homogeneous, steady-state transient, we conduct the primary statistical analysis of the Yaglom-Braginskii law. Notably, our numerical simulations intentionally avoid any external ad-hoc driving mechanisms, ensuring the absence of artificial injection disturbances and enabling a natural validation of the law. \tTo qualitatively characterize the global behavior of the system, we investigated the temporal evolution of the density variance.  The latter can be obtained by the continuity Eq.~(\\ref{continuity_eq}) multiplied by $n$, which after volume averaging becomes \t\tT + F_b + F_\\chi + F_E + F_G + F_\\nu + \\varepsilon = 0. \tThis global conservation law, typical of turbulence analysis \\cite{ManzEA15}, resembles the structure of the Yaglom-Braginskii balance in Eq.~(\\ref{final-law}). } The values of each term in Eq.~(\\ref{density_variance}) are displayed in Figure \\ref{fig:density_variance}. The evolution begins with a rapid increase in the electric field term $F_E$ and the divergence term $F_\\chi$, driven by shear that injects energy into the system. This is followed by the growth of dissipative terms, signaling the onset of the cascade. The most significant fluctuations arise from the boundary flux term $F_b = \\left \\langle \\nabla \\cdot \\left( \\frac{n^2}{2} \\mathbf{u} \\right) \\right \\rangle$, which exhibits strong anti-correlation with the time variation term $T$. These anti-correlated bursts are attributed to events of plasma flux through the open boundaries. For instance, $F_b$ ($T$) peaks around $t = 200$, coinciding with the formation and development of structures across the LCFS, leading to a flux of particle density at the open boundaries, as will be demonstrated later. \tThe dissipation (cascade) rate $\\varepsilon$ gradually increases after $t=200$, after which the quasi-steady-state is achieved. The peak occurs around $t \\sim 450$, coinciding with the formation of the first blob structures in the SOL, which triggers an efficient turbulent cascade toward smaller scales. At this stage, the system is expected to reach a balance between anisotropic energy injection, boundary fluxes at the edges, and the cascade toward small scales. The term $F_\\nu$ remains relatively small but non-zero, as it represents the flux of viscous terms through the boundaries. Additionally, we computed the sum of all terms in Eq.~(\\ref{density_variance}) and confirmed that the balance is achieved, with a small error of (at most) 7\\%, that rapidly converges to zero at longer times. The latter is consistent with the use of finite differences and the fact that during the initial stages of the simulation, the system is still reacting to the initial conditions. \tThe numerical code reproduces the turbulent dynamics of the outer region of fusion devices, as illustrated in Fig.~(\\ref{fig:fields}), which displays 2D shaded contours of the density $n$, temperature $T$, vorticity $\\omega$, and electrostatic potential $\\phi$. Prominent high-density and high-temperature structures are evident, originating near the LCFS at $x=100$, and propagating into the SOL for $x>100$, eventually reaching the near-wall region. These structures exhibit a characteristic mushroom-like shape with elongated tails, consistent with observations in fusion devices \\cite{FurnoEA08, MullerEA09, DIppolitoEA11, DecristoforoEA20, BisaiEA22}. In contrast, the inner region ($x<50$) is dominated by plumes that later evolve into blob-like structures. The electrostatic potential $\\phi$ exhibits a gradient correlated with the presence of polarization and the associated $\\mathbf{E} \\times \\mathbf{B}$ drift. \tBefore focusing on the high-order exact law, it is essential to measure and understand the characteristic scales of turbulence through classical statistical analysis. To this end, we investigate the properties of turbulent density fluctuations by analyzing increments at various spatial lags. Specifically, we evaluate the auto-correlation function and the second-order structure function of the density field. We examine these statistical quantities as functions of the spatial lag $\\mathbf{r}$. This analysis is conducted at a specific time during the steady-state evolution ($t = 1400$) within a restricted domain characterized by fully developed homogeneous turbulence (from $x = 80$ to $x = 300$, in the SOL), thereby excluding the inner and outer boundaries. For consistency, we also performed the analysis at different times and under varying initial conditions, obtaining similar results (see later). \tThe auto-correlation function of the density fluctuations is defined as \t\tC( {\\bf r} ) = \\langle \\tilde n( \\mathbf{x} + \\mathbf{r} ) \\tilde n( \\mathbf{x} ) \\rangle = \\frac{1}{V} \\int \\tilde n( \\mathbf{x} + \\mathbf{r} ) \\tilde n( \\mathbf{x} ) d \\mathbf{x},  \twhere $\\tilde n = n - \\langle n \\rangle$ represents the fluctuations around the mean density. To extract isotropic information about the characteristic length scales, we average the auto-correlation function along $r_x$ and $r_y$, yielding $C(r) = [C(r_x)+C(r_y)]/2$ \\cite{SorrisoEA02} (with $r_x=r_y\\equiv r$). This isotropic correlation function, normalized to the variance $C(0)$, is shown in Figure \\ref{fig:Correlation} and exhibits the typical profile of stochastic fluctuations in turbulence \\cite{Frisch1995}. The normalized function $C(r)/C(0)$ provides insight into the typical size of energy-containing eddies, $\\lambda_c$, which can be determined either by integrating $C(r)/C(0)$ or empirically as its $e$-folding length. For these numerical experiments, as illustrated in the same figure, we measure  \t\t$\\lambda_c \\simeq 34$.  }\t \tThis scale, which qualitatively corresponds to the size of bursty blobs \\cite{ScarivaglioneEA23, ServidioEA08}, is significantly smaller than the domain size, $L_x = L_y = 400$, indicating a state of relatively homogeneous turbulence. In the top-right panel of Figure \\ref{fig:Correlation}, we display the 2D turbulent density pattern, highlighting the scale of $\\lambda_c$.",
        "summarize_figure": "2505.03029v1_figure1",
        "summarization": "The image shows the temporal evolution of terms in the global density fluctuation balance equation as functions of normalized time tω_ci. The time derivative term T exhibits large oscillations, strongly anti-correlated with the boundary flux term Fb, which peaks near t ≈ 200, indicating strong density outflow through open boundaries. The electric field term FE and the gradient term Fχ rise early and then stabilize, reflecting initial energy injection via shear. The dissipation rate ε gradually increases, peaking at t ≈ 450, marking the onset of turbulent cascades due to blob formation. The gravity-related term FG remains nearly constant, and the viscous flux Fν stays small yet non-zero. Together, these terms maintain the conservation balance T + Fb + Fχ + FE + FG + Fν + ε ≈ 0, reflecting the dynamics of quasi-steady turbulent transport."
    },
    "111": {
        "figure1": "2505.03029v1_figure2",
        "label1": "fig:fields",
        "caption1": "Two-dimensional spatial structure of particle density (top-left), temperature (top-right), vorticity (bottom-left), and potential (bottom-right), at  $t=1400$.",
        "text": "We aim to mimic the conditions of a typical discharge, where turbulence emerges at the sharp edge gradients of the SOL. Eq.s~(\\ref{continuity_eq})-(\\ref{vorticity_eq}) are solved in a decaying turbulence regime, with an initial imposition of sharp gradients in both density and temperature. The equilibrium is then perturbed with minimal random fluctuations. Following an initial transient phase, a quasi-steady-state turbulence is established, which persists for an extended period. During this phase, energy is injected from the large-scale shear of density and temperature and subsequently dissipated at very small scales through the viscous terms in the equations. Furthermore, turbulence generated by large-scale gradients propagates outward in the $x$-direction, forming blob-like structures that continuously leave the domain, resembling the bursty behavior of edge turbulence observed in laboratory plasmas \\cite{HidalgoEA03, DevynckEA06, GarciaEA07, DudsonEA08, IonitaEA09, KallmanEA10, SechrestEA11, FasoliEA2010, BoedoEA14}. Within this quasi-homogeneous, steady-state transient, we conduct the primary statistical analysis of the Yaglom-Braginskii law. Notably, our numerical simulations intentionally avoid any external ad-hoc driving mechanisms, ensuring the absence of artificial injection disturbances and enabling a natural validation of the law. \tTo qualitatively characterize the global behavior of the system, we investigated the temporal evolution of the density variance.  The latter can be obtained by the continuity Eq.~(\\ref{continuity_eq}) multiplied by $n$, which after volume averaging becomes \t\tT + F_b + F_\\chi + F_E + F_G + F_\\nu + \\varepsilon = 0. \tThis global conservation law, typical of turbulence analysis \\cite{ManzEA15}, resembles the structure of the Yaglom-Braginskii balance in Eq.~(\\ref{final-law}). } The values of each term in Eq.~(\\ref{density_variance}) are displayed in Figure \\ref{fig:density_variance}. The evolution begins with a rapid increase in the electric field term $F_E$ and the divergence term $F_\\chi$, driven by shear that injects energy into the system. This is followed by the growth of dissipative terms, signaling the onset of the cascade. The most significant fluctuations arise from the boundary flux term $F_b = \\left \\langle \\nabla \\cdot \\left( \\frac{n^2}{2} \\mathbf{u} \\right) \\right \\rangle$, which exhibits strong anti-correlation with the time variation term $T$. These anti-correlated bursts are attributed to events of plasma flux through the open boundaries. For instance, $F_b$ ($T$) peaks around $t = 200$, coinciding with the formation and development of structures across the LCFS, leading to a flux of particle density at the open boundaries, as will be demonstrated later. \tThe dissipation (cascade) rate $\\varepsilon$ gradually increases after $t=200$, after which the quasi-steady-state is achieved. The peak occurs around $t \\sim 450$, coinciding with the formation of the first blob structures in the SOL, which triggers an efficient turbulent cascade toward smaller scales. At this stage, the system is expected to reach a balance between anisotropic energy injection, boundary fluxes at the edges, and the cascade toward small scales. The term $F_\\nu$ remains relatively small but non-zero, as it represents the flux of viscous terms through the boundaries. Additionally, we computed the sum of all terms in Eq.~(\\ref{density_variance}) and confirmed that the balance is achieved, with a small error of (at most) 7\\%, that rapidly converges to zero at longer times. The latter is consistent with the use of finite differences and the fact that during the initial stages of the simulation, the system is still reacting to the initial conditions. \tThe numerical code reproduces the turbulent dynamics of the outer region of fusion devices, as illustrated in Fig.~(\\ref{fig:fields}), which displays 2D shaded contours of the density $n$, temperature $T$, vorticity $\\omega$, and electrostatic potential $\\phi$. Prominent high-density and high-temperature structures are evident, originating near the LCFS at $x=100$, and propagating into the SOL for $x>100$, eventually reaching the near-wall region. These structures exhibit a characteristic mushroom-like shape with elongated tails, consistent with observations in fusion devices \\cite{FurnoEA08, MullerEA09, DIppolitoEA11, DecristoforoEA20, BisaiEA22}. In contrast, the inner region ($x<50$) is dominated by plumes that later evolve into blob-like structures. The electrostatic potential $\\phi$ exhibits a gradient correlated with the presence of polarization and the associated $\\mathbf{E} \\times \\mathbf{B}$ drift. \tBefore focusing on the high-order exact law, it is essential to measure and understand the characteristic scales of turbulence through classical statistical analysis. To this end, we investigate the properties of turbulent density fluctuations by analyzing increments at various spatial lags. Specifically, we evaluate the auto-correlation function and the second-order structure function of the density field. We examine these statistical quantities as functions of the spatial lag $\\mathbf{r}$. This analysis is conducted at a specific time during the steady-state evolution ($t = 1400$) within a restricted domain characterized by fully developed homogeneous turbulence (from $x = 80$ to $x = 300$, in the SOL), thereby excluding the inner and outer boundaries. For consistency, we also performed the analysis at different times and under varying initial conditions, obtaining similar results (see later). \tThe auto-correlation function of the density fluctuations is defined as \t\tC( {\\bf r} ) = \\langle \\tilde n( \\mathbf{x} + \\mathbf{r} ) \\tilde n( \\mathbf{x} ) \\rangle = \\frac{1}{V} \\int \\tilde n( \\mathbf{x} + \\mathbf{r} ) \\tilde n( \\mathbf{x} ) d \\mathbf{x},  \twhere $\\tilde n = n - \\langle n \\rangle$ represents the fluctuations around the mean density. To extract isotropic information about the characteristic length scales, we average the auto-correlation function along $r_x$ and $r_y$, yielding $C(r) = [C(r_x)+C(r_y)]/2$ \\cite{SorrisoEA02} (with $r_x=r_y\\equiv r$). This isotropic correlation function, normalized to the variance $C(0)$, is shown in Figure \\ref{fig:Correlation} and exhibits the typical profile of stochastic fluctuations in turbulence \\cite{Frisch1995}. The normalized function $C(r)/C(0)$ provides insight into the typical size of energy-containing eddies, $\\lambda_c$, which can be determined either by integrating $C(r)/C(0)$ or empirically as its $e$-folding length. For these numerical experiments, as illustrated in the same figure, we measure  \t\t$\\lambda_c \\simeq 34$.  }\t \tThis scale, which qualitatively corresponds to the size of bursty blobs \\cite{ScarivaglioneEA23, ServidioEA08}, is significantly smaller than the domain size, $L_x = L_y = 400$, indicating a state of relatively homogeneous turbulence. In the top-right panel of Figure \\ref{fig:Correlation}, we display the 2D turbulent density pattern, highlighting the scale of $\\lambda_c$.",
        "summarize_figure": "2505.03029v1_figure2",
        "summarization": "The image illustrates the spatial distribution of plasma quantities at t = 1400 in a quasi-steady turbulent regime. Both particle density n and temperature T show mushroom-shaped coherent structures originating near x = 100 and propagating outward, indicating strong turbulence with nonlinearly evolved blobs. The vorticity ω reveals several dipolar eddies, characteristic of shear-driven turbulence. The electrostatic potential φ displays a smooth gradient along x, correlated with E × B drift and the underlying polarization field. These fields collectively highlight strong coupling between density, temperature, and vorticity, and the role of background potential in blob dynamics."
    },
    "112": {
        "figure1": "2505.03029v1_figure3",
        "label1": "fig:Correlation",
        "caption1": "Normalized auto-correlation function $C(r)/C(0)$, evaluated at $t = 1400$, as a function of the increment $r$. The correlation length $\\lambda_c$, defined as the $e$-folding distance of $C(r)/C(0)$, is indicated by a vertical dashed line. In the top-right inset, the density contours are displayed, featuring a characteristic blob structure.",
        "text": "We aim to mimic the conditions of a typical discharge, where turbulence emerges at the sharp edge gradients of the SOL. Eq.s~(\\ref{continuity_eq})-(\\ref{vorticity_eq}) are solved in a decaying turbulence regime, with an initial imposition of sharp gradients in both density and temperature. The equilibrium is then perturbed with minimal random fluctuations. Following an initial transient phase, a quasi-steady-state turbulence is established, which persists for an extended period. During this phase, energy is injected from the large-scale shear of density and temperature and subsequently dissipated at very small scales through the viscous terms in the equations. Furthermore, turbulence generated by large-scale gradients propagates outward in the $x$-direction, forming blob-like structures that continuously leave the domain, resembling the bursty behavior of edge turbulence observed in laboratory plasmas \\cite{HidalgoEA03, DevynckEA06, GarciaEA07, DudsonEA08, IonitaEA09, KallmanEA10, SechrestEA11, FasoliEA2010, BoedoEA14}. Within this quasi-homogeneous, steady-state transient, we conduct the primary statistical analysis of the Yaglom-Braginskii law. Notably, our numerical simulations intentionally avoid any external ad-hoc driving mechanisms, ensuring the absence of artificial injection disturbances and enabling a natural validation of the law. \tTo qualitatively characterize the global behavior of the system, we investigated the temporal evolution of the density variance.  The latter can be obtained by the continuity Eq.~(\\ref{continuity_eq}) multiplied by $n$, which after volume averaging becomes \t\tT + F_b + F_\\chi + F_E + F_G + F_\\nu + \\varepsilon = 0. \tThis global conservation law, typical of turbulence analysis \\cite{ManzEA15}, resembles the structure of the Yaglom-Braginskii balance in Eq.~(\\ref{final-law}). } The values of each term in Eq.~(\\ref{density_variance}) are displayed in Figure \\ref{fig:density_variance}. The evolution begins with a rapid increase in the electric field term $F_E$ and the divergence term $F_\\chi$, driven by shear that injects energy into the system. This is followed by the growth of dissipative terms, signaling the onset of the cascade. The most significant fluctuations arise from the boundary flux term $F_b = \\left \\langle \\nabla \\cdot \\left( \\frac{n^2}{2} \\mathbf{u} \\right) \\right \\rangle$, which exhibits strong anti-correlation with the time variation term $T$. These anti-correlated bursts are attributed to events of plasma flux through the open boundaries. For instance, $F_b$ ($T$) peaks around $t = 200$, coinciding with the formation and development of structures across the LCFS, leading to a flux of particle density at the open boundaries, as will be demonstrated later. \tThe dissipation (cascade) rate $\\varepsilon$ gradually increases after $t=200$, after which the quasi-steady-state is achieved. The peak occurs around $t \\sim 450$, coinciding with the formation of the first blob structures in the SOL, which triggers an efficient turbulent cascade toward smaller scales. At this stage, the system is expected to reach a balance between anisotropic energy injection, boundary fluxes at the edges, and the cascade toward small scales. The term $F_\\nu$ remains relatively small but non-zero, as it represents the flux of viscous terms through the boundaries. Additionally, we computed the sum of all terms in Eq.~(\\ref{density_variance}) and confirmed that the balance is achieved, with a small error of (at most) 7\\%, that rapidly converges to zero at longer times. The latter is consistent with the use of finite differences and the fact that during the initial stages of the simulation, the system is still reacting to the initial conditions. \tThe numerical code reproduces the turbulent dynamics of the outer region of fusion devices, as illustrated in Fig.~(\\ref{fig:fields}), which displays 2D shaded contours of the density $n$, temperature $T$, vorticity $\\omega$, and electrostatic potential $\\phi$. Prominent high-density and high-temperature structures are evident, originating near the LCFS at $x=100$, and propagating into the SOL for $x>100$, eventually reaching the near-wall region. These structures exhibit a characteristic mushroom-like shape with elongated tails, consistent with observations in fusion devices \\cite{FurnoEA08, MullerEA09, DIppolitoEA11, DecristoforoEA20, BisaiEA22}. In contrast, the inner region ($x<50$) is dominated by plumes that later evolve into blob-like structures. The electrostatic potential $\\phi$ exhibits a gradient correlated with the presence of polarization and the associated $\\mathbf{E} \\times \\mathbf{B}$ drift. \tBefore focusing on the high-order exact law, it is essential to measure and understand the characteristic scales of turbulence through classical statistical analysis. To this end, we investigate the properties of turbulent density fluctuations by analyzing increments at various spatial lags. Specifically, we evaluate the auto-correlation function and the second-order structure function of the density field. We examine these statistical quantities as functions of the spatial lag $\\mathbf{r}$. This analysis is conducted at a specific time during the steady-state evolution ($t = 1400$) within a restricted domain characterized by fully developed homogeneous turbulence (from $x = 80$ to $x = 300$, in the SOL), thereby excluding the inner and outer boundaries. For consistency, we also performed the analysis at different times and under varying initial conditions, obtaining similar results (see later). \tThe auto-correlation function of the density fluctuations is defined as \t\tC( {\\bf r} ) = \\langle \\tilde n( \\mathbf{x} + \\mathbf{r} ) \\tilde n( \\mathbf{x} ) \\rangle = \\frac{1}{V} \\int \\tilde n( \\mathbf{x} + \\mathbf{r} ) \\tilde n( \\mathbf{x} ) d \\mathbf{x},  \twhere $\\tilde n = n - \\langle n \\rangle$ represents the fluctuations around the mean density. To extract isotropic information about the characteristic length scales, we average the auto-correlation function along $r_x$ and $r_y$, yielding $C(r) = [C(r_x)+C(r_y)]/2$ \\cite{SorrisoEA02} (with $r_x=r_y\\equiv r$). This isotropic correlation function, normalized to the variance $C(0)$, is shown in Figure \\ref{fig:Correlation} and exhibits the typical profile of stochastic fluctuations in turbulence \\cite{Frisch1995}. The normalized function $C(r)/C(0)$ provides insight into the typical size of energy-containing eddies, $\\lambda_c$, which can be determined either by integrating $C(r)/C(0)$ or empirically as its $e$-folding length. For these numerical experiments, as illustrated in the same figure, we measure  \t\t$\\lambda_c \\simeq 34$.  }\t \tThis scale, which qualitatively corresponds to the size of bursty blobs \\cite{ScarivaglioneEA23, ServidioEA08}, is significantly smaller than the domain size, $L_x = L_y = 400$, indicating a state of relatively homogeneous turbulence. In the top-right panel of Figure \\ref{fig:Correlation}, we display the 2D turbulent density pattern, highlighting the scale of $\\lambda_c$.",
        "summarize_figure": "2505.03029v1_figure3",
        "summarization": "The image shows the normalized auto-correlation function C(r)/C(0) of density fluctuations at t = 1400, revealing a monotonic decay with spatial lag r. The correlation length λ_c, defined as the e-folding scale where C(r)/C(0) ≈ exp(−1), is approximately 34. This indicates the typical size of energy-containing eddies. The inset displays a 2D density field with blob-like structures, visually confirming the scale λ_c. This length is significantly smaller than the system size (400), indicating spatially homogeneous turbulence and providing a key reference for energy transfer scales."
    },
    "113": {
        "figure1": "2505.03032v1_Weibull_ER_Rho_COV10",
        "label1": "fig:ER_Nse_Weibull",
        "caption1": "\\ac{MRT} as a function of the system load. \\textbf{Note:} \\ac{COV} = 10 and n = 10.}  \\vspace{-0.325cm",
        "text": "One can see that each policy’s performance varies smoothly with \\(\\rho\\), with \\ac{RR} (red) generally yielding the highest mean response times at moderate to high loads, and \\ac{CARD} (cyan) consistently remaining the best-performing single-stage policy across the entire load range. \\Cref{fig:ER_Nse_Weibull} illustrates that the optimized two-stage \\ac{RR} variant (dashed red) significantly narrows the gap between single-stage \\ac{RR} and more sophisticated policies, highlighting how selectively separating large jobs can substantially improve overall performance.  Nevertheless, at extremely high loads close to \\(\\rho = 1\\), the advantage of knowing job sizes (\\ac{LWL}) or at least the state of the server (\\ac{JIQ}) still emerges in slightly lower response times.",
        "summarize_figure": "2505.03032v1_Weibull_ER_Rho_COV10",
        "summarization": "The image shows the normalized mean response time E[R]/E[R_MG1] versus system load ρ for various load balancing strategies using a log-scale y-axis. RR exhibits the highest delay, especially under heavy load, while CARD consistently performs best across all ρ. Two-stage variants, particularly Two-Stage RR, significantly reduce response times compared to their single-stage counterparts. JIQ and LWL offer moderate performance, and their two-stage versions improve further. The purple curve shows the theoretical lower bound. Results indicate that job size or server state awareness enhances performance near saturation."
    },
    "114": {
        "figure1": "2505.03032v1_Weibull_ER_Nse_COV10",
        "label1": "fig:ER_Rho_Weibull",
        "caption1": "\\ac{MRT} as a function of the number of servers. \\textbf{Note:} \\ac{COV} = 10 and $\\rho = 0.8$.}  \\vspace{-0.325cm",
        "text": "In \\Cref{fig:ER_Rho_Weibull}, we fix \\(\\rho = 0.8\\) and vary the number of servers \\(n\\) to clearly illustrate the impact of server parallelism. When \\(n\\) is small, the two-stage policies perform poorly; however, increasing the server count to around 10--20 dramatically reduces the mean response time.  In realistic scenarios, where one does not employ an extremely small number of servers (which would require each server to be exceptionally powerful to maintain a constant total capacity), the \\ac{RR} policy, in particular, fails to achieve satisfactory performance. Eventually, when \\(n\\) is very large, the lines tend to converge, indicating that high parallelism weakens the distinction among policies.  In all configurations, \\ac{CARD} continues to demonstrate the best single-stage performance, while the threshold-optimized two-stage algorithms remain close behind, underscoring the effectiveness of migrating heavier jobs to a second stage to shield latency-sensitive, shorter jobs from undue delays.",
        "summarize_figure": "2505.03032v1_Weibull_ER_Nse_COV10",
        "summarization": "The image shows normalized mean response time E[R]/E[R_MG1] versus the number of servers n under load ρ = 0.8 and coefficient of variation 10. When n is small, two-stage policies perform poorly but improve rapidly for n between 10 and 20. CARD consistently achieves the lowest response time among single-stage policies. RR performs worst, especially as n increases. Two-stage JIQ and LWL nearly reach the theoretical lower bound, showing strong delay control. As n grows, differences among policies diminish due to parallelism, but CARD and optimized two-stage strategies remain near-optimal."
    },
    "115": {
        "figure1": "2505.03032v1_Google_ER_Rho",
        "label1": "fig:Google_ER_Rho",
        "caption1": "\\ac{MRT} as a function of the system load. Google data has been used including the task-level data structure. \\textbf{Note:} $n = 10$}  \\vspace{-0.325cm",
        "text": "Focusing first on \\Cref{fig:Google_ER_Rho}, it is apparent that exploiting state or size information makes little difference among \\ac{RR}, \\ac{JIQ}, and \\ac{LWL}, while changing the architecture of the server cluster from one to two-stage brings about a major improvement of mean response time performance. It is however remarkable that a good part of the performance advantage gained by \\ac{CARD} over single-stage policies can be reaped by adopting the baseline \\ac{RR} policy but moving to two-stage architecture.",
        "summarize_figure": "2505.03032v1_Google_ER_Rho",
        "summarization": "The image shows mean response time E[R] (in hours) versus system load ρ for n = 10 servers, comparing seven load balancing strategies using a logarithmic scale. As load increases from 0.65 to 1.0, all strategies experience rising delays, but CARD consistently achieves the lowest response times. RR performs the worst, especially under heavy load. Introducing two-stage architectures to RR, JIQ, and LWL significantly improves their performance, with Two-Stage RR closing much of the gap with CARD. The results indicate that architectural changes offer greater benefits than merely exploiting state or size information."
    },
    "116": {
        "figure1": "2505.03034v1_SAREvd-Kappa2-0.1",
        "label1": "fig:1",
        "caption1": "Expanding a Gaussian SAR process to a Non-Gaussian SAR process using Gumbel\\((0,1)\\) and GEV\\(\\bigl(1,\\xi,\\xi\\bigr)\\) innovations \\(\\bigl(\\xi>0\\bigr)\\) to generate spatial extreme fields. A fixed uniform sample is used, and then transformed for different scenarios, and the resulting spatial fields are subsequently scaled to the range \\([0,1]\\).",
        "text": "g(\\bm{s}) = \\sum_{j=1}^{m} \\varphi_j(\\bm{s}) c_j, where $\\varphi_j(\\bm{s})$ are fixed radial basis functions (e.g., Wendland functions \\citep{wendland1995, wendland1998}), $m$ represents the number of basis functions, and $c_j$ are random coefficients.  We further model the coefficient vector as $B\\bm{c} = \\bm{e}$ where $\\bm{e}$ follow a GEV distribution. In this model, the shape parameter $\\xi$ of the GEV distribution controls the heavy-tailed behavior of the spatial process, while spatial dependence is incorporated through the SAR matrix $B$, which is tri-diagonal with $B_{j,j} = \\kappa^2$, $B_{j,j+1} = -1$, and $B_{j+1,j} = -1$, generalizing a similar construction for Gaussian process modeling \\citep{nychka2015multiresolution}, where $\\kappa^2$ controls the spatial dependence.   We assume that $\\bm{e} \\overset{\\text{iid}}{\\sim} \\text{GEV}(\\mu, \\sigma, \\xi)$, where we set $\\mu = 1$ and $\\sigma = \\xi > 0$, allowing $\\xi$ to vary. Fixing $\\mu = 1$ and $\\sigma = \\xi > 0$ ensures that the distribution corresponds to the standard Fréchet form, with the distribution standardized to have location 0 and scale 1 after transformation. Figure~\\ref{fig:1} shows the transformation of a spatial field from a Gaussian process to a non-Gaussian spatial extremes field with fixed $\\kappa^2=0.1$, under varying GEV shape parameters. As illustrated in Figure~\\ref{fig:1}, the spatial field becomes increasingly extreme as the shape parameter increases from $\\xi = 0$ to $\\xi = 1$, with significant changes in spatial dependence patterns compared to the Gaussian case. It is also important to note that, for a given set of parameters, the simulation of these fields is highly efficient due to the strategic use of sparsity in the key matrices involved in the construction.\nFinally, we create 50,000 parameter configurations for training and validation, sampling \\(\\xi \\in (0.01, 0.9)\\), \\(\\kappa^2 \\in (0.001, 2)\\), and \\(\\tau^2 \\in (0.0001, 0.1)\\) uniformly, as shown in Figure~\\ref{fig:parameterConfig}.  The ranges for \\(\\xi\\) and \\(\\kappa^2\\) are guided by our initial study, which shows that \\(\\xi\\) close to 1 introduces an extreme scenario, and \\(\\kappa^2\\) close to 2 implies weak spatial dependence. The nugget variability \\(\\tau^2\\) spans 0.01\\% to 10\\% of the spatial-field noise, as we are working in a multiplicative setup. Requiring \\(\\xi \\in (0.01, 0.9)\\) guarantees that the simulated fields exhibit a range of extremes, from none to more pronounced as \\(\\xi\\) approaches 1 (see Figure~\\ref{fig:1}).",
        "summarize_figure": "2505.03034v1_SAREvd-Kappa2-0.1",
        "summarization": "The figure shows spatial fields generated under fixed spatial dependence κ² = 0.1, using Gaussian, Gumbel, and GEV innovations with varying shape parameters ξ. The top row illustrates Gaussian, Gumbel, and low-ξ GEV fields, exhibiting smooth, correlated structures. The bottom row presents fields with increasing ξ from 0.5 to 1, where high values become sparse and isolated, while the background flattens. As ξ increases, spatial extremity intensifies, and continuity weakens. The results demonstrate how the GEV shape parameter governs the transition from smooth Gaussian-like fields to sparse, extreme-valued spatial structures."
    },
    "117": {
        "figure1": "2505.03034v1_SARextremes-VaryingTaus",
        "label1": "fig:SARII",
        "caption1": "Spatial fields under fixed \\(\\xi = 0.5\\) and \\(\\kappa^2 = 0.1\\) with varying nugget variance \\(\\tau^2\\), ranging from $0.01\\%$ to $10\\%$ of the total variability. This highlights the influence of the nugget term on the spatial field.",
        "text": "To illustrate the effect of the nugget term in the model and justify the choice of its distribution, we fixed the shape and spatial dependence parameters, $\\xi$ and $\\kappa^2$, in the $g(\\bm{s})$ representation and varied the nugget parameter $\\tau^2$ to generate $\\varepsilon$'s. In Figure~\\ref{fig:SARII}, we present the simulated spatial fields $y=g(\\bm{s})\\cdot \\varepsilon$ for fixed $\\xi=0.5$ and $\\kappa^2=0.1$, with $\\tau^2$ varying from 0.01\\% to 10\\%, to show the influence of the nugget term.",
        "summarize_figure": "2505.03034v1_SARextremes-VaryingTaus",
        "summarization": "The image shows spatial fields under fixed ξ = 0.5 and κ² = 0.1 with increasing nugget variance τ² from 0.001 to 0.1. As τ² increases, the spatial fields transition from smooth, structured patterns to noisy, less correlated forms. At τ² = 0.001, strong spatial coherence is evident; at τ² = 0.01, partial structure remains with added noise; and at τ² = 0.1, the field resembles spatial white noise, dominated by random variation. This highlights the critical role of the nugget effect in disrupting spatial dependence."
    },
    "118": {
        "figure1": "2505.03034v1_Parameter-Configuration-TrainingCNN",
        "label1": "fig:parameterConfig",
        "caption1": "Parameter configurations are generated for training and validating the CNN model. The spatial parameters $\\kappa^2$ and $\\tau^2$ are generated on the log scale.",
        "text": "Finally, we create 50,000 parameter configurations for training and validation, sampling \\(\\xi \\in (0.01, 0.9)\\), \\(\\kappa^2 \\in (0.001, 2)\\), and \\(\\tau^2 \\in (0.0001, 0.1)\\) uniformly, as shown in Figure~\\ref{fig:parameterConfig}.  The ranges for \\(\\xi\\) and \\(\\kappa^2\\) are guided by our initial study, which shows that \\(\\xi\\) close to 1 introduces an extreme scenario, and \\(\\kappa^2\\) close to 2 implies weak spatial dependence. The nugget variability \\(\\tau^2\\) spans 0.01\\% to 10\\% of the spatial-field noise, as we are working in a multiplicative setup. Requiring \\(\\xi \\in (0.01, 0.9)\\) guarantees that the simulated fields exhibit a range of extremes, from none to more pronounced as \\(\\xi\\) approaches 1 (see Figure~\\ref{fig:1}).",
        "summarize_figure": "2505.03034v1_Parameter-Configuration-TrainingCNN",
        "summarization": "The image shows histograms of parameter configurations for training and validating a CNN model, covering shape parameter ξ, spatial dependence κ², and nugget variance τ². ξ is nearly uniformly distributed over (0.01, 0.9), ensuring diverse extreme behaviors. The 4 + κ² histogram is right-skewed, indicating most values reflect strong spatial dependence. τ² is concentrated at low values, suggesting limited nugget effect in most samples. These configurations span a wide range of spatial characteristics for robust model generalization."
    },
    "119": {
        "figure1": "2505.03042v1_domain_manip_illust",
        "label1": "fig:domain_manip_illust",
        "caption1": "Domain operations on 1D/2D inputs.} \\end{subfigure}\\hfill \\begin{subfigure}{.475\\linewidth} \\includegraphics[width=\\linewidth]{imgs/domain_manip_illust.png} \\subcaption{Effects of domain manipulation on network inputs.} \\end{subfigure} \\caption{Illustration of domain manipulation at different input and feature dimensions.",
        "text": "The idea of domain manipulation extends naturally to higher dimensions and unlocks more flexibility in manipulating the MLP domain as each piecewise linear segment has exponentially more boundary vertices. For instance, when $N = F = 2$, the original scale-up/down operations are converted into a more expressive version of \\textbf{sheering}, and the flipping operation splits into \\textbf{surface flipping} and \\textbf{surface twisting}. Some examples of this are visualized in Fig \\ref{fig:domain_manip_illust} (a). These additional operations create more options to repeatedly utilize the linear segments of the MLP, deeming the grid a more effective method to increase the general expressivity of the overall network. the addition of operations such as twisting also reduces the ``rate'' and ``area'' at which segments would have to overlap, which reduces the negative effects of the two aforementioned restrictions.\nHowever, it is only when we increase the feature dimension $F$ to greater than $N$ that these constraints are completely alleviated as it removes the necessity of having overlapping MLP segments to save expressivity. As shown in Fig \\ref{fig:domain_manip_illust}(b), a grid with 1-dimensional input traces a piecewise linear path on a 2-dimensional manifold. When $F > 1$, each segment will rarely intersect with another segment entirely but rather would intersect at most at a single point. In fact, for any $N$-dimensional input, as long as $F > N$, segment overlaps would rarely occur.",
        "summarize_figure": "2505.03042v1_domain_manip_illust",
        "summarization": "The image illustrates spatial domain manipulations of neural network inputs across different input and feature dimensions. It shows how 1D inputs can be transformed via operations like stretching or flipping, altering their geometric relations. In higher dimensions (F > N), input segments are less likely to overlap, reducing constraints and enhancing expressivity. Complex transformations such as sheering and twisting are introduced to more effectively utilize linear segments, allowing richer mappings while preserving input continuity."
    },
    "120": {
        "figure1": "2505.03042v1_visualization",
        "label1": "fig:grid_1d",
        "caption1": "Visualizing the effects of a 1-dimensional, single-resolution, no-hashing grid when fitting to a 1-dimensional signal.",
        "text": "We build our analysis of NGP incrementally and start with the bare-bones effects of a 1-dimensional, single-resolution, no-hashing feature grid on a 1-dimensional signal. The grid partitions the 1-dimensional input domain into equal-lengthed segments, where the endpoints of each segment (i.e. vertices of the grid) get remapped to a new value in the MLP's domain. The interior of the segments is interpolated linearly between the values of the endpoints. \\textbf{The mapping of the grid could hence be understood as two orthogonal operations on the input domain: (1) scaling up/down; (2) ``flipping''.} Take the visualization in Fig \\ref{fig:grid_1d} as an example, where the domain $[0,1]$ is partitioned into 5 equal parts. The red line in the right figure is the function that the MLP is representing, while the blue line is a visualization of the grid values, with the $x$-axis being the MLP domain and the left $y$-axis being the input domain. The first segment $[0,0.2]$ is an example of \\textbf{flipping}, where the domain gets remapped to start at $\\sim0.45$ and ends at $\\sim-0.20$. On the other hand, the second and third segments are examples of \\textbf{scaling down} where the segment $[0.2, 0.6]$ gets mapped to a very narrow domain between $[-0.2, 0.0]$.",
        "summarize_figure": "2505.03042v1_visualization",
        "summarization": "The image illustrates the modeling behavior of a neural network using a one-dimensional, single-resolution, no-hashing grid. The left plot shows close alignment between model predictions and the target signal, indicating effective fitting. The right plot visualizes how input segments are remapped into the MLP domain, with some segments undergoing flipping or scaling. The blue curve shows the grid's input remapping, while the red curve reflects the MLP function output. This demonstrates how even simple grid structures enable nonlinear reparameterization for improved expressivity."
    },
    "121": {
        "figure1": "2505.03045v1_Presentation1",
        "label1": "fig:whole_spectrum",
        "caption1": "The experimentally observed spectrum (in red) and the simulated spectrum (in black) showing the (0,0)$A^2\\Pi$~--~$X^2\\Sigma^+$ and (0,0)$A^2\\Pi$~--~$X^2\\Sigma^+$ bands.",
        "text": "The experimental and simulated spectra are shown in Figure \\ref{fig:whole_spectrum}. The input data set consisted of 144, 21, and 72 lines for the \\ce{^40CaF}, \\ce{^42CaF}, and \\ce{^44CaF} isotopologues, respectively (Table~\\ref{tbl:properties}). Since our primary goal was to investigate the transitions of less-abundant isotopologues, we targeted specific regions of the spectrum where \\ce{^42CaF} and \\ce{^44CaF} lines were not heavily overlapped with \\ce{^40CaF} lines. As a result, the full $A^2\\Pi$~--~$X^2\\Sigma^+$ band was not recorded. The unblended spectra features have a full width at half maximum of approximately 40 MHz.",
        "summarize_figure": "2505.03045v1_Presentation1",
        "summarization": "The image compares experimentally observed (red) and PGOPHER - simulated (black) spectra for (0,0) transitions of A²Π₁/₂ → X²Σ⁺ and A²Π₃/₂ → X²Σ⁺. Highlighted regions around 16480–16510 cm⁻¹ and 16555–16585 cm⁻¹ show alignment between observed and simulated peaks, with some discrepancies in zoomed insets. Targeting unblended lines of ⁴²CaF and ⁴⁴CaF to avoid ⁴⁰CaF overlap led to partial recording of the full A²Π - X²Σ⁺ band. Observed linewidths are about 40 MHz, indicating high resolution."
    },
    "122": {
        "figure1": "2505.03049v1_overview-figure",
        "label1": "fig:overview",
        "caption1": "The LLM-Powered Research Constellation. At each stage of the research process, from initial ideation through experimental execution and communication of results, LLMs provide a constellation of capabilities spanning hypothesis generation, property prediction, novel interfaces, education, material design, automation, data management, scientific communication, and more. This constellation demonstrates the possibility of LLMs and multimodal models to drive a more efficient, rapid, and creative scientific discovery process through integrations across the research lifecycle.",
        "text": "The integration of large language models (LLMs) into scientific workflows is reshaping how researchers approach data-driven discovery, automation, and even scientific reasoning and hypothesis generation~\\cite{Jablonka_Schwaller_Ortega-Guerrero_Smit_2024,Bhattacharya2024Large, choudhary2024atomgpt, yin2024comparative}. In chemistry and materials science, fields characterized by complex data modalities, heterogeneous data formats, sparse experimental datasets, and fragmented knowledge ecosystems, LLMs are emerging as versatile tools capable of bridging gaps between computational methods, experimental data, literature and text sources, and domain expertise~\\cite{gupta2022matscibert,liu2025alchembert,rubungo2023llm,kim2024explainable,ganose2019robocrystallographer,dunn2020benchmarking,petretto2018high, BazgirAdibICLR}.  Early applications have already demonstrated potential applicability in tasks ranging from molecular property prediction~\\cite{jacobs2024regressionlargelanguagemodels, rubungo2024llm4mat, qian2023largelanguagemodelsempower} to automated laboratory workflows~\\cite{Boiko2023Autonomous, Tom2024Self} and development of novel user interfaces~\\cite{Zhang2024HoneyComb, Bran2023ChemCrow}. As illustrated in \\autoref{fig:overview}, we note that there is a significant opportunity for these broad new capabilities to be incorporated throughout the scientific research lifecycle; from initial ideation through experimental execution to communication, learning, and further iteration.\nCollectively, this constellation of capabilities, shown in ~\\autoref{fig:overview}, is applicable to long-standing challenges across the research lifecycle, creating a flywheel of improvements that promises to empower researchers with new capabilities and to speed the research process. We next discuss the constellation of capabilities in more detail and highlight exemplar projects across each key application~area.",
        "summarize_figure": "2505.03049v1_overview-figure",
        "summarization": "The image illustrates the constellation of capabilities enabled by large language models across seven key stages of the research lifecycle: extraction and reasoning, hypothesis generation, data management, property prediction, molecular design, automation, and communication. It emphasizes their integration and synergy, showing how LLMs can accelerate scientific discovery by supporting tasks like literature extraction, hypothesis refinement, structured data handling, and AI-driven material design and communication tools."
    },
    "123": {
        "figure1": "2505.03049v1_learning_lobsters",
        "label1": "fig:lobster",
        "caption1": "Schematic depicting the prompt for fine-tuning the LLM with Alpaca prompt format.",
        "text": "During the hackathon, one Llama model was fine-tuned with the Alpaca prompt format using both Robocrystallographer and LobsterPy text descriptions, and another one using solely Robocrystallographer input. \\autoref{fig:lobster} depicts the prompt used to fine-tune an LLM to predict the last phonon DOS peak. The train/test/validation split was 0.64/0.2/0.16. The models were trained for 10 epochs with a validation step after each epoch. The textual output was converted back into numerical frequency values for the computation of MAEs and RMSEs. The results show that including bonding-based information improved the model's prediction. The results also corroborate the team's previous finding that quantum-chemical bond strengths are relevant for this particular target property. Both model performances (Robocrystallographer: 44 cm$^{-1}$, Robocrystallographer+LobsterPy: 38 cm$^{-1}$) are comparable to other models of the MatBench test suite, with MAEs ranging from 29 cm$^{-1}$ to 68 cm$^{-1}$ as per the time of writing~\\cite{matbench_phonon_2024}.",
        "summarize_figure": "2505.03049v1_learning_lobsters",
        "summarization": "The image illustrates a workflow where Llama 3 is fine-tuned using structure-derived text prompts to predict the highest frequency optical phonon mode for 1264 materials. Inputs include crystallographic descriptions from Robocrystallographer and bonding information from LobsterPy. The model, trained on such textual prompts, outputs phonon peak frequencies with a mean absolute error of 38 cm⁻¹. Incorporating bonding data improves accuracy over using crystallography alone (44 cm⁻¹), performing comparably to models in the MatBench suite."
    },
    "124": {
        "figure1": "2505.03049v1_figure_section2",
        "label1": "fig:fig_section2",
        "caption1": "Workflow overview. The ReAct agent looks up guidelines for designing low band gap MOFs from research papers and suggests a new MOF (likely with a lower band gap). It then checks the validity of the new SMILES candidate and predicts the band gap with epistemic uncertainty estimation using an ensemble of surrogate fine-tuned MOFormers. b. Band gap predictions for new MOF candidates as a function of agent iterations",
        "text": "Metal-organic frameworks (MOFs) are known to be excellent candidates for electrocatalysis due to their large surface area, high adsorption capacity at low CO$_2$ concentrations, and the ability to fine-tune the spatial arrangement of active sites within their crystalline structure~\\cite{li2022review}. Low band gap MOFs are crucial as they efficiently absorb visible light and exhibit higher electrical conductivity, making them suitable for photocatalysis, solar energy conversion, sensors, and optoelectronics. This submission aims at using chemistry-informed ReAct~\\cite{yao2022react} AI Agents to optimize the band gap property of MOFs. The overview of the workflow is presented \\ref{fig:fig_section2}a.  The agent takes as inputs a textual representation of the initial MOF structure as a SMILES (Simplified Molecular Input Line-Entry System) string representation, and a short description of the property optimization task (i.e., reducing band gap), all in natural language. This is followed by an iterative closed-loop suggestion of new MOF candidates with a lower band gap with uncertainty quantification, by adjusting the initial MOF given a set of design guidelines automatically obtained from the scientific literature. A detailed analysis of this methodology, including its application to various classes of materials such as surfactants, ligands, and peptides can be found in reference~\\cite{ansari2024dziner}, which supports both closed-loop and human-in-the-loop feedback cycles and thus enables real-time property inference for human-AI collaboration in molecular design.\nThe new MOF candidates and their corresponding inferred band gap are represented in ~\\autoref{fig:fig_section2}b. The agent starts by retrieving the following design guidelines for low band gap MOFs from research papers: 1. Increasing the conjugation in the linker.  2. Selecting electron-rich metal nodes. 3. Functionalizing the linker with nitro and amino groups.  4. Altering linker length. 5. Substitute functional groups (i.e., substituting hydrogen with electron-donating groups on the organic linker). Note that the metal node adaptations are restrained by simply changing the system input prompt. The agent iteratively implements the above strategies, makes changes to the initial MOF, and suggests a new SMILES. The new SMILES is validated using the Chemical Feasibility Evaluator tool, and if found invalid, the agent uses a self-correction feedback loop to suggest new candidates, accounting for the extracted design guidelines. After each valid modification, the band gap of the new MOF is then assessed using the fine-tuned ensemble of surrogate MOFormers to ensure a lower band gap.  The self-correction feedback loop also handles new MOFs with undesired higher band gaps with respect to the initial MOF, by reverting to the most recent valid MOF candidate with the lowest band gap identified throughout the iterations.",
        "summarize_figure": "2505.03049v1_figure_section2",
        "summarization": "The image illustrates an AI-driven workflow for optimizing the band gap of metal-organic frameworks (MOFs) using natural language input. The ReAct agent iteratively suggests new SMILES structures based on literature-derived design rules to lower the band gap, validates chemical feasibility, and predicts the band gap with uncertainty using ensemble MOFormer models. The band gap of a valid MOF is reduced from 2.33 eV to 1.61 eV across iterations, demonstrating effective property control and structure refinement. Chemically feasible structures are marked in green, and invalid ones in blue."
    },
    "125": {
        "figure1": "2505.03051v1_3D_large_g0.5",
        "label1": "fig:3D-large-g0.5",
        "caption1": "Dynamics of an $N=3$-dimensional large lattice of NN electrically coupled homogeneous Rulkov neurons with $\\zeta=50$ neurons per side. The neurons are in the synchronized chaotic bursting regime with electrical coupling strength $g=0.5$. Each row displays a snapshot of the voltages of the three-dimensional lattice at a certain timestep $k$, and each column displays a certain two-dimensional $(i_1,i_2)$ slice of the lattice. The darker points indicate lower voltage, and the lighter points indicate higher voltage. The snapshots are taken at the transition between chaotic spiking and silence.",
        "text": "In Fig. \\ref{fig:3D-large-g0.5}, we present a visualization of the three-dimensional lattice in the synchronized chaotic bursting regime with electrical coupling strength $g=0.5$. Similar to the two-dimensional case (Fig. \\ref{fig:2D-bursting-snapshots}), we show the lattice at the transition between chaotic spiking and silence because the voltages in the chaotic spiking phase look qualitatively identical to unsynchronized spikes. To visualize the evolution of the three-dimensional lattice, Fig. \\ref{fig:3D-large-g0.5} shows snapshots of select two-dimensional $(i_1,i_2)$ slices of the lattice, with slices taken at seven constant $i_3$ values spaced by $\\Delta i_3 = 5$. The time axis points downwards in the figure, with each row showing evenly spaced snapshots ($\\Delta k=5$) of the two-dimensional $(i_1,i_2)$ slices of the lattice at a given timestep $k$. At the top row, when $k=410$, most of the neurons are still in the chaotic spiking phase. There are also a few small black spots in most of the slices, representing neurons that have already transitioned. In a similar manner to the two-dimensional case (Fig. \\ref{fig:2D-bursting-snapshots}), the quiescent neurons pull their neighboring chaotic spiking neurons down to the silence phase, causing the low voltage black regions to spread radially throughout the lattice until the whole lattice is in the silence phase. We note that this transition between chaotic spiking and silence happens faster in this three-dimensional large lattice than the associated two-dimensional large lattice in Sec. \\ref{subsec:2D-large} (Notice that the time between snapshots in Fig. \\ref{fig:2D-bursting-snapshots} is $\\Delta k = 10$, while the time between snapshots in Fig. \\ref{fig:3D-large-g0.5} is $\\Delta k = 5$.). This occurs because although the number of neurons in the three-dimensional lattice is larger, its effective size is smaller due to the smaller side length ($\\zeta = 50$ in the three-dimensional lattice vs. $\\zeta=300$ in the two-dimensional lattice) and greater number of coupling connections, which makes neurons that are already in the quiescent phase able to pull its many adjacent neurons down more quickly.",
        "summarize_figure": "2505.03051v1_3D_large_g0.5",
        "summarization": "The image shows the time evolution of a three-dimensional lattice of Rulkov neurons with electrical coupling strength g=0.5, transitioning from chaotic bursting to silence. Each row represents a timestep (k from 410 to 455 with Δk=5), and each column a 2D (i₁,i₂) slice at different i₃ values (10 to 40). Bright regions indicate high voltage (active bursting), while dark regions indicate silent neurons. Initially, most neurons are active, with isolated silent spots. Over time, silent regions expand and suppress surrounding activity, leading to a full lattice quiescence by k=455. The transition is faster than in 2D due to denser coupling and smaller lattice side length ζ=50."
    },
    "126": {
        "figure1": "2505.03051v1_3D_large_g1",
        "label1": "fig:3D-large-g1",
        "caption1": "Dynamics of an $N=3$-dimensional large lattice of NN electrically coupled homogeneous Rulkov neurons with $\\zeta=50$ neurons per side. The neurons are in the (quasi-)synchronized hyperchaotic regime with electrical coupling strength $g=1$. Each row displays a snapshot of the voltages of the three-dimensional lattice at a certain timestep $k$, and each column displays a certain two-dimensional $(i_1,i_2)$ slice of the lattice. The darker points indicate lower voltage, and the lighter points indicate higher voltage. The snapshots are taken at the transition between a period of higher voltage and a period of lower voltage.",
        "text": "In Fig. \\ref{fig:3D-large-g1}, we present a similar visualization of the three-dimensional lattice in the synchronized hyperchaotic regime with electrical coupling strength $g=1$. Here, we display the dynamics of the lattice at the transition between a period of higher voltage and a period of lower voltage and take evenly spaced snapshots with $\\Delta k = 10$. As we discovered in Sec. \\ref{subsec:2D-large}, this regime for the two-dimensional large lattice case is better described as ``quasi-synchronized'' or ``locally synchronized.'' This also holds true in the three-dimensional case, where the entire lattice follows the general trend of alternation between higher voltages and lower voltages, and in the lower voltage phase, the lattice exhibits splotches of locally quasi-synchronized neurons. Figure \\ref{fig:3D-large-g1} also clearly shows the radial spreading of low voltages at the displayed transition: a small splotch of low voltage neurons appears in the $i_3=20$ slice at $k=680$, and as time progresses, the low voltages spread both radially in the $i_1$ and $i_2$ directions within the same slice and radially in the $i_3$ direction to adjacent slices. In the lower voltage phase, similar to the two-dimensional case, we can see clear splotches of locally quasi-synchronized neurons and waves of low voltage propagating throughout the three-dimensional lattice.\nSomething that does appear in the two-dimensional large lattice in the synchronized hyperchaotic regime (Fig. \\ref{fig:2D-hyperchaos-snapshots}) but that is abundantly clear in the three-dimensional large lattice (Fig. \\ref{fig:3D-large-g1}) is the appearance of a checkerboard pattern, with alternating high and low voltage neurons. This is present at all of the timesteps in Fig. \\ref{fig:3D-large-g1}, but it is most clear in the lower voltage phase ($k=750, 760, 770$). This checkerboard pattern does not appear in the less strongly coupled regimes, and it is a bit counterintuitive since we know that the positive electrical coupling strength $g$ leads to a synchronization of adjacent neurons, not an anti-synchronization. Indeed, rather than anti-synchronization, this observation is a manifestation of an extreme form of lag synchronization \\cite{lag-sync-1, lag-sync-2, lag-sync-3, lag-sync-4, lag-sync-model}, which often appears in hyperchaotic systems. Since the Rulkov map is a discrete-time system, it is to be expected that there is a very small amount of lag with regard to the injection of current into a neuron, since the neuron's voltage will only reflect the injection one timestep after injection begins (see Sec. \\ref{sec:the-model}). For more weakly coupled lattices, this effect does not manifest since the amount of flowing current is relatively small and slow to change. However, when $g=1$, the current flow is equal to the difference in the voltages between adjacent neurons [Eq. \\eqref{eq:coup-param}], so the magnitudes of the flowing current are large, and their direction can change instantaneously. Therefore, the one-timestep lag synchronization can manifest itself in the synchronized hyperchaotic regime, which is made especially clear in Fig. \\ref{fig:3D_lag}. In Fig. \\ref{fig:3D_lag_xvk}, we plot the voltages $x(k)$ of two adjacent neurons in the three-dimensional large lattice with $g=1$ over a short time interval. As we can see, the two neurons are spiking in antiphase, which is equivalent to the one-timestep lag synchronization. Here, the strong current between the two neurons switches at every timestep: the current flows out of the higher voltage neuron into the lower voltage neuron, reversing their voltages in the next timestep, which causes the current to flow in the opposite direction, and the process repeats. To see this lag synchronization on a larger scale, we plot four consecutive snapshots of a whole two-dimensional $(i_1,i_2)$ slice of the lattice at $i_3=20$. The checkerboard pattern is evident in the lattice, and looking closely, we can see that the pattern alternates between each timestep. Specifically, a dark neuron turns light in the next timestep, and vice versa, which is the defining characteristic of this extreme lag synchronization.",
        "summarize_figure": "2505.03051v1_3D_large_g1",
        "summarization": "The image displays the voltage dynamics of a large 3D lattice of electrically coupled Rulkov neurons with coupling strength g=1, in a synchronized hyperchaotic regime. Each row corresponds to a timestep k (from 680 to 770), and each column shows a 2D (i₁,i₂) slice at varying i₃ indices (14 to 26). Initially, low-voltage patches emerge (first at i₃=20), and gradually spread radially through space. From k=750 onwards, a distinct checkerboard voltage pattern appears, indicating an extreme form of lag synchronization due to strong coupling, where adjacent neurons alternate voltage states across timesteps."
    },
    "127": {
        "figure1": "2505.03063v1_both",
        "label1": "fig:prediction",
        "caption1": "\\texttt{DiTSNe} Prediction - Ground Truth and associated 1$\\sigma$ uncertainties (First Row) for spectra from the test set predicted over five different phases and conditioned on realistic LSST light curves. The prediction of the baseline SALT3 model is shown in green. Our method achieves a lower mean error rate than SALT3 at all phases. We also show coverage plots (Second Row) from the model, finding values near nominal starting +10 days after peak light; as well as the width of the width of the 90\\% CI from \\texttt{DiTSNe} predictions (Third Row) and its realized coverage as a function of nominal coverage (Fourth Row).",
        "text": "We provide a summary of our results in \\Cref{tab:summaryresults}, and in \\Cref{fig:prediction}. We find that \\texttt{DiTSNe-Ia} achieves better predictive performance across the test set while having a lower average bias than the SALT3 method. \\texttt{DiTSNe-Ia} yields an overall mean squared error of 0.108—five times lower than SALT3’s 0.508—and an after-peak error of just 0.0191, an order of magnitude smaller than SALT3’s 0.305. Uncertainty quantification achieves coverage near nominal values in predictions greater than 10 days after peak light with an overall coverage of 0.845 for 90\\% interval (see also last row in Figure~\\ref{fig:prediction}); although the predictions at earlier phases (especially 10 days before peak) are under-covered. This is unsurprising given the limited photometric information available at early times. SALT3, in contrast, has small CIs but is severely under-covered. %In addition, our method achieves one magnitude smaller MSE than the baseline SALT3 model in phases other than -10 days before the peak (ours 0.019 vs SALT3 0.305), and five times smaller MSE at -10 days (ours 0.108 vs SALT3 0.508).",
        "summarize_figure": "2505.03063v1_both",
        "summarization": "The image compares DiTSNe and SALT3 across five supernova phases from -10 to +30 days post-peak, showing residuals, confidence interval (CI) coverage, CI width, and calibration. DiTSNe consistently yields lower and more stable residuals than SALT3. From +10 days onward, DiTSNe achieves CI coverage close to the ideal value of 1, while SALT3 under-covers throughout. Though DiTSNe has wider CIs, they better match the ground truth. Calibration plots indicate DiTSNe’s CIs align closely with nominal levels after peak, unlike SALT3, which remains poorly calibrated. This highlights DiTSNe’s superiority in both predictive accuracy and uncertainty quantification."
    },
    "128": {
        "figure1": "2505.03063v1_spetra_inference_figs",
        "label1": "fig:architecture",
        "caption1": "The architecture of our diffusion transformer for generation of SN~Ia spectra conditioned on light curve data. Positional embeddings are used to preserve correlations from data irregularly sampled in both time and wavelength.",
        "text": "We used a variational diffusion model for spectral generation \\citep{kingma2021variational}. We used a transformer (DiT) architecture for the score model \\citep{peebles2023scalable} because of the flexibility of modeling sequences that are not from a regular grid such as light curves and spectra. Each DiT block consists of one attention block and one cross-attention block separated by one layer normalization block. Each (cross)attention block also consists of multi-headed (cross)attention, layer normalization, and a Multi-Layer Perceptron (MLP), as well as skip connections (Figure~\\ref{fig:architecture}).\nWe show an overview of the \\texttt{DiTSNe-Ia} architecture in Figure~\\ref{fig:architecture}. The model has a dimension $D=256$. The MLP used in each DiT block consists of a single hidden layer with dimension 512. In both multi-headed attention and cross-attention, we use 4 heads and 6 layers of DiT blocks.",
        "summarize_figure": "2505.03063v1_spetra_inference_figs",
        "summarization": "The image presents the architecture of a diffusion transformer for generating SN Ia spectra conditioned on light curve data. Inputs include embedded wavelength, noisy latent flux, time and phase embeddings, and light curve embeddings. These are concatenated and passed through N DiT blocks, each consisting of a self-attention and cross-attention module with layer normalization. Attention blocks include multi-headed attention, MLP, and skip connections. The final output undergoes normalization, linear transformation, and reshaping to yield noise prediction. This design preserves irregular correlations in both time and wavelength, enabling effective modeling of sparse astronomical sequences."
    },
    "129": {
        "figure1": "2505.03064v1_fig1",
        "label1": "fig:intro",
        "caption1": "a) The VTP process depositing a line of stacked coils with key parameters highlighted: $F$ (translational speed), $U$ (extrusion speed), $H$ (print height), and $D_T$ (thread diameter). b) A ``rotating squares’’ metamaterial produced with FDE by changing the coiling parameters. Zoomed insets show the different densities and characteristics of dense and sparse coiling. c) The FDE process takes an explicitly defined macrostructure and combines it with an implicit stochastic microstructure (schematically represented here by a Voronoi diagram), to produce variable-density entangled structures.",
        "text": "We parameterize the coiling via two dimensionless parameters---height \\hstar{} and speed \\vstar{}---which broadly determine coil radius and coil period, respectively \\cite{Yuk2018-br, Brun2015-qm}.  By adjusting these parameters, the filament can be made to form dense, tightly wound loops or looser, more open coils---thereby controlling stiffness. In practice, these parameters are defined as H^* \\;=\\; \\frac{H}{D_T} V^* \\;=\\; \\frac{F}{U}, where \\(H\\) is the nozzle height above the substrate, \\(D_T\\) is the extruded filament diameter, \\(F\\) is the nozzle translation speed, and \\(U\\) is the filament extrusion speed at the nozzle exit (\\autoref{fig:intro}a). With a standard extrusion-based 3D printer, we control the coiling by varying \\vstar{} and \\hstar{} via the print height $H$ and nozzle speed $F$, while keeping the nozzle thread $D_T$ and extrusion speed $U$ fixed. This enables us to produce a wide range of characteristic coiling with minimal control parameters.\nOur FDE methodology, illustrated in~\\autoref{fig:intro}, integrates VTP and IH to impose higher-order patterns onto the inherently stochastic VTP coiling process.  By spatially varying VTP parameters according to an IH-derived design, we create quasi-two-phase entangled structures with programmed local density and tailored macroscopic properties.\nFirst, the optimized density field $\\rho$ directly informs the VTP process, where $\\rho=0$ corresponds to the sparse, compliant \\vhstar{} pair, and $\\rho=1$ corresponds to the dense, rigid \\vhstar{} pair (\\autoref{fig:intro}b).  Whereas conventional TO often uses a solid-void material set for the density field, with stiffness contrasts such as $10^9{:}1$, our FDE approach operates with a quasi-two-phase entanglement system with stiffness $\\sim30{:}1$. While this stiffness ratio is substantially lower than conventional TO work, restricting the available property space, it is nevertheless comparable to those found in traditional composites~\\cite{Jones2018-yl}, indicating that an engineered response is achievable. Furthermore, since FDE translates the density map into variations along a single, continuous filament path via VTP, the resulting structure is inherently continuous and well-connected---directly realizing the domain continuity in the TO FEM formulation---and is thus free from the disconnected features or ill-defined interfaces that can plague conventional TO.",
        "summarize_figure": "2505.03064v1_fig1",
        "summarization": "The image illustrates a fabrication-directed entanglement (FDE) strategy that integrates 3D printing parameters to create structures with both patterned macrostructures and stochastic mesostructures. In (a), extrusion speed, nozzle height, and translation speed control coil density via dimensionless parameters H* = H/DT and V* = F/U. Panel (b) shows a rotating-square metamaterial with distinct dense and sparse coiling regions. Panel (c) conceptualizes the FDE approach, combining an explicit macro-pattern with a Voronoi-based entangled microstructure to generate spatially varying density networks with enhanced continuity and tunable mechanical properties."
    },
    "130": {
        "figure1": "2505.03064v1_fig3_with_strain",
        "label1": "fig:DIC",
        "caption1": "DIC analysis of a compression test of the handed shearing material (\\autoref{fig:array_modes}c). The foam is mirrored about the horizontal axis to prevent shear stresses at the test boundaries. a-b) The material at rest and 30\\% compressive strain (45 mm of compression). c) X-Displacement of the foam showing the center of the material displacing to the right. The banded nature indicates how the material shifts uniformly on top and bottom. d) Y-Displacement of the foam showing uniform vertical compression. e) The tensorial Eulerian-Almansi shear strain ($\\varepsilon_{xy}=\\gamma_{xy}/2$) showing symmetric shear strain mirrored about the middle. %The RGB coordinate system in the bottom-right indicates the print orientation, whereas the XY arrows in the top-left indicate the measurement coordinate system. ",
        "text": "The targeted vertical-shear coupling is clearly visualized via digital image correlation (DIC) during compression testing, shown in \\autoref{fig:DIC}.  The foam, compressed vertically ($x_2$), exhibits a clear shear deformation coupled with the compression, visible in the x- and y-displacement fields (\\autoref{fig:DIC}c-d) and the Euler-Almansi shear strain field (\\autoref{fig:DIC}e).  Note, the tested sample was mirrored horizontally to mitigate boundary effects, thus the shear strains seen in \\autoref{fig:DIC}e have opposite signs from top to bottom.  The measured relatively low Poisson's ratio ($\\nu_{21}\\approx0.14$) confirms that the observed horizontal displacement is dominated by the programmed normal-shear coupling, not the Poisson effect.",
        "summarize_figure": "2505.03064v1_fig3_with_strain",
        "summarization": "The image presents DIC analysis of a handed shear foam under 30% vertical compression. The X-displacement map reveals a uniform lateral shift, while the Y-displacement shows homogeneous vertical compression. Shear strain appears symmetrically mirrored across the center, with opposing signs at the top and bottom. This shear response stems from engineered normal-shear coupling rather than Poisson effects, supported by the low measured Poisson’s ratio of approximately 0.14."
    },
    "131": {
        "figure1": "2505.03067v1_flow400",
        "label1": "fig1",
        "caption1": "Flowchart depicting the multiscale modelling workflow, starting from organ-level CT segmentation and progressing through tissue-level domain definition, cell-level CPM dynamics and parallel continuum-level PDE computations. Everything takes place in the Compucell3D Steppables with exception of the Parallel execution.",
        "text": "Our framework integrates cell-level interactions with tissue-scale dynamics. This multiscale modelling covers a lower-scale process occurring over seconds, namely solute diffusion and reaction processes that shape the nutrient and chemical gradient, while also modelling cellular proliferation that unfolds over hours, resulting in growth and motility over days, linking molecular processes to tumour progression. Building on these capabilities, the goal of this study is to simulate the proliferation of MPM within the pleural space, the region between the parietal and visceral pleura of the lungs. This multiscale workflow is depicted in Fig.~\\ref{fig1}.\nFor the simulation of tumour growth and cell proliferation we implement a CPM, as visualized in Fig.\\ref{fig1}. The dynamics of oxygen ($C_O$), nutrients ($C_n$) and cytokines (IL6 and IL8) is described by a system of partial differential equations:\nDue to the heavy computational load of the PDE solver, a separate script is developed outside of the sequential Steppables used by CompuCell3D, as can be seen in Fig.~\\ref{fig1}. This script is called in parallel using \\texttt{mpi4py} after the \\texttt{FipySteppable} initializes the bounding box around the initial tumour in the pleural space and wrote the initial data (mesh, oxygen, nutrients, pleural mask and cytokines) to a shared file.",
        "summarize_figure": "2505.03067v1_flow400",
        "summarization": "The image illustrates a multiscale modelling workflow for simulating MPM growth within the pleural space, beginning with CT-based lung segmentation and progressing through pleural domain definition, cell-level CPM dynamics, and continuum-scale PDE computations. The workflow integrates sequential execution of CPM dynamics with parallel PDE solving using mpi4py and the finite volume method. The system spans multiple biological scales—organ, tissue, cell, and continuum—and captures phenomena ranging from rapid solute diffusion to slow cellular proliferation and tumor progression over days."
    },
    "132": {
        "figure1": "2505.03069v1_lipschitz_diagram",
        "label1": "fig:lipschitz",
        "caption1": "The Scaled Relative Graph \\cite{chaffey2023graphical} is defined as a set of complex numbers with $z(u, v) := \\left\\{ \\frac{\\| y - z \\|}{\\| u - v \\|} e^{\\pm j \\angle (u - v, y - z)} \\;\\middle|\\; y = \\G(u), z = \\G(v) \\right\\}$, where $\\angle (u, y) := \\text{acos} {(\\operatorname{Re} \\langle u , y \\rangle}{\\|u\\|^{-1} \\|y\\|^{-1})} \\in [0, \\pi]$, illustrated with green dots. This SRG depicts the incremental input-output properties of a $(\\mu,\\nu)$-biLipschitz operator $\\G$. The ring (blue area) is for all $(\\mu,\\nu)$-biLipschitz operator $\\G$ while the small circle (red area) is for the operator $\\G$ satisfying the strong input-output monotonicity condition \\eqref{eq:in_mono}.",
        "text": "We introduce the strong input-output monotonicity which will be used to formulate the basic building block of bi-Lipschitz systems.     System \\eqref{eq:system} is said to be ($\\xi, \\rho$)-\\emph{strongly input-output monotone} with $\\xi, \\rho>0$ if for all $u,v\\in\\ell^m$ we have        2 \\langle \\Delta y , \\Delta u\\rangle_T \\geq \\xi\\|\\Delta u\\|^2_T+\\rho\\|\\Delta y \\|^2_T     where $\\Delta y =\\G_a(u)-\\G_a(v)$, $\\Delta u = u-v$. We show that if an operator satisfies the strong input-output monotonicity, then it is bi-Lipschitz, as depicted in Fig.~\\ref{fig:lipschitz}.         If an operator $\\G:\\ell^m\\mapsto\\ell^m$ satisfies \\eqref{eq:in_mono} with         then it is $(\\mu,\\nu)$-biLipschitz. For any pair of input sequences $u, v\\in \\ell_m$, we define $\\Delta u = u-v$ and $\\Delta y=y-z$ where $y=\\G_a(u)$ and $z=\\G_a(v)$. From \\eqref{eq:in_mono}, we can obtain     2 \\langle \\Delta y , \\Delta u\\rangle_T\\geq \\frac{2}{\\mu+\\nu} (\\mu\\nu\\|\\Delta u_t\\|^2+\\|\\Delta y_t\\|^2), \\;\\forall T\\in \\mathbb{N}. By Cauchy-Schwarz inequality, we have     (\\mu+\\nu)\\|\\Delta y\\|_T\\|\\Delta u\\|_T\\geq \\mu\\nu \\|\\Delta u\\|_T^2+\\|\\Delta y\\|_T^2), which further implies     (\\|\\Delta y\\|_T-\\mu \\|\\Delta u\\|_T)(\\nu\\|\\Delta u\\|_T-\\|\\Delta y\\|_T)\\geq 0. Thus, $\\G$ is $(\\mu,\\nu)$-biLipschitz.  $z(u, v) := \\left\\{ \\frac{\\| y - z \\|}{\\| u - v \\|} e^{\\pm j \\angle (u - v, y - z)} y = \\G(u), z = \\G(v) \\right\\}$, where $\\angle (u, y) := \\text{acos} {(\\operatorname{Re} \\langle u , y \\rangle}{\\|u\\|^{-1} \\|y\\|^{-1})} \\in [0, \\pi]$, illustrated with green dots. This SRG depicts the incremental input-output properties of a $(\\mu,\\nu)$-biLipschitz operator $\\G$. The ring (blue area) is for all $(\\mu,\\nu)$-biLipschitz operator $\\G$ while the small circle (red area) is for the operator $\\G$ satisfying the strong input-output monotonicity condition \\eqref{eq:in_mono}.}\\label{fig:lipschitz}",
        "summarize_figure": "2505.03069v1_lipschitz_diagram",
        "summarization": "The image presents a complex-plane visualization of the Scaled Relative Graph (SRG) for an operator G, capturing its incremental input-output behavior. The blue ring depicts the region for all (μ, ν)-biLipschitz operators G, while the inner red circle corresponds to those additionally satisfying strong input-output monotonicity. Each point z(u, v) = ‖y − z‖ / ‖u − v‖ · e^(±jθ) represents a normalized and angle-encoded measure of change between inputs u, v and outputs y = G(u), z = G(v). Green dots illustrate specific examples. The position of μ and ν on the real axis defines the bounding region, and the red zone highlights operators with stronger coupling properties under the monotonicity condition."
    },
    "133": {
        "figure1": "2505.03069v1_factorization_test",
        "label1": "fig:fact",
        "caption1": "Open loop simulation of the outer system, the original system and the composed system under the Gaussian noise (Top), the impulse response of the inner system (Bottom).",
        "text": "In this section we present experiments which illustrate the utility of our proposed models, system identification via inner-outer factorization, and robust inversion of an minimum phase nonlinear system. In the first example, we consider learning the inner-outer factorization of a nonlinear system with time delay $\\dot x(t) = 0.9\\text{tanh}x(t)+u(t-1)$, which is a stable nonlinear system coupled with a time delay. The inner-outer factorization is a composition of an all-pass filter and a stable invertible system which naturally fits our proposed approach with a composition of a dynamics orthogonal layer and a biLipREN as the inner and outer system respectively. We plot the open-loop response of the outer system and the original system under the Gaussian noise input to demonstrate the learned factorization in Figure \\ref{fig:fact}.",
        "summarize_figure": "2505.03069v1_factorization_test",
        "summarization": "The figure contains two subplots illustrating the performance of an inner-outer factorized model of a nonlinear system under Gaussian noise and impulse input. The top plot compares open-loop responses of the outer, original, and composed systems. The original and composed systems show nearly identical trajectories, confirming that the learned factorization effectively reconstructs the original dynamics, while the outer system alone exhibits larger and less stable variations. The bottom plot shows the impulse response of the inner system, characterized by a sharp peak followed by rapid decay, indicating a stable filter with fast attenuation. This confirms the efficacy of the factorization in capturing and reconstructing the behavior of the nonlinear time-delayed system."
    },
    "134": {
        "figure1": "2505.03069v1_msd",
        "label1": "fig:msd",
        "caption1": "Four coupled nonlinear mass spring dampers.",
        "text": "The data is generated by simulating the system for $15$s and sampling $100$ batch data points with $100$ time steps under a Gaussian noise input. We fit the $(0.1,5)$-biLipschitz dynamic models to the data by minimizing the fitting error in $\\ell_2$-norm $J = \\|\\bm{F}(u_k)-y_k\\|^2$, where $u_k$ and $y_k$ are the input and output of the $k$th batch data respectively. BiLipREN has $3$ internal states and $30$ neurons, and the dynamic orthogonal layer has $30$ internal states. sequences.  In the second experiment, we learn a robustly-invertible model of a nonlinear mechanical system with four coupled mass spring dampers with nonlinear spring characteristic, adapted from \\cite{revay2020convex}. A schematic is shown in Figure \\ref{fig:msd}. Since the linearized system is easily verified to be minimum phase phase, we attempt to learn a nonlinear invertible model, such that the system input can be robustly reconstructed from the output, even with the addition of measurement noise and uncertain initial conditions.",
        "summarize_figure": "2505.03069v1_msd",
        "summarization": "The image illustrates a nonlinear mechanical system composed of four masses coupled through nonlinear springs and dampers, with system input u and output y. Each mass is connected by nonlinear springs and dampers fixed to the ground. The system simulates the challenge of learning a robustly invertible nonlinear model to reconstruct the input from output under measurement noise and uncertain initial states. Data were generated over 15 seconds under Gaussian noise, sampled into 100 batches with 100 time steps each. A (0.1, 5)-biLipschitz dynamic model is fitted by minimizing the L2 error J = ‖F(uk) - yk‖². The BiLipREN model uses 3 internal states and 30 neurons, while the dynamic orthogonal layer has 30 internal states."
    },
    "135": {
        "figure1": "2505.03073v1_figure-1",
        "label1": "fig:1",
        "caption1": "Audio amplitudes over time for a 5 minute, 38 second song sampled at 44.1 kHz from a WAV file with concurrent (simulated) heart rate data sampled from PPG and a derived ``multiplier'' for the real-time tempo of the playing audio.} \\Description{Three stacked plots: a plot of music audio amplitudes, a plot of simultaneously derived heart rate data from PPG, and a plot of the derived multiplier value.",
        "text": "Figure \\ref{fig:1} shows three stacked plots of values over time: a plot of music audio amplitudes for a 5 minute and 38 second song sampled at 44.1 kHz and loaded from a WAV file, a plot of simultaneously derived (simulated) heart rate data sampled from PPG, and a plot of the derived ``multiplier'' to change the real-time tempo of the playing audio.",
        "summarize_figure": "2505.03073v1_figure-1",
        "summarization": "The figure presents three stacked plots: one showing the music audio amplitudes over time for a 5-minute and 38-second song sampled at 44.1 kHz from a WAV file, one showing the concurrently derived heart rate data sampled from PPG, and one showing the derived multiplier for adjusting the real-time tempo of the audio playback. The audio amplitude is fluctuating significantly over time, while the heart rate data shows a relatively stable trend with slight fluctuations. The multiplier, which adjusts the tempo of the audio, exhibits periodic variations, suggesting a correlation with the heart rate data."
    },
    "136": {
        "figure1": "2505.03075v1_ranks",
        "label1": "fig:recall",
        "caption1": "Recall@\\textit{K} (k=1, 3, 5) score of the initial retrieval (Colbertv2.0), two re-ranking baselines (i.e., RankVicuna and RankZephyer) and our selection model $\\theta_{s}$, respectively.",
        "text": "In addition to the end-to-end evaluation presented in Table~\\ref{tab:main}, we further evaluate the Recall@\\textit{K} of our selection models on documents re-ranking task. Following~\\cite{karpukhin2020dense, kwiatkowski-etal-2019-natural}, we set the Recall@K to 1 if the top-K documents contains the ground-truth answer. As shown in Figure~\\ref{fig:recall}, our selection model improves the recall of the initial retrieval of ColBERTv2.0 by on average 17.78\\%, such as pushing the accuracy@1 from 49.71 to 60.89 on NQ dataset. We also compared with the similar list-wise re-ranking baselines, where we find that the selection model, trained within \\ours, achieves the highest recall. This validates the need to enable end-to-end supervision for knowled    ge selection process.",
        "summarize_figure": "2505.03075v1_ranks",
        "summarization": "The chart shows the Recall@K (K = 1, 3, 5) scores of the initial retrieval (Colbertv2.0), two re - ranking baselines (RankVicuna and RankZephyer), and the selection model on NQ, HotpotQA, MuSiQue, and 2Wiki datasets. As K increases, the recall rates of various methods generally rise. The selection model significantly improves the recall rate. For example, on the NQ dataset, Recall@1 increases from 49.71 of the initial retrieval to 60.89. Compared with the other two re - ranking baselines, the selection model achieves the highest recall rate."
    },
    "137": {
        "figure1": "2505.03075v1_variance1",
        "label1": "fig:variance",
        "caption1": "Variance during the training process of our method (logarithmic scale).",
        "text": "In our maximization step, we employ an importance weight $w(\\perm)$ to calibrate the optimization expectation. Our theoretical analysis in Section~\\ref{sec:variance} shows that the training variance decreases as training progresses, since $w(\\perm) \\propto p(y \\mid x, \\doc_{\\perm}; \\theta_g)$. To validate this finding, we compute the variance of sampling from $p(y \\mid x; \\doc_{\\perm})$ at each training iteration. We vary the number of samples from 1 to 16 to compute the variance, respectively. See Figure~\\ref{fig:variance}. The variance substantially decreases with the optimization of model $\\theta_g$ during training progress, validating the correctness of our theoretical analysis from Section~\\ref{sec:stability}. Besides, in our experiments, we observe that increasing the number of samples per iteration reduces variance at each training step, offering a straightforward strategy to improve training robustness, especially during early-stage training (See Section~\\ref{sec:sample-number} from more details).}",
        "summarize_figure": "2505.03075v1_variance1",
        "summarization": "The chart shows the variance during the training process of the method (in logarithmic scale). Theoretical analysis indicates that the training variance decreases as training progresses, as the importance weight w(perm) is proportional to p(y | x, doc_perm; θg). The variance of sampling from p(y | x; doc_perm) at each training iteration is calculated for verification. The result shows that the variance drops significantly with the optimization of model θg during training, validating the correctness of theoretical analysis. Also, the experiment shows that increasing the number of samples per iteration reduces variance in training steps, providing an intuitive strategy for improving training robustness, especially in the early - stage training."
    },
    "138": {
        "figure1": "2505.03075v1_ablation",
        "label1": "fig:ablation",
        "caption1": "Ablation study on five datasets to demonstrate the effectiveness of training the selection model $\\theta_s$ and generator $\\theta_g$.",
        "text": "Figure~\\ref{fig:ablation} reports the F1 scores across five datasets (NQ, HotpotQA, MuSiQue, 2WikiMultihopQA, and WoW) over five training iterations. We observe consistent and notable improvements of the full \\ours method over both ablated variants. On the \\textbf{NQ} dataset, the vanilla method achieves an F1 score of \\textbf{51.01} at iteration 5, while the two variants, i.e., \\ours-\\textit{w/o Selector} and \\ours-\\textit{w/o Generator}, obtain 46.77 and 48.71 respectively, suffering from drops of 4.24 and 2.30 points.  Similar patterns are observed in other datasets: These results demonstrate two key findings. First, training the selection model $\\theta_s$ is critical for learning high-quality document permutations that benefit the generator. Second, optimizing the generator $\\theta_g$ to better utilize selected documents further amplifies performance. The combined training of both components yields cumulative gains that neither alone can achieve.",
        "summarize_figure": "2505.03075v1_ablation",
        "summarization": "The chart shows the ablation study results on five datasets (NQ, HotpotQA, MusiQue, 2wiki, WoW) for the effectiveness of training the selection model θs and generator θg, presenting F1 scores over five training iterations. On the NQ dataset, the full method reaches an F1 score of 51.01 at the fifth iteration, while \"Ours w/o Selection\" and \"Ours w/o Generator\" have scores of 46.77 and 48.71 respectively, with significant drops. Similar trends are seen in other datasets. This indicates that training the selection model is crucial for learning high - quality document permutations, optimizing the generator to better utilize selected documents can enhance performance, and the combined training of both brings cumulative gains that individual training cannot achieve."
    },
    "139": {
        "figure1": "2505.03078v1_coo_consensus_th3",
        "label1": "fig:coo_consensus",
        "caption1": "Network and actions at (a) $t=0$ and (b) $t=600$. Nodes are in green if $\\sgn(y_i)=x_i=+1$ and in red if $\\sgn(y_i)=x_i=-1$. In (c), the temporal evolution of the opinion is shown to converge to consensus.",
        "text": "Consider a network of $n=30$ agents partitioned into two groups, $\\mathcal{V}_p= \\{1,\\ldots,15\\}$ and $\\mathcal{V}_n=\\{16,\\ldots,30\\}$. Given $\\vect z(0)$, we set weights uniformly at random and then re-scale them so that conditions in \\eqref{eq:condition_consensus_VnVp_coo_thm} are met with $\\mathcal{V}_p=\\mathcal{V}_{1}^{+}(0)$ and $\\mathcal{V}_n = \\mathcal{V}_{-1}^{-}(0)$, and weights are row-stochastic. The network obtained is illustrated in Fig.~\\ref{fig:coo_consensus}(a). The parameters are $\\lambda=0.8,\\beta=0.6$.  As shown in Fig.~\\ref{fig:coo_consensus}(b) and ~\\ref{fig:coo_consensus}(c), the final actions and opinions converge to a consensus.",
        "summarize_figure": "2505.03078v1_coo_consensus_th3",
        "summarization": "The chart shows the states of a network of 30 agents at the initial time (t = 0) and at t = 600, as well as the evolution of opinions over time. At the initial (Figure a) and final (Figure b) times, nodes are green when sgn(yi)=xi = +1 and red when sgn(yi)=xi = -1. Figure c shows the temporal evolution of opinions, which gradually converge to a consensus as time progresses. With parameters λ = 0.8 and β = 0.6, after randomly setting and rescaling the weights to meet specific conditions, the final actions and opinions tend to be consistent."
    },
    "140": {
        "figure1": "2505.03078v1_2D_anti_coo_contour",
        "label1": "fig: 2D_anti_coo_contour",
        "caption1": "Contour plot of the RHS in \\eqref{eq:condition_A} in Theorem~\\ref{thm:consensus_ equilibrium_anti} when $\\alpha=0$. The black regions in the plot represent areas where the contour lines are denser, indicating faster changes in values.",
        "text": "We give a 2D contour plot of the RHS of the inequality in Theorem~\\ref{thm:consensus_ equilibrium_anti} to observe whether a given influence network $\\mathcal{G}[A]$ easily satisfies the condition when $\\lambda$ and $\\beta$ take different values with $\\alpha=0$.   Fig.~\\ref{fig: 2D_anti_coo_contour} depicts the condition in \\eqref{eq:condition_A} with $\\alpha = 0$. From the contour plot, it appears that $\\frac{2(1-\\lambda)\\beta}{(1-\\beta)}$ is maximized for high $\\beta$ values, and low to medium $\\lambda$ values. In other words, the condition in \\eqref{eq:condition_A} is more likely to be satisfied when $\\beta$ is large and $\\lambda$ is small. Intuitively, this suggests that it is easier to reach a consensus equilibrium when the decision-making process is more affected by the opinion exchange. We also see a clear distinction between  coordinating agents and anti-coordinating agents. For the former, there is always a consensus equilibrium~\\cite[Proposition~2]{raineri2025}. For the latter, the existence of such equilibrium depends on the parameters.  In what follows, we present an example that satisfies the conditions of Theorem~\\ref{thm:anti_convergence_all}.",
        "summarize_figure": "2505.03078v1_2D_anti_coo_contour",
        "summarization": "The chart is a contour plot of the right - hand side (RHS) of the inequality in the theorem when α = 0. The black areas in the plot have dense contour lines, indicating rapid value changes. From the plot, (2(1 - λ)β) / (1 - β) is maximized for high β values and low to medium λ values. That is, the condition in the theorem is more likely to be satisfied when β is large and λ is small. This implies that it is easier to reach a consensus equilibrium when the decision - making process is more influenced by opinion exchange. The plot also clearly distinguishes between coordinating and anti - coordinating agents. The former always has a consensus equilibrium, while the existence of such an equilibrium for the latter depends on the parameters."
    },
    "141": {
        "figure1": "2505.03078v1_anti_coo_th4",
        "label1": "fig:anti_coo_ER",
        "caption1": "Network and actions at (a) $t=0$ and (b) $t=600$. Nodes are in green if $\\sgn(y_i)=x_i=+1$ and in yellow if $\\sgn(y_i)\\neq x_i$. In (c), the temporal evolution of the opinion shows convergence to consensus.",
        "text": "Consider a network of $n=30$ agents with weights sampled uniformly at random and then re-scale so that the matrix is row-stochastic and so that conditions in \\eqref{eq:condition_A} are met with $\\alpha=0$. The network obtained is illustrated in Fig.~\\ref{fig:anti_coo_ER}(a). The parameters are $\\lambda=0.5,\\beta=0.8$. As shown in Fig.~\\ref{fig:anti_coo_ER}(b) and ~\\ref{fig:anti_coo_ER}(c), the final actions and opinions converge to a consensus.",
        "summarize_figure": "2505.03078v1_anti_coo_th4",
        "summarization": "The chart shows the states of a network of 30 agents at the initial time (t = 0) and at t = 600, as well as the evolution of opinions over time. In Figure (a) (initial time) and Figure (b) (final time), nodes are color - coded based on the relationship between sgn(yi) and xi. Nodes are green when sgn(yi)=xi = +1 and yellow when sgn(yi)≠xi. Figure (c) shows the evolution of opinions over time steps. As time progresses, the final actions and opinions tend to reach a consensus. The parameters are set as λ = 0.5 and β = 0.8. The network weights are sampled randomly and rescaled to meet the row - stochastic matrix condition and specific formula conditions (α = 0)."
    },
    "142": {
        "figure1": "2505.03082v1_Fig1",
        "label1": "fig:metric-comparison",
        "caption1": "Repository-engagement and popularity features comparison of diverse and non-diverse AI repositories ($\\bullet$ represents the mean value)",
        "text": "We examined the extracted features of both diverse and non-diverse AI repositories to analyze their impact on the repositories. Figure ~\\ref{fig:metric-comparison} highlights notable patterns observed in diverse repositories.  For repository-engagement features, we found that, with the exception of the \\textit{num-watchers} feature, diverse repositories generally scored higher. For instance, in at least half of the AI repositories, the \\textit{num-forks} feature in diverse AI repositories is almost two times higher than in non-diverse AI repositories, i.e., the median value of forks is 22 in non-diverse repositories and the median value of forks is 42 in diverse repositories. However, we observed little difference in the number of open and closed pull requests between the groups. Conversely, there are significant differences in the number of open and closed issues between the repositories. On the other hand, for quartile analysis of the popularity features, we excluded the \\textit{has-model} and \\textit{has-shell} features from Fig.~\\ref{fig:metric-comparison}, as their presence in both groups was minimal. Specifically, 153 out of 156 non-diverse repositories contained no pre-trained models, and 121 out of 156 lacked shell scripts. Similarly, in the diverse group, 35 out of 39 repositories had no pre-trained models, and 31 out of 39 contained no shell scripts.  These findings show that our selected diverse and non-diverse repositories rarely included pretrained models and shell scripts. The few instances where these features were present suggest that they were handled by a small number of contributors, likely outliers in our dataset. Examining the remaining popularity features from Fig.~\\ref{fig:metric-comparison}, we observed that diverse AI teams tend to maintain larger repositories. For example, in at least half of the repositories, diverse teams maintain nearly twice as many code files (median: 26) as non-diverse teams (median: 16). In addition, both the \\textit{num-code-line} and the \\textit{num-modules} features are higher in diverse repositories. In terms of documentation, README files in diverse repositories include significantly more structured elements, such as lists, code blocks, and GitHub links, with a median of 8 lists in the README file compared to 4 in non-diverse repositories. This suggests that diverse AI teams prioritize clearer documentation and code readability, which may enhance maintainability and accessibility for contributors. Notably, there is little difference in the \\textit{num-img} and \\textit{has-license} features between the two repositories.",
        "summarize_figure": "2505.03082v1_Fig1",
        "summarization": "The chart compares the repository - engagement and popularity features of diverse and non - diverse AI repositories (dots represent mean values). In terms of repository - engagement features, except for the \"number of watchers\" feature, diverse repositories generally scored higher. For example, the median of \"number of forks\" in diverse repositories is almost twice that in non - diverse ones. However, there is little difference in the number of pull requests, but a large difference in the number of issues. Regarding popularity features, after excluding the \"has - model\" and \"has - shell\" features (which were rare in both groups), diverse teams tend to maintain larger repositories, with the median number of code files nearly twice that of non - diverse teams, and higher numbers of code lines and modules. In terms of documentation, README files in diverse repositories have more structured elements, indicating that diverse teams prioritize clear documentation and code readability for better maintainability and accessibility, while there is little difference in the \"number of images\" and \"has - license\" features."
    },
    "143": {
        "figure1": "2505.03082v1_Fig2",
        "label1": "fig:code-quality",
        "caption1": "Code quality comparison of diverse and non-diverse AI repositories ($\\bullet$ represents the mean value).",
        "text": "As shown in Fig.~\\ref{fig:code-quality}, we found significant differences in code quality between diverse and non-diverse repositories across all three categories (Section~\\ref{subsec:annotate-diversity}). Specifically, non-diverse repositories showed a higher number of code smells, particularly in small and medium repositories. However, in large repositories, the distributions overlap more for diverse repositories. Additionally, both cyclomatic complexity and cognitive complexity were generally lower in diverse repositories. These findings suggest that gender-diverse teams incorporate a broader range of cognitive styles and problem-solving approaches, which leads to improved code quality \\cite{page2008difference, park2015effects}. Furthermore, gender diversity may enhance communication styles and collaborative strategies, contributing to better software development practices. Catolino et al. \\cite{catolino2019gender} also found that the presence of women reduces community smells, which in turn mitigates code smells and enhances overall software quality \\cite{palomba2018beyond}.",
        "summarize_figure": "2505.03082v1_Fig2",
        "summarization": "The chart compares the code quality of diverse and non - diverse AI repositories (dots represent mean values), covering small, medium, and large repositories. In terms of the number of code smells, non - diverse repositories have more in small and medium - sized ones, and in large repositories, the distribution overlaps more for diverse repositories. The cyclomatic complexity and cognitive complexity in diverse repositories are generally lower. This indicates that gender - diverse teams improve code quality by incorporating a wider range of cognitive styles and problem - solving approaches. Also, gender diversity may enhance communication styles and collaborative strategies, which is beneficial to software development. Research has also found that the participation of women can reduce community smells, thereby alleviating code smells and improving the overall software quality."
    },
    "144": {
        "figure1": "2505.03082v1_Fig3",
        "label1": "file_ratio",
        "caption1": "Ratio of multiple-authored, unknown-authored, male-authored and female-authored code files.",
        "text": "While our analysis of RQ2 demonstrates that gender-diverse teams generally produce higher-quality code than non-diverse teams, the individual contributions within these diverse teams remain unclear. Therefore, in this research question, we investigated whether the code authored by female contributors demonstrates higher quality than that authored by male developers within diverse AI repositories. As shown in Fig.~\\ref{file_ratio}, the male-authored code files are significantly higher in both medium and large repositories, whereas female-authored code files are comparatively less. Notably, in medium repositories, we observed no code files authored exclusively by female contributors. Similarly, in large repositories, only 16.14\\% (169 out of 417) of files were authored solely by female contributors. However, small repositories exhibited the highest proportion of multiple-authored code files, suggesting a greater tendency for collaboration in these projects.    Next, we examined the code quality of male-authored and female-authored files. Since no exclusively female-authored code files were found in medium-sized repositories, we excluded them from the comparative analysis.",
        "summarize_figure": "2505.03082v1_Fig3",
        "summarization": "The chart shows the ratios of multiple - authored, unknown - authored, male - authored, and female - authored code files in small, medium, and large repositories. In medium and large repositories, male - authored code files have a significantly higher proportion, while female - authored ones have a lower proportion. There are no code files authored solely by female contributors in medium - sized repositories, and in large repositories, female - authored code files account for only 16.14%. The proportion of multiple - authored code files is highest in small repositories, indicating a stronger tendency for collaboration in such projects."
    },
    "145": {
        "figure1": "2505.03084v1_New_Heat",
        "label1": "fig:example",
        "caption1": "setup{justification=centering} \\caption{Matrix summarizing existing research on cross-modal attacks. Rows indicate the impacted modality, columns indicate the attack modality and the type of attack execution. Section refers to the specific location within this survey that provides a discussion of the relevant literature and its source.",
        "text": "In Figure~\\ref{fig:example}, we summarize the threat landscape with cross-modal influence. We observe that optimization-based attacks are the most studied in the literature. However, other attacks typically accompany optimization attacks, and those other attacks are the main goals of the attacker. Backdoor is the next most well-studied attack, followed by Membership Inference and Model Inversion. As we can see, the threat landscape is growing quickly, and the grid is getting dense. Examples of research like \\cite{carlini2021extracting} can produce a hybrid attack, which is concerning. Research from \\cite{bagdasaryan2024adversarial} targets the embedding space, which is hard to detect even for seasoned ML practitioners.",
        "summarize_figure": "2505.03084v1_New_Heat",
        "summarization": "The chart is a matrix summarizing existing research on cross - modal attacks. Rows indicate the impacted modality, columns indicate the attack modality and execution type, with \"Section\" corresponding to the specific location in the survey for relevant literature. Colors mark research status: green for done research, gray for no research, and white for the same modality. It shows that optimization - based attacks are most studied, followed by backdoor attacks, with membership inference and model inversion less studied. The threat landscape is evolving rapidly and becoming denser. Some research can produce hybrid attacks, and others target the embedding space, which is hard to detect."
    },
    "146": {
        "figure1": "2505.03088v1_main_block_diagram",
        "label1": "fig:fdi-architecture",
        "caption1": "Global task-aware fault detection and isolation for a distributed spacecraft netwrok.",
        "text": "A fault detection, isolation, and recovery (FDIR) architecture is essential to accommodate potential faults at both the network and individual agent levels to continue the mission with graceful degradation. In this work, we present a new FDI method, as shown in~\\cref{fig:fdi-architecture}, that detects the failure at the network level using an abstraction of the global task objective $\\costH$ and local sensing information and informs the agent level FDIR algorithm to perform necessary actions for recovery. For example, a fault at the network level could be due to communication loss, a global task sensor fault (for inspection task), a global task actuator fault (for on-orbit construction), and a fault at the agent level could be due to thruster or reaction wheels. We propose a simulation vs. real comparison using the $\\costH$ and its higher-order gradients. We detect the fault by computing the off-nominal behavior from the expected global task objective $\\costH$ by monitoring the individual task using a residual vector sensitive to the agent's faults. The global task objective $\\costH$ is designed to be a function of the state of agents in the network and a model of the task sensor (for inspection) or actuator (for construction). The residual vector is a function of the local relative state estimates of the agents in the network. We propose a metric that computes the deviation of the $\\costH$ from the expected performance, which is used as an indicator for faults at the network level, and uses higher-order derivatives of $\\costH$ to infer if the agent-level faults, as shown in~\\cref{fig:fdi-architecture}.\nThe function $\\sigma (\\cdot,\\cdot)$ corresponds to information per pixel. It incorporates sensor chracteristics such as the current uncertainty of the spacecraft's pose estimate, the accuracy of the sensor based on the distance between $\\vecp$ and $\\vecs$, or the lighting conditions. Here, we use a simple RGB camera sensor and no environmental noise~\\cite{DBLP:journals/ijrr/SchwagerRS11}: where $\\operatorname{dist}(\\vecp,\\vecs)$ is the Euclidean distance between POI $\\vecs$ and pose $\\vecp$. We compute $\\sigma$ using visbility checking. The offline solution to problem \\ref{prob:info_multi_agent_control} is used to predict the nominal system behavior in terms of the information-based cost $\\costH_{nom}$ over a finite time interval (1 or 2 orbits). As described in~\\cref{fig:fdi-architecture}, we precompute the nominal behaviour $\\costH_{nom}$ and compare it to the real-time behaviour $\\costH$ over the time hotizon $t$ as follows: If the real-time value deviates from the nominal behavior by a threshold $\\Delta \\costH_{\\mathrm{threshold}}$ then a fault is detected. In the following section, we discuss on how we modify the cost function to construct the FDI architecture in~\\cref{fig:fdi-architecture}.\nThe function $\\sigma (\\cdot,\\cdot)$ corresponds to information per pixel. It incorporates sensor chracteristics such as the current uncertainty of the spacecraft's pose estimate, the accuracy of the sensor based on the distance between $\\vecp$ and $\\vecs$, or the lighting conditions. Here, we use a simple RGB camera sensor and no environmental noise~\\cite{DBLP:journals/ijrr/SchwagerRS11}: where $\\operatorname{dist}(\\vecp,\\vecs)$ is the Euclidean distance between POI $\\vecs$ and pose $\\vecp$. We compute $\\sigma$ using visbility checking. In the following, we discuss on how we modify the cost function to construct the FDI architecture in~\\cref{fig:fdi-architecture}.",
        "summarize_figure": "2505.03088v1_main_block_diagram",
        "summarization": "The chart shows the global task - aware fault detection and isolation architecture for a distributed spacecraft network. The \"Global Task - Awareness\" part on the left includes information - based GNC simulation and real - world modules, outputting inspection cost simulation and real - world values respectively. These are fed into the \"Fault Detector and Identifier\" in the \"Fault Detection and Identification\" part on the right. This part also uses \"Threshold\" and \"Metric\" to detect and identify faults, outputting failed agent IDs to drive the \"Actuator\" and \"Sensor\" for fault recovery, ensuring the mission can continue with graceful degradation."
    },
    "147": {
        "figure1": "2505.03088v1_fdi_faults",
        "label1": "fig:faults",
        "caption1": "An overviwe of the type of global and local faults detected and identified using the proposed FDI architecture, metrics and the threshold.",
        "text": "The goal of the work is to detect the global and local faults in a multi-spacecraft system performing global tasks such as: on-orbit inspection and on-orbit construction. The global behaviour faults and local faults detected using the proposed approach are described in the following Figure~\\ref{fig:faults}. Note that the global behaviour fault for inspection task could lead to both deteriorating and improved performance. The improved performance is due two reasons: 1) failure in the controller at the agent level that leads to better exploration of the inpection target, and 2) suprious signals being communicated by the neighbours. The deteriorating performance is due to failure in the inspection sensor or the pointing controller of the spacecraft. The behaviour fault detection is used as an input to identify the agent and the type of fault. The fault detection and identification problem addressed in this work is summarized as follows:      Given the nominal expected global task performance $\\costH_{nom}$ and the real-time performance $\\costH$, detect the global and agent-level faults in the multi-spacecraft system performing a global task (collaborative inspection or construction). The global and agent-level fault tree is described in the~\\cref{fig:faults}. In the following sections, we describe the derivation of the metrics and threshold used in the two FDIR blocks shown in Figure~\\ref{fig:faults} for fault detection and identification.",
        "summarize_figure": "2505.03088v1_fdi_faults",
        "summarization": "The chart shows the types of global and local faults detected and identified using the proposed Fault Detection and Isolation (FDI) architecture, metrics, and threshold. Global faults are divided into deteriorating and improved performance. The former is caused by inspection sensor or spacecraft pointing controller failures, while the latter results from agent - level controller failures or spurious signals from neighbors. Global faults are detected by the global task cost metric and further analyzed by the higher - order cost gradient - based metric. After fault detection, the faulty agent ID is determined to identify whether the fault occurs in the global task sensor/actuator, control actuator, or on - board sensor and other local locations."
    },
    "148": {
        "figure1": "2505.03090v1_sistema2",
        "label1": "sismod",
        "caption1": "ISAC system model composed by one BS and 2 RIS.",
        "text": "The user localization process is performed by the BS through exclusive sensing signals {(Sensing Frame, Figure \\ref{fig:tdd})}. These signals are transmitted to cover a region {with a wide beam }and, upon interacting with environmental elements, are reflected omnidirectionally before returning to the BS through three distinct paths. These paths are described below and illustrated in Figure \\ref{sismod}:\nFirst, the equations of spheres centered at the points $(BS_x, BS_y, BS_z)$, $(R1_x, R1_y, R1_z)$ and $(R2_x, R2_y, R2_z)$ are presented, as pointed in Figure \\ref{sismod}. These equations are organized into the system below, &(x - BS_x)^2 + (y - BS_y)^2 + (z - BS_z)^2 = d_1^2; \\label{eq1} \\\\ &(x - R1_x)^2 + (y - R1_y)^2 + (z - R1_z)^2 = d_2^2; \\label{eq2}\\\\ &(x - R2_x)^2 + (y - R2_y)^2 + (z - R2_z)^2 = d_3^2, \\label{eq3} in which $\\mathbf{d}=[d_1,d_2,d_3]^T$ represents the distances from the UE's point of interest to the anchors.\nThe ToA metric is calculated by the system to measure the propagation time of the sensing signal across the three paths depicted in Figure \\ref{sismod}. {Although delays are typically derived from known distances, in practical scenarios the distances are unknown and must be estimated from the observed delays. This estimation can be performed, for example, by identifying the correlation peak between the transmitted and reflected signals. It is important to note that the estimated time delays include inherent errors, as they represent the true delays plus a certain estimation error $\\epsilon$. Therefore, for the purposes of this work, we adopt the approximation that these estimated delays are sufficiently accurate for subsequent processing.}\nThe two RISs must also be properly configured to reflect the signals received from the user to reflect back to the BS. A random configuration of the RIS would lead to disordered reflections, forming a hemispherical omnidirectional pattern with low power gain, which is insufficient for system requirements.  Therefore, it is essential to leverage RIS technology to create an ordered reflection pattern that strategically covers the region, particularly within the first quadrant (1Q), as illustrated in Figure \\ref{sismod}. This configuration ensures that the RIS can redirect reflections toward the BS with sufficient power gain.",
        "summarize_figure": "2505.03090v1_sistema2",
        "summarization": "The picture shows an ISAC system model consisting of one Base Station (BS) and two Reconfigurable Intelligent Surfaces (RISs). The coordinates of the BS (P_BS), two RISs (P_RIS1 and P_RIS2), and the User Equipment (P_UE) are marked as (BS_x, BS_y, BS_z), (R1_x, R1_y, R1_z), (R2_x, R2_y, R2_z), and (x*, y*, z*) respectively. The distances d1 from BS to UE, d2 from the first RIS to UE, and d3 from the second RIS to UE are also indicated. The user localization is carried out by the BS through exclusive sensing signals, which return via three paths. Sphere equations can be established based on these path - distances for localization. Moreover, the two RISs need proper configuration to reflect signals back to the BS, avoiding disordered reflections and ensuring sufficient power gain."
    },
    "149": {
        "figure1": "2505.03090v1_TDD3",
        "label1": "fig:tdd",
        "caption1": "Proposed ISAC Frame Structure with Time Allocation.",
        "text": "The user localization process is performed by the BS through exclusive sensing signals {(Sensing Frame, Figure \\ref{fig:tdd})}. These signals are transmitted to cover a region {with a wide beam }and, upon interacting with environmental elements, are reflected omnidirectionally before returning to the BS through three distinct paths. These paths are described below and illustrated in Figure \\ref{sismod}:\nReflections enable the BS to determine the location of the user information using the trilateration technique. However, employing three anchors results in two possible location points for the user. {Our system is designed to provide the user's height (inside the Permission and Response Frame, Figure \\ref{fig:tdd}) as additional data to BS identify which of these two candidate points corresponds to the actual user location}.\nAdditionally, the communication and sensing are structured as a system similar to TDD in which specific time slots are allocated to each function as illustrated in Figure~\\ref{fig:tdd}.\nThe methodology is based on a periodic time loop of $0.5$ seconds, as illustrated in Figure \\ref{fig:tdd}. At each time instant within this loop, multiple samples are generated using the Monte Carlo method. For each sampling instance, the ToA is recorded, the anchors are identified using the AoA, and with the response frame containing height identification. Consequently, the estimated user position, $\\hat{\\mathbf{p}}_i$, is determined for the $i$-th sample.",
        "summarize_figure": "2505.03090v1_TDD3",
        "summarization": "The chart shows the proposed ISAC frame structure and its time allocation. The cycle is 0.5 seconds. It includes a 20 - microsecond Sensing Frame for the base station to send sensing signals for user localization, followed by a 0.3 - microsecond Guard Time. Then comes a 20 - microsecond Permission and Response Frame, which provides additional data like user height to the base station to determine the actual user location, followed by another 0.3 - microsecond Guard Time. Finally, there is a 0.4999594 - second Communication Frame for data communication. Each function proceeds in an orderly manner according to this time allocation."
    },
    "150": {
        "figure1": "2505.03090v1_AoA2",
        "label1": "fig:AoA",
        "caption1": "Path identification based on AoA technique.",
        "text": "{Figure \\ref{fig:AoA} illustrates how the BS functions as the AoA sensor in the proposed system, and how the steering vectors toward the RISs are implemented for comparison with the sensing signals received at the BS. }% in which TBPD$_{BS_i}$ means time-based power detection at the i-th Antenna of the BS}",
        "summarize_figure": "2505.03090v1_AoA2",
        "summarization": "The picture shows path identification based on the Angle of Arrival (AoA) technique. It marks the Base Stations (BS1 and BSM), User Equipment (P_UE), and two Reconfigurable Intelligent Surfaces (P_RIS1 and P_RIS2). The arrows from the base stations to the RISs represent steering vectors, a(θ_RIS1) and a(θ_RIS2). In this system, the base station acts as an AoA sensor. By comparing these steering vectors with the sensing signals received at the base station, path identification is achieved."
    },
    "151": {
        "figure1": "2505.03092v1_prisma_2",
        "label1": "prisma_dispersa",
        "caption1": "\\baselineskip=8pt{\\small Dispers\\~{a}o da luz por um prisma.",
        "text": "Apesar de os experimentos de espalhamento de feixes de part\\'{\\i}culas pela mat\\'{e}ria, realizados no per\\'{\\i}odo de 1909--1911,  sob a lideran\\c{c}a de Ernest Rutherford, terem sidos determinantes no estabelecimento da estrutura dos \\'{a}tomos, pode-se tra\\c{c}ar tamb\\'{e}m a origem de muitas das propriedades e ideias sobre a estrutura at\\^{o}mica \\`{a}s investiga\\c{c}\\~{o}es realizadas ao final do s\\'{e}culo XIX, entre 1855 e 1863, por Robert Bunsen e Gustav Kirchhoff,  ao observarem a luz emitida por diversas subst\\^{a}ncias, associadas a elementos qu\\'{\\i}\\-mi\\-cos distintos, quando aquecidas pelo famoso bico de Bunsen. Nessas observa\\c{c}\\~{o}es, a luz emitida incide sobre um prisma e, ent\\~{a}o, \\'{e} dispersada de tal modo que as componentes de diferentes cores -- correspondendo a diferentes fre\\-qu\\^{e}n\\-cias, ou comprimentos de onda -- s\\~{a}o refra\\-ta\\-das em diferentes \\^{a}ngulos (Figura~\\ref{prisma_dispersa}).\nSe os raios difratados s\\~{a}o interceptados por um anteparo, como a lente de uma luneta que tem uma escala milimetrada, observa-se, no caso de gases ou vapores contendo, por exemplo, os elementos hidrog\\^{e}nio, merc\\'{u}rio ou s\\'{o}dio, um pa\\-dr\\~{a}o de linhas luminosas de v\\'{a}rias cores, como mostrado na Figura~\\ref{espectro_h} para o hidrog\\^{e}nio. Cada uma dessas  linhas est\\'{a} associada a um comprimento de onda ($\\lambda$), ou a uma frequ\\^{e}ncia ($\\nu$), que pode ser determinado pelo \\^{a}n\\-gulo ($\\theta$) entre a dire\\c{c}\\~{a}o do feixe de luz incidente no prisma e a dire\\c{c}\\~{a}o do raio emergente cor\\-respondente (Figura~\\ref{prisma_dispersa}).\\footnote{\\, Esse \\^{a}ngulo ($\\theta$) depende da abertura angular ($A$) do prisma, e do \\'{\\i}ndice de refra\\c{c}\\~{a}o associado ao respectivo comprimento de onda.} Esse conjunto de linhas \\'{e} chamado \\textit{espectro do elemento qu\\'{\\i}mico},\\footnote{\\, Em geral as subst\\^{a}ncias exibem linhas espectrais, devidas aos \\'{a}tomos, e regi\\~{o}es onde a radia\\c{c}\\~{a}o est\\'{a} dis\\-tri\\-bu\\'{\\i}\\-da continuamente -- bandas espectrais -- tendo como origem as mol\\'{e}culas.} e cada elemento est\\'{a} associado a um conjunto de linhas espectrais caracter\\'{\\i}stico, como se fosse uma esp\\'{e}cie de ``impress\\~{a}o digital'', \\'{u}nica para cada elemento.  }",
        "summarize_figure": "2505.03092v1_prisma_2",
        "summarization": "The picture shows the dispersion of light by a triangular prism. On the left, a substance is heated by a Bunsen burner, and the incident light (luz incidente) shines on the prism (A). After passing through the prism, the light is dispersed into different - colored lights, corresponding to different frequencies or wavelengths, and refracted at different angles (θ), forming a rainbow - like spectrum. This phenomenon reflects the discovery of Robert Bunsen and Gustav Kirchhoff in the 19th century, that is, the light emitted by different elements will be refracted at different angles after dispersion by a prism, forming a characteristic spectrum."
    },
    "152": {
        "figure1": "2505.03092v1_balmer-espectro_3",
        "label1": "espectro_h",
        "caption1": "\\baselineskip=6pt{\\small Espectro do \\'{a}tomo de hidrog\\^{e}nio na faixa vis\\'{\\i}vel, de frequ\\^{e}ncias da ordem de $10^{15}$~Hz. Os comprimentos de onda ($\\lambda$) est\\~{a}o expressos em angstron (\\AA), sendo  1\\,\\AA\\, = $10^{-8}$\\,m.} ",
        "text": "Se os raios difratados s\\~{a}o interceptados por um anteparo, como a lente de uma luneta que tem uma escala milimetrada, observa-se, no caso de gases ou vapores contendo, por exemplo, os elementos hidrog\\^{e}nio, merc\\'{u}rio ou s\\'{o}dio, um pa\\-dr\\~{a}o de linhas luminosas de v\\'{a}rias cores, como mostrado na Figura~\\ref{espectro_h} para o hidrog\\^{e}nio. Cada uma dessas  linhas est\\'{a} associada a um comprimento de onda ($\\lambda$), ou a uma frequ\\^{e}ncia ($\\nu$), que pode ser determinado pelo \\^{a}n\\-gulo ($\\theta$) entre a dire\\c{c}\\~{a}o do feixe de luz incidente no prisma e a dire\\c{c}\\~{a}o do raio emergente cor\\-respondente (Figura~\\ref{prisma_dispersa}).\\footnote{\\, Esse \\^{a}ngulo ($\\theta$) depende da abertura angular ($A$) do prisma, e do \\'{\\i}ndice de refra\\c{c}\\~{a}o associado ao respectivo comprimento de onda.} Esse conjunto de linhas \\'{e} chamado \\textit{espectro do elemento qu\\'{\\i}mico},\\footnote{\\, Em geral as subst\\^{a}ncias exibem linhas espectrais, devidas aos \\'{a}tomos, e regi\\~{o}es onde a radia\\c{c}\\~{a}o est\\'{a} dis\\-tri\\-bu\\'{\\i}\\-da continuamente -- bandas espectrais -- tendo como origem as mol\\'{e}culas.} e cada elemento est\\'{a} associado a um conjunto de linhas espectrais caracter\\'{\\i}stico, como se fosse uma esp\\'{e}cie de ``impress\\~{a}o digital'', \\'{u}nica para cada elemento.  }\nDe particular import\\^{a}ncia no desen\\-vol\\-vi\\-men\\-to da F\\'{\\i}sica At\\^{o}mica foi a caracteriza\\c{c}\\~{a}o do espectro de linhas mais simples, correspondente tamb\\'{e}m ao \\'{a}tomo mais simples -- o \\'{a}tomo de hidrog\\^{e}nio. O espectro do hidrog\\^{e}nio (Figura~\\ref{espectro_h}) foi observado por  Anders  {\\AA}ngstr\\\"{o}m, %~\\cite{Angstrom}, em 1853, e sua regularidade, matematizada por Johann  Balmer, %~\\cite{Balmer}, cerca de trinta anos depois, em 1885. Baseando-se nas medidas de {\\AA}ngstr\\\"{o}m, Balmer encontra uma express\\~{a}o capaz de reproduzir os comprimentos de onda de cada linha do espectro observado. A f\\'{o}rmula foi escrita de forma mais sugestiva por Johannes  Rydberg, em 1888, %~\\cite{Rydberg}, em 1888, como sendo o comprimento de onda ($\\lambda$) expresso em metros, e $R_H \\simeq 1,\\!097 \\times 10^7$\\,m$^{- \\scriptstyle 1}$, a constante de Rydberg.\\footnote{\\, O valor recomendado pelo \\textit{Committee on Data for Science and Technology} (CODATA), em 2022, \\'{e} $10\\, 973\\, 731,\\! 568\\, 157\\, (12)$~m$^{- \\scriptstyle 1}.$}",
        "summarize_figure": "2505.03092v1_balmer-espectro_3",
        "summarization": "The picture shows the spectrum of the hydrogen atom in the visible light band, with frequencies on the order of 10¹⁵Hz, and wavelengths (λ) in angstroms (Å), where 1Å = 10⁻⁸m. Different vertical lines in the figure correspond to different n values (3, 4, 5, 6), and the corresponding wavelength values below are 6562.8, 4861.3, 4340.5, and 4101.7. These are characteristic spectral lines of the hydrogen atom, which are of great significance for the development of atomic physics. Its pattern was observed by Anders Ångström, mathematically expressed by Johann Balmer, and given a more suggestive formula by Johannes Rydberg."
    },
    "153": {
        "figure1": "2505.03098v1_CRBFig1",
        "label1": "fig:1",
        "caption1": " CRB tests for single sinusoid (averaged over $10000$ random realizations). We consider $K=1$ sinusoid with parameters: $\\omega_1 T = 1.05$, $a_1 = 1$, $\\epsilon = 9.83 \\times 10^{-6}$, and $N = 100$ samples, resulting in $M = 6$ spikes. (a) Scatterplot of the retrieved frequencies. (b) Performance evaluation compared to the \\crlb in \\thmref{thm:2}.",
        "text": "The spectral estimation results obtained using the matrix pencil method \\cite{Hua:1990:J} are presented in \\fig{fig:1}. As shown in \\subfig{fig:1}{b}{}, the algorithm's performance can be classified into three regions based on the PSNR value:",
        "summarize_figure": "2505.03098v1_CRBFig1",
        "summarization": "This picture shows the Cramer - Rao Bound (CRB) test results for a single sinusoid (averaged over 10000 random realizations). The left plot is a scatterplot of retrieved frequencies, with blue dots representing Sine - 1. The x - axis is the input Peak Signal - to - Noise Ratio (PSNR) and the y - axis is the digital frequency, showing the distribution of frequency data with respect to PSNR. The right plot is the standard deviation of estimates. The blue curve is for Sine - 1 and the gray curve is for CRB (Sine - 1). The x - axis is PSNR and the y - axis is Root - Mean - Square Error (RMSE). It indicates that as PSNR increases, RMSE decreases, and the algorithm saturates when impulsive noise dominates."
    },
    "154": {
        "figure1": "2505.03099v1_NumericalApproximation2",
        "label1": "NumApprox",
        "caption1": "The state of the astronaut at each moment in time dictates the state of the astronaut at a moment in time 0.5 seconds later, which in turn dictates the state of the astronaut 0.5 seconds after that, and so on until the astronaut reaches the surface of the planet.",
        "text": "For the Falling Astronaut Problem, can approximate $g$ to be constant over both short spatial intervals and short time intervals. Indeed, $g$ will not change substantially over a brief duration of time such as 0.5 seconds. This is the key insight behind the numerical approximation technique we will use in this paper. For any given position of the astronaut, we can calculate $g$ at that position using Equation \\ref{needapproximation}. We then assume that $g$ is approximately constant for the next 0.5 seconds. With this known and approximately constant acceleration, we can calculate how far the astronaut moves over the course of 0.5 seconds. In other words, we calculate her new position at the end of that time interval. At that point, we use the new value of position to calculate the new acceleration, which we approximate as constant for 0.5 more seconds, and the cycle repeats. A diagram visualizing the process is shown in Figure \\ref{NumApprox}.",
        "summarize_figure": "2505.03099v1_NumericalApproximation2",
        "summarization": "This picture shows the numerical approximation calculation process for the Falling Astronaut Problem. Starting from the given initial position x and velocity v, first calculate the gravitational acceleration g and approximate it as constant for 0.5 seconds. Then calculate the new position x and new velocity v after 0.5 seconds. Next, use the new position to calculate the new gravitational acceleration g, which is also approximated as constant for the next 0.5 seconds. This cycle repeats until the astronaut reaches the surface of the planet."
    },
    "155": {
        "figure1": "2505.03099v1_numericalgraph",
        "label1": "numericalgraph.png",
        "caption1": "The position of the astronaut above the center of the Earth is plotted against time using the Euler method, a numerical approximation.",
        "text": "We can also plot every position value $x_n$ with its corresponding time $t$ on the horizontal axis, as shown in Figure \\ref{numericalgraph.png}. As in the previous section, this graph constitutes an answer to part $(b)$ of the question. However, we do not need to read the data directly from the graph. Rather, for any time $t$, we can iterate Equation \\ref{key} an appropriate number of times until we arrive at the corresponding $x_n$. In this way, we can find the astronaut's position for any given time.\nNote that Figure \\ref{numericalgraph.png} provides approximately the same data as in Figure \\ref{graph}. As in part $(a)$, both the analytical derivation and the numerical approximation yield approximately the same results.\nStudents are encouraged to experiment with the features in the Chart Editor and the ``Customize\" sub-menu. For example, upon clicking on the ``Vertical Axis\" tab, students can adjust the minimum and maximum values that the graph displays on the position-axis. A student can set the minimum value to be 6371000 m, causing the graph to closer resemble Figure \\ref{numericalgraph.png}. Note that students are encouraged to set a minimum value for 0 or greater, as Equation \\ref{GoogleSheet} become un-physical for negative values of position.",
        "summarize_figure": "2505.03099v1_numericalgraph",
        "summarization": "This picture is a graph of the position of an astronaut relative to the center of the Earth over time, plotted using the Euler numerical approximation method. The blue straight line represents the radius of the Earth, which remains constant. The red curve represents the position of the astronaut, which decreases over time. As time increases from 0 to about 1250 seconds, the astronaut's position gradually decreases from about 10000000 meters to close to the Earth's radius of about 6000000 meters."
    },
    "156": {
        "figure1": "2505.03100v1_proposed-method",
        "label1": "fig:proposed-method",
        "caption1": "The proposed methodology for quantitative evaluation of LLM-based timeline analysis",
        "text": "To assess the performance of an LLM for timeline analysis, several aspects are important as depicted in Fig.~\\ref{fig:proposed-method}.  We must define one or more tasks (Sec.~\\ref{sec:task_definition}) that we expect the LLM to perform. This involves designing a prompt to interact with the system, such as summarizing events into high-level insights or identifying indicators of compromise. In addition, a ground-truth dataset is needed that can be used to assess the outcome of an LLM (Sec.~\\ref{sec:ground_truth}). Lastly, evaluation metrics are required that allow us to compare the ground truth with LLM output (Sec.~\\ref{subsec:evaluation-metrics}).  While starting with the tasks may seem natural, we recommend beginning with the evaluation metric, as it defines the required output, which in turn influences the task and prompt.",
        "summarize_figure": "2505.03100v1_proposed-method",
        "summarization": "This picture shows the proposed methodology for the quantitative evaluation of LLM - based timeline analysis. First, define timeline analysis tasks and prompt design, clarifying the tasks expected of the LLM and designing prompts for interaction. Then, build a ground - truth dataset to assess the LLM's output. Finally, define evaluation metrics to compare the ground truth with the LLM's output. The entire process shows a sequential logical relationship."
    },
    "157": {
        "figure1": "2505.03100v1_example-chat",
        "label1": "fig:example-chat",
        "caption1": "A conversation sample between an investigator and ChatGPT. Note that the prompt should be accurate following the OpenAI prompt engineering guides",
        "text": "The interaction between the user and ChatGPT is outlined in Fig.~\\ref{fig:example-chat}. We provide a persona, such as stating a role (e.g., forensic investigator), including detailed information about the task (e.g., event type or data format), and offer additional tools to improve accuracy. These steps help the system to manage responses more accurately. The prompt uses a space delimiter to provide suitable spacing to separate key pieces of information.\nA sample result of the given prompts and the ChatGPT answers is depicted in Fig.~\\ref{fig:example-chat}.  The evaluation results for the used datasets are shown in Table~\\ref{tab:experimental-results} where the metric values represent the mean values for each task.\nOur research indicates that ChatGPT uses a virtual environment to run Python code when responding to user prompts. This means we can install the \\texttt{dftpl} Python wheel installer within that virtual environment.  To respond to the user prompts, ChatGPT generates Python code as shown in Fig.~\\ref{fig:example-chat}. For example, if the parser example is designed to work for all supported events, ChatGPT can summarize a specific event, such as the last shutdown event on Windows. One can click the `[$>$\\_]'-button to view the generated Python source code. Thus, experienced investigators may validate the code and with it the answer. Finally, the results can be downloaded in a JSON format and this file will be quantitatively evaluated based on the ground truth from Sec.~\\ref{subsubsec:event-reconstruction}.\nIn contrast, the result for a single event with additional knowledge, i.e., using the \\texttt{dftpl} library, shows near-perfect performance, with a BLEU score of 0.999 and ROUGE-1, ROUGE-2, and ROUGE-L scores, all at 1.000. This indicates that the ChatGPT output almost exactly matched the reference in terms of precision, word overlap, and sequence structure. The high scores suggest that, with additional knowledge, the system was able to mimic the expected results. The reason is that we gave a Python library that can summarize events based on the method described in \\citet{Hargreaves2012} to ChatGPT (Fig.~\\ref{fig:example-chat}). Although we did not explicitly instruct ChatGPT to follow a particular order, the ground truth output produced by the \\texttt{dftpl} library is chronologically ordered by timestamp. For the multiple event summarization task, the evaluation scores were lower because ChatGPT generated the correct events but in a different order than the ground truth.  The beginning of the file displays timestamps that increase or remain the same, indicating a mostly sorted order. Similarly, the end of the file follows a chronological pattern. However, the middle sections break this order, with some events appearing earlier than preceding ones. This discrepancy in ordering affected the BLEU and ROUGE scores, which are sensitive to the sequence of words or structures. Importantly, while the order differed, the extracted content was sometimes semantically correct and forensically valid. Future work may include implementing order-invariant evaluation metrics or normalizing the output order before comparison to address this issue.",
        "summarize_figure": "2505.03100v1_example-chat",
        "summarization": "The picture shows a conversation sample between an investigator and ChatGPT. The user first sets the role (forensic investigator) and provides task details (such as using the dftpl tool to summarize high - level events in the \"message\" column of a CSV file), and also provides a Python wheel file to install the dftpl tool. ChatGPT responds that the tool is successfully installed. At the user's request, it provides a Python code example for using the tool and the results of summarizing events in the CSV file according to the specified event type (last - shutdown). The results can be downloaded as a JSON file for evaluation. ChatGPT generates Python code to answer, and experienced investigators can verify the code and the answer. This interaction demonstrates how to make ChatGPT complete specific data analysis tasks through accurate prompts."
    },
    "158": {
        "figure1": "2505.03100v1_building-datasets",
        "label1": "fig:building-ground-truth",
        "caption1": "Building ground truth for LLM evaluation",
        "text": "The first step was to create a dataset as no appropriate dataset was available. The procedure is illustrated in Fig.~\\ref{fig:building-ground-truth} and the dataset is shared through Zenodo.  Our test bed was a Windows 11 machine within a virtual environment simulating regular computer usage. All activities were recorded using screen capture (video) and therefore are documented (written notes).",
        "summarize_figure": "2505.03100v1_building-datasets",
        "summarization": "The picture shows the process of building a ground - truth dataset for Large Language Model (LLM) evaluation. First, use Windows 11 Enterprise (Evaluation) to run in the VMware Fusion virtual machine. During this, perform several activities like Google search and record the activity time and details as ground - truth data. Then save it as a VMDK file. Finally, use this file to build a forensic timeline, thus creating a ground - truth dataset for LLM evaluation."
    },
    "159": {
        "figure1": "2505.03102v1_core2",
        "label1": "fig:hw:vortex_extension",
        "caption1": "Vortex core architecture. Modified sections (highlighted in yellow) support cooperative groups and warp-level functions}  \\vspace{-1mm",
        "text": "To support warp-level functions and cooperative groups at the hardware level, we need architectural modifications and ISA extensions. \\Cref{fig:hw:vortex_extension} shows the modified Vortex core architecture and \\Cref{tab:instructions} presents the introduced ISA.",
        "summarize_figure": "2505.03102v1_core2",
        "summarization": "The picture shows the Vortex core architecture. The sections highlighted in yellow are modified to support cooperative groups and warp - level functions. The architecture process starts from Schedule, including components like Warp Mask and Warp Scheduler; goes through Icache Bus in the Fetch stage, to Decoder in the Decode stage; then to the Issue stage involving Ibuffer, Scoreboard, etc.; the Execute stage has execution units such as Tile and SFU; and finally, the Commit unit in the Commit stage completes the operation. These modifications support relevant functions at the hardware level."
    },
    "160": {
        "figure1": "2505.03110v1_Figures_Observation-noise-model_IIP",
        "label1": "Fig_with_and_without_observation-noise",
        "caption1": "Comparison of seasonal adjustment with observation noise (left) and without observation noise (right) for IIP data. Upper plots: $m_1=1$, bottom plots: $m_1=2$. In each plots, the panels from top to bottom show the original data and the trend component, the seasonal component, the AR component, and the observation noise component, respectively.} \\end{center",
        "text": "Figure \\ref{Fig_with_and_without_observation-noise} shows the decomposition results for the four cases, differentiated by the presence or absence of observation noise and the trend order ($m_1 = 1$ or $2$), with the AR order selected to minimize the AIC. The top panels correspond to the first-order trend model, while the bottom panels correspond to the second-order trend model. The left-hand panels display results with observation noise, and the right-hand panels without it.\nFigure \\ref{Fig_with_and_without_observation-noise} presents the decomposition results from the seasonal adjustment model, where the AR order corresponds to the minimum AIC. The results indicate that models with and without observation noise yield nearly identical decompositions. When the trend order is set to 1, the trend component reduces to a constant, and the trend information is largely absorbed into the AR component. In contrast, with a trend order of 2, the trend captures the long-term structure of the time series, while the short-term fluctuations are represented by the AR component.",
        "summarize_figure": "2505.03110v1_Figures_Observation-noise-model_IIP",
        "summarization": "The picture compares the seasonal adjustment results of Industrial Production Index (IIP) data with and without observation noise. The upper - left and lower - left sub - plots are for the case with observation noise, while the upper - right and lower - right sub - plots are for the case without it. The upper - row sub - plots correspond to the trend order M1 = 1, and the lower - row sub - plots correspond to M1 = 2. Each sub - plot shows the original data, trend, seasonal, autoregressive (AR), and observation noise components from top to bottom. The results show that the decomposition results of models with and without observation noise are nearly identical. When the trend order is 1, the trend component approaches a constant, and trend information is mostly absorbed by the AR component. When the trend order is 2, the trend captures the long - term structure of the time series, and the AR component represents short - term fluctuations."
    },
    "161": {
        "figure1": "2505.03110v1_Figures_Observation-noise-model",
        "label1": "Fig_with_and_without_observation-noise",
        "caption1": "Comparison of seasonal adjustment with observation (left) noise and without observation (right) for Blsallfood data. Upper plots: $m_1=1$, right: $m_1=2$. } \\end{center",
        "text": "Figure \\ref{Fig_with_and_without_observation-noise} shows the decomposition results for the four cases, differentiated by the presence or absence of observation noise and the trend order ($m_1 = 1$ or $2$), with the AR order selected to minimize the AIC. The top panels correspond to the first-order trend model, while the bottom panels correspond to the second-order trend model. The left-hand panels display results with observation noise, and the right-hand panels without it.\nFigure \\ref{Fig_with_and_without_observation-noise} presents the decomposition results from the seasonal adjustment model, where the AR order corresponds to the minimum AIC. The results indicate that models with and without observation noise yield nearly identical decompositions. When the trend order is set to 1, the trend component reduces to a constant, and the trend information is largely absorbed into the AR component. In contrast, with a trend order of 2, the trend captures the long-term structure of the time series, while the short-term fluctuations are represented by the AR component.",
        "summarize_figure": "2505.03110v1_Figures_Observation-noise-model",
        "summarization": "The picture compares the seasonal adjustment results of Blsallfood data with and without observation noise. The upper - left and lower - left sub - plots are for the case with observation noise, while the upper - right and lower - right sub - plots are for the case without it. The upper - row sub - plots correspond to the trend order M1 = 1, and the lower - row sub - plots correspond to M1 = 2. Each sub - plot shows the data decomposition, including the original data (with a trend line) and trend, seasonal, autoregressive (AR) components, etc. The results show that the decomposition results of models with and without observation noise are nearly identical. When the trend order is 1, the trend component approaches a constant, and trend information is mostly absorbed by the AR component. When the trend order is 2, the trend captures the long - term structure of the time series, and the AR component represents short - term fluctuations."
    },
    "162": {
        "figure1": "2505.03110v1_Figure_L2-estimates",
        "label1": "Fig_L2-estimates",
        "caption1": "Change of estimated parameters under $L_2$ regularization. $\\lambda = 0$, and $10^{j/10}, j=-8,\\ldots ,16$ ",
        "text": "The left panel of Figure \\ref{Fig_L2-estimates} illustrates how the partial autocorrelation (PARCOR) coefficients vary with respect to the regularization weight parameter $\\lambda$ in the model with a second-order trend ($m_1=2$)  and AR order $m_3=8$, applied to the Blsallfood data. The horizontal axis represents the value of $\\lambda$, while the vertical axis shows the corresponding PARCOR coefficients. As $\\lambda$ increases, the PARCOR values exhibit a smooth and gradual decline. The right panel provides a closer examination by displaying the trajectories of seven PARCOR coefficients, excluding $b_1$, which has the largest absolute value.\nj=-8,\\ldots ,16$ }\\label{Fig_L2-estimates}",
        "summarize_figure": "2505.03110v1_Figure_L2-estimates",
        "summarization": "The picture shows the change of estimated parameters under L2 regularization. The left plot corresponds to the case where a model with second - order trend (M1 = 2) and autoregressive order M3 = 8 is applied to Blsallfood data, showing how the partial autocorrelation (PARCOR) coefficients vary with the regularization weight parameter λ. The horizontal axis is the value of λ, and the vertical axis is the PARCOR coefficient. As λ increases, the PARCOR values show a smooth downward trend. The right plot shows the trajectories of seven PARCOR coefficients except b1 with the largest absolute value, presenting the change more meticulously."
    },
    "163": {
        "figure1": "2505.03112v1_confusion",
        "label1": "fig:confusion_deepseek32B",
        "caption1": "Confusion matrix for DeepSeek‑R1‑Distill‑Qwen‑32B using the I+C+S prompt on noisy signals.",
        "text": "Figure~\\ref{fig:confusion_deepseek32B} presents the confusion matrix top 5 classes for this model under noisy conditions using the $I+C+S$ prompt. While the model performs well for certain classes like 16PAM, significant confusion exists between spectrally similar modulations, notably 4ASK and 4PAM, which are frequently misclassified as each other. Other notable misclassifications include CPFSK being predicted as OOK or failing classification, and 8ASK being mistaken for 16PAM.",
        "summarize_figure": "2505.03112v1_confusion",
        "summarization": "The picture shows the confusion matrix of the DeepSeek - R1 - Distill - Qwen - 32B model using the I+C+S prompt on noisy signals. The vertical axis is the true class and the horizontal axis is the predicted class. The model performs well for some classes like 16PAM, with most predictions being correct (value of 181) for the true class of 16PAM. However, there is significant confusion among spectrally similar modulations. For example, 4ASK and 4PAM are often misclassified as each other. CPFSK is likely to be predicted as OOK or fail classification, and 8ASK may be mistaken for 16PAM."
    },
    "164": {
        "figure1": "2505.03117v1_atco-system",
        "label1": "fig:atco-system",
        "caption1": "Explanation as an enabler in ATCO-AI interaction",
        "text": "Integrating XAI into ATM is crucial for several key reasons. Firstly, XAI has been recognized as one of the fundamental building blocks for a trustworthy AI\\cite{easa2024}, an essential attribute in safety-critical domains like ATM. Beyond merely improving user acceptance, AI integration in ATM is expected to enhance human decision-making performance by providing ATCOs with meaningful context-aware explanations. Furthermore, if ATCO-generated explanation can be integrated into the AI system—clarifying why one advisory is accepted while another is rejected—the system stands to benefit by learning from its users (see Fig.~\\ref{fig:atco-system}). Overall, XAI is envisioned to serve a critical role in enabling a more effective human-AI interaction in ATM~\\cite{kirwan2025human}, from trust calibration to supporting co-evolution~\\cite{pham2024hah}.",
        "summarize_figure": "2505.03117v1_atco-system",
        "summarization": "The picture shows the facilitating role of explanations in the interaction between Air Traffic Controllers (ATCOs) and the system. ATCOs can generate and comprehend explanations, which are exchanged bidirectionally between ATCOs and the system. Integrating Explainable Artificial Intelligence (XAI) into Air Traffic Management (ATM) is crucial. It is a foundation for building trustworthy AI, enhancing user acceptance and aiding ATCO decision - making through context - aware explanations. If ATCO - generated explanations can be integrated into the AI system, the system can learn from it. XAI helps achieve more effective human - AI interaction."
    },
    "165": {
        "figure1": "2505.03119v1_Figure2",
        "label1": "fig2",
        "caption1": "Plots of the the true $\\gen_{12}(\\cdot)$ in quadratic, cubic and quartic forms  (black) and the corresponding estimated  version using Bayesian approach ((posterior mode, red) and frequentist approach (blue).}  \\includegraphics[scale=.8]{Figure2.png",
        "text": "A number of visual and numerical  summaries attest strongly to the good performance  of our method.  Figure \\ref{fig2}  compares  the true curve of the nonparametric transition arm, $\\lngen_{12}(\\cdot)=\\ln\\gen_{12}(\\cdot)$, (black) with the estimated ones in Bayesian (red) and Frequentist (blue) settings respectively.  Evidently they are  very similar in regions of high data concentration and tapers off slightly on the extremities with  low data frequencies.",
        "summarize_figure": "2505.03119v1_Figure2",
        "summarization": "The picture shows the true  (black curve) in quadratic, cubic, and quartic forms, along with the corresponding estimated curves using the Bayesian method (red, posterior mode) and the frequentist approach (blue). From left to right, N takes the values of 100, 500, and 1000 respectively. In the region of high data concentration, the true curve is very similar to the estimated curves. In the extreme regions with low data frequencies, the estimated curves deviate slightly. This indicates that both estimation methods perform well in the region of high data concentration."
    },
    "166": {
        "figure1": "2505.03119v1_Figure3_three_2",
        "label1": "fig3",
        "caption1": "Plots of averaged MSE for estimated $\\lngen_{12}(\\cdot)$ over 5 simulated datasets, with true function given by quadratic, cubic and quartic functions in polynomial settings versus sample size N}  \\includegraphics[scale=.8]{Figure3_three_2.png",
        "text": "Figure \\ref{fig3} shows the relationship of the averaged MSE (across 5 replicates) in the three different polynomial settings with one arm treated nonparametric. There is a clear inverse relationship between sample size and error rates when $\\lngen_{12}$ is quadratic. This decrease in error rates is markedly  prominent from N=100 to $\\sim <600$ and Frequentist estimation performs slightly better than Bayesian  when N is 100. However, it is outperformed  by Bayesian for greater sample sizes (between 200 and 400). While in the cubic setting, the Frequentist estimation generates lower error rates than Bayesian in all sample sizes, demonstrated as the opposite trend in quartic setting.  The results on the both arms reinforce those findings. %Tables \\ref{tab2} displays the asymptotic probabilities of $d_{absorption}$ metric for  N across 100, 500, 1000 averaged across 5 replicates in quadratic setting.",
        "summarize_figure": "2505.03119v1_Figure3_three_2",
        "summarization": "The chart shows the plots of the averaged mean - squared error (MSE) for the estimated function lngen₁₂(·) over 5 simulated datasets. The true functions are quadratic, cubic, and quartic functions in polynomial settings, with the vertical axis being the sample size N. When lngen₁₂ is quadratic, there's a clear negative correlation between sample size and error rate, with a significant decrease when N ranges from 100 to about 600. At N = 100, frequentist estimation performs slightly better than Bayesian estimation; for N between 200 and 400, Bayesian estimation outperforms. In the cubic setting, frequentist estimation has lower error rates than Bayesian at all sample sizes, while the opposite is true in the quartic setting. Results from both arms reinforce these findings."
    },
    "167": {
        "figure1": "2505.03120v1_FIT501",
        "label1": "fig4:swatpic",
        "caption1": "An example of attack data, showing flow meter readings from sensor FIT501.} \\vspace{-0.1in",
        "text": "By analyzing the water level data, if an attack lowers the readings, it could falsely trigger (1) the pump to operate and (2) increase the risk of overflow. This inevitably results in more pumping events, leading to increased energy consumption. It is essential to monitor the float level and identify any false positives or false negatives. Similarly, in the case of the flow meter in Fig. \\ref{fig4:swatpic}, the attack data reveals significant variations in flow, which can lead to potential malfunctions. In the following section, we propose an ML framework that uses attack data while incorporating adversarial sampling.",
        "summarize_figure": "2505.03120v1_FIT501",
        "summarization": "The chart shows the flow meter readings of sensor FIT501. The blue line represents the readings with intrusion, and the red dots indicate the intrusion period. From 13:00 - 14:15 and 14:30 - 15:00, the readings are relatively stable. Between 14:15 - 14:30, there is a significant drop during the intrusion period. Such attack - induced significant flow variations may cause equipment malfunctions. When analyzing water level data, attacks reducing readings can falsely trigger pump operation and increase overflow risks, leading to more pumping and higher energy consumption. Thus, monitoring float levels and identifying false positives/negatives is crucial."
    },
    "168": {
        "figure1": "2505.03125v1_baseProfilesComp",
        "label1": "fig:baseProfilesCompare",
        "caption1": "Equilibria parameters for DIII-D shot 180636 at 3900ms. \\textbf{(a)} Shows the manual expert (blue) and CAKE 2-dimensional cross sections (green). Note that the JAKE equilibria do not provide a full 2-dimensional cross section of the plasma. \\textbf{(b)} Shows the total plasma pressure for manual expert (blue), CAKE (green), and JAKE (red) reconstructions. \\textbf{(c-e)} Visualize the manual expert, CAKE, and JAKE equilibria for the safety factor (q), parallel current profile, and electron temperature, respectively.",
        "text": "In this section, we discuss the three types of equilibrium reconstruction used in this study. The first is a set of manual kinetic equilibria constructed by physics experts over several decades at the DIII-D tokamak. These are compared to two automated kinetic equilibrium calculation methods: CAKE \\cite{xing_cake_2021} and JAKE \\cite{bechtel_accelerated_2022}. Each equilibrium represents a two-dimensional cross section of the plasma, as seen in Figure \\ref{fig:baseProfilesCompare}.a, which is characterized by a number of scalar shape parameters defined in Table \\ref{tab:parameter_comp} and a 2-dimensional grid of magnetic field vectors and flux functions. Ideal MHD profiles, found by solving the Grad-Shafranov (GS) equation for radial force balance, comprise equilibrium information internal to the separatrix. These are given in Figure \\ref{fig:baseProfilesCompare}.b-e. The timeslice shown in Figure \\ref{fig:baseProfilesCompare} shows reasonable visual agreement between the expert and CAKE cross sections and equilibrium profiles, whereas the JAKE reconstruction is less physical and in poorer agreement for this example profile. This alone is not necessarily representative of JAKE quality throughout the dataset as both the CAKE and manual expert datasets have timeslices with equally unrealistic profiles for isolated examples.\nThere was an overlap of 1194 equilibria between the CAKE and Manual datasets and a significantly smaller overlap of 29 equilibria between the JAKE and Manual datasets. Because there were significantly fewer JAKEs that overlapped with the Manual dataset, we will also compare JAKEs to a larger dataset of CAKEs later. To quantify variance in these scalar profiles, we move from raw values to percent differences to normalize all quantities. We calculate the percent difference of the CAKE and JAKE profiles from each Manual equilibria and report the Mean$\\pm$Standard Deviation of percent differences for each parameter. We calculate percent differences as $(Manual-CAKE)/Manual$ and $(Manual-JAKE)/Manual$. This metric is reported in Figure \\ref{fig:baseProfilesCompare} for each scalar parameter. We color each parameter so the intensity is proportional to the mean percent difference divided by the standard deviation. The effect is such that when the reconstruction methods agree, the mean is close to $0$ and the color is lighter. When the equilibria disagree and the mean percent difference is comparatively further from $0$, the intensity is brighter.",
        "summarize_figure": "2505.03125v1_baseProfilesComp",
        "summarization": "The chart shows the equilibrium parameters for DIII - D shot 180636 at 3900ms. In sub - figure (a), the blue curve represents the 2D cross - section analyzed by manual experts, and the green curve is the result from the CAKE method. The JAKE method does not provide a full 2D cross - section. Sub - figure (b) shows the total plasma pressure for manual expert (blue), CAKE (green), and JAKE (red), all decreasing. Sub - figure (c) is the safety factor q, with curves of the three methods close at first and the JAKE curve rising significantly later. Sub - figure (d) is the parallel current profile, with large differences in curve trends. Sub - figure (e) is the electron temperature, all decreasing. The results of manual experts and CAKE show good visual agreement, while the JAKE reconstruction has poor agreement with the other two in this example."
    },
    "169": {
        "figure1": "2505.03125v1_cakeVariationsComp",
        "label1": "fig:cakeVariationsComp",
        "caption1": "CAKE variations for DIII-D shot 189652 at time 3100ms. The horizontal axis is the normalized poloidal flux $\\Psi$ (also reffered to as $\\Psi_N$) where the left column is the full range of $\\Psi=[0,1]$ and the right column is the pedestal region of $\\Psi=[0.9,1]$. \\textbf{(a)} and \\textbf{(b)} show the effect of varying $T_e$ at the LCFS where \\textbf{(b)} is a zoom in on the edge to better see the effect on the pedestal. The value of $T_e$ at the LCFS is given in the legend of \\textbf{(a)}. \\textbf{(c)} and \\textbf{(d)} show the effect of varying the spline knot smoothing on the $P^\\prime$ profile, with the scanned smoothing values given in the legend. \\textbf{(e)} and \\textbf{(f)} show the effect of the number of splining knots on the pressure profile. ",
        "text": "To quantify the effects of these settings, we have scanned various user settings over our original dataset of CAKE equilibria. Here we present a single base equilibrium with changed CAKE settings while later we will look at a large database scale. The effect of three of these setting variations is shown in Figure \\ref{fig:cakeVariationsComp}. The full impact of changing these settings is discussed below in Section \\ref{sec:cakeVarComp}. The details regarding CAKE is available in a previous publication\\cite{xing_cake_2021}, and here we summarize the CAKE settings examined:      The minimum uncertainty assigned to the measurement location in $\\psi_n$, as in the uncertainty in x assumed for the CAKE fitting scheme. CAKE uses an Monte Carlo fitting process to account for position uncertainty. The default values are between 0.005 and 0.01 depending on the diagnostic.     Minimum value for spline knot location in the $P^\\prime$ profile. The default limit is $0$, meaning unconstrained, and we vary this parameter from $0.0$ to $0.95$.      Minimum value for the spline know location in the $FF^\\prime$ profile, where $F=2\\pi R\\frac{B_\\phi}{\\mu_0}$. The default location is $0$, meaning unconstrained, and we vary this from $00$ to $0.95$.      The number of spline smoothing knots in $P^\\prime$ and $FF^\\prime$ profiles. The default value is $6$ and we vary this from $2$ to $10$.\nSome of the effects of manipulating the CAKE settings are immediately obvious. In Figure \\ref{fig:cakeVariationsComp}.b, the increasing $T_e$ at the LCFS has a clear effect on raising the $T_e$ pedestal without a significant impact on the core $T_e$ profile seen in Figure \\ref{fig:cakeVariationsComp}.a. While there is not a clear trend in the effect of changing smoothing knot locations, it has a definitive effect on both the core of the $P^\\prime$ profile as well as the pedestal as seen in Figure \\ref{fig:cakeVariationsComp}.c-d. Finally, in Figure \\ref{fig:cakeVariationsComp}.f, increasing the number of knots appears to increase the pedestal pressure, while there is some minor effect on the core pressure when there are a very low number of splining knots.",
        "summarize_figure": "2505.03125v1_cakeVariationsComp",
        "summarization": "The chart shows CAKE parameter variations for DIII - D shot 189652 at 3100ms with normalized poloidal flux Ψ on the horizontal axis. The left column has Ψ = [0, 1] and the right is the pedestal region Ψ = [0.9, 1]. Sub - figures (a) and (b) show the effect of varying Te at the LCFS, with (b) a zoom - in; higher Te raises the pedestal Te with little core impact. (c) and (d) display the effect of spline knot smoothing on the P′ profile, and (e) and (f) show the impact of the number of spline knots on the pressure profile."
    },
    "170": {
        "figure1": "2505.03125v1_scalarPlots",
        "label1": "fig:scalarPlots",
        "caption1": "Plotted are equilibrium scalar parameters from manual EFITs (horizontal axes) versus CAKEs (vertical axes). \\textbf{(a)} plots major radius $R_0$. The color contour $Z_{axis}$ illustrates the level of disagreement and the histograms above and below show the distribution of the Manual and CAKE datasets, respectively. Plots \\textbf{(b)-(f)} follow the same layout for plasma current, toroidal magnetic field, edge plasma pressure, core plasma pressure, and safety factor at $\\psi_N=0.95$, respectively.",
        "text": "We visualize the comparison of scalar parameters, like plasma current and core pressure, in Figure \\ref{fig:scalarPlots}. Good agreement follows a diagonal line (i.e. $R^2=1$), and disagreement is represented by spread from this line (i.e. $R^2<1$). Good agreement can be seen in plasma current in Figure \\ref{fig:scalarPlots}.b, toroidal magnetic field in Figure \\ref{fig:scalarPlots}.c, and $q_{95}$ in Figure \\ref{fig:scalarPlots}.f. There is significantly more spread in edge pressure in Figure \\ref{fig:scalarPlots}.d and core pressure in Figure \\ref{fig:scalarPlots}.e, while major radius in Figure \\ref{fig:scalarPlots}.a falls somewhere in between.",
        "summarize_figure": "2505.03125v1_scalarPlots",
        "summarization": "The chart compares equilibrium scalar parameters from manual EFIT (horizontal axis) and CAKE (vertical axis). Sub - figures (a) - (f) show major radius, plasma current, toroidal magnetic field, edge plasma pressure, core plasma pressure, and safety factor at ψₙ = 0.95 respectively. Colored contours represent disagreement, with histograms showing dataset distributions. Good agreement follows the diagonal line; plasma current, toroidal magnetic field, and q₉₅ show good agreement, while edge and core pressure have more spread, and major radius is intermediate."
    },
    "171": {
        "figure1": "2505.03127v1_LSTM",
        "label1": "LSTM",
        "caption1": "skip}{-0.2cm} \\setlength{\\belowcaptionskip}{-0.2cm} %调整图片标题与下文距离 \\caption{Proposed LSTM encoder-decoder structure.} \\vspace{-0.1cm",
        "text": "As shown in (\\ref{decoder}), when the control information is not received at the Rx, the predicted status information by the SFR module is used, i.e., $\\boldsymbol{\\tilde{x}}_i=\\mathcal{R} \\left(\\boldsymbol{\\tilde{x}}_{i-1} \\right)$. Specifically, the Rx is to reconstruct the semantic features of the motion control information based on the previously received information. In this paper, we apply the LSTM encoder-decoder architecture network to achieve this purpose. For brevity, the details of the LSTM are omitted, where interested readers can refer to \\cite{LSTM}. In the LSTM network training, we construct an empirical buffer using $N_l$ historical data $\\boldsymbol{x}_{r,l},\\ l \\in [1,N_l]$ consecutively transmitted from the Tx to the Rx. As shown in Fig. \\ref{LSTM}, an encoder reads the input sequence $\\boldsymbol{S}_{in} = \\left[ \\boldsymbol{x}_{r,l+1}, \\boldsymbol{x}_{r,l+2}, \\cdots, \\boldsymbol{x}_{r,l+T}  \\right]$ with length $T$ and extracts the semantic feature information to obtain a feature vector. The decoder adopts the output of the encoder as the initial hidden state vector and employs the final input state $\\boldsymbol{x}_{r,l+T}$ of the encoder as its input. Finally, the decoder produces a deduced semantic feature information sequence $\\boldsymbol{S}_{out} = \\left[ \\boldsymbol{y}_{1}, \\boldsymbol{y}_{2}, \\cdots, \\boldsymbol{y}_{M} \\right]$ with length $M$. In addition, $\\boldsymbol{h}_{n_t},\\ n_t \\in \\{0,1,\\cdots,T\\}$ and $\\boldsymbol{h}_j^{\\prime},\\ j \\in \\{0,1,\\cdots,M-1\\}$ are hidden status vectors in Fig. \\ref{LSTM}. As such, the  decoder policy $\\mathcal{R}(\\cdot)$ in \\eqref{decoder} selects the first predicted value $\\boldsymbol{y}_1$ as the output, which is given by Although we only utilize the first predicted value in practical applications, we employ a strategy of training the LSTM model to forecast values for the subsequent \\( M \\) time steps. This approach was implemented to mitigate the “laziness” phenomenon in predictive models, where the model tends to output values that are close to or identical to the last input value, thereby achieving a lower loss value. By adopting this strategy, we encourage the model to learn more predictive features, thereby enhancing its forecasting performance.\nIt is worth noting that the above semantic feature encoder-decoder characterizes the conditional probability distribution of $\\boldsymbol{S}_{in}$ given $\\boldsymbol{S}_{out}$. Specifically, given the input sequence $\\boldsymbol{S}_{in}$, the conditional probability of the feature vector $\\boldsymbol{h}_T$ can be approximated as \\cite{LSTM} \tp\\left(\\boldsymbol{h}_T \\mid \\boldsymbol{S}_{in} \\right) \\approx  \\prod_{n_t=1}^{T} p\\left(\\boldsymbol{h}_{n_t} \\mid \\boldsymbol{h}_{{n_t}-1}, \\boldsymbol{x}_{r,l+{n_t}} \\right) , where the initial hidden vector $\\boldsymbol{h}_0$ is randomly generated. As shown in Fig. \\ref{LSTM}, the decoder successively produces the probability distribution of the predicted status $\\boldsymbol{y}_j$, which is approximated as \\cite{LSTM} \tp\\left(\\boldsymbol{y}_j \\mid \\boldsymbol{S}_{in} \\right) \\approx  \\prod_{n_t=1}^{j} p\\left(\\boldsymbol{h}_{n_t}^{\\prime} \\mid \\boldsymbol{h}_{n_t-1}^{\\prime}, \\boldsymbol{x}_{r,l+T} \\right)  \t  p\\left(\\boldsymbol{y}_j \\mid \\boldsymbol{h}_{n_t}^{\\prime} \\right), where the initial hidden vector $\\boldsymbol{h}_0^{\\prime}$ can be obtained from $\\boldsymbol{h}_T$ via linear mapping, i.e., $\\boldsymbol{h}_0^{\\prime} = \\boldsymbol{h}_T$.",
        "summarize_figure": "2505.03127v1_LSTM",
        "summarization": "The picture shows a proposed Long Short - Term Memory (LSTM) encoder - decoder structure. The encoder reads an input sequence of length T and extracts semantic feature information to obtain a feature vector. The decoder takes the output of the encoder as the initial hidden state vector and uses the final input state of the encoder as its input to generate a deduced semantic feature information sequence. Hidden state vectors exist in both the encoder and decoder. This structure is used to reconstruct the semantic features of motion control information based on previously received information when control information is not received at the receiving end. By training the LSTM model to predict values for subsequent time steps, it mitigates the \"laziness\" phenomenon in predictive models and improves prediction performance. It also involves the approximate calculation of the probability distribution of feature vectors and predicted states under the input sequence."
    },
    "172": {
        "figure1": "2505.03127v1_HR-MADRL",
        "label1": "fig-HR-MADRL",
        "caption1": "skip}{-0.0cm} \\setlength{\\belowcaptionskip}{-0.2cm} %调整图片标题与下文距离 \\includegraphics[scale=0.85]{fig/HR-MADRL.eps} \\caption{Proposed HR-MADRL architecture.}  \\vspace{-0.4cm",
        "text": "As shown in Fig. \\ref{fig-HR-MADRL}, we present the network architecture of HR-MADRL. Agent $\\mathcal{T}\\left(\\cdot\\right)$ produces discrete action using a  Q-network, while agent $\\mathcal{F}\\left(\\cdot\\right)$ generates continuous action utilizing an Actor-Critic (AC) frameworks. The agents operate asynchronously: agent $\\mathcal{T}\\left(\\cdot\\right)$ first determines its action; then, the action of $\\mathcal{T}\\left(\\cdot\\right)$ is input into the agent $\\mathcal{F}\\left(\\cdot\\right)$. Agent $\\mathcal{F}\\left(\\cdot\\right)$ interacts with the environment, receives reward $v_i$, and sends feedback to agent $\\mathcal{T}\\left(\\cdot\\right)$ to compute its environmental reward $r_i$. These rewards and Q-values are used to update the network parameters via gradient backpropagation.",
        "summarize_figure": "2505.03127v1_HR-MADRL",
        "summarization": "The picture shows the proposed HR-MADRL architecture. In this architecture, agent T generates discrete actions through a Q-network, while agent F generates continuous actions using an Actor - Critic (AC) framework. The agents operate asynchronously. Agent T first determines its action, which is then input into agent F. Agent F interacts with the environment, receives the reward vi, and sends feedback to agent T to calculate the environmental reward ri. These rewards and Q-values are used to update the network parameters via gradient backpropagation."
    },
    "173": {
        "figure1": "2505.03128v1_uhumans2_office_3dsg_labeled",
        "label1": "fig:3dsg",
        "caption1": "3D Scene Graph generated from the uHumans2 office scene dataset \\cite{Rosinol21ijrr-Kimera} using Hydra \\cite{hughes2022hydra}. The graph comprises five layers, as shown in the figure. Room nodes are denoted as R($\\cdot$), while building nodes are represented as B($\\cdot$).",
        "text": "We model the environment as a hierarchical semantic graph (HSG). Specifically, let $G = (V,E,\\mathcal{K})$ be a graph with $n$ layers, where $V$ is the set of nodes, $E \\subseteq V \\times V$ is the set of edges, and $\\mathcal{K} = {1, \\ldots K}$ represents a set of semantic classes ordered by decreasing priority. We denote the set of layers as $L$, where $\\ell=0$ is the lowest layer and $\\ell=n$ is the highest layer (root).  Each layer $\\ell$ forms a connected weighted subgraph $G^l = (V^\\ell,E^\\ell, \\mathcal{K})$, with an associated weight function $w: E^\\ell \\to \\Re^+$.  We assume that each node in layer $\\ell-1$, for $\\ell = 1, \\ldots n$, is connected to exactly one node in layer $\\ell$, which we refer to as its parent node. More generally, we define an ancestor as a node's parent or any higher-layer predecessor in the hierarchy. We define the projection function $p: V \\times L \\to V$ that maps each node to its corresponding ancestor in layer $\\ell$. An example of an HSG is the 3D Scene Graph (3DSG) shown in Figure~\\ref{fig:3dsg}.\nWe perform two sets of simulations on the publicly available uHumans2 office 3DSG, shown in Figure~\\ref{fig:3dsg} and constructed using Hydra \\cite{hughes2022hydra}.  In the figure, the corresponding layers are also depicted. The \\textsf{places} layer $\\ell=0$ is a subgraph where each node represents an obstacle-free location, and edges indicate straight-line traversability. The rooms at layer $\\ell=1$ consist of nodes representing room centers, with edges connecting neighboring rooms. The number of border nodes (bn) and places in each room are: $\\\\$ R(0): 49 bn, 330 places, R(1): 22 bn, 259 places, R(2): 30 bn, 196 places, R(3): 32 bn, 195 places, R(4): 4 bn, 63 places, R(5): 35 bn, 206 places, R(6): 14 bn, 65 places.\nIn this section, we demonstrate a path-planning scenario in the 3DSG shown in Figure \\ref{fig:3dsg}. The starting node $v_s$ is in R(2), while the goal $v_g$ is in R(1). For the environment's semantic classes, we assign $\\phi_V^0(v) = 2$, for all $v \\in \\text{R(0)}$ (e.g., R(0) represents a typically crowded area, which the robot should try to avoid). Additionally, we define $\\phi_V^0(v) = 3$, for all $v \\in C$, where $C$ is the set of \\textsf{place} nodes located within a disk centered around specific objects.  Formally, $C = \\{ v \\in G^0: \\| x(v) - x(o)\\|_2 \\leq r ,\\hspace{0.5em}  \\forall o \\in O\\}$, where $x(\\cdot)$ denotes the location of a \\textsf{place} node $v$ or an object $o$, and $O$ is a set of objects. In this scenario, we set $r = 3$m and define $O$ as the set of all computers in the 3DSG.  This means that the robot should avoid passing too close to computers for safety reasons, as they might be in use by humans. Figure~\\ref{fig:uhumans2_all}(\\subref{fig:uhumans2}) depicts the \\textsf{places} layer of the environment along with the start and goal nodes.",
        "summarize_figure": "2505.03128v1_uhumans2_office_3dsg_labeled",
        "summarization": "The image shows a 3D Scene Graph from the uHumans2 office scene dataset, generated with Hydra. It has five layers: Buildings, Rooms (nodes R(·)), Places, Objects, and 3D Mesh (building nodes B(·)). Details include the number of border nodes and places in each room. Path - planning simulations are performed, with rules for the robot to avoid crowded areas like R(0) and objects such as computers for safety."
    },
    "174": {
        "figure1": "2505.03128v1_GNN",
        "label1": "fig:GNN",
        "caption1": "Proposed GNN architecture for node $P \\in V^\\ell$ ($\\ell \\neq 0$) classification, utilizing the semantic classes (green, blue, red) of nodes in $\\ell=0$. The dataset $\\mathcal{D} = \\{G^{0'}(P),\\phi_V^\\ell(P)\\}$ is generated by running COA* on subgraphs of induced nodes. The network consists of two MLPs for pre-processing and post-processing, three GNN layers with skip connections, and an average pooling operator. Training is conducted using cross-entropy loss over the semantic classes.",
        "text": "The proposed model is illustrated in Figure~\\ref{fig:GNN}.  We use border nodes as input features for the network, which are concatenated with the semantic classes of the nodes in $G^{0'}$ represented as one-hot encodings. A 2-layer MLP is used to preprocess the input.",
        "summarize_figure": "2505.03128v1_GNN",
        "summarization": "The picture shows the Graph Neural Network (GNN) architecture for node P∈V^ℓ (ℓ≠0) classification, using the semantic classes (green, blue, red) of nodes in ℓ = 0. The dataset D = {G^0'(P), ϕ_V^ℓ(P)} is generated by running COA* on subgraphs of induced nodes. The network consists of two Multi - Layer Perceptrons (MLPs) for pre - and post - processing, three GNN layers with skip connections, and an average pooling operator. Training is carried out using cross - entropy loss over the semantic classes."
    },
    "175": {
        "figure1": "2505.03133v1_bic_vs_MSE",
        "label1": "fig:enter-labelpaper4",
        "caption1": "Pareto-efficiency plot returned after the termination criterion.",
        "text": "After termination, the user will receive a list of Pareto optimal solutions, along with the following performance metrics. An en example output can be seen below: Elapsed time: 0:01:34.087934 Pareto Solutions: [{'aic': 2168.614, 'bic': 2225.0140,...}] The user will also receive a graphical representation of a Pareto frontier. This illustrates all potential solutions that cannot be strictly deemed more efficient than one another with respect to the objective functions, without causing a deterioration in at least one of them. For instance, consider the following output of a multi-objective search for a model as presented in  \\cref{fig:enter-labelpaper4}. The model yields two distinct solutions, which, according to the Bayesian Information Criterion (BIC) and the Mean Squared Error (MSE), cannot be considered strictly superior to one another. This offers analysts a list of noncompromising solutions to assess and derive insights from. Depending on the characxtxristics of the problem, numerous additional compromising solutions could have been identified and suggested. There exist further termination criteria within all algorithms. These criteria revolve around the absence of improvements in the solution arguments. The default parameter \\code{arg: - \\_max\\_iterations\\_improvement = 50} can be modified. If no solutions are accepted for consideration in the search process by any algorithm, which includes the potential acceptance of a worse solution, then the search will terminate after 50 iterations. If any solution is found that updates the population, is accepted for consideration, or is pareto-efficient, then the iteration count will be reset.",
        "summarize_figure": "2505.03133v1_bic_vs_MSE",
        "summarization": "The picture is a Pareto - efficiency plot obtained after the termination criterion is met, showing the relationship between the Bayesian Information Criterion (BIC) and the Mean Squared Error (MSE). As the BIC value increases from about 2225.0 to 2228.0, the MSE value decreases from around 165.28 to 165.20, showing a negative correlation. The plot indicates that there are different potential solutions under these two metrics, which cannot be strictly determined as superior to each other in terms of their respective objective functions, providing analysts with a list of non - compromising solutions for evaluation and gaining insights."
    },
    "176": {
        "figure1": "2505.03133v1_combined_heatmap",
        "label1": "figpap4:all",
        "caption1": "Covering Arrays for Compared",
        "text": "Note that the results shown in \\cref{figpap4:all} relate to this specific experiment conducted on a single dataset. Different results may be observed when using datasets with significantly different characteristics, observations, or objectives for the search itself. The framework is designed to incorporate as much information as possible, allowing models to be searched with minimal analyst experience. In \\cref{lab:constaintssection}, an initial solution was embedded into the framework. A similar test is now being considered, but with a focus on restricting certain hypotheses to be tested.",
        "summarize_figure": "2505.03133v1_combined_heatmap",
        "summarization": "The picture is a combined heatmap of the normalized weighted scores across algorithms, showing the normalized weighted scores for different hyperparameters (de, hs, sa). The scores corresponding to de and sa are 0.48, and the score for hs is 0.44. The shade of color represents the level of the normalized weighted score; the darker the color, the higher the score. The results of this heatmap are related to a specific experiment conducted on a single dataset, and different results may be obtained when the characteristics of the dataset, observations, or search objectives vary."
    },
    "177": {
        "figure1": "2505.03136v1_experiment",
        "label1": "fig:experiment",
        "caption1": "Experiment overview by~\\citet{ji2024characterizing}.",
        "text": "We used user study data collected by~\\citet{ji2024characterizing}. In the study, participants focused on a cross in the centre of a blank screen for 4 seconds. Then, one of the 12 topic titles appeared, and participants rated the familiarity on a 5-point scale. Next, a backstory was provided for the query formulation, where the backstory was selected from the \\textit{InformationNeeds} dataset~\\cite{bailey2015information} to evoke the users' realisation of the knowledge gap or information need. The Tobii Pro Fusion eye-tracker\\footnote{\\textcolor{blue}{\\href{https://www.tobii.com/products/eye-trackers/screen-based/tobii-pro-fusion}{https://www.tobii.com/products/eye-trackers/screen-based/tobii-pro-fusion}}} was used to collect eye tracking data (60Hz). \\autoref{fig:experiment} shows an overview of the procedure.",
        "summarize_figure": "2505.03136v1_experiment",
        "summarization": "The picture shows the experimental procedure studied by ji in 2024. At the start of the experiment, participants were required to focus on a cross in the center of a blank screen for 4 seconds. Then, one of 12 topic titles would appear, and participants rated its familiarity on a 5 - point scale. After that, a backstory for query formulation was provided based on the \"InformationNeeds\" dataset to evoke participants' awareness of knowledge gaps or information needs. During the whole process, the Tobii Pro Fusion eye - tracker (sampling frequency 60Hz) was used to collect eye - tracking data."
    },
    "178": {
        "figure1": "2505.03139v1_2",
        "label1": "fig:hetfedft",
        "caption1": "Federated fine-tuning over heterogeneous wireless networks.} \\vspace{-0.5cm",
        "text": "Despite these advancements, the inherent heterogeneity in edge devices, i.e., both in computation capabilities and data modalities, continues to pose obstacles for effective FedFT implementation. To tackle this, we propose a FedFT framework tailored for heterogeneous edge networks. This framework optimizes both the LoRA structure and constrained wireless resources to maximize performance, as shown in Fig. \\ref{fig:hetfedft}. Each edge device independently fine-tunes a set of low-rank matrices with varying ranks, while the edge server aggregates these diverse low-rank matrices using unicast communication. Furthermore, to manage the variability of aggregated matrices, we introduce a projection framework where the edge server applies zero-padding to standardize incoming matrices and utilizes truncation to redistribute them back to edge devices. This approach differs significantly from conventional FL frameworks, as it employs unicast for downlink communication instead of broadcast and aggregates low-rank matrices instead of global model gradients. To further align the deployment of local LoRA modules with the computation and communication capabilities of edge devices, we propose a joint device selection and bandwidth allocation optimization strategy. This strategy minimizes computation and communication delays while preserving the integrity of unified matrix information, ultimately enhancing overall learning performance.",
        "summarize_figure": "2505.03139v1_2",
        "summarization": "The picture shows a framework for Federated Fine - Tuning (FedFT) over heterogeneous wireless networks. Edge devices have different computing capabilities; some can fine - tune Low - rank Adaptation (LoRA) modules with varying ranks while some modules are frozen. The edge server standardizes incoming matrices with zero - padding, aggregates them using FedAvg, and redistributes them via truncation. The framework also approximates by exploiting sparsity, considering the fine - tuned Language Adaptation Model (LAM) as the sum of the pre - trained LAM and updated weights. Using unicast communication, it differs from traditional frameworks and proposes an optimization strategy for device selection and bandwidth allocation to improve learning performance."
    },
    "179": {
        "figure1": "2505.03139v1_3",
        "label1": "fig:fu4lam",
        "caption1": "Wireless federated unlearning for edge LAMs.} \\vspace{-0.5cm",
        "text": "To address the aforementioned challenges, we shall propose a FU framework for edge LAMs over wireless networks. This framework enables collaborative model training while allowing opting-out devices to remove their data influence through modified gradient updates—preserving global utility, as illustrated in Fig. \\ref{fig:fu4lam}. A key innovation lies in the orthogonal projection mechanism, which decouples unlearning updates from those that preserve retained knowledge by projecting gradients onto an orthogonal subspace before dissemination. Unlike conventional approaches that aggregate only gradients from opting-out devices and broadcast a single global model, our method leverages information from all participating devices and delivers personalized updates via unicast transmission. This enhances both efficiency and adaptability in heterogeneous edge environments. To improve training stability, we introduce a bounded loss function by modifying the logarithmic term in cross-entropy, which effectively mitigates gradient explosion with negligible impact on learning efficiency.  Furthermore, differential privacy techniques can be adopted to enhance the privacy protection level, where the exchanged gradient can be manually added with adjustable noise.  Therefore, a theoretical framework needs to be established to analyze the relationship between FU convergence, non-linear projected gradients under wireless fading, and privacy level, diverging from conventional linear gradient-based analyses. By integrating these insights into transmission scheme design, a resource allocation algorithm can be developed to improve FU convergence while preserving edge LAM utility and data privacy. Edge LAM inference deploys sophisticated AI models at the network edge to deliver real-time intelligent services. Unlike conventional single-step edge AI paradigms, which rely on straightforward feature-to-prediction mappings, LAMs require sequential execution of specialized computation modules, ranging from latent feature extraction to prompt-driven reasoning. This approach enables complex IoT services but comes at the cost of increased neuron counts and computation intensity. Unlike conventional models partitioned by parameters, LAMs demand functional decomposition aligned with architectural roles, necessitating collaborative inference across resource-constrained edge devices. However, this introduces critical challenges: monolithic architectures cause redundant computations across inference stages, while heterogeneous device capabilities complicate coordination under strict latency constraints. To address these issues, we shall propose a novel microservice-based framework that virtualizes the functional modules of edge LAMs into microservices based on computational capabilities. This is achieved by leveraging the characteristics of the Mixture of Experts (MoE) architecture and the Chain-of-Thought (CoT) inference process to coordinate edge devices, thereby improving resource utilization and reducing inference latency. State-of-the-art (SOTA) LAMs exploit the sparsity of feed-forward networks to support conditional computation, utilizing a MoE framework to improve inference efficiency \\cite{10707053}. In this setup, computation-intensive decoders are split into lightweight expert models managed by a gating system. Despite its strengths, this framework demands the activation of multiple experts in parallel and sequences of gates, leading to substantial computation requirements and increased latency. Existing solutions typically focus on reducing computation and communication overhead within a single compressed model, often overlooking the increased response latency caused by the repeated scheduling of experts across different downstream tasks. Moreover, conventional MoE-based edge LAM inference relies on synchronizing the MoE layer, where a corrupted expert can lead to computation anomalies in subsequent layers.",
        "summarize_figure": "2505.03139v1_3",
        "summarization": "The picture shows a Wireless Federated Unlearning (FU) framework for edge Language Adaptation Models (LAMs). In this framework, the edge server receives the gradients of the LoRA module from the remaining devices. It first aggregates them via FedAvg, then constructs a subspace using Singular Value Decomposition (SVD), and finally projects for the opting - out devices to determine the orthogonal steepest descent direction. The remaining devices receive the global LoRA module, while the opting - out devices receive the projected LoRA gradient. This framework enables opting - out devices to remove the influence of their data through modified gradient updates via an orthogonal projection mechanism, preserving global utility. Different from traditional methods, it uses information from all participating devices and provides personalized updates via unicast transmission. It also introduces a bounded loss function to improve training stability."
    },
    "180": {
        "figure1": "2505.03139v1_moe",
        "label1": "fig:microservice4moe",
        "caption1": "Microservice architecture for edge LAM inference with MoE.} \\vspace{-0.5cm",
        "text": "To address these issues, we propose a microservice-based edge LAM inference framework. In this framework, experts of each MoE layer are virtualized as microservices deployed on edge devices, while the edge server handles attention calculations and gate scheduling. Fig. \\ref{fig:microservice4moe} illustrates an example of this framework. Inference tasks are transformed into a unidirectional acyclic graph of microservices, where gate function scheduling in sequential MoE layers can be formulated as an online microservice orchestration problem. The goal is to minimize long-term system costs (e.g., communication latency, energy cost) by optimizing the device selection policy. We apply the Lyapunov optimization technique, breaking down the long-term optimization problem into sequential one-shot device scheduling problems. This yields a tractable upper bound on the Lyapunov drift based on the current scheduling policy and real-time computation load. To find the optimal balance between system cost and limited edge resources, we develop an online optimization method that schedules edge devices while ensuring inference latency.",
        "summarize_figure": "2505.03139v1_moe",
        "summarization": "The picture compares conventional Mixture of Experts (MoE) inference with microservice - based MoE inference. In the conventional approach, the entire Language Adaptation Model (LAM) is deployed on a single device. The input is scheduled by the gate network, with inefficient expert activation, resulting in poor scalability and low resource utilization. In the microservice - based inference, experts of each MoE layer are virtualized as microservices deployed on edge devices. The edge server handles attention calculations and gate scheduling. Devices communicate via feedback of intermediate results and broadcast embedding, avoiding multi - slot transmission. It features high scalability and resource utilization, and inference tasks are transformed into a unidirectional acyclic graph of microservices."
    },
    "181": {
        "figure1": "2505.03146v1_figure2_explosion",
        "label1": "workflow",
        "caption1": "The Scheme of the Robot Hydrodynamic Testing and Optimization. Panel A shows the overview of the swimming quadruped robot. Panel B shows the hydrodynamic experiment of the leg peddling in the water tunnel. Panel C depicts the EFD-LSTM model's structure, inputs, and outputs. Panel D reports the towing test for determining body hydrodynamics. Panel E highlights the key process of the robot swimming gait optimization, which is deployed in the open water test, shown in Panel F.",
        "text": "Our system stems on the Pupper quadruped robot \\cite{pupper}, which has a symmetric design with three joints per leg, shown in Fig.~\\ref{workflow}B. The joint angle of Hip Abduction / Adduction (HAA) $\\theta^A$, responsible for lateral movement, is driven by a single servo motor. The joint angle of Hip Flexion / Extension (HFE) $\\theta^H$ and the joint angle of Knee Flexion / Extension (KFE) $\\theta^K$ control the swing of the thigh and calf, a quadrilateral linkage mechanism couples these two joints, allowing coordinated leg adjustments using two servo motors.\nWe modify the robot by adding a rigid square web ($36\\, cm^2$, $3\\, mm$ thick) to the midsection of each leg (Fig.~\\ref{workflow}A). This modification allows significant production of hydrodynamic forces without interfering with terrestrial movement.\nAs shown in Fig. \\ref{workflow}B, experiments of leg paddling are conducted in a water tunnel, where a six-component DMI D6095XA transducer is used to measure the instantaneous forces and torques acting on the robotic leg. The prescribed motion of the leg is governed by the following equations:\nTo model the nonlinear and unsteady hydrodynamics of the paddling leg, we deploy a Long Short-Term Memory (LSTM) network, trained using fluid experiment data. The FED-LSTM model, depicted in Fig.~\\ref{workflow}C, comprises two LSTM layers, each with 64 hidden units, and a dropout rate of 0.21. The hidden states from the final LSTM time step are passed through a fully connected layer to predict hydrodynamic forces and torques. The learning rate is adaptively adjusted, ranging between 0.001 and 0.1.",
        "summarize_figure": "2505.03146v1_figure2_explosion",
        "summarization": "The picture shows the scheme of robot hydrodynamic testing and optimization. Panel A presents an overview of the swimming quadruped robot, which is modified from the Pupper quadruped robot with rigid square webs added to the legs. Panel B shows the leg paddling hydrodynamic experiment in a water tunnel, using a six - component sensor to measure forces and torques. Panel C depicts the EFD - LSTM model structure, with two LSTM layers of 64 hidden units each, for modeling the nonlinear and unsteady hydrodynamics of the paddling leg. Panel D is the towing experiment for determining body hydrodynamics. Panel E highlights the key process of robot swimming gait optimization, including population initialization, objective value calculation, etc., to obtain the best solutions. Panel F is the hardware experiment in open water."
    },
    "182": {
        "figure1": "2505.03146v1_figure3_emperical_formula",
        "label1": "fig:exampleq",
        "caption1": "Definition of leg kinematics and fluid force model: The trajectory of the web midpoint Q is calculated from $\\theta_H$ and $\\theta_K$, and is used to compute the empirical force components. The force direction corresponding to the motion and fluid direction is highlighted in the dashed box. ",
        "text": "The planar motion of the rigid web can be modeled as a 2D slender body in uniform flow, shown in Fig.~\\ref{fig:exampleq}. The orientation and trajectory of the web midpoint Q is calculated based on the joint angles $\\theta^H$ and $\\theta^K$ using the geometry of points A, B, and C, given $OA = BC = 0.035\\,m$, $OC = AB = 0.125\\,m$, and BQ is $2.5$ times the length of $BC$.",
        "summarize_figure": "2505.03146v1_figure3_emperical_formula",
        "summarization": "The picture defines the leg kinematics and fluid force model. The trajectory of the web midpoint Q is calculated based on the joint angles θ^H and θ^K using the geometric relationships of points A, B, and C, and is used to compute the empirical force components. The force direction corresponding to the motion and fluid direction is highlighted in the dashed box. The planar motion of the rigid web can be modeled as a 2D slender body in uniform flow, with OA = BC = 0.035m, OC = AB = 0.125m, and BQ being 2.5 times the length of BC."
    },
    "183": {
        "figure1": "2505.03147v1_WWW_CTI_F1_tram_improve",
        "label1": "fig:tram_f1_improv",
        "caption1": "Performance of TRAM using different Configurations proposed in our work on ATD dataset.",
        "text": "To address RQ2, we evaluated TRAM under different configurations and confidence levels (25\\% and 80\\%). The results, shown in Figure~\\ref{fig:tram_f1_improv}, highlight the impact of configuration changes. We can observe that first summarising the report with GPT-3.5 and then SciBERT classification results in slightly better performance than the default setting of SciBERT.\nTo address class imbalance and overfitting in TRAM, we retrained the SciBERT model on a rebalanced dataset. This approach resulted in a median F1-score increase of approximately seven percentage points compared to the baseline model, as shown in Figure~\\ref{fig:tram_f1_improv}. Classification results for a few selected techniques using the best-performing retrained SciBERT model are presented in Table~\\ref{tab:sci_bert_results}. We observed that this model performs well on many of the top 50 most prevalent techniques in the MITRE ATT\\&CK framework, achieving an F1-score of up to 0.92.",
        "summarize_figure": "2505.03147v1_WWW_CTI_F1_tram_improve",
        "summarization": "The picture shows the F1 - score performance of different methods on the ATD dataset. Box plots present the results of various configurations such as Original SciBert, aCTIon, SciBert fine - tuned, and SciBert retrained at 25% and 80% confidence levels. There are differences in F1 - scores among different methods. Summarizing the report with GPT - 3.5 first and then using SciBert for classification performs slightly better than the default SciBert setting. Retraining the SciBert model on a rebalanced dataset increases the median F1 - score by about seven percentage points compared to the baseline model. The best - performing retrained SciBert model performs well on many common techniques in the MITRE ATT&CK framework, with an F1 - score of up to 0.92."
    },
    "184": {
        "figure1": "2505.03150v1_diagram0_grid",
        "label1": "diagram_m0",
        "caption1": "Diagram $R\\times L$ for case I, where $m^{\\ast}=0$ is an absorbing state, while $m=1$ is not.",
        "text": "In this scenario, one has $\\overline{A^{(1)}}=0$, and the equation of motion is cast as $\\dot m=F_{0}(m)$, where  &= -m\\left[ L - \\left(R+2L\\right)m + \\left(R+L+A^{(4)}\\right)m^{2} \\right]. As seen from \\eqref{F0}, apart from the stationary point $m^{\\ast}=0$, two other stationary solutions, $m_{+}^{(0)}$ and $m_{-}^{(0)}$ , can emerge depending on the choice of the pair $(L,R)$. These values are given by m_{\\pm}^{(0)} := \\frac{R+2L\\pm\\sqrt{ R^{2} - 4LA^{(4)} }}{2\\left(R+L+A^{(4)}\\right)}. The stationary points in this case are m^{\\ast}\\in\\{0,m_{\\pm}^{(0)}\\}. If $A^{(4)}=0$, the system will also have $m^{\\ast}=1$ as one of its stationary points. Since this case ($0$ and $1$ being both fixed points of the dynamical system) will be examined later, \\underline{it will be assumed that $A^{(4)}\\neq 0$ (or $A^{(4)}>0$) in this section}. The stability analysis is going to be performed through the (one-dimensional) isocline method, which stems on the analysis of the sign of \\eqref{F0}. Consequently, the relative positions of the points $0$, $m_{+}^{(0)}$ and $m_{-}^{(0)}$ plays a major rule in this procedure and is detailed in the Appendix \\ref{caseI}, and the results are summarized in figure \\ref{diagram_m0}.\nDefining the regions with $\\Omega_{0}^{(0)}\\cup\\Omega_{+}^{(0)}\\cup\\Omega_{0+}^{(0)}=\\mathbb{R}^{2}$. As shown in figure \\ref{diagram_m0}, the regions $\\Omega_{0}^{(0)}$ and $\\Omega_{+}^{(0)}$ contain a single stationary point, which are $m^{\\ast}=0$ and $m^{\\ast}=m_{+}^{(0)}$, respectively. On the other hand, the region $\\Omega_{0+}^{(0)}$ displays two stationary points, $0$ and $m_{+}^{(0)}$; the initial condition $m(t=0)=m_{0}$ determines the trajectory toward one of these two fixed points in $\\Omega_{0+}^{(0)}$. In Appendix \\ref{caseI}, it is shown that $0<m_{-}^{(0)}<m_{+}^{(0)}<1$ in this region, and 0 &,& m_{0} < m_{-}^{(0)} \\\\ m_{+}^{(0)} &,& m_{0}>m_{-}^{(0)} In other words, the point $m=m_{-}^{(0)}$, which is an unstable fixed point, is a separatrix between these two stationary points, $0$ and $m_{+}^{(0)}$.",
        "summarize_figure": "2505.03150v1_diagram0_grid",
        "summarization": "The R×L diagram for case I shows different regions. In the green region, m∗=m+(0)​; in the red region, m∗=0; in the gray region, m∗∈{0,m+(0)​}. With m∗=0 as an absorbing state and m=1 not being one, the equation of motion m˙=F0​(m) gives stationary points. Depending on (L,R), stationary solutions m+(0)​ and m−(0)​ can exist. In region Ω0+(0)​, m−(0)​ (an unstable fixed point) is a separatrix between 0 and m+(0)​."
    },
    "185": {
        "figure1": "2505.03150v1_diagram1_grid",
        "label1": "diagram_m1",
        "caption1": "Diagram $R\\times L$ for case II, where $m^{\\ast}=1$ is an absorbing state, while $m=0$ is not.",
        "text": "In this scenario, one has $A^{(4)}=0$ and $\\overline{A^{(1)}}=0$, and the equation of motion is cast as $\\dot m=F_{1}(m)$, where &= \\left(1-m\\right)\\left[ \\overline{A^{(1)}} - \\left(2\\overline{A^{(1)}}+L\\right)m + \\left(R+L+\\overline{A^{(1)}}\\right)m^{2}\\right] Two more stationary points, $m_{+}^{(1)}$ and $m_{-}^{(1)}$, are potentially available, depending on the choice of the parameters, and are given by m_{\\pm}^{(1)} := \\frac{2\\overline{A^{(1)}}+L\\pm\\sqrt{ L^{2} - 4R\\overline{A^{(1)}} }}{2\\left(R+L+\\overline{A^{(1)}}\\right)}. The stationary points in this case are m^{\\ast}\\in\\{1,m_{\\pm}^{(1)}\\}. Analogously in the previous section, the case $\\overline{A^{(1)}}=0$ is excluded in the analysis here, since it will be explored in the next section. Moreover, the technical details of the stationary analysis of \\ref{F1} is shown in Appendix \\ref{caseII}, and the results can be summarized in the diagram of figure \\ref{diagram_m1}.\nDefining the regions with $\\Omega_{1}^{(1)}\\cup\\Omega_{-}^{(1)}\\cup\\Omega_{1-}^{(1)}=\\mathbb{R}^{2}$. As shown in figure \\ref{diagram_m1}, the regions $\\Omega_{1}^{(1)}$ and $\\Omega_{-}^{(1)}$ contain a single stationary point, which are $m^{\\ast}=1$ and $m^{\\ast}=m_{-}^{(1)}$, respectively. On the other hand, the region $\\Omega_{1-}^{(1)}$ displays two stationary points, $1$ and $m_{-}^{(1)}$; the initial condition $m(t=0)=m_{0}$ determines the trajectory toward one of these two fixed points in $\\Omega_{1-}^{(1)}$. In Appendix \\ref{caseII}, it is shown that $0<m_{-}^{(1)}<m_{+}^{(1)}<1$ in this region, and 1 &,& m_{0} > m_{+}^{(1)} \\\\ m_{-}^{(0)} &,& m_{0}<m_{+}^{(1)} In other words, the point $m=m_{+}^{(1)}$, which is an unstable fixed point, is a separatrix between these two stationary points, $1$ and $m_{-}^{(1)}$.",
        "summarize_figure": "2505.03150v1_diagram1_grid",
        "summarization": "The R×L diagram for case II shows distinct regions. In the purple region, m∗=1; in the green region, m∗=m−(1)​; in the gray region, m∗∈{m−(1)​,1}. With m∗=1 as an absorbing state and m=0 not being one, the equation of motion m˙=F1​(m) gives stationary points. Depending on parameters, stationary solutions m+(1)​ and m−(1)​ can exist. In region Ω1−(1)​, m+(1)​ (an unstable fixed point) is a separatrix between 1 and m−(1)​."
    },
    "186": {
        "figure1": "2505.03150v1_diagram01",
        "label1": "diagram_m01",
        "caption1": "Diagram $R\\times L$ for case III, where $m^{\\ast}=0$ and $m^{\\ast}=1$ are absorbing states.",
        "text": "In this last scenario, one has $A^{(4)}=0$ and $\\overline{A^{(1)}}=0$, and the equation of motion is cast as $\\dot m=F_{01}(m)$, where  &= m\\left(1-m\\right)\\left[\\left(R+L\\right)m-L\\right], and the stationary points $m^{\\ast}=0$ and $m^{\\ast}=1$ are naturally present. Apart from them, it is also possible to have a third one, depending on the choice of the pair $(L,R)$. The set of stationary points in this case are, therefore, m^{\\ast}\\in\\{0,1,\\tilde{m}^{(01)}\\}. The stability analysis is summarized in figure \\ref{diagram_m01}, while the technical details are found in Appendix \\ref{caseIII}.\nDefining the regions with $\\Omega_{0}^{(01)}\\cup\\Omega_{1}^{(01)}\\cup\\Omega_{\\tilde m}^{(01)}\\cup\\Omega_{01}^{(01)}=\\mathbb{R}^{2}\\setminus\\{(0,0)\\}$. As shown in figure \\ref{diagram_m01}, the regions $\\Omega_{0}^{(01)}$, $\\Omega_{1}^{(01)}$ and $\\Omega_{\\tilde m}^{(01)}$ contain a single stationary point, which are $m^{\\ast}=0$, $m^{\\ast}=1$ and $m^{\\ast}=\\tilde{m}^{(01)}$, respectively. On the other hand, the region $\\Omega_{01}^{(01)}$ displays two stationary points, $0$ and $1$; the initial condition $m(t=0)=m_{0}$ determines the trajectory toward one of these two fixed points in $\\Omega_{01}^{(01)}$. In Appendix \\ref{caseIII}, it is shown that $0<\\tilde{m}^{(01)}<1$ in this region, and 0 &,& m_{0} < \\tilde{m}^{(01)} \\\\ 1 &,& m_{0} > \\tilde{m}^{(01)} In other words, the point $m=\\tilde{m}^{(01)}$, which is an unstable fixed point, is a separatrix between these two stationary points, $0$ and $1$.",
        "summarize_figure": "2505.03150v1_diagram01",
        "summarization": "This picture is a coordinate plot of R and L, divided into four regions. In the purple region, m* equals 1; in the green region, m* equals m~(01); in the red region, m* equals 0; in the gray region, m* takes values of 0 or 1. The caption indicates that in case III, m* = 0 and m* = 1 are absorbing states. Analysis of the equation of motion shows different stationary points. The stationary - point situations vary in different regions, which are divided by the values of R and L. Also, m~(01) is an unstable separatrix between the two stationary points 0 and 1."
    },
    "187": {
        "figure1": "2505.03159v1_framework",
        "label1": "fig:framework",
        "caption1": "\\textbf{Overview of the Proposed PID Auto-Tuning Framework.} The diagram shows two main modules: a Configurations Generator that produces tuning trials by pairing different initial PID states with exploration–exploitation levels, and a Trials Executer that applies these configurations using Bayesian Optimization and Differential Evolution in mobile robotics experiments. ",
        "text": "This paper introduces a novel framework, shown in Figure \\ref{fig:framework}, that streamlines the integration of various exploration-exploitation levels and initial states to generate a unique set of different configurations for Bayesian and Differential Evolution optimizers, the backbone of the auto-tuning process. This enables finding the most suitable configuration-optimizer pair, ensuring that the auto-tuning process yield optimal PID control performance. To demonstrate its practical utility, experiments were conducted on two different robotic platforms, a differential-drive mobile robot and an omnidirectional robot, with each executing PID controlled 90-degree in-place rotations while adhering to predefined overshoot percentage and rise time constraints, and aiming to minimize settling time. Moreover, the experimentation seeks to answer the following Research Questions (RQs): (1)  How does the exploration-exploitation trade-off affect PID auto-tuning convergence across different robot types, in terms of settling time and convergence percentage; (2) How does the initial state impact PID auto-tuning outcomes, specifically settling time and convergence percentage, across various robot types; (3) How do BO and DE differ when used for PID auto-tuning, given the variation in the exploration-exploitation levels and initial states.",
        "summarize_figure": "2505.03159v1_framework",
        "summarization": "The picture shows an overview of the proposed PID auto - tuning framework. There are two main modules: the Configurations Generator combines different initial PID states with exploration - exploitation levels to generate tuning trials; the Trials Executer applies these configurations using Bayesian Optimization and Differential Evolution in mobile robotics experiments. The framework integrates various exploration - exploitation levels and initial states to find the most suitable configuration - optimizer pair for optimal PID control performance. Experiments on different robotic platforms are conducted to answer research questions regarding exploration - exploitation trade - off, initial states, and differences between the two optimization methods."
    },
    "188": {
        "figure1": "2505.03159v1_trials-executer",
        "label1": "fig:trial-workflow",
        "caption1": "\\textbf{Workflow of the Trials Executer.} The diagram outlines the process by which each auto-tuning configuration (trial) is sequentially executed using both Bayesian Optimization and Differential Evolution on mobile robotic platforms. ",
        "text": "Trials executer is responsible of conducting each generated trial and collecting results data. Figure \\ref{fig:trial-workflow} presents the trials executer workflow.",
        "summarize_figure": "2505.03159v1_trials-executer",
        "summarization": "The picture shows the workflow of the Trials Executer. It starts with triggering a trial, initializing the optimizer based on initial gain values (Kp, Ki, Kd), optimizer type (DE or BO), optimizer configuration, objective threshold (desired settling time), and constraints on overshoot percentage and rise time. Then, the first experiment is run with given initial gain values, angular data is collected by IMU to calculate overshoot percentage (OP) and rise time (RT). If OP and RT are within constraints, the settling time (ST) is calculated and data is logged. If ST is less than or equal to the objective threshold, it terminates; otherwise, the optimizer updates gain values based on previous results and a new experiment is run. This loops until the termination condition is met."
    },
    "189": {
        "figure1": "2505.03161v1_fig1",
        "label1": "fig:fig1",
        "caption1": "Illustration of 6G usage scenarios and the corresponding threats. 6G introduces new usage scenarios such as artificial intelligence and communication, ubiquitous connectivity, and massive communication, which bring more security threats including DDoS and advanced persistent threat (APT).}  \\vspace{-7mm",
        "text": "While 6G SAGINs introduce novel usage scenarios, the cross-layer and cross-domain features, along with emerging applications of 6G, expand attack surfaces and increase complexity in detecting and mitigating security threats, as illustrated in Figure \\ref{fig:fig1}. For instance, malicious actors can launch DDoS attacks to disrupt services such as autonomous vehicles and smart urban infrastructures. Additionally, unauthorized individuals might eavesdrop on wireless signals, thereby leaking sensitive data transmitted in the space-to-ground link. Therefore, 6G security frameworks are needed to ensure the security of 6G SAGINs.",
        "summarize_figure": "2505.03161v1_fig1",
        "summarization": "The picture illustrates 6G usage scenarios and corresponding security threats. 6G introduces new scenarios like Immersive Communication, Integrated Sensing and Communication, etc. Meanwhile, it brings various security threats such as DDoS attacks, network viruses, Advanced Persistent Threats, eavesdropping attacks, side-channel attacks, and authorization bypass. These threats affect the security of 6G services and data transmission, so a 6G security framework is needed to ensure security."
    },
    "190": {
        "figure1": "2505.03161v1_fig2",
        "label1": "fig:fig2",
        "caption1": "Illustration of our proposed self-evolving security framework of 6G SAGINs and the 6G Simulator. The proposed LLM-6GNG receives threat information from our 6G Simulator, and feeds back security strategies to the 6G Simulator. Meanwhile, our 6G-INST collects data from the proposed LLM-6GNG, trains the Twin Specific Strategy Agent, and updates the LLMs.}  \\vspace{-7mm",
        "text": "To simulate a more realistic 6G SAGINs communication system and address emerging threats, we construct the 6G Simulator and the self-evolving security framework of 6G SAGINs as depicted in Figure \\ref{fig:fig2}. Our system consists of three components: 6G Simulator, LLM-6GNG, and 6G-INST. Threat information collected in the 6G Simulator is passed to the LLM-6GNG, which analyzes the threat information and generates corresponding security strategies in real-time. Concurrently, the 6G-INST empowers our LLM-6GNG with self-evolution capability.",
        "summarize_figure": "2505.03161v1_fig2",
        "summarization": "The picture illustrates the self - evolving security framework of 6G SAGINs and the 6G Simulator. The system consists of three components: 6G Simulator, LLM - 6GNG, and 6G - INST. Threat information collected by the 6G Simulator is sent to LLM - 6GNG. LLM - 6GNG analyzes the information through threat information processing modules and generates security strategies in real - time via the security strategy generation module, which are then fed back to the 6G Simulator. Meanwhile, 6G - INST collects data from LLM - 6GNG to train the Twin Specific Strategy Agent and update large language models, enabling self - evolution for LLM - 6GNG. The 6G Simulator includes an ns - 3 - based simulator and an OAI - based semi - physical simulator, equipped with components like intrusion detection systems, honeypots, and network monitors."
    },
    "191": {
        "figure1": "2505.03161v1_fig3",
        "label1": "fig:fig3",
        "caption1": "Illustration of our LLM-6GNG and 6G-INST. The top layer provides an example of threat information from the three subnets within our 6G Simulator. The middle part of the diagram describes the process of the LLM-6GNG, with the left side depicting the scenario of intra-network communication, and the right side showing the scenario of inter-Network Communication. The bottom layer of the diagram describes the process by which our 6G-INST automatically generates new datasets to assist the LLM-6GNG in its self-evolution.}  \\vspace{-6mm",
        "text": "Our LLM-6GNG comprises two modules, namely the threat information processing module and the security strategy generation module. As shown in Figure \\ref{fig:fig3}, we illustrate the procedure of our LLM-6GNG. The procedure of the threat information processing module is depicted in the upper half of the middle layer of Figure \\ref{fig:fig3}. The threat information collected from the 6G Simulator is processed by the Condensation Agent, the 6G attack correlation (6G-AC) algorithm, and the 6G key information extraction (6G-KWE) algorithm to extract keywords from the threat information.  The procedure of the security strategy generation module is depicted in the lower half of the middle layer of Figure \\ref{fig:fig3}. After being processed by the threat information processing module, we utilize the Prompt Agent, the Specific Strategy Agent, and the Consolidating Agent to analyze the processed key data and generate security strategies, which are then fed back to our 6G Simulator.  In our LLM-6GNG, CoT reasoning and the multi-agent collaboration mechanism are employed, which effectively analyze threat information and generate accurate security strategies. To meet the real-time requirements for security strategy generation, we chose Llama3-8b to build LLM-6GNG, ensuring its high operational efficiency. Next, we will provide a detailed introduction to LLM-6GNG.\nThe 6G attack correlation algorithm processes the aggregated data within each subnet for subsequent processing. It regroups the threat information based on the semantic and feature similarity between them, ultimately generating correlated information. As shown in Figure \\ref{fig:fig3}, for intra-network communications, the procedure of 6G-AC is as follows:\nFor inter-network communications, as shown in Figure \\ref{fig:fig3}, we aggregate threat information from all subnets involved in the inter-network communications. The aggregated information is then processed using the 6G-AC algorithm to achieve effective attack clustering across networks.\nAs shown in the bottom layer of Figure \\ref{fig:fig3}, we illustrate the procedure of our proposed 6G-INST. Our 6G-INST can collect and filter data from the Specific Strategy Agent in real-time and can automate the expansion of training datasets through the Instruction Generation Agent and Strategy Generation Agent. Through our proposed 6G-INST, we can address the high cost of collecting instruction datasets and use the expanded training datasets to fine-tune our LLM-6GNG, enabling its self-evolution.",
        "summarize_figure": "2505.03161v1_fig3",
        "summarization": "The picture illustrates the workflows of LLM - 6GNG and 6G - INST. The top layer shows threat information examples from three subnets in the 6G Simulator. The middle part, LLM - 6GNG, has an intra - network communication scenario on the left and an inter - network communication scenario on the right. Threat information collected by the 6G Simulator is processed by the Condensation Agent, 6G - AC algorithm, and 6G - KWE algorithm to extract keywords. Then, the Prompt Agent, Specific Strategy Agent, and Consolidating Agent analyze the processed key data to generate security strategies, which are fed back to the 6G Simulator. The bottom layer, 6G - INST, collects and filters data from the Specific Strategy Agent in real - time, and automatically expands the training dataset through the Instruction Generation Agent and Strategy Generation Agent to fine - tune LLM - 6GNG for self - evolution."
    },
    "192": {
        "figure1": "2505.03163v1_Methodology",
        "label1": "fig:research-methodology-flow",
        "caption1": "Research Methodology Flowchart",
        "text": "The overall research methodology is summarized in the diagram provided in Figure~\\ref{fig:research-methodology-flow}.",
        "summarize_figure": "2505.03163v1_Methodology",
        "summarization": "The picture shows a flowchart of the research methodology, which includes five steps: Participant Recruitment, Study Introduction, Semi - Structured Interviews, Data Handling, and Data Analysis. In the Participant Recruitment stage, 23 student volunteers were recruited from Rajasthan and Delhi. In the Study Introduction stage, participants were briefed on research objectives and AI in education. In the Semi - Structured Interviews stage, online interviews were conducted on key areas. In the Data Handling stage, interviews were recorded and transcribed. In the Data Analysis stage, thematic analysis was used to analyze the data."
    },
    "193": {
        "figure1": "2505.03165v1_TRUNK",
        "label1": "fig:TRUNKvsMonolithic",
        "caption1": "(a) Monolithic Architectures vs (b) TRUNK \\cite{TRUNK_goel}. We will use TRUNK, a type of hierarchical neural network, as a case study to demonstrate our guidelines for reproducibility. ",
        "text": "Monolithic networks use a single DNN to identify every feature associated with all the categories to make a decision (Figure \\ref{fig:TRUNKvsMonolithic}(a)). On the other hand, hierarchical networks, like that of TRUNK, are a collection of multiple shallow DNNs in the form of a tree and the leaf nodes represent each individual category of a particular dataset (Figure \\ref{fig:TRUNKvsMonolithic}(b)). This hierarchical structure enables TRUNK to limit the number of redundant floating point operations that occur in order to classify a particular image \\cite{goel_modular_2020}, therefore making it an efficient DL method for computer vision.\nTRUNK stands out among efficient hierarchical neural networks because it is one of the few \\cite{goel_modular_2020, TRUNK_goel, bcnn, hdcnn, roy_tree-cnn:_2018} with an official GitHub repository that is entirely implemented in PyTorch.  We preferred TRUNK over simpler, monolithic networks like VGG-16 \\cite{simonyan_very_2014} or AlexNet \\cite{alexnet} (Figure \\ref{fig:TRUNKvsMonolithic}(a)) because of the added complexity it offers.  Traditional DL models feature a consistent architecture which we train in a single pass. In contrast, TRUNK's architecture adapts based the dataset, as shown in Figure \\ref{fig:treeDataset}, due to the visual similarity criteria \\cite{goel_modular_2020, TRUNK_goel}. This tree structure of TRUNK is why it requires individual training for each node. This unique approach not only tests the robustness of the reproducibility guidelines under complex training scenarios but also challenges our current computing resources. By focusing on TRUNK, we aim to rigorously assess the effectiveness of the guidelines in a demanding yet controlled environment, ensuring they can manage varying complexities in training deep learning models.",
        "summarize_figure": "2505.03165v1_TRUNK",
        "summarization": "The picture compares Monolithic Architectures and the hierarchical neural network TRUNK. In Figure (a), Monolithic Architectures use a single Large CNN to identify features of various categories like cat, dog, duck, owl, pizza for decision - making. In contrast, Figure (b) shows that TRUNK is a hierarchical structure, composed of multiple shallow neural networks in a tree form. The root node is divided into clusters such as Cluster 1, Cluster 2, Cluster 3, and further into Cluster 4 - Cluster 7. Leaf nodes correspond to specific categories like cat, dog, bike. This hierarchical structure reduces redundant floating - point operations in image classification, making it more efficient than monolithic architectures. Moreover, its architecture adjusts according to the visual similarity criteria of datasets. Due to complex training, it can be used to rigorously assess the effectiveness of reproducibility guidelines in deep - learning model training."
    },
    "194": {
        "figure1": "2505.03165v1_reproExp",
        "label1": "fig:reproExp",
        "caption1": "Experiment Methodology used to Test the Reproducibility of TRUNK",
        "text": "The experimental methodology to assess the robustness of the current reproducibility guidelines through the use of a case study is outlined in Figure \\ref{fig:reproExp}. Initially, we will attempt to replicate the software environment used by the developers of TRUNK and explore ways to enhance the current methodology.  Subsequently, we will assess the importance of providing pre-trained weights for enhancing reproducibility. This will be accomplished by verifying the results reported in the TRUNK paper and comparing them with those obtained from reproducing the training process of TRUNK. Throughout this process, we will introduce improvements to the TRUNK software to bolster its reproducibility. Additionally, we will explore the impact of minor architectural modifications and variations in training recipes on the reproducibility of training deep learning models.",
        "summarize_figure": "2505.03165v1_reproExp",
        "summarization": "The picture shows the experimental methodology for testing the reproducibility of TRUNK. First, replicate the software environment following the instructions provided by TRUNK's authors. Then, evaluate and verify the provided pre - trained weights. Next, train the TRUNK network from scratch using the original training scripts. After that, make enhancements to the TRUNK software to improve its reproducibility. Finally, conduct ablation experiments on small architectural changes and training recipes to analyze their impact on reproducibility. This process aims to assess the robustness of current reproducibility guidelines through a case study."
    },
    "195": {
        "figure1": "2505.03165v1_svhn_colored_tree",
        "label1": "fig:treeDataset",
        "caption1": "We choose TRUNK for our case study analysis due to its complexity of the network architecture design varying by dataset (a) EMNIST (b) CIFAR-10 (c) SVHN. The red node is the root node of the tree, the gray nodes are the supergroups, and the green nodes are the leaf nodes.",
        "text": "TRUNK stands out among efficient hierarchical neural networks because it is one of the few \\cite{goel_modular_2020, TRUNK_goel, bcnn, hdcnn, roy_tree-cnn:_2018} with an official GitHub repository that is entirely implemented in PyTorch.  We preferred TRUNK over simpler, monolithic networks like VGG-16 \\cite{simonyan_very_2014} or AlexNet \\cite{alexnet} (Figure \\ref{fig:TRUNKvsMonolithic}(a)) because of the added complexity it offers.  Traditional DL models feature a consistent architecture which we train in a single pass. In contrast, TRUNK's architecture adapts based the dataset, as shown in Figure \\ref{fig:treeDataset}, due to the visual similarity criteria \\cite{goel_modular_2020, TRUNK_goel}. This tree structure of TRUNK is why it requires individual training for each node. This unique approach not only tests the robustness of the reproducibility guidelines under complex training scenarios but also challenges our current computing resources. By focusing on TRUNK, we aim to rigorously assess the effectiveness of the guidelines in a demanding yet controlled environment, ensuring they can manage varying complexities in training deep learning models.",
        "summarize_figure": "2505.03165v1_svhn_colored_tree",
        "summarization": "The picture shows the tree - structured architecture of the TRUNK network. The red node is the root of the tree, the gray nodes represent supergroups, and the green nodes are leaf nodes. As a hierarchical neural network, TRUNK stands out because its architecture adjusts according to the visual similarity criteria of datasets (such as EMNIST, CIFAR - 10, SVHN). Unlike traditional deep - learning models, each node of TRUNK requires individual training. This unique structure not only tests the robustness of reproducibility guidelines in complex training scenarios but also challenges current computing resources. Therefore, it is selected as a case - study object to rigorously assess the effectiveness of relevant guidelines in a complex environment."
    },
    "196": {
        "figure1": "2505.03175v1_Fig_architecture",
        "label1": "fig:architecture",
        "caption1": "The architecture of the proposed CL-MA array.}  \\end{center",
        "text": "To address this challenge, we propose in this paper a novel architecture of cross-linked MA (CL-MA) array. As shown in Fig. \\ref{fig:architecture}, the CL-MA array consists of multiple horizontal and vertical sliding tracks on the 2D plane. Each MA element/subarray is installed at the cross point of a horizontal track and a vertical track. With the aid of a motor mounted on the upper side of the array (see Fig. \\ref{fig:architecture}), each vertical track can move along the horizontal direction, which changes the horizontal positions of all MAs in its corresponding column. Similarly, each row of MAs can collectively move along the vertical direction with the aid of a motor mounted on the left side of the array. Note that for conventional MA arrays with element/subarray-wise independent movement, the number of motors required for 2D movement is at least twice the total number of antennas/subarrays (e.g., an $M\\times N$ 2D array with element-wise movement requires $2MN$ motors with the 2D-moving capability). In comparison, the required number of motors for the proposed CL-MA architecture is equal to the number of horizontal and vertical tracks (i.e., $M+N$ motors for the same $M\\times N$ 2D array), which can significantly reduce hardware cost and control complexity for 2D arrays with large values of $M$ and $N$.\nWe consider a sector of the BS shown in Fig. \\ref{fig:system}, where a CL-MA array is employed to serve $K$ users in the uplink\\footnote{Due to the duality between the uplink and downlink, the optimized positions of MAs based on uplink channels can also improve the downlink communication performance \\cite{zhu2023MAmultiuser,Boche2002duality}.}. The users are each equipped with a single FPA and randomly distributed within the coverage sector of the BS. As shown in Fig. \\ref{fig:architecture}, the CL-MA array consists of $M$ vertical and $N$ horizontal sliding tracks on the 2D plane. Each MA element is installed at the cross point of a horizontal track and a vertical track, which is connected to the RF chain via a flexible cable to enable movement\\footnote{The proposed CL-MA architecture can also accommodate a subarray at each cross point, maintaining the same hardware complexity for the movement module while accommodating more antennas.}. Thus, the total number of antennas is $MN$. With the aid of $M$ motors mounted on the upper side of the array, the vertical tracks can move along the horizontal direction, which changes the horizontal positions of each column of MAs. Similarly, with the aid of $N$ motors mounted on the left side of the array, each row of MAs can collectively move along the vertical direction. Let $\\mathbf{x}=[x_{1},x_{2},\\dots,x_{M}]^{\\mathrm{T}}$ and $\\mathbf{y}=[y_{1},y_{2},\\dots,y_{N}]^{\\mathrm{T}}$ denote the horizontal and vertical APVs, respectively. Specifically, $x_{m}$ represents the horizontal coordinate of the $m$-th column of MAs, while $y_{n}$ represents the vertical coordinate of the $n$-th row of MAs. Without loss of generality, we sort the coordinates in an increasing order, i.e., $x_{1} < x_{2} < \\dots <x_{M}$ and $y_{1} < y_{2} < \\dots <y_{N}$. The antenna moving region is defined as a rectangle of size $[0, x_{\\max}] \\times [0, y_{\\max}]$.",
        "summarize_figure": "2505.03175v1_Fig_architecture",
        "summarization": "The picture shows the architecture of the proposed cross - linked reconfigurable intelligent surface (CL - MA) array. This array consists of multiple horizontal and vertical sliding tracks on a 2D plane. Each MA element or subarray is installed at the intersection of horizontal and vertical tracks. Motors installed on the upper and left sides of the array enable vertical tracks to move horizontally, changing the horizontal positions of corresponding columns of MAs, and horizontal tracks allow corresponding rows of MAs to move vertically. Compared with traditional MA arrays that require a large number of motors for independent 2D movement of elements, the CL - MA array requires a number of motors equal to the sum of horizontal and vertical tracks (for an M×N 2D array, only M + N motors are needed), significantly reducing the hardware cost and control complexity of large - sized 2D arrays."
    },
    "197": {
        "figure1": "2505.03175v1_Fig_system",
        "label1": "fig:system",
        "caption1": "The proposed CL-MA array enabled BS for multiuser communication.}  \\end{center",
        "text": "We consider a sector of the BS shown in Fig. \\ref{fig:system}, where a CL-MA array is employed to serve $K$ users in the uplink\\footnote{Due to the duality between the uplink and downlink, the optimized positions of MAs based on uplink channels can also improve the downlink communication performance \\cite{zhu2023MAmultiuser,Boche2002duality}.}. The users are each equipped with a single FPA and randomly distributed within the coverage sector of the BS. As shown in Fig. \\ref{fig:architecture}, the CL-MA array consists of $M$ vertical and $N$ horizontal sliding tracks on the 2D plane. Each MA element is installed at the cross point of a horizontal track and a vertical track, which is connected to the RF chain via a flexible cable to enable movement\\footnote{The proposed CL-MA architecture can also accommodate a subarray at each cross point, maintaining the same hardware complexity for the movement module while accommodating more antennas.}. Thus, the total number of antennas is $MN$. With the aid of $M$ motors mounted on the upper side of the array, the vertical tracks can move along the horizontal direction, which changes the horizontal positions of each column of MAs. Similarly, with the aid of $N$ motors mounted on the left side of the array, each row of MAs can collectively move along the vertical direction. Let $\\mathbf{x}=[x_{1},x_{2},\\dots,x_{M}]^{\\mathrm{T}}$ and $\\mathbf{y}=[y_{1},y_{2},\\dots,y_{N}]^{\\mathrm{T}}$ denote the horizontal and vertical APVs, respectively. Specifically, $x_{m}$ represents the horizontal coordinate of the $m$-th column of MAs, while $y_{n}$ represents the vertical coordinate of the $n$-th row of MAs. Without loss of generality, we sort the coordinates in an increasing order, i.e., $x_{1} < x_{2} < \\dots <x_{M}$ and $y_{1} < y_{2} < \\dots <y_{N}$. The antenna moving region is defined as a rectangle of size $[0, x_{\\max}] \\times [0, y_{\\max}]$.",
        "summarize_figure": "2505.03175v1_Fig_system",
        "summarization": "The picture shows a system where a base station (BS) equipped with a cross - linked reconfigurable intelligent surface (CL - MA) array is used for multi - user communication. The BS is equipped with a CL - MA array composed of multiple orange elements. On the right side, there are K users (User 1 to User K), each equipped with a single fixed - polarization antenna (FPA) and randomly distributed within the coverage area of the BS. The CL - MA array adjusts the positions of antennas through horizontal and vertical sliding tracks in a 2D plane to provide uplink communication services for multiple users. Due to the duality between uplink and downlink, the positions of MAs optimized based on uplink channels can also improve downlink communication performance."
    },
    "198": {
        "figure1": "2505.03175v1_Fig_optimal_APV",
        "label1": "fig:optimalAPV",
        "caption1": "Illustration of the optimal APVs for $M \\times N =2 \\times 4$ and $K=3$.}  \\end{center",
        "text": "The key idea for constructing the optimal APVs in Theorem \\ref{theo_APV} is by ensuring the SVO condition for each pair of users sequentially, subject to the constraint on the minimum inter-antenna spacing over both horizontal and vertical directions. In Fig. \\ref{fig:optimalAPV}, we show an example of the system with $M \\times N =2 \\times 4$ CL-MAs at the BS serving $K=3$ users, with their virtual AoAs given by $\\vartheta_{1}=0.1,\\varphi_{1}=-0.3$, $\\vartheta_{2}=-0.4,\\varphi_{2}=0.5$, $\\vartheta_{3}=-0.2,\\varphi_{3}=0.7$. The optimal APVs can be constructed as follows. Specifically, we set $\\mathcal{P}_{1}=\\{(1,2)\\}$ and $\\mathcal{P}_{2}=\\{(1,3), (2,3)\\}$. For the horizontal APV, the 1st column of MAs are deployed at $x_{1}^{\\star}=0$. Then, the 2nd column of MAs are deployed at $x_{2}^{\\star}=x_{1}^{\\star}+[\\mathbf{d}_{x}]_{1}=\\lambda$ to guarantee the CVO condition for users 1 and 2. For the vertical APV, the 1st row of MAs are deployed at $y_{1}^{\\star}=0$. Then, the 2nd row of MAs are deployed at $y_{2}^{\\star}=y_{1}^{\\star}+[\\mathbf{d}_{y}]_{1}=0.5\\lambda$ to guarantee the CVO condition for users 1 and 3. Finally, the 3rd and 4th rows of MAs are respectively deployed at $y_{3}^{\\star}=y_{1}^{\\star}+[\\mathbf{d}_{y}]_{2}=2.5\\lambda$ and $y_{4}^{\\star}=y_{2}^{\\star}+[\\mathbf{d}_{y}]_{2}=3\\lambda$ to guarantee the CVO for users 2 and 3. The CVO between any pair of two users is guaranteed by the constructed APVs, which are thus optimal to problem \\eqref{eq_problem_ist} for achieving the lower bound on the total transmit power. As can be observed in Fig. \\ref{fig:optimalAPV}, the size of the antenna moving region required to achieve the lower bound on the transmit power is only $\\lambda \\times 3\\lambda$, demonstrating the ease of realization of the proposed optimal solution presented in Theorem \\ref{theo_APV}.",
        "summarize_figure": "2505.03175v1_Fig_optimal_APV",
        "summarization": "The picture shows an example of optimal Antenna Position Vectors (APV) for M×N = 2×4 and K = 3. Horizontally, the position of the first - column antennas x1* is 0, and the second - column antennas x2* is x1* plus the first value of the horizontal spacing dx (equal to the wavelength lambda) to meet the condition for users 1 and 2. Vertically, the position of the first - row antennas y1* is 0. The second - row antennas y2* is y1* plus the first value of the vertical spacing dy (0.5 times lambda) for users 1 and 3. The third row y3* is y1* plus the second value of dy (2.5 times lambda), and the fourth row y4* is y2* plus the second value of dy (3 times lambda) for users 2 and 3. The constructed APV ensures the condition between any two users, and the antenna movement area needed to achieve the lower bound of transmit power is only lambda×3lambda, showing the easy implementation of the scheme."
    },
    "199": {
        "figure1": "2505.03177v1_param_dist_ABC_area",
        "label1": "fig:ABC_param",
        "caption1": "Asymptotic consistency of the posterior distributions of $\\bm{\\theta}$ and $\\bm{w}$ across iterations.",
        "text": "The performance improvement is further illustrated in the estimated posterior distributions of $\\bm{\\theta}$ and $\\bm{w}$ as shown in Figure~\\ref{fig:ABC_param}. Here we select one representative parameter for each regulatory mechanism: $K_{m,\\text{EGLC}}$ is the dissociation constant in Reaction 2; $K_{i,\\text{ELACtoHK}}$ captures the non-competitive inhibition of ELAC on EGLC uptake in Reaction 2; and $K_{a,\\text{EGLN}}$ reflects the allosteric activation effect of EGLN on ELAC production in Reaction 3.  The results show the MALA with adjoint SA produces posterior distributions with higher concentration—defined as posterior mass surrounding the true parameter value $\\bm{\\theta}^c$—and achieves faster convergence toward $\\bm{\\theta}^c$. In addition, as shown in the last column of Figure~\\ref{fig:ABC_param}, the weight $w_{k^*}$ of the true model, corresponding to the correct combination of regulatory modules R1–R4 in Figure~\\ref{fig:MetaNetwork}, also converges to 1 more rapidly compared to other state-of-the-art methods.",
        "summarize_figure": "2505.03177v1_param_dist_ABC_area",
        "summarization": "The picture shows the posterior distribution estimates of parameters KmEGLC, KiELACtoHK, KaEGLN and weight Weight by ABC, MALA, and MALA with adjoint SA at different iteration numbers (Iter 1, Iter 10, Iter 20, Iter 100). As the number of iterations increases, the posterior distribution obtained by MALA with adjoint SA is more concentrated around the true parameter value (marked by the dashed line). Compared with other methods, it converges faster to the true parameter value, and the weight wk* of the true model corresponding to the correct combination of regulatory modules also converges to 1 more quickly."
    },
    "200": {
        "figure1": "2505.03178v1_clustered_trajs-compressed",
        "label1": "Fig:DynamicsCheck",
        "caption1": "Tokenized dynamics check. (a) Schematic for dynamics check. This module refines the raw trajectory by finding the closet motion token from a set of motion vocabulary. (b) Visualization of motion token vocabulary constructed from the rounD dataset~\\cite{krajewski2020round}. The vocabulary contains $1024$ tokens, and here for better visualization, we show $512$ tokens.",
        "text": "For efficient dynamics check, we design a tokenized dynamics check module in this paper, motivated by recent GPT-style trajectory generation or prediction techniques~\\cite{wu2024smart,philion2023trajeglish,shi2022motion}, which have demonstrated strong performance on discrete trajectory tasks, particularly in small datasets. As shown in Fig.~\\ref{Fig:DynamicsCheck}(a), the core idea is to constrain trajectory generation within a set of valid state transitions, referred to as motion tokens. Specifically, we define a motion vocabulary $V=\\{\\boldsymbol{a}\\}$, where each token $\\boldsymbol{a}$ represents a state transition from current state $\\boldsymbol{s}$ to next state $\\boldsymbol{s}'$. This vocabulary is constructed by applying a k-disks clustering algorithm~\\cite{philion2023trajeglish} to all motion transitions observed in the dataset; see Fig.~\\ref{Fig:DynamicsCheck}(b) for illustration of the constructed motion vocabulary.",
        "summarize_figure": "2505.03178v1_clustered_trajs-compressed",
        "summarization": "The picture shows the visualization of the motion token vocabulary constructed from the rounD dataset. The motion vocabulary consists of 1024 tokens, and for better visualization, 512 tokens are presented in the figure. These tokens, in the form of blue arrows, are distributed on the x (m) and y (m) coordinate plane, representing the state transition from the current state s to the next state s'. The motion token vocabulary is constructed by applying the k - disks clustering algorithm to all motion transitions observed in the dataset, aiming to provide valid state transition constraints for the tokenized dynamics check module to refine the raw trajectory."
    },
    "201": {
        "figure1": "2505.03179v1_UserStudy_Design_v2",
        "label1": "fig:studydesign",
        "caption1": "\\textbf{Study Design:} In format A, participants complete the task independently first, followed by collaboration with AI. In format B, the order is reversed, starting with AI collaboration before working independently. Both formats are followed by a questionnaire.",
        "text": "In our study, each participant engaged in a three-phase process, as shown in~\\autoref{fig:studydesign}:",
        "summarize_figure": "2505.03179v1_UserStudy_Design_v2",
        "summarization": "The picture shows two user study design formats. In Study Format A, participants first complete the task independently using Problem Set 1, and then collaborate with AI to complete the task using Problem Set 2. In Study Format B, the order is reversed. Participants first collaborate with AI to complete the task using Problem Set 1, and then complete the task independently using Problem Set 2. After the tasks in both formats are completed, a questionnaire survey is required."
    },
    "202": {
        "figure1": "2505.03181v1_token_based_draft",
        "label1": "fig:token_based",
        "caption1": "\\textbf{Converting Turns $\\rightarrow$ Tokens.} We treat each token of the agent's reply as a single action by passing consecutive turns of dialogue through the VLM and finding output representations corresponding to tokens where an action decision is made. Outputs corresponding to the prompt tokens are masked and ignored. The resulting sequence of RL input states is numbered $(0, 1, 2, 3, 4)$.",
        "text": "Where $\\cancel{\\nabla}$ is a stop-gradient that puts the VLM in inference mode. The expanded batch of RL transitions is: $(h^0, \\tilde{a}_t^0, r^1 = 0, d^1 = 0, h^1), (h^1, \\tilde{a}_t^1, r^2 = 0, d^2 = 0, h^2), \\dots, (h^{l}, \\tilde{a}_t^{l}, r^{l + 1} = r_{t + 1} , d^{l + 1} = d_{t+1}, h^{l + 1} = h')$. This process is visualized in Figure \\ref{fig:token_based}. After the conversion, we can apply RL exactly as we would in any other setting, and gradients will flow back to the base VLM.",
        "summarize_figure": "2505.03181v1_token_based_draft",
        "summarization": "The picture shows the process of converting dialogue turns into tokens. Consecutive dialogue turns are processed through a Vision - Language Model (VLM), treating each token in the agent's reply as a single action. The input includes image features, prompt features, and action features, taking turn - based observations as input. The VLM processes and outputs actions. The environment provides rewards based on actions and generates the next observation. Outputs corresponding to prompt tokens are masked and ignored, ultimately resulting in a Reinforcement Learning (RL) input state sequence that is numbered."
    },
    "203": {
        "figure1": "2505.03181v1_gym_cards_base_model_evals_2",
        "label1": "fig:gym_cards_base_models",
        "caption1": "\\textbf{Gym Cards Model Evaluations.} We compare the task success and the rate of valid model output syntax of base models in two Gym Cards environments. LLM prompts replace the image with an equivalent text description.",
        "text": "Figure \\ref{fig:gym_cards_base_models} compares the performance of the three base VLMs in Table \\ref{tbl:vlm_intro} using only prompting and without further fine-tuning. We see that it is surprisingly difficult to get these smaller open-weight VLMs to cooperate with the expected action syntax. Action syntax is closely related to tool-use (Section \\ref{sec:background}), where larger models tend to be more accurate, and this capability is more reliable in larger LLMs. We can demonstrate this by using privileged environment data to add a text summary of the image to the observation prompt. LLMs can then be used in a nearly equivalent setup by simply ignoring the visual input. xLAM \\citep{zhang2024xlam} --- an LLM specifically targeted at tool-use and function-calling applications --- achieves high accuracy. Off-policy RL fine-tuning will let us learn from the interactions of larger models (including LLMs with alternate text prompts). However, even when models have reasonable syntax accuracy, they might only output certain subsets of the action space or make incorrect semantic decisions that will not solve the task. Appendix \\ref{app:implementation} Figure \\ref{fig:additional_prompting} repeats this experiment in the BabyAI \\citep{chevalier-boisvert2018babyai} domain and explores the addition of ``thought''-style prompting. We find all VLMs output inaccurate syntax, and only xGen-MM has a non-zero success rate.",
        "summarize_figure": "2505.03181v1_gym_cards_base_model_evals_2",
        "summarization": "The picture shows the comparison of the task success rate (blue) and valid action accuracy rate (gray) of multiple models in the NumberLine and Blackjack environments of Gym Cards. In the NumberLine environment, Random Valid Actions has a task success rate of 58% and a valid action accuracy rate of 100%; xGen-MM has a task success rate of 53% and a valid action accuracy rate of 83%; xLAM (LLM) and Gemma 2B (LLM) both reach 100% in task success rate and valid action accuracy rate, while MoonDream2 (Min. Format) and PaliGemma (Min. Format) are 0. In the Blackjack environment, Random Valid Actions has a task success rate of 28% and a valid action accuracy rate of 100%; xGen-MM has a task success rate of 32% and a valid action accuracy rate of 55%; xLAM (LLM) has a valid action accuracy rate of 100% and a task success rate of 13%, and other models have some indicators at 0. It reflects the differences in the task execution capabilities of different models in the two environments. Smaller models have more difficulty in conforming to the expected action syntax, while larger models perform more accurately."
    },
    "204": {
        "figure1": "2505.03181v1_gym_cards_horizontal_epg",
        "label1": "fig:gym_cards_offline_to_online",
        "caption1": "\\textbf{Offline-to-Online Gym Cards.} \\textbf{(Left, Center Right)} Task success rate ($[0, 1]$) compared against a reference score for a similar setup in \\citet{zhai2024fine}. A \\textcolor{gray}{gray} background indicates offline learning on a fixed dataset. A white background indicates that online environment interaction is enabled. \\textbf{(Center Left, Right)} Action syntax accuracy during evaluation.}  \\vspace{-7mm",
        "text": "We can improve performance by taking these same VLM-agents and fine-tuning them with $\\mathcal{L}_{\\text{VLMQ}}$ (Eq. \\ref{eq:vlmq}). Figure \\ref{fig:gym_cards_offline_to_online} demonstrates the technique in two Gym Cards tasks. We initialize a dataset $\\mathcal{D}$ to the actions generated by a random policy; this data makes sub-optimal (random) decisions that are properly formatted as text. We also mix in data collected by prompting base VLMs --- which are both inaccurate and low-performance. More detail about the initial offline datasets used in our experiments is provided in Appendix \\ref{app:offline_datasets}. For demonstration purposes, the advantage threshold is initialized to $\\beta = -\\infty$ (Eq. \\ref{eq:filter}) such that the language head (RL actor) is optimizing SFT on this sub-optimal dataset while beginning to optimize the critic value regression loss (Eq. \\ref{eq:critic}). This quickly brings both xGenMM and MoonDream2 to a similar performance level. After a few hundred gradient steps, we schedule the advantage threshold to reach $\\beta = 0$; the critic is now filtering sub-optimal tokens within each turn's reply, and the agent's performance improves to match or exceed the RL4VLM \\citep{zhai2024fine} reference score. Because $\\mathcal{L}_{\\text{VLMQ}}$ is equally applicable online and offline, we are free to use our current RL agent to collect additional data. In this case, the agents have already converged to high performance but maintain this level as the buffer fills with fresh data. In RL, it is not uncommon to see a dip in performance during the transition between offline and online learning due to distribution shift and related optimization challenges \\citep{zhang2024perspective}, but Eq. \\ref{eq:fsft} avoids many of these obstacles, and we do not observe instability.\nFigure \\ref{fig:gym_cards_offline_to_online} includes results from an alternative actor update that replaces Eq. \\ref{eq:fsft} with a more standard online objective that directly maximizes $Q$-values according to the current critic: $\\mathcal{L}_{\\text{EPG}}(i) =  -\\frac{1}{l} \\sum_{j=0}^{l} L_{\\phi}(h_i^j)^{\\intercal} \\cancel{\\nabla} Q_{\\psi}(h_i^j)$ \\citep{JMLR:v21:18-012, christodoulou2019soft}. While this objective is effective in discrete actor-critics in more typical RL settings, it is not effective here due to  miscalibrated $Q$-values in the critic network's output space resulting from large token vocabularies.",
        "summarize_figure": "2505.03181v1_gym_cards_horizontal_epg",
        "summarization": "The picture shows the task success rate and valid action accuracy rate of different models in the offline learning (gray background) and online learning (white background) phases in the NumberLine and Blackjack tasks of Gym Cards. In the NumberLine task, models such as xGenMM+SFT, xGenMM+AFSFT, MoonDream2+EPG, and MoonDream2+AFSFT show different trends in task success rate and valid action accuracy rate as the number of training epochs increases. When the advantage filter is activated, the performance of some models improves. In the Blackjack task, each model also shows different changes in success rate and accuracy rate at different stages, and finally, it is compared with the reference score of RL4VLM (Final Eval) to reflect the optimization effect."
    },
    "205": {
        "figure1": "2505.03185v1_FlowChart",
        "label1": "fig:flowchart",
        "caption1": "Flow chart of SLR process based on the PRISMA guidelines",
        "text": "Our overall multi-staged screening and selection process is depicted in Figure \\ref{fig:flowchart}. The final set of review paper set includes 136 papers overall. Annual publication count is shown in Figure \\ref{fig:annual_count}, illustrating a growing research interest in ingestion health in recent years. We identified 93 distinct publication venues across all included studies, with full details provided in \\ref{tab:venues list}. Among them, the following venues featured two or more included papers: ACM Conference on Human Factors in Computing Systems (CHI, n=16), ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)/UbiComp (N=16), ACM Interaction Design and Children Conference (IDC, n=2), ACM Designing Interactive Systems Conference (DIS, n=4), IEEE Sensors (n=2), IEEE International Conference on Robot and Human Interactive Communication (RO-MAN, n=2), ACM/IEEE International Conference on Human-Robot Interaction (HRI, n=2), PervasiveHealth (n=2), SIGGRAPH Asia (n=2), International Conference on Tangible, Embedded, and Embodied Interaction (TEI, n=2), Innovations in Power and Advanced Computing Technologies (i-PACT, n=2), International Conference on Computing Communication and Networking Technologies (ICCCNT, n=2), World Congress on Medical Physics and Biomedical Engineering (n=2).",
        "summarize_figure": "2505.03185v1_FlowChart",
        "summarization": "The picture is a flowchart of the Systematic Literature Review (SLR) process based on PRISMA guidelines. In the identification stage, records are obtained from ACM - DL (679 records), IEEE Xplore (911 records), Google Scholar (100 records), citation searching (29 records), and own bibliography (9 records). Before screening, 22 duplicate records and 5 extended versions of included records are removed. In the abstract/title screening stage, 1663 records are screened, excluding those not accessible/not in English (3 records), not ingestion - related/out of scope (1129 records), without behavioral intervention (214 records), and without sensor/perceptual computing (68 records). In the full - text screening stage, 249 records are screened, excluding those in clinical or therapeutic contexts (18 records), with non - health - oriented goals (42 records), and not primary research (91 records). Finally, 98 publications from databases and 38 from other methods are included in the synthesis."
    },
    "206": {
        "figure1": "2505.03185v1_annual_publications_2",
        "label1": "fig:annual_count",
        "caption1": "Total number of papers published overall (split to process behavior only, context factor only, and both included)",
        "text": "Our overall multi-staged screening and selection process is depicted in Figure \\ref{fig:flowchart}. The final set of review paper set includes 136 papers overall. Annual publication count is shown in Figure \\ref{fig:annual_count}, illustrating a growing research interest in ingestion health in recent years. We identified 93 distinct publication venues across all included studies, with full details provided in \\ref{tab:venues list}. Among them, the following venues featured two or more included papers: ACM Conference on Human Factors in Computing Systems (CHI, n=16), ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)/UbiComp (N=16), ACM Interaction Design and Children Conference (IDC, n=2), ACM Designing Interactive Systems Conference (DIS, n=4), IEEE Sensors (n=2), IEEE International Conference on Robot and Human Interactive Communication (RO-MAN, n=2), ACM/IEEE International Conference on Human-Robot Interaction (HRI, n=2), PervasiveHealth (n=2), SIGGRAPH Asia (n=2), International Conference on Tangible, Embedded, and Embodied Interaction (TEI, n=2), Innovations in Power and Advanced Computing Technologies (i-PACT, n=2), International Conference on Computing Communication and Networking Technologies (ICCCNT, n=2), World Congress on Medical Physics and Biomedical Engineering (n=2).",
        "summarize_figure": "2505.03185v1_annual_publications_2",
        "summarization": "The picture shows the annual publication count of ingestion health research from 2002 - 2025. The blue line represents all research, the orange line represents research on process behavior only, the green line represents research on context factors only, and the red line represents research on both. Overall, the publication numbers of various types of research fluctuate in different years, and there is an upward trend in recent years, reflecting growing attention to ingestion health research. The research on context factors only has a relatively significant increase in some years."
    },
    "207": {
        "figure1": "2505.03185v1_closed-loop_overview",
        "label1": "fig:close-loop overview",
        "caption1": "Behavior closed-loop overview",
        "text": "Specifically, the stages in behavioral closed-loop system include identifying target behaviors, sensing behaviorally relevant data, interpreting the observed phenomena through reasoning processes, developing feedback or intervention strategies, and monitoring subsequent behavioral adjustments to close the loop. This structure guides the subsequent analysis and provides a detailed understanding of how perceptual computing techniques are applied across different stages of intervention design. An overview of the primary behavioral closed-loop paradigm and its staged evolution is illustrated in Figure \\ref{fig:close-loop overview}. While existing studies often focus on specific stages rather than achieving a fully closed-loop design, we structure the subsequent analysis and synthesis based on the ideal of a complete behavioral closed-loop framework.",
        "summarize_figure": "2505.03185v1_closed-loop_overview",
        "summarization": "The picture shows an overview of the behavioral closed - loop system. Target factors and behaviors are divided into contextual factors (food choices, intake timing, etc.) and process behaviors (intake quantity, ingestion pace, etc.). Sensing and data collection are carried out through human - based (auditory, visual, etc.) and environment - based (acoustic, spectral, etc.) methods. After reasoning, feedback or intervention is provided in various forms such as visual and auditory. Finally, evaluation is conducted through metrics like usability, effectiveness, user acceptance, and ingestive literacy and awareness, reflecting the application of perceptual computing techniques in different stages of intervention design."
    },
    "208": {
        "figure1": "2505.03194v1_plot",
        "label1": "fig:simulation",
        "caption1": "$\\WTwo$ error in multi-step sampling.",
        "text": "We simulate three instantiations of $\\crl*{\\sampleStep{i}}_{i=1}^N$ defined in~(\\ref{eq:sampling-time-schedule}), i.e. the sequence of time steps for our multi-step sampling defined in~(\\ref{eq:sampling-time-schedule}): In Figure~\\ref{fig:simulation}, we plot the $\\WTwo$ error in multi-step sampling. We present the revolution of $\\WTwo$ error in a sampling time schedule on a single curve. Specifically, we plot each curve by: Because the sampling time step $\\sampleStep{i}$ decreases in the multi-step sampling by definition. We reverse the $x$-axis of the plot for presentation purposes.",
        "summarize_figure": "2505.03194v1_plot",
        "summarization": "The picture shows the W2 error in multi - step sampling as the sampling time step changes. Four curves represent \"our schedule\" (blue triangles), \"our upper bound\" (orange stars), \"baseline 1\" (green dots), and \"baseline 2\" (red dots). As the sampling time step t decreases from 20.0, the W2 error corresponding to each curve generally shows a downward trend, reflecting the differences in the change of W2 error under different settings during the multi - step sampling process."
    },
    "209": {
        "figure1": "2505.03195v1_IJCAI-2025-FIG1",
        "label1": "fig:prediction",
        "caption1": "\\textbf{An example of breaking data dependencies by a predictor in the superscalar processor design.} \\textbf{(a)} For the program slice (Inst.1 and Inst.2) under Read-after-Write (RAW) dependency, without any predictors, these two instructions have to execute one after another. \\textbf{(b)} With the data dependency predictor, the predicted data of register $r1$ can be obtained in advance by Inst.2 so that it is no longer blocked by Inst.1. }  \\vspace{-5pt",
        "text": "In this paper, we propose to automatically design a superscalar processor by learning data dependencies with a novel hardware-friendly machine learning model \\emph{Stateful Binary Speculation Diagram} (State-BSD). Surpassing the state-of-the-art automated processor design method Binary Speculation Diagram (BSD), State-BSD captures the inter-instruction dependencies with a small set of reusable states. In this way, it can be trained to design a highly precise on-the-fly predictor in the processor for instruction-level parallelism, consisting of 1) a lightweight state-selector to select the reusable processor's internal states and store them in a small buffer and 2) a highly precise state-speculator to accurately speculate the state transition and predict the dependent data. More specifically, to meet the lightweight challenge, a small set of states are selected from the vast high-dimensional processor internal-state space by simulated annealing, and stored in a small buffer for further reuse. To meet the precision challenge, the state-speculator uses the Binary Speculation Diagram method to precisely speculate the predictable dependencies with buffered states. According to the human design paradigm, the dependent data can be separated into predictable dependencies and unpredictable dependencies. The state-speculator can 100\\%-precise predict the predictable dependencies with BSD expansion, which is proved in the previous work~\\cite{cheng2023pushing}, meanwhile bypassing the unpredictable dependencies to avoid the potential functional errors caused by misprediction. With State-BSD, we can automatically design an on-the-fly predictor for instruction-level parallelism, in which the predictable dependent instructions can be paralleled by predicting the dependent data, as shown in Figure~\\ref{fig:prediction}.",
        "summarize_figure": "2505.03195v1_IJCAI-2025-FIG1",
        "summarization": "The picture shows an example of breaking data dependencies by a predictor in superscalar processor design. In the left part (a), for the program slice Inst.1 (mul r1, r3, r4) and Inst.2 (mul r2, r5, r1) with Read - after - Write (RAW) dependency, without a predictor, these two instructions must execute sequentially, with r1 first written by Inst.1 and then read by Inst.2. In the right part (b), with the data dependency predictor, Inst.2 can predict the data of r1 in advance through the execution trace and is no longer blocked by Inst.1, enabling instruction - level parallelism."
    },
    "210": {
        "figure1": "2505.03195v1_IJCAI-2025-FIG2",
        "label1": "fig:State-BSD",
        "caption1": "\\textbf{The design overview.} (a) The automated designed superscalar processor. It consists of a predictor automatically designed by State-BSD and $k$ execution units. Every clock cycle, the predictor outputs $m$ indicating how many instructions can be execute in parallel, and $\\Psi$ indicating the predicted dependent data. (b) The hardware implement of the dependency predictor, consisting of two components: the state-selector and state-speculator. The state-selector is a multiplexer with small buffer, and the state-speculator is a high-precision combinational module. (c) Train the dependency predictor with State-BSD: The selector is trained by simulated annealing, and the speculator is trained by BSD expansion. %With dependent prediction, the superscalar processor design is formalized as a machine learning problem, i.e. time-series prediction. We use the State-BSD architecture for the problem, consisting of (1) a state-selector to select and store the reusable processor intermediate states and (2) a state-speculator to accurately speculate the state transition and predict the dependent data. More specifically, the state-selector is a simulated annealing method searching for a better circuit embedding, and the state-speculator is a BSD expansion method from the state-of-the-art automated single-cycle processor design method. ",
        "text": "In this section, we provide an overview of the design, specifically focusing on how the superscalar processor is constructed with a learned data predictor.  The input of the design flow is a single-cycle processor, which can be automatically designed by the previous works~\\cite{cheng2023pushing}, and the output of the design flow is a superscalar processor. The design flow designs a dependency predictor to control the instruction-level parallelism with multiple execution units, as illustrated in Fig.~\\ref{fig:State-BSD} (a). For a processor with $n$ execution units, in the clock cycle, the predictor checks the following $n$ instructions to be executed and whether their operands can be predicted. If the predictor can predict all the data of the following $m$ instructions ($m \\leq n$), the superscalar processor executes $m$ instructions in parallel. Following the paradigm of human-designed processors, we identify three types of dependency potentials:  the data dependency in the load/store unit for memory (MEM), the data dependency in the program counter for instructions (PC), and the data dependency in general purpose registers (GPR). Accordingly, we propose three sub-predictors, each dedicated to one of these data dependencies. For each sub-predictor, its input is the instruction $\\phi$, the current processor states $\\psi$, and its output is a control signal $m$ indicating the dependent data in the following $m$ instruction can be predicted by the predictor, along with the predicting data itself. For an ongoing instruction, it can be predicted by the dependent predictor if and only if all three sub-predictor can predict the corresponding potential dependent data. For hardware implementation, we train these three sub-predictors independently (details of the training process are in Section 3.2) and implement them using Verilog code. All the sub-predictors are implemented as a combinational module and then integrated into the processor. The training phase must ensure the design's correctness, as the implementation cannot be modified after the processor is manufactured.",
        "summarize_figure": "2505.03195v1_IJCAI-2025-FIG2",
        "summarization": "The figure shows the design overview of an automatically - designed superscalar processor. Part (a) on the left is the superscalar processor, consisting of a predictor designed by State - BSD and k execution units. The predictor outputs m (number of instructions that can be executed in parallel) and Ψ (predicted dependent data) per clock cycle. Part (b) on the upper right is the hardware implementation of the dependency predictor, with a state - selector (a multiplexer with a small buffer) and a state - speculator (a high - precision combinational module). Part (c) on the lower right is training the dependency predictor with State - BSD, where the selector is trained by simulated annealing and the speculator by BSD expansion."
    },
    "211": {
        "figure1": "2505.03195v1_IJCAI-2025-FIG3",
        "label1": "fig:CPU performance",
        "caption1": "\\textbf{CPU performance comparison with the state-of-the-art.} We compare the performance of the automated designed CPU designed by our method and the state-of-the-art. The red data points are human-designed commercial CPUs with similar performance. The result shows that our design is comparable to Arm Cortex A53 (2010s CPU), while the state-of-the-art automated design is only comparable to Intel 486 (1990s CPU).}  \\vspace{-10pt",
        "text": "We compare the design performance of State-BSD with both the overall human-design CPU and the human-design predictor within the same computer architecture. The experimental results show that State-BSD significantly narrows the gap between automated and human design abilities. Experimental results in Figure~\\ref{fig:CPU performance} show that our method significantly improves the automated design ability from the CPUs in the 1990s (16K Dhrystone/s) to the CPUs in the 2010s (6.2M Dhrystone/s). The results show that State-BSD exploits most of the optimization potentials, i.e., 94\\%  possible parallel instructions human designers find while eliminating massive human design effort.",
        "summarize_figure": "2505.03195v1_IJCAI-2025-FIG3",
        "summarization": "The figure shows the CPU performance comparison between the automatically - designed CPU by this method and the state - of - the - art. The y - axis is throughput (Dhrystone/s), and the x - axis has \"SOTA\" for the state - of - the - art automated design and \"Ours\" for the CPU designed by this method. Red cross data points are human - designed commercial CPUs with similar performance. The results show that the CPU designed by this method is comparable to ARM Cortex A53 in the 2010s, while the state - of - the - art automated design is only comparable to Intel 486 in the 1990s, indicating a significant improvement in automated design ability."
    },
    "212": {
        "figure1": "2505.03201v1_AUC",
        "label1": "fig:filter_based",
        "caption1": "The average AUC of Deletion Score for filterd WG.",
        "text": "We further analyze the effect of removing low-fitness baselines incrementally, comparing WG with Expected Gradients. Figure~\\ref{fig:filter_based} plots the average AUC of Deletion Score as we remove 1 to 5 low-fitness baselines.\nFigure~\\ref{fig:filter_based} reveals that WG’s AUC remains stable, starting at 1.6872 and reaching a minimum of approximately 1.4 after removing 3 baselines, before slightly rising to 1.5 after 5 removals. The consistent gap (1.3--1.5 AUC units) underscores WG’s robustness, as filtering low-fitness baselines maintains or improves performance while reducing computational overhead.",
        "summarize_figure": "2505.03201v1_AUC",
        "summarization": "The figure shows the average AUC of Deletion Score for filtered WG (our method) as the number of removed low - fitness baselines changes. The x - axis is the average number of removed baselines, and the y - axis is the average AUC. The \"Filtered WG (ours)\" curve shows that WG's AUC starts at 1.6872, drops to about 1.4 when 3 baselines are removed, and slightly rises to 1.5 after 5 removals, remaining relatively stable overall. Compared with \"EG (4.4474)\", there is a stable gap of 1.3 - 1.5 AUC units, highlighting the robustness of WG. Filtering low - fitness baselines can maintain or improve performance while reducing computational overhead."
    },
    "213": {
        "figure1": "2505.03208v1_Backdoor_Attack_block_diagram",
        "label1": "fig:Backdoor Attack",
        "caption1": "Backdoor Attack Experimental Setup.",
        "text": "An overview of the backdoor attack experimental setup is depicted in Figure~\\ref{fig:Backdoor Attack}.",
        "summarize_figure": "2505.03208v1_Backdoor_Attack_block_diagram",
        "summarization": "The picture shows the experimental setup process for backdoor attacks. The green training dataset is turned into a poisoned dataset (red) by inserting text, numeric, visual triggers and changing the target label. A model is trained on the poisoned dataset and then the backdoored model is deployed. Eventually, this model makes malicious predictions on poisoned samples, potentially leading to evasion of toxic content detection and fake news detection."
    },
    "214": {
        "figure1": "2505.03208v1_Chaos_based_metric_block_diagram",
        "label1": "fig:PDS",
        "caption1": "Methodology for the Chaos based Precision Matrix Dependency Score for Backdoor Trigger Detection.",
        "text": "Dependency Score (PDS) for Backdoor Detection} An overview of the methodology used in the Chaos based PDS approach is depicted in Figure~\\ref{fig:PDS} and is elaborated next.",
        "summarize_figure": "2505.03208v1_Chaos_based_metric_block_diagram",
        "summarization": "The chart shows the methodology for the chaos - based Precision Matrix Dependency Score (PDS) for backdoor trigger detection. First, input features or sentence embeddings for each class label are normalized. Then, through Neurochaos Learning (NL), Uniform Manifold Approximation and Projection (UMAP), DBSCAN Clustering and other steps, the clusters are evaluated. If the clusters are not distinct, NL hyperparameters are retuned. After obtaining the best NL hyperparameters, the covariance matrix, precision matrix, and PDS for each cluster are computed successively."
    },
    "215": {
        "figure1": "2505.03209v1_strategy_induction",
        "label1": "fig:strategy_induction",
        "caption1": "An example strategy induction process from expert demonstrations in GPT-4o \\cite{openai2024gpt4} for the Dynamic Obstacles RL environment from Minigrid \\cite{MinigridMiniworld23}. See Appendix \\ref{app:strategy_evolvement} for the complete list of strategies induced in this example.}  \\vspace{-0.1in",
        "text": "Recent research works have demonstrated the ability of LLMs to automatically extract generalizable rules, knowledge and insight from examples~\\cite{zhu2024large,Zhao_Huang_Xu_Lin_Liu_Huang_2024}. Inspired by these works, here we focus on using LLMs to automatically induce useful and generalizable strategies for completing tasks in reinforcement learning environments from trajectories of expert demonstrations. We adapt and extend the prompting method in \\cite{Zhao_Huang_Xu_Lin_Liu_Huang_2024} to design our prompt for automatic RL strategy induction. Our prompt has three components: \\textit{Description of the RL Environment}, \\textit{Expert Demonstration Trajectories} and \\textit{Strategy Query Prompt}. The \\textit{Expert Demonstration Trajectories} component includes a full textual description for each of the expert demonstration trajectories in $\\mathcal{D}$ including its goal and a concatenation of the textual descriptions of all \\{observation, action\\} pairs in sequential order. The \\textit{Strategy Query Prompt} component describes our expectations for the kind of strategies that the LLM should induce from expert demonstrations and generate for us. Figure \\ref{fig:strategy_induction} demonstrates a concrete example of this prompt and our strategy induction process from expert demonstrations in GPT-4o \\cite{openai2024gpt4} for an RL environment called Dynamic Obstacles from the Minigrid library \\cite{MinigridMiniworld23}. As we can see in Figure \\ref{fig:strategy_induction}, the list of strategies induced by GPT-4o is indeed very relevant to successfully completing tasks in this Dynamic Obstacles RL environment, and also coincides with human intuition.",
        "summarize_figure": "2505.03209v1_strategy_induction",
        "summarization": "The chart shows the process of inducing strategies from expert demonstrations via GPT - 4o in the \"Dynamic Obstacles\" RL environment of MiniGrid. The strategy - inducing prompt on the left has three parts: description of the RL environment, asking the model to imagine as an RL agent in the \"Dynamic Obstacles\" environment of MiniGrid; expert demonstration trajectories, providing 5 successful expert demonstration action trajectories; and strategy query prompt, asking the agent to induce generalizable strategies from these demonstrations. On the right, GPT - 4o generates a list of strategies, like moving forward when safe and the goal is within reach, or when the goal is directly ahead."
    },
    "216": {
        "figure1": "2505.03209v1_modeling_pipeline",
        "label1": "fig:modeling_pipeline",
        "caption1": "An overview of our proposed new modeling pipeline - Dynamic Strategy Induction with LLMs for Reinforcement Learning (DYSTIL). The steps depicted in \\textcolor{greenarrow}{green} arrows corresponds to \\textit{Initialization of the RL Agent Model}, \\textit{Initial Strategy Induction from Expert Demonstrations} and \\textit{Behavioral Cloning with Induced Strategies}; the steps depicted in \\textcolor{bluearrow}{blue} arrows corresponds to \\textit{Experience Collection and Advantage Estimation} and \\textit{Induction of New Candidate List of Strategies}; and the steps depicted in \\textcolor{magentaarrow}{magenta} arrows corresponds to \\textit{Strategy-Integrated Proximal Policy Optimization}.",
        "text": "As we can see, in DYSTIL these last three steps \\textit{Experience Collection and Advantage Estimation}, \\textit{Induction of New Candidate List of Strategies} and \\textit{Strategy-Integrated Proximal Policy Optimization} will be executed in cycles to iteratively train the RL agent model to improve its performance. Our DYSTIL learning framework is illustrated in Figure \\ref{fig:modeling_pipeline} and also summarized in Algorithm \\ref{alg:algorithm}. Also see Appendix \\ref{app:complexity} for the complexity analysis of DYSTIL.",
        "summarize_figure": "2505.03209v1_modeling_pipeline",
        "summarization": "The chart shows the modeling pipeline of Dynamic Strategy Induction with LLMs for Reinforcement Learning (DYSTIL). Green - arrowed steps include RL agent model initialization, initial strategy induction from expert demonstrations, and behavioral cloning with induced strategies. Blue - arrowed steps are experience collection and advantage estimation, and induction of a new candidate list of strategies. Magenta - arrowed steps are strategy - integrated proximal policy optimization. In DYSTIL, the last three steps are executed cyclically to iteratively train the RL agent model for performance improvement."
    },
    "217": {
        "figure1": "2505.03209v1_ablation_results",
        "label1": "fig:ablation_results",
        "caption1": "The results of our ablation study.",
        "text": "In our ablation study, we remove the dynamic strategy update component from our DYSTIL procedures, and run experiments in the four RL environments to see how that will affect the performance of RL training. After the removal of the dynamic strategy update component, the RL agent will keep using the initial list of strategies that it obtains from the Strategy-Generating LLM (before behavioral cloning) for the whole PPO training process without updating it, and we call this ablated method $\\text{DYSTIL}_{\\text{BC+PPO}} \\text{-Static}$. The results of our ablation study are plotted in Figure \\ref{fig:ablation_results}. As we can see, on average the success rate drops by 7.75\\% and the mean return drops by 0.062 (31\\% in ratio) after removing the dynamic strategy update component from DYSTIL, which shows that the dynamic strategy update component is indeed critical in achieving the best reinforcement learning performance with DYSTIL.",
        "summarize_figure": "2505.03209v1_ablation_results",
        "summarization": "The chart shows the results of the ablation study. The left chart is for success rate ablation, and the right one for mean return ablation, comparing DYSTILBC + PPO and DYSTILBC + PPO-Static in four RL environments (Dynamic Obs, Unlock Pickup, Key Corridor, Put Next) and the average case. After removing the dynamic strategy update component (DYSTILBC + PPO-Static), the average success rate drops by 7.75% and the mean return drops by 0.062 (31% in ratio). This shows that the dynamic strategy update component is crucial for DYSTIL to achieve optimal RL performance."
    },
    "218": {
        "figure1": "2505.03210v1_PIC5",
        "label1": "FIGWBA5",
        "caption1": "Exponential convergence of the  weighted quasi-Monte Carlo method  in different settings. For the oscillating curves, red corresponds to $ p = q = 1/2 $, pink corresponds to $ p = q = 1 $, blue corresponds to $ p = 1$ and $ q = 2 $, and green corresponds to $ p = q = 2 $, respectively.} \t",
        "text": "The terms ``quasi-periodic'' and ``almost periodic'' are used here because the corresponding rotations are nonresonant; for example, they satisfy certain finite-dimensional or infinite-dimensional Diophantine conditions as in Definitions \\ref{Finite-dimensional Diophantine nonresonance} and \\ref{Infinite-dimensional Diophantine nonresonance}. To simplify the exposition, we prefer not to state these concepts in Theorem \\ref{QMC}. These rotations are universal in the sense of measure, meaning that the set of them admits full Lebesgue measure or full probability measure, as previously mentioned. This weighted quasi-Monte Carlo method is \\textit{not} only effective for quasi-periodic, almost periodic, and periodic cases, but also yields similar results for general decaying waves \\cite[Theorems 1.1 and 1.2]{TLdecaying}. As a corollary, when the observational data can be decomposed into a linear combination of quasi-periodic, almost periodic, and periodic components, as well as decaying waves, this method also exhibits excellent acceleration effects. Here, we also mention that the ``super-analyticity'' imposed for the quasi-periodic case in (II)--(6) of Theorem \\ref{QMC} is not as strong as it might appear at first glance (of course, the corresponding ``super-analyticity'' in the finite-dimensional case is indeed strong). This is because, unlike the finite-dimensional case, the infinite-dimensional case requires consideration of the cardinality estimates brought by the $\\mathbb{Z}_*^{\\infty}$ lattice, which inevitably leads to stronger decay of individual Fourier coefficients.  To facilitate the reader's understanding of Theorem \\ref{QMC}, we present the numerical simulations in Figure \\ref{FIGWBA5}. For convenience, we denote the actual error in the $\\ell^{\\infty}$-norm as ${\\rm{Error}}^{p,q}_N$.  We consider a quasi-periodic orbit generated by $f(x) = \\sin(2\\pi x) + \\cos(10\\pi x)$, with an initial value $ \\theta=0 $ and a rotation number  $\\rho = 1/2\\pi$, which is universal (i.e., not specifically constructed to achieve the ``best'' convergence). The choice of $f$ as a finite-order trigonometric polynomial is motivated by practical considerations: in numerical simulations, we cannot truly achieve infinity, and extremely high orders would lead to inaccurate computations.  According to Theorem \\ref{QMC}, specifically the part (I)--(4), the theoretical error is $\\mathcal{O}\\left(\\exp(-cN^{\\zeta_2})\\right)$, where $ {\\zeta _2} = {\\left( {1 + 1/\\min \\left\\{ {p,q} \\right\\}} \\right)^{ - 1}} $ and $c>0$ is a universal constant that may depend on $p$ and $q$. To visualize the results more intuitively, we take the \\textit{logarithm} of both the actual error and the theoretical error (and thus the theoretical error becomes $\\mathcal{O}\\left(-N^{\\zeta_2}\\right)$, where $ {\\zeta _2} = {\\left( {1 + 1/\\min \\left\\{ {p,q} \\right\\}} \\right)^{ - 1}} $). This transformation results in curves that are more curved than $\\mathcal{O}^{\\#}(-N)$. We present four cases: $p = q = 1/2$, $p = q = 1$, $p = 1$ and $q = 2$, and $p = q = 2$. It can be  observed from Figure \\ref{FIGWBA5} that as $\\min\\{p, q\\}$ increases, the exponential convergence rate of the actual error indeed accelerates as we proved, closely matching the changes in the order of the theoretical error (although we do not estimate the control constant $ c>0 $ as in \\cite{TLdecaying} for general decaying waves, which would have provided a more precise result). Notably, for the cases $p = q = 1$ and $p = 1$, $q = 2$, the actual errors are \\textit{almost identical}, aligning well with our theoretical conclusions, as $\\min\\{1, 1\\}=\\min\\{1, 2\\}=1$ at this point.  In addition to the results in the general sense mentioned above, \\cite[Theorems 2.1, 2.2, 3.1 and 3.2]{MR4768308} and \\cite[Theorems 3.1--3.4]{TLmultiple} also investigate results based on Liouville rotations. Specifically, for any Liouville type rotation, as long as the observable is sufficiently smooth, the weighted Birkhoff average can converge at an exponential rate. This is quite different from the phenomena observed in classical dynamical systems: Firstly, many dynamical systems essentially impose restrictions on the irrationality of rotations, such as the Bruno condition, and \\textit{do not} allow arbitrary Liouville rotations. Secondly, even for the specific case of irrational translations, without weighting, such results are absolutely impossible according to the co-homological equation argument. However, when the regularity of the observable is insufficient, such weak nonresonance will reduce the convergence rate in the weighted case, which is entirely reasonable from the perspective of dynamical systems.",
        "summarize_figure": "2505.03210v1_PIC5",
        "summarization": "The chart shows the exponential convergence of the weighted quasi - Monte Carlo method in different settings. The horizontal axis is N, and the vertical axis is the actual and theoretical errors in logarithmic form. Among the oscillating curves, red corresponds to p = q = 1/2, pink to p = q = 1, blue to p = 1, q = 2, and green to p = q = 2. As min {p, q} increases, the exponential convergence rate of the actual error accelerates, consistent with the change in the order of the theoretical error. For example, when p = q = 1 and p = 1, q = 2, the actual errors are almost identical, in line with the theoretical conclusion."
    },
    "219": {
        "figure1": "2505.03211v1_geodesics2",
        "label1": "fig:cyclic",
        "caption1": "The events $\\Omega $ and $\\Sigma$. When $x\\ge k^{\\delta }$, the event $\\Omega $ depicted on the left, forces the geodesic $\\tilde{\\gamma}_u$ (in blue) to intersect $\\mathcal C_x$ only inside $B(u,k^\\delta )$. When $x< k^{\\delta }$, the event $\\Sigma $ on the right, forces the geodesic $\\tilde{\\gamma}_u$ to intersect $\\mathcal C_x$ only inside $B(u,2h)$ with $h=\\Theta (k^{\\delta })$.",
        "text": "We start with the case $x\\ge k^{\\delta }$, so that $B(u,k^{\\delta }) \\subseteq R$. Consider the vertices         v^-=v^-(u):=(x+y,0) \\quad \\text{and}\\quad v^+=v^+(u):=(x+k-y,k).     These are the two points on the boundary of $R$ that are at $-45$ and $45$ degrees respectively from $u$. Define the event     In words, $\\Omega ^-$ is the event that the geodesic $\\gamma (u,v^-)$ is contained inside the open square with opposite corners $u$ and $v^-$ except for short parts close to its endpoints (see Figure~\\ref{fig:cyclic}). Symmetrically, we define the event  and let $\\Omega :=\\Omega ^+ \\cap \\Omega ^-$. By Corollary~\\ref{cor:controlgeodesic} we have that $\\mathbb P (\\Omega )\\ge 1-C\\exp (-k^{c_\\epsilon })$.\nThe last proof fails when $x<k^{\\delta }$ as the geodesics $\\gamma (u,w^{\\pm })$ may turn backward from $u$ for a short distance and exit $R$ (so that $\\tilde{\\gamma }(u,w^\\pm )\\neq \\gamma (u,w^\\pm )$). To handle this case, we define slightly different events. Fix $M$ sufficiently large depending only on the edge distribution, let $h:=\\lceil Mk^{\\delta } \\rceil <k^{\\epsilon }/2$ and consider the vertices     u^-:=(0,y-h ), \\quad  u^+:=(0,y+h ), \\quad      v^-:=(y-h ,0), \\quad  v^+:=(k-y-h,k). Define the events See Figure~\\ref{fig:cyclic}.  Next, note that when $M$ is sufficiently large depending on the angle $\\theta <\\pi /2$ from Proposition~\\ref{prop:limit}, any point $x_0$ in the set $([0,k^\\delta]\\times [0,k])\\setminus B(u^\\pm,h)$, satisfies  $x_0=u^\\pm+Re^{i\\varphi}$ with $|\\varphi|\\ge \\theta$ and $R\\ge h$. Thus, for such $M$, as in Corollary~\\ref{cor:controlgeodesic}, it follows from Proposition~\\ref{prop:limit} that $\\mathbb P (\\Sigma )\\ge 1-C\\exp (-k^{c_\\epsilon })$.",
        "summarize_figure": "2505.03211v1_geodesics2",
        "summarization": "The figure shows two diagrams. The left - hand diagram corresponds to the event Ω. When x≥k^δ, the blue geodesic γ~u intersects Cx only inside the square region B(u,k^δ ) centered at u with radius k^δ. The right - hand diagram corresponds to the event Σ. When x<k^δ, the geodesic γ~u intersects Cx only inside the region B(u,2h) (where h = Θ(k^δ ) ) centered at u. The figure also marks several vertices such as v^-, v^+, u^-, u^+ etc., and some curves connecting these vertices to illustrate the intersection of geodesics."
    },
    "220": {
        "figure1": "2505.03212v1_Example",
        "label1": "fig:exmaple",
        "caption1": "Example of Label Containing Nearest Neighbor Search}  \\begin{exmp} Fig.~\\ref{fig:exmaple} illustrates the containing label search scenario in online shopping. Customers may search for an item with a given photo and an extra label requirement. In detail, the $x_1,...,x_4$ is the image embedding vector of items, and $q$ is the query vector from the photo of customers. The customer requires the most similar item of $q$ with a specific brand and time. Then, the $x_1$ and $x_3$ will be filtered out, and $x_4$ will be the nearest neighbor of $q$. \\end{exmp}\\vgap\\vgap",
        "text": "The $k$ nearest neighbor (KNN) search over high-dimensional vectors has become a fundamental operator in modern data systems, powering applications including recommendation systems~\\cite{CF-2007-recommender-sys}, data mining~\\cite{NN-datamining-1967-TIT}, face recognition~\\cite{AnalyticDB-VLDB-2020}, product search~\\cite{AnalyticDB-VLDB-2020}, and retrieval-argument generation (RAG) for large language models~\\cite{LLM-RAG-NIPS-2020}.  In production environments, vector embeddings are usually accompanied by structured attributes (e.g., product categories, geolocations). For instance, the customer may search for items similar to a photo and specify the brand and year in the e-commerce scenario. As illustrated in Fig.~\\ref{fig:exmaple}, the item search can be achieved by vector nearest neighbor search with label-specific. However, the exact KNN search in high-dimensional space suffers from \\textit{curse-of-dimensionality}~\\cite{Curse-of-dim-1998}, where algorithms for exact solutions have extremely high computational cost in high dimensions. Therefore, researchers turn to studying the approximate $k$ nearest neighbor (AKNN) search, which can greatly improve efficiency by trading search accuracy. Consequently, approximate $k$ nearest neighbor search with specific keywords and attributes has attracted extensive attention recently~\\cite{UNG-SIGMOD-2025,Acorn-SIGMOD-2024}. Specifically, the entries in the database $S$ consist of two parts: the vector embedding part and the label set $L$. Given a query vector $q$ and a query label set $L_q$, the problem is searching the approximate nearest neighbor of $q$ in $S$ that the vector label set contains the query label set, where $L_q\\subseteq L$. This problem involves a hybrid search combining label containment and vector similarity, aiming to achieve a better trade-off between accuracy and efficiency.\nFig.~\\ref{fig:exmaple} illustrates the containing label search scenario in online shopping. Customers may search for an item with a given photo and an extra label requirement. In detail, the $x_1,...,x_4$ is the image embedding vector of items, and $q$ is the query vector from the photo of customers. The customer requires the most similar item of $q$ with a specific brand and time. Then, the $x_1$ and $x_3$ will be filtered out, and $x_4$ will be the nearest neighbor of $q$.",
        "summarize_figure": "2505.03212v1_Example",
        "summarization": "The figure demonstrates the label - containing nearest - neighbor search scenario in online shopping. There are four items (Item #1 - Item #4), each with details of brand, size, color, and year, and corresponding image embedding vectors x1 - x4. The customer provides a query vector q (from a photo) and sets filter conditions \"brand=A\" and \"year=2025\". After filtering, x1 and x3 are excluded, and x4 becomes the nearest neighbor vector most similar to q."
    },
    "221": {
        "figure1": "2505.03212v1_Motivate-Example",
        "label1": "fig:motivate-exmaple",
        "caption1": "The Motivate Example}  \\begin{exmp} In Fig.~\\ref{fig:motivate-exmaple}, we consider a vector dataset with labels A, B, and C. The number of vectors in label $L$ is denoted as \\{$L$\\}[\\textit{number of vectors}]. Each label group is connected to its minimum superset by an arrow. As shown in Fig.~\\ref{fig:motivate-exmaple}(a), there are 400 vectors with label A only, marked as A[400]. For a hybrid AKNN query that needs to contain label A, an index with data in label group \\{A,AB,AC,ABC\\} is built, a total of 1000 vectors as illustrated in Fig.~\\ref{fig:motivate-exmaple}(b). For all possible query label sets, all vector groups in Fig~\\ref{fig:motivate-exmaple}(c) need to be indexed, requiring a total of 5400 entries, which is 2.75x of the dataset cardinality. \\end{exmp}\\vgap\\vgap",
        "text": "In Fig.~\\ref{fig:motivate-exmaple}, we consider a vector dataset with labels A, B, and C. The number of vectors in label $L$ is denoted as \\{$L$\\}[\\textit{number of vectors}]. Each label group is connected to its minimum superset by an arrow. As shown in Fig.~\\ref{fig:motivate-exmaple}(a), there are 400 vectors with label A only, marked as A[400]. For a hybrid AKNN query that needs to contain label A, an index with data in label group \\{A,AB,AC,ABC\\} is built, a total of 1000 vectors as illustrated in Fig.~\\ref{fig:motivate-exmaple}(b). For all possible query label sets, all vector groups in Fig~\\ref{fig:motivate-exmaple}(c) need to be indexed, requiring a total of 5400 entries, which is 2.75x of the dataset cardinality.\nAlthough the set of all possible labels may be large, the label set of a single entry may be sparse in practice because some attribute values are orthogonal to others. For example, the set of all possible product brands may be large, but a single product can only have one brand. Meanwhile, insignificant keywords can be integrated into the vector as features to avoid a single entry having a large number of labels. Even if the label set $L$ of a single entry is not large, the brute-force approach still requires indexing $2^{|L|}$ entries. As shown in Fig.~\\ref{fig:motivate-exmaple}, for an entry with a label set of \\{ABC\\}, it needs to be inserted into the 8 groups \\{$\\emptyset$, A, B, C, AB, AC, BC, ABC\\}, corresponding to the 8 possible label containing query label sets, where $\\emptyset$ is the case without considering the label. When the average label set size is 6-10, the entries that need to be indexed are 64x-1024x of the original, resulting in extremely high construction time and space.\nIn this paper, we consider constructing indices with near-optimal search performance with limited space and time. Instead of building indexes for all possible label combinations as the brute-force approach, we selectively build partial indexes and exploit the covering relationship of sets to make the corresponding hybrid queries share the index. We use the \\textit{elastic factor} to model the overlap coverage ratio of entries between label sets, which can also serve as a performance indicator. A higher \\textit{elastic factor} means more search efficiency in both theory and practice. For instance, in Fig~\\ref{fig:motivate-exmaple} right, the entries in group \\{A\\} contain group \\{AB\\} data with an overlap ratio of 0.5. Using the index built by entries in group \\{A\\} can efficiently answer \\{AB\\}-containing AKNN queries with the filter search strategy. The data entries in group \\{B\\} also cover group \\{AB\\} and have a higher coverage ratio/elastic factor. Using the index of the group \\{B\\} to perform a filter search to answer \\{AB\\}-containing queries is more efficient. Therefore, we use the greedy algorithm to select some indices to meet the search performance requirements. In addition, we use the monotonicity of the selection strategy to achieve the optimal index configuration under limited space and time. Compared with the current methods in Table~\\ref{tab:features}, our approach has higher flexibility, theoretical performance guarantees, better practical search efficiency, and the ability to trade space for near-optimal search performance.",
        "summarize_figure": "2505.03212v1_Motivate-Example",
        "summarization": "The figure shows a vector dataset with labels A, B, C. Sub - figure (a) details the number of vectors for single labels (A[400], B[200], C[200]) and combinations (AB[400], AC[100], BC[200], ABC[100]). Arrows connect label groups to their minimum supersets, indicating label containment. For a hybrid AKNN query with label A, an index of label group {A, AB, AC, ABC} with 1000 vectors is built as in (b). Indexing all vector groups in (c) for all query label sets needs 5400 entries, 2.75x the dataset size. The paper aims to build near - optimal search performance indices with limited resources by selectively building partial indexes, using the elastic factor for overlap coverage and a greedy algorithm for index selection, achieving better flexibility and efficiency than current methods."
    },
    "222": {
        "figure1": "2505.03212v1_graph-search",
        "label1": "fig:graph-search",
        "caption1": "The Graph Search Example}  \\vgap\\vgap\\vgap",
        "text": "The $\\PRE$ and $\\POST$ approaches are basic strategies that utilize graph indexes for a filtered search. The advantage lies in the unchanged index structure, which only needs specific filtering conditions during the graph traverse. The $\\PRE$ strategy removes the filtered node and its neighbor during graph search. As illustrated in Fig.~\\ref{fig:graph-search}, the nodes $x_4$ and $x_5$ are filtered out with two specific queries. When searching for query~1, the $\\PRE$ strategy removes the outgoing edges of $x_5$, making the nearest neighbor $x_6$ unreachable from the entry node $x_1$. The $\\POST$ strategy retains the filtered node information for routing without recording it in the result set. When searching for query~2 in Fig.~\\ref{fig:graph-search}, the $\\POST$ algorithm visit node $x_1,x_2,x_3,x_4,x_7$ at each search step. The outgoing edge of $x_4$ is still used for routing, and finally, the filtered nearest neighbor $x_7$ is searched. Since the filtered-out nodes will not be retained in the result set, the final result returned is $x_7$, the nearest neighbor in the filtered nodes. For the top $k$ filtered search, the $\\POST$ method only needs to keep searching the $k$+1 nearest neighbors and accumulate $k$ unfiltered results, but the time complexity is affected by the selectivity of the query. If most of the points are filtered out, it may visit near $O(N)$ points to accumulate $k$ results. The $\\PRE$ method encounters the same challenge. With a small selectivity, it is difficult to ensure that the nearest neighbor can be reached from the entry node, making its correctness and time complexity difficult to analyze.",
        "summarize_figure": "2505.03212v1_graph-search",
        "summarization": "The figure is a graph search example, showing two basic strategies, PRE and POST, for filtered search using graph indexes. Node x1 is the entry node, and x4 and x5 are filtered nodes. For Query 1, the PRE strategy removes the filtered node x5 and its outgoing edges during graph search, making the nearest neighbor x6 unreachable from x1. For Query 2, the POST strategy retains information of the filtered node x4 for routing. The algorithm visits nodes x1, x2, x3, x4, x7 in sequence and finally searches and returns the filtered nearest neighbor x7."
    },
    "223": {
        "figure1": "2505.03213v1_error_analysis_grid_loglog",
        "label1": "fig:loglog",
        "caption1": " Log-log plot of the final errors $\\sqrt{\\mathcal{L}_{\\mathrm{e, valid}}}$ (top) and $\\sqrt{\\mathcal{L}_{\\mathrm{f, valid}}}$ (bottom) against the number of data points $N_{\\mathrm{data}}$ on the $x$-axis for H$_2$O, CH$_3$O, CH$_3$CHO, and ethanol from left to right. Cold colours (solid lines) correspond to the \\emph{Energy only} training, while warm colours (dotted lines) indicate the \\emph{Energy \\& Force} training. Crosses represent the initial QNN with $N_{\\mathrm{a}}-1$ qubits, triangles the initial QNN with $2(N_{\\mathrm{a}}-1)$ qubits, and circles the revised QNN.",
        "text": "We train on datasets of varying sizes, selecting $N_{\\mathrm{data}} \\in \\{60, 100, 200, 400, 600, 800, 1000\\}$ random samples from the $100,000$-step MD trajectory. Training samples are drawn by randomly selecting indices from the predefined $1000$ point training splits provided in the rMD17 dataset. For validation, we evaluate errors on a fixed set of $1000$ validation points, independent of $N_{\\mathrm{data}}$, using the corresponding validation splits. We denote the mean squared errors (MSE) on the validation energy and validation force in the final epoch as $\\mathcal{L}_{\\mathrm{e, valid}}$ and $\\mathcal{L}_{\\mathrm{f, valid}}$, respectively. For the \\emph{Energy \\& Force} case, we include force and energy labels only for $N_{\\mathrm{data}}/4$ configurations, ensuring a total of $N_{\\mathrm{data}}$ labels in the training set. From experience with classical machine learned force fields, it is expected that the prediction accuracy on both energy and force labels is higher (and thus the loss is lower) for the training~\\cite{christensen2020RoleGradientsMachinea}.      $1-\\mathcal{L}_{\\mathrm{e, valid}}/\\sigma^2_{\\mathrm{E}}$     and $1-\\mathcal{L}_{\\mathrm{f, valid}}/\\sigma^2_{\\mathrm{F}}$, where     $\\sigma_{\\mathrm{E}}$ and $\\sigma_{\\mathrm{F}}$ denote the standard deviation     of the energy and force data points}. The black horizontal     line marks zero, and symbols, colors, and line styles follow \\cref{fig:loglog}.\nThe final validation errors are presented in \\cref{fig:loglog}. Energy prediction errors (top row) show only a slight improvement with increasing data points, remaining within a narrow range of $50$ meV. This is in stark contrast to classical models, which exhibit an improvement of orders of magnitude with more training data~\\cite{christensen2020RoleGradientsMachinea}. Furthermore, energy errors increase with molecular complexity, with ethanol exhibiting the highest errors.",
        "summarize_figure": "2505.03213v1_error_analysis_grid_loglog",
        "summarization": "This log - log plot shows the final errors (Le,valid​​ at top and Lf,valid​​ at bottom) against the number of data points for H₂O, CH₃O, CH₃CHO, and ethanol. Cold - colored solid lines are for Energy only training, warm - colored dotted lines for Energy & Force training. Crosses are initial QNN with Na​−1 qubits, triangles with 2(Na​−1) qubits, and circles the revised QNN. Energy prediction errors slightly improve with more data points, remaining around 50 meV, different from classical models. They increase with molecular complexity, highest for ethanol. Force prediction errors vary by system and training method."
    },
    "224": {
        "figure1": "2505.03213v1_error_analysis_grid",
        "label1": "fig:minus_std",
        "caption1": " Plot of the coefficients of determination $R^2$ given by for the energy and force data points respectively by $1-\\mathcal{L}_{\\mathrm{e, valid}}/\\sigma^2_{\\mathrm{E}}$ and $1-\\mathcal{L}_{\\mathrm{f, valid}}/\\sigma^2_{\\mathrm{F}}$, where $\\sigma_{\\mathrm{E}}$ and $\\sigma_{\\mathrm{F}}$ denote the standard deviation of the energy and force data points}. The black horizontal line marks zero, and symbols, colors, and line styles follow \\cref{fig:loglog}.",
        "text": "Note that for CH$_3$CHO and ethanol, the initial QNNs with \\emph{Energy only} training shows the lowest training-validation difference. However, these models do not exhibit meaningful learning (see~\\cref{fig:minus_std}). Among models that do exhibit meaningful learning, the revised QNN remains the best performer.",
        "summarize_figure": "2505.03213v1_error_analysis_grid",
        "summarization": "This plot shows the coefficients of determination R2 for energy and force data points of H₂O, CH₃O, CH₃CHO, and ethanol. Energy R2 is 1−Le,valid​/σE2​, and force R2 is 1−Lf,valid​/σF2​. The black horizontal line is zero. Different symbols, colors, and line styles represent training methods and qubit configurations. For CH₃CHO and ethanol, initial QNNs with Energy only training have the lowest training - validation difference but no meaningful learning. Among models with meaningful learning, the revised QNN performs best."
    },
    "225": {
        "figure1": "2505.03221v1_plot2ma",
        "label1": "fig:Figure3",
        "caption1": "The plot of the integrand, $\\frac{\\sin\\sqrt{\\tau^2 x^2 + \\sigma^2}}{\\sqrt{\\tau^2 x^2 + \\sigma^2}} \\, \\frac{\\sin^{4} (\\lambda x)}{x^{4}}$ for $\\lambda =2,3,4$.",
        "text": "In equation \\eqref{eqn:example1result}, we have obtained positive orders of $\\lambda$ as $\\lambda \\to \\infty$. Further, the behavior of the integrand is illustrated in Figure \\ref{fig:Figure3}. We observe that the area under the curve of the integrand increases as $\\lambda$ increases. This shows consistency with the polynomial behavior of the obtained expansion which behaves as a polynomial with positive powers of $\\lambda$ as $\\lambda$ grows large.",
        "summarize_figure": "2505.03221v1_plot2ma",
        "summarization": "This figure shows the plot of the integrand (sin(√(τ²x² + σ²)) / √(τ²x² + σ²)) × (sin⁴(λx) / x⁴) for λ = 2, 3, 4. As x varies from 0 to 1, the curves for different λ values have distinct shapes. As λ increases from 2 to 4, the area under the curves grows larger. This is consistent with the polynomial behavior of the obtained expansion, which shows positive powers of λ as λ approaches infinity."
    },
    "226": {
        "figure1": "2505.03221v1_plot2m1a",
        "label1": "fig:Figure4",
        "caption1": "The plot of the integrand, $\\frac{\\sin\\sqrt{\\tau^2 x^2 + \\sigma^2}}{\\sqrt{\\tau^2 x^2 + \\sigma^2}} \\, \\frac{\\sin^{3} (\\lambda x)}{x^{3}}$ for $\\lambda =2,3,4$.",
        "text": "A plot of the integrand of equation \\eqref{eqn:ExampleSin3} is also illustrated in Figure \\ref{fig:Figure4}. We see there that the net area under the curve of the integral in \\eqref{eqn:ExampleSin3} increases for increasing $\\lambda$ which is consistent with the polynomial behavior of the obtained expansion for positive powers of $\\lambda$ as $\\lambda\\to\\infty$. This is contrary to the usual behavior of asymptotic expansions given by the Poincar{\\'e} asymptotic expansion which is in inverse powers of $\\lambda$ for arbitrary large $\\lambda$.",
        "summarize_figure": "2505.03221v1_plot2m1a",
        "summarization": "This figure shows the plot of the integrand (sin(√(τ²x² + σ²)) / √(τ²x² + σ²)) × (sin³(λx) / x³) for λ = 2, 3, 4. With x ranging from 0 to 1, the curves for different λ values have different shapes. As λ increases from 2 to 4, the net area under the curves grows larger. This is consistent with the polynomial behavior of the obtained expansion with positive powers of λ as λ approaches infinity, and is different from the usual asymptotic behavior of the Poincaré asymptotic expansion with inverse powers of λ."
    },
    "227": {
        "figure1": "2505.03222v1_ncvx_1d_7_1",
        "label1": "fig:1d1",
        "caption1": "$n=112, k=2$} \\end{subfigure} \\begin{subfigure}{0.48\\textwidth} \\centering \\includegraphics[width=.8\\textwidth]{fig/ncvx_1d_7_1.eps} \\caption{$n=7, k=1$} \\end{subfigure} \\caption{ \\small The MSE and N-CP versus iterations of the GD, GND, and DL-GND algorithms are evaluated by minimizing $f_{7, 1}(x)$ and $f_{112, 2}(x)$ given by \\Cref{obj:nearly_convex_repeat}. ",
        "text": "The numerical behaviors of GND and DL-GND are given in \\Cref{fig:1d1} in terms of the MSE and the N-CP. Both GND and DL-GND (\\Cref{alg:gngd,alg:dlgnd}) algorithms are observed to converge linearly to the global optimal point in both state space and probability space, while most trajectories of GD fail to converge and get trapped in local minima. Due to the additional estimation of the lower bound of the objective function value, DL-GND requires more iterations than GND, but achieves the similar accuracy.",
        "summarize_figure": "2505.03222v1_ncvx_1d_7_1",
        "summarization": "This figure shows the MSE and N - CP of GD, GND, and DL - GND algorithms versus iterations when minimizing f7,1​(x) and f112,2​(x). GND and DL - GND converge linearly to the global optimal point in state and probability spaces, while most GD trajectories get stuck in local minima. DL - GND, due to lower - bound estimation of the objective function, takes more iterations than GND but attains similar accuracy."
    },
    "228": {
        "figure1": "2505.03222v1_rast2d_default_c_01",
        "label1": "fig:2d.default",
        "caption1": "$c=0.05, d=2$} \\end{subfigure} \\begin{subfigure}{0.48\\textwidth} \\centering \\includegraphics[width=.8\\textwidth]{fig/rast2d_default_c_01.eps} \\caption{$c=0.01, d=2$} \\end{subfigure} \\caption{The convergence behavior for minimizing the two dimensional Rastrigin function \\eqref{obj:rastrigin} with $c = 0.05$ and $0.01$. The first and the second rows are the MSE and N-CP versus iterations, respectively.",
        "text": "The numerical results of GND and DL-GND, shown in \\Cref{fig:2d.default}, are evaluated based on the metrics MSE and N-CP. The results show that both GND and DL-GND achieve linear convergence to the global minimum for $c = 0.05$ and $c = 0.01$, in both the state space and the probability space. However, the DL-GND algorithm converges 4–8 times more slowly than the GND algorithm for both objectives. This slower convergence is considered acceptable, given that the initial gap $f^* - f_{\\lb}^0 = 20$ in DL-GND is significantly larger compared to the zero gap in GND.",
        "summarize_figure": "2505.03222v1_rast2d_default_c_01",
        "summarization": "This figure shows the convergence behavior of GND and DL - GND algorithms when minimizing the two - dimensional Rastrigin function. The x - axis is the number of iterations, and the y - axis shows the MSE and N - CP. For both c = 0.05 and c = 0.01, GND and DL - GND achieve linear convergence to the global minimum in both state and probability spaces. However, the DL - GND algorithm converges 4–8 times slower than the GND algorithm. This slower convergence is deemed acceptable considering the significantly larger initial gap (f* - flb^0 = 20) in DL - GND compared to the zero gap in GND."
    },
    "229": {
        "figure1": "2505.03222v1_rast2d_eta_s",
        "label1": "fig:2d1",
        "caption1": "Minimizing the two dimensional Rastrigin function \\eqref{obj:rastrigin} with $c = 0.01$, using the GND algorithms. We fix $f_{\\lb}=f^*=0$ and vary the two parameters $\\eta$ and $s$. ",
        "text": "In \\Cref{fig:2d1}, the GND algorithm is applied with the lower bound fixed at $f_{\\lb} = f^* = 0$, while varying the values of $\\eta$ and $s$. The hyperparameters are selected from the set     (\\eta, s) \\in \\{0.5, 1.0, 1.5\\} \\times \\{1.0, 2.5, 5.0\\}. The estimated MSE and N-CP for each combination of $(\\eta, s)$ are summarized in \\Cref{fig:2d1}.  The results indicate that GND converges linearly to the global minimum for all choices of $\\eta$ when $s \\ge 2.5$. However, when the noise coefficient $s$ is too small, the algorithm fails to converge to the global minimum, suggesting that insufficient noise makes it harder for the trajectory to escape local minima. When $s$ is chosen suitably large, the choice of $\\eta$ and $s$ affects the convergence speed. Larger $\\eta$ and $s$ appear to converge faster, but the converging trajectories become noisier. To achieve satisfactory performance, $\\eta$ and $s$ should not be too large.",
        "summarize_figure": "2505.03222v1_rast2d_eta_s",
        "summarization": "This figure shows the GND algorithm applied to minimize the two - dimensional Rastrigin function (c = 0.01), with flb = f* = 0 fixed and parameters η and s varied. The x - axis is the number of iterations, and the y - axis shows the MSE and N - CP. Results indicate that when s ≥ 2.5, GND converges linearly to the global minimum for all η values. When the noise coefficient s is too small, the algorithm fails to converge to the global minimum, suggesting insufficient noise hinders escaping local minima. When s is large enough, the choice of η and s affects the convergence speed. Larger η and s lead to faster convergence but noisier trajectories, so η and s should not be too large for satisfactory performance."
    },
    "230": {
        "figure1": "2505.03228v1_overview",
        "label1": "fig:overview",
        "caption1": "Overview of the proposed MGFF-TDNN architecture.",
        "text": "In this paper, we propose the MGFF-TDNN model, an enhanced TDNN-based architecture focused on multi-granularity context modeling, as illustrated in Fig.~\\ref{fig:overview}. Firstly, to enhance the model's feature modeling in the frequency domain, a two-dimensional depthwise separable convolution module \\cite{Sandler2018mobilenetv2} is employed as the front-end extractor to capture time-frequency domain features. Secondly, inspired by \\cite{chen23eresnet, tan2022ponet}, we devise a Multi-granularity Temporal Delay Neural Network (M-TDNN) module. Within M-TDNN, a phoneme-level local max-pooling module is integrated to enhance fine-grained feature modeling. Additionally, the dilation factor of the TDNN module is progressively increased to extend the temporal context for feature modeling. Subsequently, the outputs of these two components are concatenated and fed into an Squeeze-Excitation (SE) block \\cite{Hu_2018se} to capture internal dependencies among multi-granularity features. MGFF-TDNN is validated on the open-source VoxCeleb \\cite{Nagrani17voxceleb,Chung18voxceleb2} dataset, and the experimental results demonstrate that the MGFF-TDNN architecture achieves competitive performance with fewer parameters and computational resources.\nThe overall architecture of the proposed MGFF-TDNN model is depicted in Fig.~\\ref{fig:overview}. The architecture is primarily composed of two main blocks: the Depthwise Separable Module (DSM) and the Multi-Granularity Temporal Delay Neural Network Module (M-TDNN). Within the DSM, two-dimensional depth-wise separable residual network layers are introduced for the initial modeling of time-frequency features by extending the channel dimension. Subsequently, the generated feature maps are flattened along the channel and frequency dimensions and input into multiple M-TDNNBlocks, each of which contains a different number of M-TDNN layers. Each M-TDNN layer extracts features of varying granularities, which are then fused through the SE module to obtain attention weights for different granularity features.\nAs shown in Fig.~\\ref{fig:overview}, we denote the input of a layer in the M-TDNN Block as Y. Initially, Y undergoes one-dimensional convolution for preliminary feature extraction, resulting in the feature e:   e = ReLU( BN( W_{1} \\cdot Y ) ) Subsequently, the feature e proceeds into two feature extraction branches. The left TDNN layer extracts dynamic contextual features $e_t$ by controlling the dilation factors, while the right phoneme-level pooling (PLP) extracts fine-grained features $e_p$ through overlapping sliding time windows.     e_{t} &= \\tau(e) \\\\     e_{p} &= plp(e) Where $\\tau(\\cdot)$ represents the feature extraction process of the TDNN layer, and $plp(\\cdot)$ represents phoneme-level pooling. Phoneme-Level Pooling (PLP) is implemented using standard max-pooling over sliding windows to capture fine-grained features. To reduce information loss, the PLP window slides with a 50\\% overlap, and a sliding window size of 8 is utilized in this study. To ensure that the output tensor from PLP is aligned with the input tensor along the time dimension, we replicate the pooled features of the window across the time axis. This preparation facilitates subsequent feature fusion. We control the output dimensions of both TDNN and PLP, concatenate their outputs along the time axis, and then process them through the Squeeze-Excitation (SE) block to obtain $e_c$.     s = \\sigma( W_{2}\\delta( W_{1} \\cdot g \\left \\lbrack {e_{t},e_{p}} \\right\\rbrack + b_{1} ) + b_{2} ) \\\\    e_{c} = \\left\\lbrack {e_{t},e_{p}} \\right\\rbrack \\cdot s where $\\sigma(\\cdot)$ represents the sigmoid function, $\\delta(\\cdot)$ denotes the rectified linear activation function,  $[\\cdot]$ signifies the feature concatenation process, and g refers to global pooling, which extracts global representations. Lastly, the weighted multi-granularity feature $e_c$ undergoes further feature fusion through one-dimensional convolution for enhanced feature extraction and is then residual-connected with the input $Y$, ultimately yielding the feature $e_o$.     e_{o} = ReLU( {Y + ReLU( {BN( {W_{1} \\cdot e_{c}} )} )} ) The output $e_o$ serves as the output of the MTDNN layer, integrating features of different granularities. The detailed configurations are illustrated in Table \\ref{tab:architecture}, where each MTDNN module consists of varying numbers of MTDNN layers. The three MTDNN modules include [3, 6, 4] MTDNN layers with output channels of 128, 256, and 512 respectively. The MTDNN modules in Table \\ref{tab:architecture} outline the variation units within the MTDNN layers. To control the overall model parameters and computational complexity, we first employ one-dimensional convolution layers with a context of one frame to reduce the feature dimensions. This is followed by multi-granularity feature extraction. Subsequently, one-dimensional convolution is used for feature scaling and fusion, with the entire unit incorporating skip connections to enhance information flow.",
        "summarize_figure": "2505.03228v1_overview",
        "summarization": "The picture shows an overview of the proposed MGFF - TDNN architecture. It mainly consists of the Depthwise Separable Module (DSM) and the Multi - Granularity Temporal Delay Neural Network Module (M - TDNN). In DSM, two - dimensional depth - wise separable residual network layers initially model time - frequency features by expanding the channel dimension. The generated feature maps are flattened and fed into multiple M - TDNN blocks. In an M - TDNN layer, the input first goes through 1 - D convolution for preliminary feature extraction, then into two branches. The TDNN layer and phoneme - level pooling (PLP) extract dynamic contextual and fine - grained features respectively. Their outputs are concatenated, processed by the Squeeze - Excitation (SE) block, further fused by 1 - D convolution and connected with the input to integrate features of different granularities."
    },
    "231": {
        "figure1": "2505.03228v1_dwresnet",
        "label1": "fig:dwresnet",
        "caption1": "Illustration of depthwise separable residual block.",
        "text": "The vanilla time-delay neural network perform convolution operations along the temporal axis to capture time-domain features in the sequence. Although the convolutional kernels fully cover the frequency domain, a large number of filter banks are required to effectively model the frequency-domain features, which inadvertently increases the model's parameter count. To efficiently model both temporal and frequency-domain features, a two-dimensional convolution module is incorporated at the frontend. Inspired by \\cite{Sandler2018mobilenetv2, 2022mfa}, we employ depthwise separable module in this paper, which comprises multiple layers of separable residual structures. Each layer's microstructure is referred to as an Inverted Residual Block \\cite{Sandler2018mobilenetv2}. This structure was initially used in the field of computer vision, utilizing Depth-wise convolution paired with Point-wise convolution to extract features. This approach reduces the time and space complexity of convolutional layers while enhancing the network's representational capacity with minimal impact on performance. As illustrated in Fig.~\\ref{fig:dwresnet}, assuming the input is x, the output y is obtained after three convolutional operations and residual connections. This process can be formally described by the following equation: y = ReLU(x + BN( W_{3} \\cdot ReLU( BN( W_{2} \\cdot ReLU \\\\ (BN ( W_{1} \\cdot x ) ) ) ) ) ) where $W_{1}$ and $W_3$ are the weights of the point-wise convolutions with output channels being $C*t$ and $C$ respectively, where t is the expansion factor typically set to $5 \\sim 10$ (we set it to 6 in this work). $W_2$ is the weight of the depth-wise convolution, and its output channels are also $C*t$. $BN$ refers to batch normalization \\cite{ioffe2015batch}, and $ReLU(\\cdot)$ denotes rectified linear unit activation function. This module first employs point-wise convolution to increase the dimensionality of channels, then utilizes depth-wise convolution for feature extraction, and finally employs point-wise convolution for dimensionality reduction. This approach enables high-dimensional feature extraction, facilitating a comprehensive capture of the time-frequency domain characteristics in speech signals, while also strengthening the local modeling of acoustic features. Subsequently, the secondary point-wise convolution serves for dimensionality reduction, ensuring the maintenance of parameters and computational complexity within suitable bounds. Specific details are shown in Table \\ref{tab:architecture}.",
        "summarize_figure": "2505.03228v1_dwresnet",
        "summarization": "The picture illustrates the structure of the depthwise separable residual block. The input data with in_channels number of channels first goes through a 1x1 convolution to expand the channels to in_channels×t, followed by batch normalization (BN) and ReLU activation. Then a 3x3 depthwise (dw) convolution with the same in_channels×t channels is performed, again followed by BN and ReLU. Finally, a 1x1 convolution reduces the channels to out_channels and conducts BN. The final result is added to the input through a residual connection and activated by ReLU. This structure increases dimensionality first, then extracts features, and finally reduces dimensionality, enabling effective time - frequency domain feature extraction while controlling parameters and computational complexity."
    },
    "232": {
        "figure1": "2505.03228v1_tsne",
        "label1": "fig:tsne",
        "caption1": "The t-SNE visualization depicts the extracted embeddings of five speakers. These 5-second speaker embeddings are derived from ECAPA-TDNN, Res2Net, and MGFF-TDNN models.",
        "text": "In order to show the proposed model's multi granularity feature fusion ability, we evaluate the performance of different models on the VoxCeleb test set for both 3-second and 5-second duration groups. As shown in Table \\ref{tab:short_dur}, our proposed MGFF-TDNN model achieves superior performance on most metrics. Specifically, in the 3-second duration group, MGFF-TDNN shows EER reductions of 24.0\\%, 14.9\\%, and 12.0\\% across the three test sets compared to Res2Net, and also demonstrates certain advantages over ECAPA-TDNN. Additionally, we visualize the 5-second speaker embeddings using t-distributed Stochastic Neighbor Embedding (t-SNE) \\cite{tsne2008} and compare their disentanglement capabilities. As illustrated in Fig.\\ref{fig:tsne}, the speaker embeddings extracted by MGFF-TDNN demonstrate stronger clustering capabilities in short duration compared to those from ECAPA-TDNN and Res2Net, and MGFF-TDNN makes speaker embeddings more discriminative.",
        "summarize_figure": "2505.03228v1_tsne",
        "summarization": "This picture uses t - SNE visualization to show the embeddings of five speakers extracted by ECAPA - TDNN, Res2Net, and MGFF - TDNN models. Points of different colors represent embeddings of different speakers. It can be seen that the speaker embeddings extracted by the MGFF - TDNN model have stronger clustering ability in short duration compared with those from ECAPA - TDNN and Res2Net, making the speaker embeddings more discriminative. This reflects the advantage of the MGFF - TDNN model in multi - granularity feature fusion."
    },
    "233": {
        "figure1": "2505.03233v1_pipeline",
        "label1": "fig:model_pipeline",
        "caption1": "\\textbf{GraspVLA} consists of an autoregressive vision-language backbone and a flow-matching based action expert. It exploits the synergy between Internet grounding data and synthetic action data with a Progressive Action Generation mechanism: the model first predicts 2D bounding boxes of the target object for both synthetic data and web data, and additionally generates grasp pose and chunked actions for synthetic data.}  \\vspace{-4mm",
        "text": "GraspVLA integrates a Vision-Language Model (VLM) with an action expert \\cite{pi_0}, connected through a Progressive Action Generation (PAG) mechanism, as illustrated in Figure \\ref{fig:model_pipeline}. The VLM takes observation images and a text instruction for vision-language joint perception. It comprises a trainable large language model (InternLM2 1.8B \\cite{internlm2}), a vision encoder that fuses features from frozen DINO-v2 \\cite{dinov2} and SigLIP \\cite{siglip} inspired by OpenVLA \\cite{openvla}, and a trainable projector from the vision space to the language space. We use a conditional flow matching action expert \\cite{lipman2022flowmatching} for fine-grained end-effector action generation. We further introduce PAG to efficiently transfer knowledge learned from Internet grounding dataset to grasping skills.",
        "summarize_figure": "2505.03233v1_pipeline",
        "summarization": "The picture shows the pipeline of the GraspVLA model, which consists of an autoregressive vision - language backbone and a flow - matching based action expert. Through the Progressive Action Generation (PAG) mechanism, it exploits the synergy between web data (e.g., “pick up charger”) and synthetic data (e.g., “pick up box”). The Vision - Language Model (VLM) conducts token prediction, first predicting 2D bounding boxes for target objects, and generating grasping poses and action chunks for synthetic data. The action expert generates actions via flow matching. This pipeline combines different data sources and modules for knowledge transfer and action generation."
    },
    "234": {
        "figure1": "2505.03233v1_step_response_comparison",
        "label1": "fig:step_response_comparison",
        "caption1": "\\textbf{Step response comparison for different filters}. In the comparison, these filters set the same sampling frequency and cutoff frequency. The first-order Butterworth filter exhibits a large initial acceleration in its step response. The third-order Butterworth filter, third-order Chebyshev II filter, and third-order Bessel filter all exhibit overshoot to varying degrees. The triple-cascaded first-order Butterworth filter can avoid excessive initial acceleration and eliminate overshoot while maintaining convergence speed, making it an ideal filter choice.",
        "text": "To mitigate abrupt target transitions, we evaluated multiple filter implementations and selected a cascaded filter design for its superior smoothing performance (Figure ~\\ref{fig:step_response_comparison}). It achieves fast convergence without overshoot while avoiding excessive initial acceleration, which is suitable for the output characteristics of the GraspVLA model. Additionally, positional interpolation was adopted instead of temporal interpolation to address synchronization mismatches between model computation latency and control pipelines.",
        "summarize_figure": "2505.03233v1_step_response_comparison",
        "summarization": "The picture shows a step response comparison of different filters, with the same sampling and cutoff frequencies set for each. The first - order Butterworth filter has a large initial acceleration in its step response. The third - order Butterworth, third - order Chebyshev II, and third - order Bessel filters all exhibit overshoot to different extents. The triple - cascaded first - order Butterworth filter can avoid excessive initial acceleration and eliminate overshoot while maintaining convergence speed, making it an ideal choice for the output characteristics of the GraspVLA model."
    },
    "235": {
        "figure1": "2505.03237v1_transporte_estacionaria_difTfinal",
        "label1": "transporte_wbcheck_cini+dif",
        "caption1": "Well-balanced property check: transport equation. Initial condition (left) and differences between the initial and final numerical solutions for a 200-cell mesh at $T_f=10$ with the ROM (right).}  \\end{center",
        "text": "Let us consider the transport equation with linear source term \\eqref{transporte_ecuacion}, with $c=1$ and $\\alpha=1$. Let us consider the space interval  $[0, 2]$ and the time interval  $[0,10]$. As initial condition, we consider discrete stationary solution given by the projection into the POD basis of the stationary solution $$ w_0(x)=e^{x}$$  for the FOM (see Figure \\ref{transporte_wbcheck_cini+dif}, left). $L^1$-errors between the initial and final cell-averages of the ROM have been computed for different meshes (see Table \\ref{transporte_wbcheck_erroresCPU}). In particular, Figure \\ref{transporte_wbcheck_cini+dif} right shows the differences bewteen the initial and final solutions for a 200-cell mesh at $T_f=10$ with the ROM. CPU times in seconds required by the FOM and ROM to obtain the solution at final time are also shown in Table \\ref{transporte_wbcheck_erroresCPU}. As expected, the ROM is faster than the FOM, obtaining a significant reduction in computational time if the number of cells is high. Cells & Errors with ROM & \\multicolumn{2}{c|}{CPU time}\\\\  & & FOM & ROM\\\\\\hline 200&   5.17e-14 & 1.24 & 1.13 \\\\ 400&   1.26e-15 & 6.01 & 4.35 \\\\ 800&  6.40e-14 & 34.23& 16.65 \\\\ 1600& 0.0  & 201.24 & 68.02 \\\\",
        "summarize_figure": "2505.03237v1_transporte_estacionaria_difTfinal",
        "summarization": "This graph checks the well - balanced property of the transport equation. It shows the differences between the initial and final numerical solutions for a 200 - cell mesh at Tf​=10. The x - axis ranges from 0.25 to 1.75, and the y - axis (w - w0) ranges from 1e - 14 to - 6. The curve for w indicates that as x increases, w - w0 decreases. This is related to an initial condition w0​(x)=ex, and the ROM is faster than the FOM in obtaining solutions, as shown by CPU times in related tables."
    },
    "236": {
        "figure1": "2505.03238v1_title_fig.drawio",
        "label1": "fig:title",
        "caption1": "Overview of the proposed \\textit{RobotxR1} \\gls{eai} agent for autonomous driving. The agent consists of a \\textit{DecisionxR1} module and an \\textit{MPCxR1} module that work in tandem to achieve a user-specific driving behavior. At the core of each module sits a \\gls{llm} trained with both \\gls{sft} and \\gls{rl}. During \\gls{rl} training, the system interacts with its simulation environment, where it is rewarded for behavioral adherence. The trained agent is deployed on a scaled autonomous racecar for real-world experiments.",
        "text": "This work leverages the publicly available framework introduced in \\cite{rsspaper} to enable a comparison with purely \\gls{sft}-trained \\glspl{llm}, while significantly extending the \\emph{DecisionxLLM} and \\emph{MPCxLLM} architectures of \\cite{rsspaper} to support robotic reasoning in dynamic environments. The existing \\gls{rag} structure, including the use of five retrieved memories as in the baseline \\cite{rsspaper}, and the proposed \\gls{lora}-based \\gls{sft} method are leveraged. In this work, the \\emph{Qwen} family of models \\cite{bai2023qwen}, ranging from 1.5 to 7B parameters, is adopted as the primary \\gls{llm} architecture, motivated by the performance reported in \\cite{rsspaper}. The \\emph{R1-zero} framework is integrated into this setup, resulting in \\textit{RobotxR1}, which encompasses the \\emph{DecisionxR1} and \\emph{MPCxR1} modules, to enable full \\gls{rl}-based embodiment of the underlying \\glspl{llm} within a holistic pipeline that follows sequential decision making and control adaptation, as illustrated in \\Cref{fig:title}.",
        "summarize_figure": "2505.03238v1_title_fig.drawio",
        "summarization": "The picture shows an overview of the RobotxR1 Embodied AI (EAI) agent for autonomous driving. The agent consists of the DecisionxR1 and MPCxR1 modules, which work together to achieve specific driving behaviors. At the core of each module is a Large Language Model (LLM) trained with both Supervised Fine - Tuning (SFT) and Reinforcement Learning (RL). During RL training, the system interacts with a simulation environment and receives rewards for compliant behavior. The trained agent is deployed on a scaled - down autonomous race car for real - world experiments."
    },
    "237": {
        "figure1": "2505.03238v1_tracks",
        "label1": "fig:tracks",
        "caption1": "Tracks used for training and evaluation. Left: \\textbf{Circle} map used for \\emph{MPCxR1} \\gls{grpo} training. Right: \\textbf{The Grand Tour} map used for evaluation. Dashed grey and solid red lines denote the centerline and the minimum-curvature racing line (targeted by the default \\gls{mpc}), respectively.",
        "text": "The maps used in the proposed \\textit{MPCxR1} \\gls{grpo} training and evaluation procedure are shown in \\Cref{fig:tracks}. The \\textbf{Circle} map, used during training, provides a minimal and structured environment in which the \\gls{llm} can efficiently learn to interact with the robot and environment. In contrast, \\textbf{The Grand Tour} map, used for control adaptability evaluation as in \\Cref{tab:control_res}, follows a conventional racing circuit design with greater complexity and variation.",
        "summarize_figure": "2505.03238v1_tracks",
        "summarization": "The figure shows two tracks used for training and evaluation. The left is the Circle map for MPCxR1 grpo training, with dashed grey centerline and solid red minimum - curvature racing line, providing a simple structured environment for LLM - robot - environment interaction learning. The right is The Grand Tour map for evaluation, a more complex conventional racing circuit."
    },
    "238": {
        "figure1": "2505.03239v1_model2_FRC_N20_pre",
        "label1": "fig:model2_FRCs",
        "caption1": "\\small The forced response curve of periodic orbits of the two coupled oscillators~\\eqref{eq:model2_two_order} with $\\epsilon = 0.01$.",
        "text": "We further investigate the forced vibration of the system with $\\epsilon=0.01$, with particular emphasis on the system's forced response curve (FRC) near its first natural frequency (i.e., $\\Omega \\approx \\omega_1$). The FRC under different expansion orders is shown in Fig.~\\ref{fig:model2_FRCs}, where it is observed that $\\mathcal{O}(3)$ expansion is already sufficient. To validate the effectiveness of the SSM-based reductions, we applied the collocation method to the approximated ODE system of~\\eqref{eq:model2_one_order}, extracting reference solutions for the forced periodic orbits. Additionally, we utilized the \\textsc{matlab} solver 'dde23' to conduct forward simulations of the original DDEs~\\eqref{eq:model2_two_order} to validate the predicted stable periodic orbits. As shown in Fig.~\\ref{fig:model2_FRCs}, the collocation method and forward simulation results align well with the SSM-based predictions.",
        "summarize_figure": "2505.03239v1_model2_FRC_N20_pre",
        "summarization": "The picture shows the forced response curve of the periodic orbits of two coupled oscillators when ε = 0.01. The horizontal axis is Ω, ranging from 0.90 to 1.10, and the vertical axis is ||φ1||∞. There are multiple curves representing different methods or expansion orders, such as SSM - O(3) - stable, SSM - O(5) - stable, etc. The curves first rise and then fall, peaking when Ω is close to 1.00. It is found that the SSM - O(3) expansion is sufficient, and the results of the collocation method and forward simulation agree well with the SSM - based predictions."
    },
    "239": {
        "figure1": "2505.03239v1_model2_post_new",
        "label1": "fig:model2_limitcycle",
        "caption1": "\\small Time history curve of the free variable $q_1(t)$. The projections of the trajectories of the full system (blue dashed line) and the ROM~\\eqref{eq:ROM2_unstable} (red solid line)are depicted. In the left subplot, the trajectories approach the limit cycle in the steady state, while the right subplot illustrates the limit cycle of the system in its steady state. ",
        "text": "We observe from the SSM-based ROM~\\eqref{eq:ROM2_unstable} that the polynomial function $a(\\rho)$ (cf.~\\eqref{eq:red-auto}) admits a nontrivial root $\\rho^*=5.179$ such that $a(\\rho^*)=0$. This root $\\rho^*$ corresponds to a limit cycle with frequency $b(\\rho^*)$, as discussed in Sect.~\\ref{sec:ssm-prediction}. We take the initial condition $\\bs{p}_0=(4.0 e^{0.1\\mathrm{i}}, 4.0 e^{-0.1\\mathrm{i}})$, and perform forward time integration for both the ROM~\\eqref{eq:ROM2_unstable} and the approximate system~\\eqref{eq:model2_one_order} using the same initial conditions. As shown in Fig.~\\ref{fig:model2_limitcycle}, the trajectories indeed approach the limit cycle in the steady state (as depicted in the left subplot). Furthermore, we plot the limit cycle in the right subplot. The SSM-based ROM provides excellent predictions, with the full system's simulated trajectory aligning with the ROM's trajectory, confirming the accuracy of the SSM-based predictions.",
        "summarize_figure": "2505.03239v1_model2_post_new",
        "summarization": "The picture shows the time - history curve of the free variable q1(t). In the left sub - plot, the blue dashed line represents the trajectory projection of the full system, and the red solid line represents that of the reduced - order model (ROM). In the steady state, the trajectories approach the limit cycle. The horizontal axis is t, and the vertical axis is q1(t), with the curve showing periodic fluctuations. The right sub - plot shows the limit cycle of the system in the steady state. The red solid line (SSM) and the blue dashed line (Full) basically overlap, indicating that the SSM - based ROM provides accurate predictions, and the simulated trajectory of the full system is consistent with that of the ROM."
    },
    "240": {
        "figure1": "2505.03240v1_PINN",
        "label1": "fig:pinn_structure",
        "caption1": "The general structure of a Physics-Informed Neural Network (PINN).",
        "text": "Figure~\\ref{fig:pinn_structure} illustrates the general structure of a PINN. The network takes as input the spatiotemporal coordinates $(x,t)$, referred to as collocation points, and computes an approximation $u(x,t)$ of the true solution to the target problem. The outputs of the network, along with their required derivatives, are used to construct the loss function. These derivatives are efficiently evaluated using automatic differentiation techniques \\cite{Paszke2017}.",
        "summarize_figure": "2505.03240v1_PINN",
        "summarization": "The picture shows the general structure of a Physics - Informed Neural Network (PINN). The network takes spatiotemporal coordinates (x,t) as inputs (called collocation points) and computes an approximation u(x,t) of the true solution to the target problem. Through automatic differentiation (Auto grad), derivatives of u(x,t) with respect to x and t, such as ∂u/∂x, ∂²u/∂x², ∂u/∂t, etc., are obtained. These, together with u(x,t), are used to construct the loss function, including the PDE loss LPDE, boundary - condition loss LBC, and initial - condition loss LIC, which are ultimately used to train and optimize the network parameters."
    },
    "241": {
        "figure1": "2505.03240v1_StageIA",
        "label1": "fig:stageIA",
        "caption1": "The training process of FKE solver with PINN.",
        "text": "To accelerate training, we initialize the network parameters using weights from the preceding time step. For the initial time step $t \\in [\\tau_0, \\tau_1]$, the network parameters are randomly initialized. Fig.~\\ref{fig:stageIA} describes the general training process of the FKE solver.",
        "summarize_figure": "2505.03240v1_StageIA",
        "summarization": "The picture shows the training process of the FKE solver based on Physics - Informed Neural Network (PINN). The network takes spatiotemporal coordinates (x,t) (where x belongs to Ω and t belongs to [τi - 1, τi]) as inputs, with the initial condition ui(x, τi - 1). The network output ui(x,t) is used to construct the initial - condition loss LIC, boundary - condition loss LBC, and FKE equation loss LFKE, which together form the total loss L. Training is carried out by optimizing the network parameters, ultimately obtaining the FKE solution ui(x, τi). To accelerate training, network parameters are initialized with weights from the preceding time step, and for the initial time step, parameters are randomly initialized."
    },
    "242": {
        "figure1": "2505.03240v1_StageIB",
        "label1": "fig:stageIB",
        "caption1": "The training of the approximate FKE solver based on PCA.",
        "text": "A schematic of training process of this approximate FKE solver is depicted in Fig.~\\ref{fig:stageIB}. Once trained, this approximate FKE solver can then be used in Stage II for fast prediction of end time solutions with very limited storage needs, enabling near-real-time online computation.",
        "summarize_figure": "2505.03240v1_StageIB",
        "summarization": "The picture shows the training process of the approximate FKE solver based on Principal Component Analysis (PCA). Before the update, the network output is ui(x, τi) = ∑βj(τi)φj(x), and the network parameters are optimized by calculating the loss function L = ∑[β̂j(τi) - βj(τi)]². After the update, ui(x, τi - 1) = ∑αj(τi - 1)φj(x) is obtained. For time - variant cases, the time - variant functions a(t), b(t), c(t), d(t), etc., in FKE need to be evaluated at M + 1 time points t∈[τi - 1, τi]. The trained approximate FKE solver can be used in Stage II to quickly predict end - time solutions with very limited storage requirements, enabling near - real - time online computation."
    },
    "243": {
        "figure1": "2505.03246v1_FIG_003",
        "label1": "FIG_003",
        "caption1": " (a) Low-frequency (0-18 MHz) passband-filtered forward solution when the input is the projection of the relative permittivity distribution in Fig. 1(a) onto the first five PCs. (b) For the same frequency bandwidth as (a), the difference with respect to the data corresponding to the forward solution for the reference relative permittivity distribution in Fig. 1(a). (c) and (d) As in (a) and (b), but considering the full bandwidth of the data. (e)-(h) As in (a)-(d), but using 35 PCs and the 0-45 MHz bandwidth. (i)-(l) As in (a)-(d), but using 100 PCs and the 0-72 MHz bandwidth.",
        "text": "Following \\citet{meles2022bayesian}, we use the generative model to create a total of 1000 random realizations from the prior and learn a prior-knowledge-based parametrization of the input domain (i.e., $\\epsilon_r$ distributions) in terms of principal components, with the first ones linked to low-wavenumber features, and higher modes revealing small-scale details. In terms of wave propagation, the low-frequency content of the data is mainly sensitive to the first principal components. To also capture the high-frequency content in the output, it is necessary to expand and consider the higher modes of the principal components. This is demonstrated in Fig. \\ref{FIG_003} using the reference $\\epsilon_r$ distribution in Fig. \\ref{FIG_001}(e) and its truncations on 5, 35 and 100 PCs, and the corresponding data computed via FDTD modeling for low (0-18 MHz), medium (0-45 MHz), high (0-72 MHz) and full frequencies (0-300 MHz) bandwidths. The ability of the truncated input to capture the reference data set depends on the frequency content considered, with 5 PCs being able to only adequately recover low frequencies of the data (Fig. \\ref{FIG_003}(a-d)). As we expand the PCA basis elements up to 35, the residual  reduces even when considering an expanded frequency bandwidth (Fig. \\ref{FIG_003}(e-h)). Projecting the input on 100 PC components results in close agreement across the entire bandwidth of the data (Fig. \\ref{FIG_003}(i-l)).",
        "summarize_figure": "2505.03246v1_FIG_003",
        "summarization": "The figure shows multiple sets of waveform data under different conditions. (a) is the low - frequency (0 - 18 MHz) band - pass filtered forward solution with the input being the projection of the relative permittivity distribution onto the first five principal components in Fig. 1(a). (b) is the difference with respect to the forward solution data of the reference relative permittivity distribution under the same frequency bandwidth as (a). (c) and (d) are similar to (a) and (b), but consider the full bandwidth of the data. (e) - (h) use 35 principal components and a 0 - 45 MHz bandwidth, and (i) - (l) use 100 principal components and a 0 - 72 MHz bandwidth. As the number of principal components increases from 5 to 35 and then to 100, the ability of the truncated input to capture the reference data set improves under different frequency bandwidths, the residual decreases, and 100 principal components achieve good agreement across the full - bandwidth data."
    },
    "244": {
        "figure1": "2505.03248v1_NE_atPC_inRb_MUBO",
        "label1": "fig:NE_atPC_inRb",
        "caption1": "Block diagram representation of  $\\left[\\mathbf{G}^\\mathcal{B}_{P,C}(\\mathrm s)\\right]_{\\mathcal{R}_b}$: the non-linear $36\\times 12$ model of a rigid body $\\mathcal{B}$ at points $P$ and $C$.}  \\end{center",
        "text": "Let us consider that  the body $\\mathcal{B}$ has  $2$ connection points $P$ and $C$ where only external wrenches $\\mathbf{W}_{./\\mathcal{B},P}$ and $\\mathbf{W}_{./\\mathcal{B},C}$ can be applied, respectively. The kinematic model $\\boldsymbol{\\tau}_{CP}$ can be used to transport the wrench  $\\mathbf{W}_{./\\mathcal{B},C}$ from $C$ to $P$: The transport of the motion vector from $P$ to $C$ can be decomposed into: The non-linear model $\\left[\\mathbf{G}^\\mathcal{B}_{P,C}(\\mathrm s)\\right]_{\\mathcal{R}_b}$, projected in the body frame, of the body  $\\mathcal{B}$ at points $P$ and $C$ can be described by the block-diagram of Figure \\ref{fig:NE_atPC_inRb}, based on the previous model $\\left[\\mathbf{G}^\\mathcal{B}_{P}(\\mathrm s)\\right]_{\\mathcal{R}_b}$.\nThe operation to transport the motion vector from point $P$ to point $C$ is denoted $\\boldsymbol{\\upsilon_{\\overrightarrow{CP}}}$: \tThis operation, in projection in the body frame $\\mathcal R_b$, is represented by the red box in Fig. \\ref{fig:NE_atPC_inRb}).",
        "summarize_figure": "2505.03248v1_NE_atPC_inRb_MUBO",
        "summarization": "This picture shows the block - diagram representation of the non - linear 36×12 model [G^B_P,C(s)]Rb of a rigid body B at points P and C. It involves various vectors and matrices like [W./B,C]Rb, [W./B,P]_Rb, etc., and operations are carried out through matrices such as [τCP]_Rb. The operation of transporting the motion vector from point P to point C is denoted as υ_CP(·), and its projection in the body - frame Rb is represented by the red - boxed part in the figure. These elements together form the kinematic model relationship of the rigid body B at the two connection points P and C."
    },
    "245": {
        "figure1": "2505.03248v1_NE_2port_1_g_MUBO",
        "label1": "fig:NE_2port_0",
        "caption1": "Block diagram model of  $\\left[\\mathbf{R}^\\mathcal{A}_{P,C}\\right]_{\\mathcal{R}_a}$:  non-linear TITOP  model of a rigid body $\\mathcal{A}$ at points $P$ and $C$.}  \\end{center",
        "text": "Let us consider a rigid appendage $\\mathcal{A}$, its body frame $\\mathcal{R}_a$, clamped to a parent body at point $P$ and to a child body at point $C$. The Two-Input Two-Output Port (TITOP) non-linear model $\\mathbf{R}^\\mathcal{A}_{P,C}$ is  the $24\\times 24$ transfer between: and could be represented by the block-diagram model depicted in Figure \\ref{fig:NE_2port_0}. This model is directly based on the \\textsc{Newton-Euler} equation \\eqref{eq:NE_short_g}  applied to the body $\\mathcal A$: and  the motion transport operator $\\boldsymbol{\\upsilon_{CP}}$ defined in Definition \\ref{def:upsilon}. This model is static and does not involves additional states.",
        "summarize_figure": "2505.03248v1_NE_2port_1_g_MUBO",
        "summarization": "The picture shows the block - diagram model of the non - linear Two - Input Two - Output Port (TITOP) model [R^A_P,C]R_a of a rigid body A at points P and C. This model is a 24×24 transfer relationship, involving multiple matrices and vectors such as [W./A,C]_R_a, [m^A_P]_R_a, etc. Operations are carried out through matrices like [τCP]_R_a^T, [D^A_P]_R_a. This model is directly based on the Newton - Euler equation applied to the rigid body A and the defined motion transport operator υ_CP. The model is static and does not involve additional states."
    },
    "246": {
        "figure1": "2505.03251v1_fig_path",
        "label1": "fig:path",
        "caption1": "From a given position, the path towards the best (winning) position necessitates to find the best moves consecutively. The probability to reach the ideal position after $n$ half-moves is $P(n)$ (Eq.~\\ref{eq:Pn}).",
        "text": "We analyze a sequence of moves (also called a variation) from a given position in the following way. We will consider a depth of $n$ half-moves (or plies) which corresponds to $n/2$ moves.  A given position $P$ has an evaluation $E(P)$, and the best move maximizes the evaluation of the new position $P'$ (for white, while for black $E(P')$ is minimized). We denote by $E_1$ the evaluation for the best move, by $E_2$ the evaluation for the second best move (for white we have $E_1>E_2$ and for black $E_1<E_2$), and $E_i$ the evaluation for the $i^{th}$ best move. We define the following `gap' \\cite{Barthelemy2023} This quantity $\\Delta$ characterizes the difference of quality between the best move and the second best move, and has been shown to display a wide range of values \\cite{Barthelemy2023,Chacoma}. When $\\Delta$ is large, the best move is significantly better than the second best move and many players won't miss it. When $\\Delta$ is small (typically less than $100$ centipawns), it could be more difficult to find the best move. We therefore characterize a player's strength by their ability to detect the best move, quantified by the parameter $\\Delta_0$. Specifically, if $\\Delta > \\Delta_0$ the player can identify the best move; however, if $\\Delta < \\Delta_0$, the player's resolution is insufficient to determine the best move between the top two options. It is natural to assume that the probability to make the best move of the usual logit form in discrete choice theory (see for example \\cite{Train} and references therein)   p_1=\\frac{\\mathrm{e}^{E_1/\\Delta_0}}{\\sum_{i=1}\\mathrm{e}^{E_i/\\Delta_0}} We will consider the first two moves only and the probability to find the best move is then   p_1=\\frac{1}{1+\\mathrm{e}^{-\\Delta/\\Delta_0}} The sequence of $n$ half-moves is then characterized by the probability $P(n)$ to reach a winning position given by   P(n)=\\prod_{i=1}^np_{1}(i) where $p_{i}(i)$ is the probability to find the best $i^{th}$ half-move (see Fig.~\\ref{fig:path} for an illustration of this point).  The corresponding information entropy  \\cite{Shannon1948} is   S=-\\log_2\\prod_{i=1}^n where $\\Delta (i)=E_{bm}(i)-E_{2bm}(i)$ is the gap corresponding to the position at half-move $i$ ($E_{bm}(i)$ is the best move at ply $i$, and $E_{2bm}(i)$ the second best move). For a sequence of $n$ plies, the entropy is then $0\\leq S\\leq n$. This quantity $S$ is the information quantity that a player has to process when visualizing the next $n$ half-moves. Note that the player has to find his own best moves but also the ones of his opponent. If $|\\Delta(i)|\\gg \\Delta_0$ for all half-moves, then $S=0$: the choice is clear at each step. In contrast, if $|\\Delta(i)|\\ll \\Delta_0$ -- which corresponds to the worst case --  we have the largest possible entropy $S=n$. We note that this quantity should not be confused with the entropy of move distributions as discussed in \\cite{Chassy:2011}: in this work the authors weighted each move by a factor reflecting how often it appeared in their database. Here in contrast, we discuss the entropy of a sequence of moves.",
        "summarize_figure": "2505.03251v1_fig_path",
        "summarization": "The picture shows the path from a given position (the initial position, represented by a blue dot) to the best (winning) position (the ideal position, represented by a green dot). To reach the ideal position, one needs to find the best moves consecutively. Starting from the initial position, there are multiple possible positions at each step (the left side shows a set of 2 to the power of n positions). By choosing the best moves (such as p1(1), p1(2), etc.), one gradually approaches the ideal position. The probability of reaching the ideal position after n half - moves is P(n), which is related to the probability of finding the best move at each step. It reflects the information entropy that a player needs to process when visualizing the next n half - moves, and its value ranges from 0 to n, indicating the clarity of move choices."
    },
    "247": {
        "figure1": "2505.03251v1_fig_PS4_vs_ply",
        "label1": "fig:Svsply",
        "caption1": "(Top) Evolution of the average entropy $S$ versus ply (or half-move) for different levels of players (beginner, intermediate, expert). (Bottom) Evolution of the probability to observe an entropy larger than $4$ during the game, computed for different levels. These results are obtained for an average of 100 games played at the World Rapid Chess Tournament 2023 (Samarkand).",
        "text": "We computed this entropy averaged over $100$ games taken at the World Rapid Chess Tournament 2023 (Samarkand). For $\\Delta_0$, we can estimate from \\cite{Chacoma} (using the fact that for $\\Delta=\\Delta_0$, $p_1\\approx 0.73$) that the order of magnitude is $\\Delta_0\\sim 10^1$ for experts and $\\Delta_0\\sim 10^2$ for beginners. Based on this, we selected $\\Delta_0=100$ for beginners, $\\Delta_0=50$ for intermediate players, and $\\Delta_0=10$ for experts. Typically, this implies that beginners struggle to differentiate between moves even when one wins a pawn or more, while experts can detect subtle differences in move quality. We also verified that varying these values yields similar results. The evaluation was provided by the Stockfish engine \\cite{SF} (see Methods). The average entropy for three different player skill levels during the game is shown in Fig.~\\ref{fig:Svsply} (the ply represents the progression of the game is plotted on the x-axis).           (Bottom) Evolution of the probability to observe an entropy larger than $4$ during the game, computed for different levels.            These results are obtained for an average of 100 games played at the World Rapid Chess Tournament 2023 (Samarkand).}\nIn Fig.~\\ref{fig:Svsply}(Top) we represented the average entropy, but there are variations from a game to another one. In order to extract information from these fluctuations, we show in Fig.~\\ref{fig:Svsply}(bottom), the probability $P(S>4)$ to observe a value of entropy larger than 4. These results are consistent with those discussed above for average values. In particular, we see that for experts, there is a rapid decline of high-entropy situations, underscoring their superior ability to navigate the game's complexities, particularly in the critical middlegame phase.  The drop in $P(S > 4)$ for all levels indicate that complex situations requiring a large cognitive load are more seldom. There are therefore significant differences in how chess players of varying skill levels manage the complexity of positions throughout a game. Experts excel at reducing complexity as the game progresses, particularly in the middlegame and endgame, while beginners and intermediates continue to face challenging, high-entropy positions for much longer. The persistence of high entropy for non-experts underscores the importance of tailored training and simplified recommendations to help them navigate through complex positions.",
        "summarize_figure": "2505.03251v1_fig_PS4_vs_ply",
        "summarization": "The picture shows the evolution of the probability of observing an entropy larger than 4 during the game for different levels of players (beginners, intermediate players, experts) based on the average data of 100 games in the 2023 World Rapid Chess Tournament (Samarkand). Blue dots represent beginners, orange squares represent intermediate players, and green triangles represent experts. As the number of plies increases, the probability for all three groups decreases. Expert players show a rapid decline and can effectively handle complex situations in the middlegame. Beginners and intermediate players still face many high - entropy complex situations for a long number of plies, indicating significant differences in the ability of players at different levels to handle the complexity of the game."
    },
    "248": {
        "figure1": "2505.03251v1_fig_PE_cumu",
        "label1": "fig:PE",
        "caption1": "(Top) Probability to have a Stockfish $E$ (in centipawns). (Bottom) Cumulative distribution of the absolute value $|E|$. We observe here that positions with evaluation $|E|<100$ correspond to about $\\approx 64\\%$ of all positions (results computed for all positions for 100 games during the World Rapid chess championship 2023). ",
        "text": "The evaluation for all positions is distributed according to the result shown in Fig.~\\ref{fig:PE}(top). The average is slighly positive as on average players with white win more often (the average is $\\overline{E}=23$ and the median is $19$). We also observe that a majority of positions during these games (about $64\\%$) have an evaluation less than $|E|<100$ (Fig.~\\ref{fig:PE}(bottom)).",
        "summarize_figure": "2505.03251v1_fig_PE_cumu",
        "summarization": "The picture contains two sub - graphs. The upper graph shows the probability distribution of the Stockfish evaluation value E (in centipawns) for chess positions, based on 100 games in the 2023 World Rapid Chess Championship. The average is slightly positive, reflecting that players with white win more often on average. The lower graph is the cumulative distribution of the absolute value |E|, showing that about 64% of the positions have an evaluation value |E| less than 100, indicating that most positions have relatively small evaluation values."
    },
    "249": {
        "figure1": "2505.03252v1_fast_sketch",
        "label1": "fig:fast and slow",
        "caption1": "Relations \\eqref{serre_speed-amp} between the velocity of a fast (slow) solitary wave and the corresponding long-wave velocity $V_+$ ($V_-$) are represented schematically on the left (right).",
        "text": "The solitary wave limit of \\eqref{serre_TW} is achieved when $h_2 \\to h_1$ so that $m \\to 1$. Its explicit form is   h(x,t)= \\hb + a \\,\\mathrm{sech}^2 \\left ( \\frac{\\sqrt{3a}}{\\hb Fast ($\\sigma=+1$) and slow ($\\sigma=-1$) elevation solitary waves propagate on the background $h=\\hb, u=\\ub$, and are characterised by the speed-amplitude relation \tc=c_s(a, \\bar  h, \\ub) \\equiv \\ub +\\sigma \\sqrt{\\hb +a}. Fast solitary waves move faster than the dispersionless long-wave velocity $V_+ = \\ub + \\sqrt{\\hb}$, and slow solitary waves move slower than  the dispersionless long-wave velocity $V_- = \\ub - \\sqrt{\\hb}$ (see Figure \\ref{fig:fast and slow}).       a fast (slow) solitary wave and the corresponding long-wave       velocity $V_+$ ($V_-$) are represented schematically on the left       (right).}",
        "summarize_figure": "2505.03252v1_fast_sketch",
        "summarization": "The picture shows a schematic diagram of the relationship between the velocities of fast and slow solitary waves and the corresponding long - wave velocities. The blue curve in the figure represents a certain trend of change, on which two velocity relations are marked. dx/dt = cs represents the velocity of the fast solitary wave, and dx/dt = V_ represents the velocity of the slow solitary wave. The velocity of the fast solitary wave is faster than the non - dispersive long - wave velocity V_+, and the velocity of the slow solitary wave is slower than the non - dispersive long - wave velocity V_-. This reflects the characteristics of the propagation velocities of solitary waves in a specific background."
    },
    "250": {
        "figure1": "2505.03252v1_Deltaq",
        "label1": "fig:fitting_exact_RI",
        "caption1": "Left: comparison between the exact Riemann invariant $Q_+$ \\eqref{eq:f} (solid line) and  its convex approximation $\\tilde {Q}_+$\\eqref{eq:qfit} (dashed line); Right: the difference $\\Delta= (Q_+ - \\tilde Q_+)/ \\overline h$. ",
        "text": "convex approximation $\\tilde {Q}_+$\\eqref{eq:qfit} (dashed line); Right: the difference $\\Delta= (Q_+ - \\tilde Q_+)/ \\overline h$.         } One can see that the expression \\eqref{eq:qfit} does not coincide with the rigorous asymptotic result \\eqref{eq:f} for $Q_+$ corresponding to $\\mu=\\sigma=1$. However, it provides a quite accurate approximation in the physically relevant range of amplitude-depth ratios $z^2=a/\\hb$. Indeed, the small amplitude expansion of the fitting approach result \\eqref{eq:qfit} is   -\\frac{37}{864} z^6+ {\\cal O}(z^8) \\right ).  The first two terms in this expansion correspond to the Riemann invariant of the KdV modulation system in the soliton limit ($\\mathcal{O}(z^2)$) as $a \\to 0$. But the fitting approach improves upon the classical KdV prediction since \\eqref{eq:small_amp} and the exact result \\eqref{eq:qp} asymptotically agree up to $\\mathcal{O}(a^2)$ ($\\mathcal{O}(z^4)$). Even further, the difference between the two expansions at $\\mathcal{O}(a^3)$ ($\\mathcal{O}(z^6)$) is just $\\sim 10^{-3} a^3$, i.e. the exact \\eqref{eq:f} and convex approximation \\eqref{eq:qfit} curves are in excellent agreement for physically admissible $a/\\hb < a_{\\rm max}/\\hb \\approx 0.8$ \\cite{el_unsteady_2006}, see Fig.~\\ref{fig:fitting_exact_RI}. At the same time, the existence of such a discrepancy poses the important question:  what is the cause of this $\\mathcal{O}(a^3)$ asymptotic discrepancy and what does it mean for the asymptotic validity of the DSW fitting method, particularly for non-integrable systems?",
        "summarize_figure": "2505.03252v1_Deltaq",
        "summarization": "The picture shows the curve of the difference Δ = (Q_+ - tilde{Q}+)/overline{h} between the exact Riemann invariant Q+ and its convex approximation tilde{Q}+ with respect to Z. The horizontal axis is Z and the vertical axis is Δ. The curve shows that in the physically relevant range of the amplitude - depth ratio z^2 = a/overline{h}, the convex approximation tilde{Q}+ provides a fairly accurate approximation to the exact Riemann invariant Q_+. In the small - amplitude expansion, the two are asymptotically consistent at certain orders. In the physically admissible range of a/overline{h} < a_max/overline{h} ≈ 0.8, the two curves agree well, but there is a difference of about 10^(-3) a^3 at the O(a^3) order, which raises questions about the asymptotic validity of the DSW fitting method."
    },
    "251": {
        "figure1": "2505.03255v1_percentage_nonzero_amps",
        "label1": "fig:percentage_nnz_amps",
        "caption1": "Percentage of non-zero to zero vertex amplitudes at different cutoffs with the total number of states $N_s$ shown for each cutoff.",
        "text": "For small enough cutoffs, such datasets can be obtained simply by full enumeration of the space $\\mathcal{S}^{(j_{max})}$. To understand the distribution of the potential training data, one can compare the number of non-zero $\\logtenj{\\mathbf{S^{(j_{max})}}}$ for all $\\mathbf{S^{(j_{max})}}$ in different cutoffs. It is immediately evident then, as shown in Figure \\ref{fig:percentage_nnz_amps}, that there is an imbalance in the number of configurations which yield a zero amplitude as compared to non-zero amplitude. In fact, as the cutoff increases, the percentage of non-zero amplitude configurations increases drastically, reaching over 60\\% already at $j_{max} = 2.5$. Further, the value of $(\\tenjsymbol(\\mathbf{S^{(j_{max})}}))^2$ can vary by more than 22 orders of magnitude in even small cutoffs as $j_{max} = 2.0$. This therefore poses the following hurdles: (i) a class imbalance for classification and (ii) very large range in the values of $\\logtenj{\\mathbf{S^{(j_{max})}}}$ for the regression.",
        "summarize_figure": "2505.03255v1_percentage_nonzero_amps",
        "summarization": "The picture is a bar chart showing the percentage relationship between non - zero vertex amplitudes and zero vertex amplitudes at different cutoffs, with the total number of states Ns marked for each cutoff. The horizontal axis is the cutoff value, ranging from 0.5 to 2.5, and the vertical axis is the percentage of non - zero amplitudes. As the cutoff value increases, the percentage of non - zero amplitude configurations rises sharply, exceeding 60% when the cutoff is 2.5. This indicates an imbalance in the number of zero - amplitude and non - zero - amplitude configurations. Meanwhile, the large range of relevant values under different cutoffs poses problems of class imbalance for classification and a large numerical range for regression."
    },
    "252": {
        "figure1": "2505.03267v1_KAWSx3",
        "label1": "D",
        "caption1": "Normalized perpendicular Poynting flux vector $S_x (z)/S(0)$ of KAWs plotted against the normalized wavenumber $k_x \\rho_i$. The flux $S_x (z)/S(0)$ is computed for various $\\mathrm{T_e/T_i}$ values, with the height fixed at h = 0.05 R\\textsubscript{Sun}. This analysis demonstrates how changes in the temperature ratio influence the energy dissipation behavior as a function of wavenumber in the solar corona.",
        "text": "We examined the normalized $S_x (z)/S(0)$ of KAWs as a function of the normalized wavenumber $(k_x \\rho_i)$ for varying $\\mathrm{T_e/T_i}$, as illustrated in Figure \\ref{D}. The results indicate that $S_x (z)/S(0)$ decays more rapidly at higher $\\mathrm{T_e/T_i}$ values. When the electron-to-ion temperature is lower (e.g., $\\mathrm{T_e/T_i} \\approx 0.3$), KAWs effectively heat the coronal region over a broader wavenumber range before fully dissipating.",
        "summarize_figure": "2505.03267v1_KAWSx3",
        "summarization": "The picture shows the relationship between the normalized perpendicular Poynting flux vector Sx(z)/S(0) of kinetic Alfvén waves (KAWs) and the normalized wavenumber kxρi. Computations are carried out for different electron - to - ion temperature ratios Te/Ti (0.3, 0.4, 0.5), with the height fixed at h = 0.05 R_Sun. The results show that the higher the Te/Ti value, the faster Sx(z)/S(0) decays. When the Te/Ti value is lower (e.g., 0.3), KAWs effectively heat the coronal region over a wider wavenumber range before fully dissipating."
    },
    "253": {
        "figure1": "2505.03267v1_KS_9.0.1Rsun_Main",
        "label1": "F",
        "caption1": "Net resonant speed $\\mathrm{V_{rp}}$ of particles as a function of the normalized wavenumber $k_x \\rho_i$. $S_z (x)/S(0)$. The evaluation is performed for various electron-to-ion temperature ratios $\\mathrm{T_e/T_i}$ at a fixed height of h = 0.05 R\\textsubscript{Sun}. This analysis provides insights into how the $\\mathrm{V_{rp}}$ behavior of KAWs varies with different temperature ratios, particularly with the normalized wavenumber.",
        "text": "We also explored the resultant perpendicular resonance speed of KAWs across a range of different electron-to-ion temperatures and the normalized wavenumber $k_x \\rho_i$ values. As shown in Figure \\ref{F}, $\\mathrm{V_{rp}}$ is significant at larger $k_x \\rho_i$ values. In contrast, at smaller $k_x \\rho_i$, the curves converge, showing minimal dependence on temperature, which suggests that the influence of $\\mathrm{T_e/T_i}$ diminishes in these conditions.",
        "summarize_figure": "2505.03267v1_KS_9.0.1Rsun_Main",
        "summarization": "The picture shows the relationship between the net resonant speed Vrp of particles and the normalized wavenumber kxρi. Evaluations are carried out for different electron - to - ion temperature ratios Te/Ti (0.3, 0.4, 0.5), with the height fixed at h = 0.05 R_Sun. The results indicate that Vrp is significant at larger kxρi values. At smaller kxρi values, the curves corresponding to different Te/Ti converge, showing minimal dependence on temperature, which means the influence of Te/Ti diminishes in such cases."
    },
    "254": {
        "figure1": "2505.03269v1_tuning_results",
        "label1": "fig:tuning",
        "caption1": "Auto-tuning results of \\ccglib matrix-matrix multiplication kernel. The measured performance and energy efficiency of each combination of tuning parameters is shown.",
        "text": "The performance and energy efficiency of each combination of tuning parameters is shown in Fig.~\\ref{fig:tuning}. Typically, the most performant combination of parameters is also the most energy efficient solution. On the GH200, there is a large spread of kernels with similar performance, but up to a factor two difference in energy efficiency.",
        "summarize_figure": "2505.03269v1_tuning_results",
        "summarization": "The picture shows the auto - tuning results of the ccglib matrix - matrix multiplication kernel, presenting the performance (in TOPs/s) and energy efficiency (in TOPs/J) for different combinations of tuning parameters. Six sub - graphs correspond to different devices such as AD4000, A100, GH200, W7700, MI210, and Instinct MI300. In most cases, the combination of parameters with the best performance is also the most energy - efficient. On the GH200, kernels have similar performance, but the energy efficiency can differ by a factor of two."
    },
    "255": {
        "figure1": "2505.03269v1_roofline",
        "label1": "fig:roofline",
        "caption1": "Roofline analysis of the \\ccglib matrix-matrix multiplication kernel. For each GPU, we show the roofline ceiling of the float16 and int1 (NVIDIA only) tensor cores, as well as the normal float32 cores for comparison.",
        "text": "The resulting rooflines are shown in Fig.~\\ref{fig:roofline}. For all GPUs, the small matrix size is memory-bound. On all GPUs, but especially the NVIDIA GPUs, we reach a performance very close to the limit set by the memory bandwidth. The larger matrix size is compute bound, and reaches $50-85\\%$ of the peak tensor-core throughput. In all cases except the small matrix size on the workstation-grade GPUs, \\ccglib is faster than the theoretical maximum of the normal single-precision cores by a wide margin.",
        "summarize_figure": "2505.03269v1_roofline",
        "summarization": "The picture shows the roofline analysis results of the ccglib matrix - matrix multiplication kernel. Six sub - graphs correspond to different GPUs such as AD4000, A100, GH200, W7700, MI210, and Instinct MI300. For all GPUs, small matrix sizes are memory - bound, with performance close to the limit set by the memory bandwidth. Large matrix sizes are compute - bound, reaching 50 - 85% of the peak tensor - core throughput. Except for the small matrix size on workstation - grade GPUs, ccglib is much faster than the theoretical maximum of normal single - precision cores."
    },
    "256": {
        "figure1": "2505.03269v1_medical_realtime",
        "label1": "fig:medical_realtime",
        "caption1": "Performance of beamforming for ultrasound. The number of voxels ranges from three orthogonal planes of $128\\!\\times\\!128$ each, to the full $128^3$ data volume. The horizontal dash-dotted line indicates the minimum number of frames per second required for real-time performance.",
        "text": "In Fig.~\\ref{fig:medical_realtime} we show the number of frames per second that the TCBF can sustain on different GPUs. The processing includes the 1-bit packing and transpose of the measurement matrix. It excludes these steps for the model matrix, as this typically happens once before the experiment and does not need to be repeated. For a set of three orthogonal planes, all three GPUs can easily sustain the required real-time frame rate of 1000 frames per second. None of the GPUs can process the full $128^3$ data volume in real time, although the GH200 is capable of processing $\\sim85\\%$ of the voxels in real time. Reducing for example the number of frequencies from 128 to 64 would make real-time processing of the full data volume possible for both the A100 and GH200.",
        "summarize_figure": "2505.03269v1_medical_realtime",
        "summarization": "The picture shows the performance of ultrasound beamforming. The horizontal axis is the number of voxels, and the vertical axis is the number of frames per second. Three curves correspond to three GPUs: GH200, A100, and AD4000. The horizontal dash - dotted line indicates the minimum number of frames per second required for real - time performance. For the number of voxels in three orthogonal planes, all three GPUs can easily maintain a real - time frame rate of 1000 frames per second. For the full 128³ data volume, none of the GPUs can achieve real - time processing, but the GH200 can process about 85% of the voxels in real time. Reducing the number of frequencies can enable the A100 and GH200 to process the full data volume in real time."
    },
    "257": {
        "figure1": "2505.03280v1_Inventory-I",
        "label1": "fig:inventory_200",
        "caption1": "Inventory management case study with sensing cost $\\$200$",
        "text": "Our main takeaways are as follows. For sensing cost~\\$200, our results are shown in Figure~\\ref{fig:inventory_200}. In this case, we see that the heuristic policy (Section~\\ref{sec:heuristic}) performs quite close to the optimal policy (judging by the bound on sub-optimality gap) and the optimal policy for the truncated MDP $\\M_{k,N}$ outperforms it only after $N \\ge 7$. However, the conditions of Theorem~\\ref{thm:pi_N_optimal} are not satisfied over the depths $N$ we were able to compute for (recall that the computational complexity of solving~$\\M_{k,N}$ grows exponentially in~$N$). This is consistent with the results in Figure~\\ref{fig:inventory_200}; we continue to see small cost benefits from increasing the threshold on the number of blind actions allowed.\nOur main takeaways are as follows. For sensing cost~\\$200, our results are shown in Figure~\\ref{fig:inventory_200}. In this case, we see that the heuristic policy (Section~\\ref{sec:heuristic}) performs quite close to the optimal policy (judging by the bound on sub-optimality gap) and the optimal policy for the truncated MDP $\\M_{k,N}$ outperforms it only after $N \\ge 7$. However, the conditions of Theorem~\\ref{thm:pi_N_optimal} are not satisfied over the depths $N$ we were able to compute for (recall that the computational complexity of solving~$\\M_{k,N}$ grows exponentially in~$N$). This is consistent with the results in Figure~\\ref{fig:inventory_200}; we continue to see small cost benefits from increasing the threshold on the number of blind actions allowed.",
        "summarize_figure": "2505.03280v1_Inventory-I",
        "summarization": "The picture shows the results of an inventory management case study with a sensing cost of $200, including four sub - graphs corresponding to states 0, 1, 2, and 3. In each sub - graph, the blue solid line represents V*_{Ate,N}(s), the red dashed line is the lower bound of V**{Ate}(s), the green solid line is the situation of π*{ATM} on V*{Ate}(s), and the yellow solid line is SPI. The results show that the heuristic policy performs quite close to the optimal policy. The optimal policy for the truncated Markov decision process M{k,N} outperforms the heuristic policy only when the number of layers N≥7. Increasing the threshold on the number of allowed blind actions can bring small cost benefits."
    },
    "258": {
        "figure1": "2505.03280v1_Inventory-II",
        "label1": "fig:inventory_64",
        "caption1": "Inventory management case study with sensing cost $\\$64$",
        "text": "For the lower sensing cost of \\$64, our results are shown in Figure~\\ref{fig:inventory_64}. In this case, we see that the heuristic policy (Section~\\ref{sec:heuristic}), which does provide an improvement over always sensing, is in fact optimal for~$\\M_k$. Moreover, the optimal policy for $\\M_{k,1}$ is also found to be optimal for~$\\M_k$. However, the condition of Theorem~\\ref{thm:pi_N_optimal} is only satisfied at $N=7.$\nFor the lower sensing cost of \\$64, our results are shown in Figure~\\ref{fig:inventory_64}. In this case, we see that the heuristic policy (Section~\\ref{sec:heuristic}), which does provide an improvement over always sensing, is in fact optimal for~$\\M_k$. Moreover, the optimal policy for $\\M_{k,1}$ is also found to be optimal for~$\\M_k$. However, the condition of Theorem~\\ref{thm:pi_N_optimal} is only satisfied at $N=7.$",
        "summarize_figure": "2505.03280v1_Inventory-II",
        "summarization": "The picture shows the results of an inventory management case study with a sensing cost of $64, including four sub - graphs corresponding to states 0, 1, 2, and 3. In each sub - graph, the blue solid line represents V*_{Ate,N}(s), the red dashed line is the lower bound of V**{Ate}(s), the green solid line is the situation of π*{ATM} on V*{Ate}(s), and the yellow solid line is SPI. The results show that the heuristic policy improves compared to always sensing and is optimal for M_k. The optimal policy for M{k,1} is also optimal for M_k. However, the condition of Theorem thm:pi_N_optimal is only satisfied when the number of layers N = 7."
    },
    "259": {
        "figure1": "2505.03283v1_control",
        "label1": "fig:gen_con",
        "caption1": "\\footnotesize General control architecture proposed for autonomous steering of robots in cluttered dynamic SaR environments.",
        "text": "The control architecture that is proposed for steering a robot in dynamic cluttered SaR environments is illustrated in Figure~\\ref{fig:gen_con}:  In order to improve the computational efficiency and the responsiveness of the steering system of the robot,  both highly crucial for SaR missions \\cite{hierarchicalmpc,basescu2020direct},  we opt to separate the steering system  into a motion planning system and an optimal motion tracking system, which must follow the trajectory  that is generated by the motion planning system as closely as possible.  The output of the motion planning system (shown via a green box in Figure~\\ref{fig:gen_con})  is injected, as the reference trajectory, into the optimal motion tracking system (shown via a gray box in Figure~\\ref{fig:gen_con}),  which will request the motion planning system to re-plan the trajectory whenever needed.%\nIn practice, especially in cluttered SaR environments that are prone to various  uncontrolled stimuli, the dynamics of the obstacles cannot be perfectly captured via mathematical models.  Additionally, the perceived position of dynamic obstacles based on the images of the camera is prone to errors  (Assumption~\\ref{ass:imperfect_state_perception}).  Therefore, the path planning approach must be made robust to these uncertainties.  This is guaranteed if the robot safely navigates in the environment even when  maximum uncertainties are realized, i.e., when the forbidden areas (hollow circles around each dynamic obstacle in Figure~\\ref{fig:heuristic_path_planning}) are expanded considering the maximum  modeling or perception error.  Moreover, the cumulative errors enhance the uncertainty of the predictions. %(Figure~\\ref{fig:dyntight}).  This effect is incorporated by increasing the upper value of the errors according to the proximity of the  prediction to the current time.  In fact, a larger prediction window allows to incorporate and assess longer-term impacts of current control  inputs, which increases the chances of a safe and optimal mission and decreases the risk of recursive infeasibility.  However, in addition to heavier online computations,  prediction in larger windows leads to larger cumulative errors and thus risks to safety and degrading the performance.  This further motivates the introduction of a bi-level control architecture  that generates a reference path that approximately provides safety and  desirable performance, and that delegates the obstacle avoiding task to a reference tracking control system  (see Figure~\\ref{fig:gen_con}).    Whenever the local reference tracking control problem becomes  infeasible or the performance criteria falls under desirable thresholds,  the motion planning system  updates the reference path, based on the updated information. Meanwhile, the optimal motion tracking system keeps the motions of the robot safe and crash-free.%",
        "summarize_figure": "2505.03283v1_control",
        "summarization": "The figure shows a general control architecture for the autonomous steering of robots in complex dynamic search and rescue (SaR) environments. It consists of multiple subsystems. The supervisory system allocates tasks. The assessment and objective - setting system receives new goals and constraints and feeds back evaluations and constraint status. The path - planning system generates paths and provides reference trajectories for the optimal path - tracking system. The optimal path - tracking system, including the robot's dynamic model and constraints and the obstacle - avoidance system, controls the robot's motion via a position/speed controller. The perception system, with sensors and a state estimator, provides environmental information, and the communication system enables information exchange. These subsystems cooperate to enhance the computational efficiency and responsiveness of robot navigation in uncertain environments."
    },
    "260": {
        "figure1": "2505.03285v1_compare",
        "label1": "fig:embedding",
        "caption1": "Visualization of embeddings with the different head entities and relations using t-SNE under the settings of SimKGC and SRP-KGC. In the visualization, points with the same color represent embeddings that share the same target tail entity. ",
        "text": "The experiments demonstrate that SRP-KGC improves upon the best-performing method by 1.1\\% and 1.5\\%, respectively, in these two tasks. Through this approach, the model is able to effectively capture the underlying patterns of reasoning paths, demonstrating strong generalization ability even when handling previously unseen entities. We compared the discriminative features obtained from different head entities pointing to the same tail entity through different paths and validated the effectiveness of incorporating multiple types of positive samples into the training process to enhance the model's ability to understand reasoning paths. In the experiment, we selected 10 tail entities that are highly relevant to triples from the FB15k-237 test set. For these tail entities and their related triples, we performed path searches and combined the resulting paths with their corresponding head entities. Subsequently, we encoded these combinations using BERT models trained with SimKGC and SRP-KGC. We visualized the encoded outcomes using t-SNE in Figure \\ref{fig:embedding}. The visualization shows that after training with SRP-KGC, the embeddings for the same tail entity are significantly closer in the embedding space, demonstrating better feature discriminability. To validate the effectiveness of the soft reasoning path, we designed a series of comparative experiments, focusing on its performance in different scenarios. Specifically, we conducted comparisons under two conditions based on the existence of reasoning paths, and analyzed the impact of introducing the soft reasoning path:     Testing settings & MRR   & Hits@1   & Hits@3   & Hits@10 \\\\     R     & 26.9  & 18.0  & 29.6  & 44.1 \\\\     RS    & \\textbf{33.2} & \\textbf{24.4} & \\textbf{36.5} & \\textbf{51.0} \\\\",
        "summarize_figure": "2505.03285v1_compare",
        "summarization": "The figure visualizes the embeddings of different head entities and relations using t - SNE under the SimKGC setting. Points of the same color represent embeddings with the same target tail entity. The scattered distribution of colored point clusters in the coordinate space shows that embeddings of different tail entities are distinctly separated. This indicates that the model can distinguish different tail entities in this setting. However, the distribution also implies that there may be room for improvement in the clustering of embeddings for the same tail entity, which relates to the experiment's exploration of the model's understanding of reasoning paths and feature discriminability."
    },
    "261": {
        "figure1": "2505.03288v1_episodic_reward_RL8_R1_map2_ep100_noisy",
        "label1": "fig:8MARL_reward",
        "caption1": "Episodic reward for each RL based agent throughout the training. Each episode represents passing through the full market data. We plot the rewards divided by zone, each point represent the average reward during an episode and the shaded region the standard deviation.",
        "text": "We start by analyzing the convergence and stability of the multi-agent simulation. Our settings are particularly complicated, since the market environment is dependent on all agents decisions. We tested two settings for the MARL algorithm: one with 4 learning based producers, and one with 8. In the first case, the 4 other producers always bid their marginal price. The second case creates a very dynamic environment, since at every step the price of the market is decided based on the actions of 8 independent RL agents. To assess the stability and convergence, we consider the reward of each agent throughout the training. More precisely, we compute the average reward during an episode, this being a full pass through the data. With this data we obtain Fig.~\\ref{fig:8MARL_reward}, where we plot the episodic average reward for each agent divided by zone, considering the case with only RL based producers. Notice that the reward is not the profit, as we mention in the full description of the algorithm in the Appendix. From Fig.~\\ref{fig:8MARL_reward} we can see that each agent increases its reward during the initial episodes and eventually plateaus to a stable value. These results are particularly interesting, since they show how the algorithm we use is well suited for a medium scale simulation with competitive learning agents, providing results that we can compare with the classical solutions.",
        "summarize_figure": "2505.03288v1_episodic_reward_RL8_R1_map2_ep100_noisy",
        "summarization": "The figure shows the episodic rewards of RL - based agents during training. The horizontal axis is the number of episodes, and the vertical axis is the reward value. Different colored lines represent different agents, with shaded areas indicating standard deviations. In the Germany scenario, the reward values of each agent increase initially and then gradually stabilize at a relatively constant value. This indicates that during training, the agents can learn and optimize their behavior, ultimately achieving stable reward performance in the market environment, demonstrating the applicability of the used algorithm in medium - scale simulations of competitive learning agents."
    },
    "262": {
        "figure1": "2505.03288v1_double_avg_cost",
        "label1": "fig:avg_cost",
        "caption1": "Average cost of running the market, obtained with different algorithms.",
        "text": "We start by analyzing the average cost to run the market. This value is computed by summing the average profits of each producer over the full data available. In particular, for the MARL producers, we consider the cost for the final episode of their training, where we assume they reached stable bidding strategies. The results are reported in Fig.~\\ref{fig:avg_cost}. We can immediately see how the two exact algorithms produce higher costs than the learning-based counterpart, which can be explained by the form of the constraints, as detailed above, and by the lack of actual competition. In the integrated solution, the problem is solved by an exogenous external coordinator, so by definition there will be no competition; while for the best response, each producer is free to update its decision variables uncoupled from the others. We can see the average cost for the MARL simulation with only 4 learning producers is very low, but this is also affected by the presence of static producers that always bid at marginal cost, driving the electricity price down.\nTo quickly evaluate all of these values we use the MARL simulations, in both cases (4 and 8 RL agents). We report in Fig.~\\ref{fig:avg_cost_export} the average cost to run the market, computed in the same way as for Fig.~\\ref{fig:avg_cost}, for the range of constraints values. In particular, we trained the agents for the two cases with $c_G=0.4,c_A=0.04$, and used them on all other values. When analyzing these results, the actual costs are not particularly relevant, as they depend on the demand and other external factors, while the trend of increase/decrease is what we have to interpret.",
        "summarize_figure": "2505.03288v1_double_avg_cost",
        "summarization": "The figure shows the average cost of running the market using different algorithms. The horizontal axis lists four algorithm categories: 4RL, 8RL, BR, and POT, while the vertical axis represents the average cost (Avg. Cost). The BR algorithm has the highest average cost, followed by the POT algorithm. The 8RL algorithm's average cost is lower than that of BR and POT but higher than 4RL, with 4RL having the lowest average cost. The results indicate that the two exact algorithms (BR and POT) incur higher costs than the learning - based algorithms (4RL and 8RL), likely due to constraint forms and lack of competition. The 4RL algorithm is influenced by static producers bidding at marginal cost, reducing electricity prices and thus the average cost."
    },
    "263": {
        "figure1": "2505.03288v1_average_cost_export_range_MARL",
        "label1": "fig:avg_cost_export",
        "caption1": "Average cost of running the market with 8 and 4 RL agents, evaluated over different values for the export constraints. Each element on the x-axis corresponds to a couple of export constraints: \\texttt{G} stands for Germany and \\texttt{A} for Austria, the constraints are in MW.",
        "text": "To quickly evaluate all of these values we use the MARL simulations, in both cases (4 and 8 RL agents). We report in Fig.~\\ref{fig:avg_cost_export} the average cost to run the market, computed in the same way as for Fig.~\\ref{fig:avg_cost}, for the range of constraints values. In particular, we trained the agents for the two cases with $c_G=0.4,c_A=0.04$, and used them on all other values. When analyzing these results, the actual costs are not particularly relevant, as they depend on the demand and other external factors, while the trend of increase/decrease is what we have to interpret.",
        "summarize_figure": "2505.03288v1_average_cost_export_range_MARL",
        "summarization": "The figure shows the average cost of running the market with 4 and 8 RL agents under different export constraints. The horizontal axis represents export constraints, with G for Germany and A for Austria, in MW, and the vertical axis is the average cost. The average cost of 8RL Germany shows a significant downward trend with changes in export constraints and is notably higher than the other three curves. The average costs of 4RL Germany, 4RL Austria, and 8RL Austria are relatively low and change gently. This indicates that export constraints have a greater impact on the average cost of 8RL Germany and a smaller impact on the other cases."
    },
    "264": {
        "figure1": "2505.03307v1_stabilizer_order",
        "label1": "fig:rank",
        "caption1": "The order $n'$ on four generators is shown as four black lines, ranging from 1 to $4^4$. The gate application's order is from qubit $0^{\\text{th}}$ to qubit $(n-1)^{\\text{th}}$, left to right. We are considering the worst cases by using 4-qubit $|XYZ+\\text{chain}\\rangle$ ansatz, the circuit representation can be found in Fig.~\\ref{fig:operator} (b).",
        "text": "Applying non-Clifford gates increases the stabilizer rank of a stabilizer state $|\\psi\\rangle$, as generators evolve from single Pauli strings into linear combinations of Pauli strings. When non-Clifford gates act on every qubit, a generator initially represented as a single Pauli string becomes a sum of $n'$ Pauli strings, where $n'$ can grow to $4^n$. Subsequent gates require iterating over all $n'$ Pauli strings, making the computation significantly slower than the state-vector approach which processes $2^n$-complex amplitudes. Figure~\\ref{fig:rank} illustrates the increasing complexity of generators due to non-Clifford and two-qubit gate applications.",
        "summarize_figure": "2505.03307v1_stabilizer_order",
        "summarization": "The figure shows the change in the order n' of four generators (represented by four black lines, ranging from 1 to 4⁴) as the number of gates increases, under the worst - case scenario using a 4 - qubit |XYZ+chain⟩ ansatz. The horizontal axis is the number of gates, and the vertical axis is the order. Vertical lines of different colors correspond to the cases of 1, 2, and 3 layers. As the number of gates increases, the order rises rapidly and then levels off after reaching a certain number of gates. This reflects that applying non - Clifford and two - qubit gates increases the complexity of generators, evolving them from single Pauli strings into linear combinations of Pauli strings, thus increasing computational complexity."
    },
    "265": {
        "figure1": "2505.03307v1_operatoring",
        "label1": "fig:operator",
        "caption1": "(a) A quantum circuit can be divided into $\\{U_k\\},\\{V_k\\}$, and end up with $U_{K-1}$ or $V_{K'-1}$ (b) $XYZ+W_{\\text{chain}}$ topology.",
        "text": "We present gates as instructions similar to Qiskit \\cite{javadiabhari2024quantumcomputingqiskit}. Each instruction is a tuple $\\{g, w, \\theta\\}$, representing the gate name, qubit (wire), and parameter value (if applicable), respectively. For gate without parameters, $\\theta$ is set as $0$.  Performing gate-by-gate on stabilizers leads to low performance; hence, we consider performing operators instead, where each operator contains gates of the same type. A quantum circuit is partitioned into $K$ $U_k$ (only 1-qubit gates) and $K'$ $V_k$ (only 2-qubit gates) because the 2-qubit gate has different behavior from other gates. These operators interleaved each other, as presented in Fig.~\\ref{fig:operator}, obey $|K-K'|\\leq 1$. The operator sequence is determined by the first instruction: if it is a single-qubit gate, the order will be $U_0, V_1, U_1, V_1,\\ldots$; otherwise, it starts with $V_0$. Within each $U_k$, gates are further grouped by qubit, with $U_{k,j}$ denoting instructions in the $k^{\\text{th}}$ single-qubit operator acting on the $j^{\\text{th}}$ qubit. By grouping gates, there is no difference between Clifford and non-Clifford gates.\nComparable software includes Pennylane 0.41.0 \\cite{bergholm2022pennylaneautomaticdifferentiationhybrid} and Qiskit 1.1.1 \\cite{javadiabhari2024quantumcomputingqiskit}, both accelerated with NVIDIA’s cuQuantum SDK (lightning.gpu 0.41.0 and cusvaer\\_simulator\\_statevector 0.13.3, respectively). Experiments were conducted on an Intel i9-10940X CPU (3.30 GHz) and an NVIDIA RTX 4090 GPU. Execution time was measured from the initialization of stabilizer generators to the computation of final generators. Each experiment was run at least 10 times, and the average execution time was calculated. The benchmarked circuits include the GHZ circuit (multi-qubit entangled state), Graph circuit (graph-based quantum state), and $|XYZ+chain\\rangle$, as shown in Figure\\ref{fig:operator} (b). The circuit structure consists of $L$ layers, with the XYZ component repeated $\\#\\text{Repeats}$ times within each layer. Experiments varied $\\#\\text{Repeats}$ from 100 to 100,000 to evaluate Qimax’s scalability. The \\#Gates increases linearly with $\\#\\text{Repeats}$ and \\#Qubits. The average stabilizer rank of generators, which impacts performance, is plotted in Figure\\ref{fig:rank2}; higher ranks correspond to lower performance in Qimax due to increased computational complexity.",
        "summarize_figure": "2505.03307v1_operatoring",
        "summarization": "The figure shows the structure of a quantum circuit. In Figure (a), a quantum circuit can be divided into {Uk} (containing only single - qubit gates) and {Vk} (containing only two - qubit gates), which are interleaved and satisfy |K - K'|≤1. The operation order is determined by the first instruction. Blue dots represent single - qubit gates, and blue rectangles represent two - qubit gates, with the circuit ending in UK - 1 or VK' - 1. Figure (b) shows the |XYZ+Wchain⟩ topology, including CX gates and Rx, Ry, Rz rotation gates, repeated according to the number of layers (#Layers) and repeats (#Repeats). These divisions and structures help in understanding the combination and operation order of different types of gates in a quantum circuit, which is crucial for analyzing quantum computing performance."
    },
    "266": {
        "figure1": "2505.03307v1_rank",
        "label1": "fig:rank2",
        "caption1": "The average stabilizer rank between circuit, while $|XYZ+chain\\rangle$ and max stabilizer rank increase exponential ($\\mathcal{O}(2^n)$ and $\\mathcal{O}(4^n)$ ), GHZ and Graph are fixed as $1$.",
        "text": "Comparable software includes Pennylane 0.41.0 \\cite{bergholm2022pennylaneautomaticdifferentiationhybrid} and Qiskit 1.1.1 \\cite{javadiabhari2024quantumcomputingqiskit}, both accelerated with NVIDIA’s cuQuantum SDK (lightning.gpu 0.41.0 and cusvaer\\_simulator\\_statevector 0.13.3, respectively). Experiments were conducted on an Intel i9-10940X CPU (3.30 GHz) and an NVIDIA RTX 4090 GPU. Execution time was measured from the initialization of stabilizer generators to the computation of final generators. Each experiment was run at least 10 times, and the average execution time was calculated. The benchmarked circuits include the GHZ circuit (multi-qubit entangled state), Graph circuit (graph-based quantum state), and $|XYZ+chain\\rangle$, as shown in Figure\\ref{fig:operator} (b). The circuit structure consists of $L$ layers, with the XYZ component repeated $\\#\\text{Repeats}$ times within each layer. Experiments varied $\\#\\text{Repeats}$ from 100 to 100,000 to evaluate Qimax’s scalability. The \\#Gates increases linearly with $\\#\\text{Repeats}$ and \\#Qubits. The average stabilizer rank of generators, which impacts performance, is plotted in Figure\\ref{fig:rank2}; higher ranks correspond to lower performance in Qimax due to increased computational complexity.",
        "summarize_figure": "2505.03307v1_rank",
        "summarization": "The figure shows the change in the average stabilizer rank of different types of quantum circuits with the number of qubits (#Qubits). The blue curve represents |ZXZ + chain⟩, and the orange curve represents Max. Their average stabilizer ranks increase exponentially with the number of qubits, being O(2ⁿ) and O(4ⁿ) respectively. The green curve represents GHZ/Graph, whose average stabilizer rank is fixed at 1 and does not change with the number of qubits. This indicates that quantum circuits with different structures have significantly different stabilizer - rank characteristics, which also have different impacts on quantum computing performance."
    },
    "267": {
        "figure1": "2505.03308v1_generative_model_nf_no_metacog",
        "label1": "fig:feedback_matrices",
        "caption1": "Generative process and generative model : the subject's brain models the neurofeedback environment to predict outcomes and optimize its actions. Remarkably, BCI training casts cognitive states as external \"environment\" variables and includes them in its generative model. It is thus necessary to make a distinction between the hidden states themselves and how they are perceived internally by the subject. The agent can only interact with its environment through boundary states (its sensory and active states).",
        "text": "A key tenet of AIF is the distinction between the generative process—the actual, often unobservable, dynamics of the environment generating sensory data—and the agent's internal generative model—a probabilistic, approximate representation of this process (see Figure \\ref{fig:feedback_matrices}). The agent interacts with the external world solely through its sensory inputs (observations) and motor outputs (actions), which constitute the statistical boundary known as a Markov blanket. Consequently, inferring the hidden states of the environment ($s^*$) based on observations ($o$) is fundamentally an inference problem, constrained by the information available at this boundary. The brain must entertain beliefs about the hidden causes of its sensations and update these beliefs in a Bayesian manner. Selecting actions that either reduce uncertainty about the world (exploration) or lead to preferred outcomes (exploitation) allows the agent to maintain accurate beliefs and ensure its continued existence. The inherent limitations on computational resources necessitate a balance between model accuracy and complexity, aligning with principles like Occam's razor, which is implicitly handled by the  VFE minimization \\cite{Murray2005}.",
        "summarize_figure": "2505.03308v1_generative_model_nf_no_metacog",
        "summarization": "The figure shows the generative model and the generative process. The generative model is the subject's brain model of the environment, including beliefs about the control state (model state), obtaining feedback observations (sensory states) through perception, and performing action planning and mental actions (active states). The generative process is the neurofeedback training environment, including true cognitive dynamics, generating sensations through the BCI pipeline and feeding them back to the subject. The subject interacts with the environment through a Markov blanket, distinguishing between hidden states and their internal perception in the generative model to predict outcomes and optimize actions."
    },
    "268": {
        "figure1": "2505.03308v1_MDP",
        "label1": "fig:ai_process",
        "caption1": "The Partially Observable Markov Decision Process (POMDP) used to model training. The environment (generative) process figures a set of hidden states ($\\mathbf{\\hat{s}_t}$), corresponding to the subject \"actual\" cognitive states in our formulation. Although impossible to see directly, each state stochastically generates stimuli observable by the subject ($o_t$) depending on a true observation function $\\mathbf{A}$. It may consist in some external feedback or in an interoceptive observer the subject must learn to interpret. Possible state transitions ($\\mathbf{B}$) and starting states ($\\mathbf{D}$) are fixed before training. The subject generative model of the cognitive regulation is shown above, featuring a set of perceived states  ($\\mathbf{s_t}$). The subject models the feedback as a realization of hidden states with the function $\\mathbf{a}$, and the effect of his/her mental actions $u_t$ on those states with the function $\\mathbf{b}$. The Active Inference agent uses this formulation to update their model on two distinct timescales by minimizing their Free Energy following the equations described in \\cite{sophisticated_inference,da_costa_active_2020}. On the timestep timescale, it infers the hidden states that best match observations and prior beliefs, and the actions that best match its habits/preferences/exploration drive. On the trial timescale, the agent updates its beliefs about the environment dynamics ($\\mathbf{a}$,$\\mathbf{b}$,$\\mathbf{d}$) to better predict and navigate it.",
        "text": "To apply AIF to NF training, we model the subject-environment interaction using a discrete Partially Observable Markov Decision Process (POMDP), as depicted in Figure \\ref{fig:ai_process}. This formalism allows us to explicitly differentiate between the true environmental dynamics (generative process) and the subject's internal representation (generative model).",
        "summarize_figure": "2505.03308v1_MDP",
        "summarization": "The figure shows the Partially Observable Markov Decision Process (POMDP) for training modeling. The environment (generative) process has a set of hidden states (ŝt ), corresponding to the subject's \"actual\" cognitive states. These states, though unobservable directly, stochastically generate stimuli (ot ) observable by the subject according to the true observation function A. State transitions (B ) and starting states (D ) are fixed before training. The subject's generative model of cognitive regulation includes a set of perceived states (st ). The subject models feedback as the realization of hidden states via function a and the effect of mental actions ut on these states via function b. The Active Inference agent uses this to update the model on two timescales by minimizing free energy, inferring hidden states and actions matching observations and prior beliefs on the timestep timescale, and updating beliefs about environmental dynamics (a, b, d ) on the trial timescale to better predict and navigate the environment."
    },
    "269": {
        "figure1": "2505.03308v1_NFModel",
        "label1": "fig:layout_full",
        "caption1": "Full model.  The environment (generative process) forwards observations to the subject as a discrete feedback value (here $o_t=4$) and reacts to subject actions $u_t \\sim \\Pi_t$. In our simulations, the process state topography was a simple graph with 5 possible states and 5 possible ooutcomes. Possible state transitions are showed with blue arrows ($\\mathbf{B}$) and starting state with red arrows ($\\mathbf{D}$). Each state may generates outcomes based on the feedback mapping ($\\mathbf{A}$) following a normal law $N(\\mu,\\sigma_{process})$ (unbiaised noisy feedback). Initially, the agent \\textit{believes} each state is correlated with a specific feedback level following a normal law $N(\\mu,\\sigma_{model})$ (unbiaised noisy feedback). Given these priors, the agent attempts to learn state transitions and starting values ($\\mathbf{b}$ and $\\mathbf{d}$) in order to achieve higher feedback levels. This is done in 3 steps : the subject uses its priors to \\textbf{1.} infer the hidden states, \\textbf{2.} pick the best actions (for exploratory or exploitative purposes) and finally \\textbf{3.} update its mapping from the observed succession of actions/observations.",
        "text": "To explore NF dynamics using this framework, we simulated AIF agents engaged in a task requiring the regulation of a single cognitive dimension, represented by discrete latent states $\\hat{s}$. The agents received feedback signal(s) $\\mathbf{o}$ and possessed prior preferences $\\mathbf{c}$ favoring higher feedback values. The objective was to learn a sequence of mental actions $u_t$ to transition from initial, less preferred states towards target states associated with higher feedback, potentially overcoming initially inaccurate models of perception ($\\mathbf{a}$) and control ($\\mathbf{b}$). Figure \\ref{fig:layout_full} illustrates the general model structure. \\\\",
        "summarize_figure": "2505.03308v1_NFModel",
        "summarization": "The figure shows the full subject generative model. The environment (generative process) sends observations as discrete feedback values (e.g., ot = 4) to the subject and reacts to the subject's actions ut. In the simulation, the process state topography is a simple graph with 5 possible states and 5 possible outcomes. Blue arrows represent possible state transitions (B ), and red arrows represent the starting state (D ). Each state generates outcomes according to the feedback mapping (A ) following a normal distribution N(μ, σprocess) (unbiased noisy feedback). Initially, the subject believes each state is correlated with a specific feedback level following a normal distribution N(μ, σmodel) (unbiased noisy feedback). Based on priors, the subject learns in three steps: 1. infer hidden states; 2. select the best actions (for exploration or exploitation); 3. update the mapping based on the observed sequence of actions/observations to achieve higher feedback levels."
    },
    "270": {
        "figure1": "2505.03309v1_Majda",
        "label1": "Fig.Majda",
        "caption1": "Computation of the periodic vortex sheet after singularity by means of a desingularization of the kernel \\eqref{Eq.kernel-K2}. The computation is shown at $t=3.9$, well after the initial singularity. Note the approximation to a double-branched spiral. This picture is \\cite[Figure 9.3]{MB}.",
        "text": "A longstanding open problem in the field concerns the behavior of the vortex sheet after singularity formation. Generally, one expects vortex sheet roll-up after the initial singularity \\cite{MB,MP,Moore1979,MS1973,Saffman1992}. In \\cite{Krasny1986}, Krasny presented compelling numerical evidence that a double-branched spiral vortex sheet forms after singularity formation for periodic perturbations of a straight sheet, as illustrated in Figure \\ref{Fig.Majda}, which is taken from \\cite{MB} (see also \\cite[Figure 3]{Krasny1986}). Another numerical result by Krasny \\cite{Krasny1987} showed that the roll-up is in good agreement with Kaden’s asymptotic spiral \\cite{Kaden}, an infinite-length algebraic spiral. Based on Pullin’s numerical results on self-similar algebraic spiral vortex sheets \\cite{Pullin1978,Pullin1989} and the method of asymptotic analysis, Pullin and Shen \\cite{PS2023} constructed a vortex sheet solution after its singularity time and observed the tearing-up behavior. Moreover, the recent work of Wu \\cite{Wu2006} indicates that, after the singularity time, the vortex sheet will fail to be in the class of chord-arc curves that do not roll up too fast near the singularity. This naturally leads one to study self-similar algebraic spiral-shaped vortex sheets, for which no rigorous existence was known prior to the present work. Let us mention recent works concerning the construction of self-similar algebraic spiral vortex patches, as reported in \\cite{E1, E2, GG, SWZ}.",
        "summarize_figure": "2505.03309v1_Majda",
        "summarization": "The figure depicts the periodic vortex sheet after singularity at t=3.9, computed via kernel desingularization, approximating a double - branched spiral. A long - standing open problem is the vortex sheet's post - singularity behavior. Generally, roll - up is expected. Krasny's numerical work shows a double - branched spiral forms for periodic straight - sheet perturbations, aligning with Kaden's asymptotic spiral. Pullin and Shen constructed a post - singularity solution and observed tearing - up. Wu's work indicates the vortex sheet leaves the chord - arc curve class post - singularity, driving research on self - similar algebraic spiral - shaped vortex sheets, whose existence was unproven before."
    },
    "271": {
        "figure1": "2505.03311v1_end_to_end_gnn",
        "label1": "end_to_end_l",
        "caption1": "setup{font=footnotesize} \\caption{This diagram illustrates the architecture of an end-to-end GNN.",
        "text": "The architecture of an end-to-end GNN is as shown in Fig. \\ref{end_to_end_l}. The process begins with the input feature matrix $\\mathbf{H}$, where each row represents a feature vector of a node. This feature matrix is processed by the GNN, which takes the graph's vertices $\\mathcal{V}$, edges $\\mathcal{E}$, and adjacency matrix $\\mathbf{A}$ as inputs. The GNN layers iteratively update and propagate edge features based on their neighboring edges, capturing the structural information of the graph. The bottom  right section of the diagram shows a loss function $\\mathcal{L}(\\boldsymbol{\\Theta})$, which evaluates the model's performance by considering the graph structure and node features. We use the negative value of our objective function as the loss function of our training, specifically setting the loss function to where $\\mathcal{D}$ is the size of the training batch.",
        "summarize_figure": "2505.03311v1_end_to_end_gnn",
        "summarization": "The figure shows the architecture of an end - to - end Graph Neural Network (GNN). The process starts with the input feature matrix H, where each row represents a node's feature vector. The GNN takes the graph's vertices V, edges E, and adjacency matrix A as inputs, iteratively updating and propagating edge features through its layers to capture the graph's structural information. The bottom - right corner shows the loss function L(Θ), which evaluates the model's performance by considering the graph structure and node features. In training, the negative value of the objective function is set as the loss function, with D being the training batch size. Parameters are updated via backpropagation to minimize the loss function and optimize the model."
    },
    "272": {
        "figure1": "2505.03311v1_end_to_end_GNN1",
        "label1": "end_to_end",
        "caption1": "setup{font=footnotesize} \\caption{Downlink LEO satellite communication system with $K$ single-antenna UTs and its graph representation.  ",
        "text": "As shown in Fig. \\ref{end_to_end}, this graph structure provides a detailed visual and mathematical representation of the communication links, enabling analysis and optimization of the downlink transmission from the satellite to the UT.",
        "summarize_figure": "2505.03311v1_end_to_end_GNN1",
        "summarization": "The figure shows the downlink of a Low Earth Orbit (LEO) satellite communication system with K single - antenna User Terminals (UTs) and its graph representation. The left figure is the graph representation, showing the channel coefficients hij between N antennas on the satellite and K UTs, reflecting the connection of communication links. The right figure is the real - world scenario, where the satellite communicates with various UTs on the ground, including people and vehicles, via a Uniform Planar Array (UPA). This graph structure provides an intuitive and mathematical way to analyze and optimize the downlink transmission from the satellite to the UTs."
    },
    "273": {
        "figure1": "2505.03311v1_Detailed_process",
        "label1": "end_to_end_net",
        "caption1": "setup{font=footnotesize} \\caption{Detailed process of the  edge $(n,k)$.",
        "text": "In the combination phase, for each edge $(n,k)$ at layer $\\ell$,we start by extracting the previous layer's edge feature $d^\\ell$ and $a_{nk}^{(\\ell-1)}$  \ta_{nk}^{(\\ell)} = \\mathbf{\\sigma }\\{\\mathbf{MLP}_3^{(\\ell)}(a_{nk}^{(\\ell-1)}, d^{\\ell})\\}, where it describes the combination process to update the edge feature $a_{nk}$ at the current layer $\\ell$. Here, $\\mathbf{MLP}_{3}$ is an MLP that combines the edge feature from the previous layer $a_{nk}^{(\\ell-1)}$ with the aggregated feature $d^\\ell$. $\\sigma$  is an activation function. Detailed process of the  edge $(n,k)$ is shown in Fig. \\ref{end_to_end_net}. The result, $a_{nk}^{(\\ell)}$ is the updated edge feature that incorporates information from both the previous edge feature and the aggregated neighborhood information, enhancing the overall representation at the current layer, as shown in Algorithm \\ref{end_end}.",
        "summarize_figure": "2505.03311v1_Detailed_process",
        "summarization": "The figure shows the detailed processing of edge (n, k). First, the previous - layer edge feature a(nk)^(ℓ - 1) and the relevant set of neighboring edge features are processed by Multi - Layer Perceptrons (MLPs) MLP1^ℓ and MLP2^ℓ respectively, and then aggregated through the aggregation function AGG^ℓ to obtain d^ℓ. Next, a(nk)^(ℓ - 1) and d^ℓ are combined through MLP3^ℓ and integrated via the activation function σ, ultimately resulting in the updated edge feature a(nk)^ℓ at the current layer. This process combines the previous - layer edge feature with the aggregated neighborhood information, enhancing the overall representation at the current layer."
    },
    "274": {
        "figure1": "2505.03323v1_figure1",
        "label1": "fig1",
        "caption1": "Difference in validation makespan over the course of training between each algorithmic component and DQN, shown separately for each instance size. Each subplot corresponds to a specific instance size. The top row displays results for the FJSP, and the bottom row for the JSSP.",
        "text": "To answer the first question, we began by examining the training convergence of Rainbow and its individual components compared to DQN. Figure~\\ref{fig1} illustrates the difference in average validation makespan between each extension and the DQN, computed over 100 validation instances at each target network update. Since the objective function is to minimize the makespan, negative values indicate improved performance over the DQN, while positive values indicate worse performance. Overall, several algorithmic components showed improved convergence behavior than DQN, notably Dueling Networks, Distributional RL, Multi-step learning and Noisy Networks. However, convergence speed varied considerably across components. For example, the PER initially converged slower than the DQN during the first 1000 to 2000 epochs, whereas the Multi-step learning model showed a faster early convergence. Another interesting aspect is that the full Rainbow implementation did not appear to generate any significant improvements over the best-performing individual components.",
        "summarize_figure": "2505.03323v1_figure1",
        "summarization": "The figure shows the difference in validation makespan during training between various algorithmic components and the Deep Q-Network (DQN), presented separately for different instance sizes. The top row is for the Flexible Job Shop Scheduling Problem (FJSP), and the bottom row is for the Job Shop Scheduling Problem (JSSP). The horizontal axis is the number of training epochs (Epochs), and the vertical axis is the difference in validation makespan compared to DQN. Negative values indicate better performance than DQN, and positive values indicate worse performance. Several algorithmic components such as Dueling Networks, Distributional RL, Multi-step learning, and Noisy Networks show better convergence than DQN, but the convergence speeds vary. For example, Prioritized Experience Replay (PER) converges slower than DQN in the first 1000 - 2000 epochs, while the Multi-step learning model shows faster early convergence. Also, the full Rainbow implementation does not significantly outperform the best - performing individual components."
    },
    "275": {
        "figure1": "2505.03323v1_figure2",
        "label1": "fig2",
        "caption1": "Average validation makespan throughout training for each algorithmic component, shown separately for all instance sizes. Each subplot corresponds to a specific instance size. The top row displays results for the FJSP, and the bottom row for the JSSP.} \\vspace{10pt",
        "text": "To address the second question, we compared the training convergence of value-based algorithms against the baseline heterogeneous GNN model, trained with PPO (Figure~\\ref{fig2}). Each curve shows the average makespan across the validation dataset, measured at every target network update step (for the PPO, we evaluated the validation dataset every 10 iterations). We find that, in smaller problems, for both the JSSP and FJSP, most value-based algorithms outperformed PPO, which struggled to converge effectively. However, this pattern shifted in larger problems (15x10 and 20x10), where the PPO algorithm demonstrated faster and more stable convergence than most value-based methods.",
        "summarize_figure": "2505.03323v1_figure2",
        "summarization": "The figure shows the average validation makespan during training for various algorithmic components, presented separately for different instance sizes. The top row is for the Flexible Job Shop Scheduling Problem (FJSP), and the bottom row is for the Job Shop Scheduling Problem (JSSP). The horizontal axis is the number of training epochs (Epochs), and the vertical axis is the average validation makespan. Curves of different colors represent different algorithms, including Proximal Policy Optimization (PPO), Deep Q-Network (DQN) and its variants. In smaller - scale problems, for both JSSP and FJSP, most value - based algorithms outperform PPO, which has difficulty converging. However, in larger - scale problems (15x10 and 20x10), the PPO algorithm converges faster and more stably."
    },
    "276": {
        "figure1": "2505.03323v1_figure3",
        "label1": "fig3",
        "caption1": "Average training runtime (minutes) for each RL algorithm.",
        "text": "Although many value-based algorithms showed a generalization comparable to PPO,  it is important to understand whether they also compete in terms of computational efficiency. Figure~\\ref{fig3} illustrates the average training runtime (in minutes) for each method evaluated in the previous sections. Additionally, we report the percentage deviation in runtime relative to the PPO baseline. We found out that, with the exception of the Multi-step learning extension and the full Rainbow model, all other value-based methods required less training time than PPO. Notably, the DQN, Dueling Network and PER algorithms achieved approximately a 20\\% reduction in runtime, highlighting their efficiency in addition to their competitive performance.",
        "summarize_figure": "2505.03323v1_figure3",
        "summarization": "The figure shows the average training runtime (in minutes) for each reinforcement learning algorithm. The Rainbow algorithm has the longest average training time of 390.2 minutes, 77.4% higher than the Proximal Policy Optimization (PPO) algorithm; the NStep algorithm has an average training time of 272.6 minutes, 23.9% higher than PPO. However, algorithms such as Deep Q-Network (DQN), Dueling Network, and Prioritized Experience Replay (PER) have lower average training times than PPO, which are 165.6 minutes, 174.1 minutes, and 176.2 minutes respectively, 24.7%, 20.9%, and 19.9% lower than PPO. Except for the Multi-step learning extension and the full Rainbow model, most value - based algorithms have shorter training times than PPO, demonstrating higher computational efficiency."
    },
    "277": {
        "figure1": "2505.03328v1_sample_coverage",
        "label1": "fig: coverage",
        "caption1": "Our sample and the population of exporters in France}  \\centering \\includegraphics[width=.5\\textwidth]{pic/sample_coverage.png} \\begin{tablenotes} \\singlespacing \\footnotesize \\item Note: The Figure reports sample coverage of French exporters from the Orbis database and population numbers from \\citet{INSEE}. We investigate manufacturing exporters to avoid the inclusion of trade intermediaries and other services firms that do not professionally deliver goods to foreign markets. \\end{tablenotes",
        "text": "In Figure \\ref{fig: coverage}, we report a snapshot of our sample coverage. If we compare with national statistics offices, we have 87\\% of exporters out of the total of manufacturing exporters in the population of firms. Figures \\ref{fig: export intensity} and \\ref{fig: tfp distribution} focus on non-temporary exporters\\footnote{Please note that descriptive evidence and baseline analyses are based on the subsample of non-temporary exporters, whereas the description of the total sample includes temporary exporters as they are used for a few robustness checks in Section \\ref{sec: robustness}.} and show how our main variables of interest are distributed. We compute export intensity as a simple ratio between export and total revenues, thus bounded in a range $[0. 1]$. Notably, we observe that the distribution of export intensity has a long right tail because most firms export less than 10\\%. Only 1.3\\% exporters reach an intensity of $100\\%$, selling exclusively abroad. Let's move our attention to the TFP distribution in Figure \\ref{fig: tfp distribution}. We know that it is much more skewed than export intensity, and we had to transform it in logs to visualize the distribution. In line with previous literature, we have that exporters have, on average, a higher TFP. See Appendix Table \\ref{tab: export_premium_stats}. When we focus on exporters, as in Figure \\ref{fig: tfp distribution}, we find non-negligible evidence of firm-level heterogeneity. Even in logs and after excluding the subsample of non-exporters, the TFPS show a skewness to the right, where a fringe of the most efficient exporters is. In the rest of the paper, we want to explain how much of this heterogeneity can be explained by a different exposure to export intensity.",
        "summarize_figure": "2505.03328v1_sample_coverage",
        "summarization": "The figure shows the sample and population of French exporters. Among all exporters, the population number is 127,586 and the sample number is 102,841; among manufacturing exporters, the population number is 32,100 and the sample number is 28,017. Compared with national statistics offices, the sample covers 87% of the population of manufacturing exporters. The study focuses on non - temporary exporters and calculates export intensity (the ratio of export value to total revenue, ranging from [0, 1]). It is found that the distribution of export intensity has a long right tail, with most firms having an export intensity of less than 10%, and only 1.3% of exporters reaching an intensity of 100%. The Total Factor Productivity (TFP) distribution is more skewed than export intensity. After taking logarithms for visualization, it shows that exporters have a higher average TFP, and there is non - negligible firm - level heterogeneity."
    },
    "278": {
        "figure1": "2505.03328v1_exp_int_TFP",
        "label1": "fig:export_intensity_TFP",
        "caption1": "Export intensity and TFP} \\centering \\includegraphics[width=0.5\\textwidth]{pic/exp_int_TFP.png}  \\begin{tablenotes} \\singlespacing \\footnotesize \\item Note: The figure focuses on non-temporary exporters to show export intensity intervals on the x-axis and TFP distribution partitioned by quartiles on the y-axis. We observe that the last TFP quartiles have a bigger support when we move to higher-intensity intervals. Yet, TFP variation is relatively large along the export intensity distribution. \\end{tablenotes",
        "text": "Table \\ref{tab: intensity and size} and Figure \\ref{fig:export_intensity_TFP} investigate joint distributions. In Table \\ref{tab: intensity and size}, we have a distribution of export intensity across firm size categories, once considering each firm-year observation present in our sample. A description of the firm size categories is provided in Appendix Table \\ref{app_tab: variables}. In Figure \\ref{fig:export_intensity_TFP}, we consider once again firm-year observations and we plot TFP quartiles along export intensity intervals.",
        "summarize_figure": "2505.03328v1_exp_int_TFP",
        "summarization": "The figure focuses on non - temporary exporters, with the horizontal axis representing export intensity intervals and the vertical axis representing the Total Factor Productivity (TFP) distribution partitioned by quartiles. As the export intensity intervals increase, the proportion of higher TFP quartiles grows. However, the variation of TFP across the export intensity distribution is relatively large. This indicates a certain correlation between export intensity and TFP, with a higher proportion of firms with higher TFP in higher export intensity intervals, but significant differences in TFP distribution under different export intensities."
    },
    "279": {
        "figure1": "2505.03328v1_DRFIC_TFP",
        "label1": "fig:drf_TFP",
        "caption1": "Dose-response function of export intensity on TFP}  \\includegraphics[width=0.6\\textwidth]{pic/DRFIC_TFP.png} \\begin{tablenotes} \\singlespacing \\footnotesize \\item \\textit{Note:} The figure reports the dose-response function obtained after estimating Eq. \\ref{eq: drf}. The grey highlighted areas identify intervals of export intensity where the dose-response function is statistically different from zero using a 5\\% significance level. \\end{tablenotes",
        "text": "In the following paragraphs, we present the results after estimating Eq. (\\ref{eq: regression}). Our primary interest lies in studying the effect of export intensity on firm-level productivity. Central to our analysis is the dose-response function illustrated in Figure \\ref{fig:drf_TFP}, which is obtained by plugging the coefficients from column (3) of Table \\ref{tab:drf tfp expint} into Eq. (\\ref{eq: drf}) and plotting the resulting curve over the export intensity support.\nPlease note how the inclusion of covariates, firm- and time-level fixed effects, contributes to improving the goodness of fit. Looking at our baseline estimates visualized in Figure \\ref{fig:drf_TFP}, we can divide in segments.\nNotably, the dose-response functions for sales and variable costs in Figure \\ref{fig:drf_channels} have an inverted U-shape that mirrors the one observed for TFP in \\ref{fig:drf_TFP}. Our findings indicate that exporting significantly impacts costs and sales only when export intensity exceeds 10\\%. Below this threshold, firms do not experience substantial changes in the operational scale.\nAt this point, we can reconcile what we observe with the evidence from the last paragraphs on TFP in Figure \\ref{fig:drf_TFP}. We argue that a critical mass is needed in either case to record a significant impact of the exporting activity. At lower levels of exporting activity, the company starts to benefit from economies of scale but also needs to invest in productive capacity. To keep up with the technological frontier is costly, and it often requires an upgrade of obsolete tangible assets. We argue that the combined evidence of rising operational capacity (sales and costs) and investment in fixed assets explains why we observe a negative albeit small productivity loss in an intermediate range of export intensity. It is only when the company operates abroad at a larger scale that positive albeit small TFP gains come as a consequence of exporting. In this case, we argue, economies of scale become evident and the capital adjustment unveils its impact on firms' performance.",
        "summarize_figure": "2505.03328v1_DRFIC_TFP",
        "summarization": "The figure shows the dose - response function of export intensity on Total Factor Productivity (TFP). The horizontal axis is the export intensity in period t - 1, and the vertical axis is the change in TFP in period t. The solid line ATE(t) represents the estimated average treatment effect, and the dashed line represents the 95% confidence interval. The grey highlighted areas indicate the export intensity intervals where the dose - response function is statistically different from zero at a 5% significance level. The results show that when the export intensity exceeds 10%, exporting has a significant impact on costs and sales. At lower export intensity levels, the firm's operational scale does not change much. In the intermediate export intensity range, although the firm benefits from economies of scale, it needs to invest in production capacity, resulting in a small loss of TFP. Only when the firm exports on a large scale will there be a small increase in TFP due to economies of scale and capital adjustment."
    },
    "280": {
        "figure1": "2505.03331v1_shape_evaluation",
        "label1": "fig:shape_evaluation",
        "caption1": "\\textbf{Results of the design parameter optimisation: (A)} Differential pressures at 3~m/s airspeed, averaged over time (1.4~s / 70 samples), are plotted as a function of AoA and AoS. Each of the five pressure sensors records the pressure difference between a pair of ports. The distribution is aligned with the direction of the hole pair of the corresponding sensor, enabling estimation of AoA and AoS. \\textbf{(B)} \\rev{Comparison of all tested hardware configurations with respect to measurement resolution across velocity and angular ranges. Each point or triangle represents a distinct hardware design, with its performance compressed into a single metric according to the procedures outlined in Table~\\ref{tab:compare_res_noise}. Statistically significant differences, as determined by independent t-tests, are marked with asterisks. The results show that conical tips yield significantly higher velocity measurement resolution compared to spherical tips, while hole spacing has no significant impact. \\textbf{(C)} Comparison of hardware designs in terms of noise generation. Conical tips produce significantly lower speed noise compared to spherical tips.",
        "text": "Sensor 1 (static pressure - tip centre pressure) shows a pressure difference that varies radially with AoA and AoS, reaching its minimum when the probe is aligned parallel to the airflow (AoA = AoS = 0), as expected. In this alignment, the channel functions similarly to a pitot tube, capturing the airflow component parallel to the probe’s axis \\cite{pitot_tube} \\rev{(See Fig.~\\ref{fig:shape_evaluation}A)}.\nWe compare the 8 designs in measurement resolution and noise generation (see Fig. \\ref{fig:shape_evaluation}B). \\rev{In order to maximise the sensors sensitivity to changes in airspeed and angles, the preferred design has a high measurement resolution and low noise (Tab. \\ref{tab:compare_res_noise}).} Both metrics are evaluated on the data of pressure sensor 1 for the airspeed and on the data of sensors 2,3,4,5 for the angular measurements. The measurement matrix has five dimensions: 8 hardware designs x 4 speeds x 5 pressure sensors x 81 angles x 70 samples. To calculate the four comparative metrics, the dimension of the data matrix is reduced using the operations shown in table \\ref{tab:compare_res_noise}.",
        "summarize_figure": "2505.03331v1_shape_evaluation",
        "summarization": "The figure shows the results of design parameter optimization: (A) plots the differential pressures of five pressure sensors at an airspeed of 3 m/s, averaged over time (1.4 s / 70 samples), as a function of Angle of Attack (AoA) and Angle of Sideslip (AoS). The differential pressure distribution aligns with the direction of the corresponding sensor's hole pair, enabling the estimation of AoA and AoS. The differential pressure of sensor 1 varies radially with AoA and AoS, reaching a minimum when the probe is parallel to the airflow (AoA = AoS = 0). (B) compares different hardware configurations in terms of measurement resolution across velocity and angular ranges. Conical tips have significantly higher velocity measurement resolution than spherical tips, while hole spacing has no significant impact. (C) shows that conical tips generate significantly lower speed noise than spherical tips. To make the sensor more sensitive to wind speed and angle changes, an ideal design should have high measurement resolution and low noise."
    },
    "281": {
        "figure1": "2505.03337v1_overview",
        "label1": "fig:overview",
        "caption1": "The proposed separation model processes the audio input through several stages. First, a Feature Extraction module extracts learned  frame-level features. These features are transformed by a Synthesis Conditioning module into relevant synthesis parameters: transcription onsets, velocities, a mixture embedding, and individual track gains. A decoder module synthesizes one-shot samples for each drum instrument conditioned on the mixture embedding and reconstructs individual drum tracks by sequencing these one-shots with the obtained transcription.  The framework encompasses three interconnected tasks: \\acf{ADT}, \\acf{OSS}, and \\acf{DSS}. \\ac{DSS} is obtained either from the decoder output  (\\textit{synthesis estimate}) or after  time-frequency masking (\\textit{masked estimate}).",
        "text": "In this work, we propose an analysis-by-synthesis approach to decompose drum mixtures into transcription information and elementary one-shot samples. As illustrated in Figure \\ref{fig:overview}, the proposed architecture processes audio mixtures through several stages. Initially, a Feature Extraction module derives frame-level representations from the input mixture. These features are subsequently transformed by a Synthesis Conditioning module into relevant parameters for synthesis (transcription and one-shot conditioning). The conditioning is used by a one-shot synth to synthesize drum samples which are sequenced by the estimated transcription. A multitask training stage enables learning of neural network parameters for all modules.",
        "summarize_figure": "2505.03337v1_overview",
        "summarization": "The figure shows the processing flow of the proposed drum sound separation model for audio input. First, the Feature Extraction module extracts frame - level features. Then, the Synthesis Conditioning module transforms these features into relevant synthesis parameters such as transcription onsets, velocities, mixture embedding, and individual track gains. The decoder module synthesizes one - shot samples for each drum instrument based on the mixture embedding and reconstructs individual drum tracks by sequencing these samples with the obtained transcription. The framework encompasses three interrelated tasks: Automatic Drum Transcription (ADT), One - Shot Sample Synthesis (OSS), and Drum Sound Separation (DSS). DSS can be obtained from the decoder output (synthesis estimate) or after time - frequency masking (masked estimate). Neural network parameters are learned through a multi - task training stage to decompose drum mixtures into transcription information and basic one - shot samples."
    },
    "282": {
        "figure1": "2505.03337v1_detail",
        "label1": "fig:detailed_diagram",
        "caption1": "Detailed diagram of the analysis-by-synthesis pipeline, containing the Feature Extraction (blue), Synthesis Conditioning (yellow), and Decoder (purple) modules. Blocks in blue indicate trainable components, blocks in orange indicate differentiable operations, and blocks in pink indicate non-differentiable operations. Loss functions are represented as green components, and grey components represent external information used during training",
        "text": "The following sections explain the proposed multitask framework and then go into detail on the Feature Extraction,  Synthesis Conditioning, and Decoder modules, which are also detailed in Figure \\ref{fig:detailed_diagram}. For presentation clarity, we left most implementation details to Section \\ref{sec:implementation}.",
        "summarize_figure": "2505.03337v1_detail",
        "summarization": "The figure shows the detailed pipeline of the analysis - by - synthesis approach, including the Feature Extraction (blue), Synthesis Conditioning (yellow), and Decoder (purple) modules. Blue blocks are trainable components, orange blocks are differentiable operations, pink blocks are non - differentiable operations, green blocks are loss functions, and grey blocks are external information used during training. The input audio undergoes log - mel transformation and then frame features are extracted by the encoder. The transcription head obtains transcription information through peak - picking, combined with track volume, etc. for synthesis conditioning. The one - shot synth generates one - shot samples, which are sequenced by the Conv1D Sequencer and then mixed to obtain the reconstructed mix and individual tracks. The training process optimizes the model through multiple loss functions."
    },
    "283": {
        "figure1": "2505.03337v1_one-shot",
        "label1": "fig:one_shot_synth",
        "caption1": "One-shot synth model architecture. White noise is fed into a \\acf{TCN} conditioned via \\acf{FiLM} on a conditioning vector \\conditioning, which has disentangled instrument class/timbre dimensions. Depending on input \\conditioning, the \\ac{TCN} allows for synthesizing various drum instruments from the different drum kits.  The TCN output is shaped by a parametrized exponential envelope. The final output is normalized to $[-1, 1]$ amplitude range.",
        "text": "Conditional \\acl{OSS} is performed with a \\acf{TCN}, similar to the work of Shier et al. \\cite{shierDifferentiableModellingPercussive2023}. However, instead of using \\ac{TCN} to enhance the transient of a sinusoidal-plus-noise signal \\cite{vermaExtendingSpectralModeling2000a}, we take a more direct path. We eliminate the sinusoidal-plus-noise input, instead feeding \\sampleduration~second zero-padded white noise directly to the \\ac{TCN}. By expanding the network's receptive field and adding a time envelope, we found the \\ac{TCN} capable of synthesizing both sinusoidal, noise, and transient components. Figure \\ref{fig:one_shot_synth} shows the architecture of the one-shot synthesis model, with specific implementation details reported in Sections \\ref{sec:tcn_architecture} to \\ref{sec:envelope}.",
        "summarize_figure": "2505.03337v1_one-shot",
        "summarization": "The figure shows the model architecture of the one - shot synth. White noise is fed into a Temporal Convolutional Network (TCN) conditioned by a Feature-wise Linear Modulation (FiLM) - based conditioning vector conditioning, which disentangles the instrument class/timbre dimensions. The TCN can synthesize sounds of various drum instruments from different drum kits according to the input conditioning. The output of the TCN is shaped by a parametrized exponential envelope (Gen.Envelope), and the final output is normalized to the amplitude range of [-1, 1]. This architecture is different from the method of using TCN to enhance the transients of a sinusoidal - plus - noise signal. Instead, it directly feeds zero - padded white noise into the TCN. By expanding the network's receptive field and adding a time envelope, it can synthesize sinusoidal, noise, and transient components."
    },
    "284": {
        "figure1": "2505.03340v1_expected_degree_of_anomaly",
        "label1": "fig: degree of anomaly",
        "caption1": " The degree of the anomaly as a function of time $t$. The parameters used are $\\tau = 200, \\delta = 0, \\beta = 2.0, m = 1.$",
        "text": "For general $ j \\in [m]$, we can extend \\eqref{eq:D-tau} to find $ \\E [D_\\tau(t,j) + \\delta ] $. Applying the recursive approach,     &= D_{\\tau}(t,j-1) + \\delta + \\frac{(t-1) \\beta+ D_{\\tau}(t,j-1) +\\delta}{(t-1)(2 m+ \\beta + \\delta)+ j-1} \\\\     & = (D_{\\tau}(t-1,m) + \\delta) \\times \\frac{t-1 + \\frac{j}{2m + \\beta+\\delta}}{t-1} + \\frac{\\beta j}{2m + \\beta+\\delta}, and taking expectation, we have We obtain \\eqref{eq:D-tau} by solving this recursion. Figure \\ref{fig: degree of anomaly} shows the growth of $D_{\\tau} (t)$ over time, with our theoretical result for $ \\E [D_{\\tau} (t)+ \\delta]$ depicted as yellow line. It is very interesting that the coefficient of the linear growth, $\\frac{\\beta}{m+\\beta+\\delta}$ is larger than the probability $\\frac{\\beta}{2m+\\beta+\\delta}$ in \\eqref{rule-2} that an edge attaches itself to an anomaly. Indeed, our assumption that vertices may attach to the anomaly also through the PA mechanism, has increased the rate of growth (rather than, say, giving rise to an extra polynomial term as we conjectured at the beginning).      The parameters used are $\\tau = 200, \\delta = 0, \\beta = 2.0, m = 1.$} To further explore the relation between $ D_{\\tau} (t)$ and the network size, we assume that $ t = a \\tau$, $ a \\geq 1$. By Stirling's formula, when $t \\to \\infty$ and $a$ is fixed (see, e.g., \\cite[(8.3.9)]{Hofstad_2016}). This approximation allows us to simplify the formula of expected degree of $v_\\tau$. Indeed, applying it to \\eqref{eq:D-tau} gives     f(a) = \\lim_{\\tau \\to \\infty} \\frac{\\E [D_{\\tau}(a \\tau) + \\delta]}{a \\tau} = \\frac{m\\beta}{m+\\beta+\\delta}\\left( 1- a^{-\\frac{m+\\beta+\\delta}{2m+\\beta+\\delta}} \\right), and     f(1) &= 0, \\qquad \\lim_{a \\to \\infty} f(a) = \\frac{m\\beta}{m+\\beta+\\delta}, \\\\     f'(a) &= \\frac{m \\beta}{2m+\\beta+\\delta} a^{-\\frac{m+\\beta+\\delta}{2m+\\beta+\\delta}-1},  We see that larger values of $a$, corresponding to an earlier anomaly, result in a larger deviation from the linear growth of $ \\E[D_{\\tau}(a \\tau)]$.",
        "summarize_figure": "2505.03340v1_expected_degree_of_anomaly",
        "summarization": "The figure shows the change of the degree of the anomaly with the network size. The blue line represents the simulated degree with the parameter τ = 200, and the yellow line represents the expected degree E[Dτ(t)] derived from Proposition 1. As the network size increases, the degree of the anomaly shows an upward trend, and the growth trends of the simulated degree and the expected degree are basically consistent. The expression for the expected degree is obtained through theoretical derivation, with parameter values τ = 200, δ = 0, β = 2.0, m = 1. The study also assumes t = aτ (a ≥ 1) and uses Stirling's formula for approximation. It is concluded that the earlier the anomaly occurs (the larger the value of a), the greater the deviation from the linear growth of E[Dτ(aτ)]."
    },
    "285": {
        "figure1": "2505.03345v1_TF-IDF_Voting",
        "label1": "fig:TF-IDF voting",
        "caption1": "Evaluation of TF-IDF voting.",
        "text": "We divided the dataset into a training set (66\\%) and a test set (34\\%), maintaining a balanced representation of different campaigns across both subsets. %This split was applied to ensure that the models generalize effectively, preventing overfitting while capturing each disinformation campaign's distinct characteristics. Then, we evaluated TF-IDF voting by conducting multiple experiments to determine the optimal similarity threshold. We systematically increased the threshold from 0.1 to 0.9 and recorded the corresponding accuracy values. Figure \\ref{fig:TF-IDF voting} illustrates the results of the evaluation for different threshold values. It is possible to notice that we achieved the highest accuracy (56\\%) with a threshold of 0.25. When the threshold was lower, more articles were classified, but the inclusion of loosely related tuples led to an increase in false positives, reducing overall precision. On the other hand, setting the threshold too high resulted in underfitting, where valid attributions were missed, leading to a drop in recall. Beyond 0.25, accuracy declined as the stricter similarity requirements prevented many articles from receiving a campaign label. These findings highlight that while TF-IDF voting benefits from flexible attribution, it remains highly dependent on threshold tuning for optimal performance.",
        "summarize_figure": "2505.03345v1_TF-IDF_Voting",
        "summarization": "The figure shows the evaluation results of the Term Frequency - Inverse Document Frequency (TF-IDF) voting method. The dataset is divided into a 66% training set and a 34% test set. By systematically increasing the similarity threshold from 0.1 to 0.9 and recording the corresponding accuracy, the optimal threshold is explored. The results show that the highest accuracy of 56% is achieved when the threshold is 0.25. A too - low threshold classifies more articles, but the inclusion of loosely related tuples increases false positives and reduces overall precision. A too - high threshold leads to underfitting, missing valid attributions and decreasing recall. Above 0.25, stricter similarity requirements prevent many articles from getting a campaign label, reducing accuracy. This shows that although the TF-IDF voting method is flexible in attribution, its performance highly depends on threshold tuning."
    },
    "286": {
        "figure1": "2505.03345v1_TF-IDF_Thresholding",
        "label1": "fig:TF-IDF thresholding",
        "caption1": "Evaluation of TF-IDF thresholding.",
        "text": "As in the previous evaluation, we tested TF-IDF thresholding with different threshold values to determine the most effective setting. However, the results, as shown in Figure \\ref{fig:TF-IDF thresholding}, showed that TF-IDF thresholding performed significantly worse than voting, with a maximum accuracy of only 12\\% at a threshold around 0.1, which is too low to effectively determine if an article belongs to a specific campaign. The main reason for this poor performance is that many articles do not contain enough extracted tuples to surpass the required minimum similarity threshold, resulting in a high number of unclassified samples. As the threshold increased, accuracy dropped further, highlighting the method’s inability to effectively handle variation in article content. Unlike TF-IDF voting, which provides attribution based on cumulative votes, this approach over-prioritizes precision, leading to a sharp decline in recall and making it impractical for real-world disinformation campaign attribution.",
        "summarize_figure": "2505.03345v1_TF-IDF_Thresholding",
        "summarization": "The figure shows the evaluation results of the Term Frequency - Inverse Document Frequency (TF-IDF) thresholding method. Similar to previous evaluations, different threshold values were tested to determine the optimal setting. The results show that the TF-IDF thresholding method performs significantly worse than the TF-IDF voting method. The maximum accuracy is only 12% at a threshold of around 0.1, which is too low to effectively determine if an article belongs to a specific campaign. Many articles do not contain enough extracted tuples to exceed the required minimum similarity threshold, resulting in a large number of unclassified samples. As the threshold increases, the accuracy drops further, highlighting the method's inability to effectively handle variations in article content. Unlike the TF-IDF voting method, which makes attributions based on cumulative votes, this method over - prioritizes precision, leading to a sharp decline in recall and making it impractical for real - world disinformation campaign attribution."
    },
    "287": {
        "figure1": "2505.03345v1_SBERT",
        "label1": "fig:SBERT",
        "caption1": "Evaluation of SBERT.",
        "text": "To assess the effectiveness of SBERT in attributing fake news to disinformation campaigns, we employed the same accuracy metric defined in the lexical similarity analysis, calculated as the ratio between the number of correctly classified articles and the total number of fake news articles. Figure \\ref{fig:SBERT} illustrates the results of the evaluation for different threshold values. These results demonstrate that SBERT outperformed TF-IDF-based approaches, achieving an accuracy of 67.5\\% with a similarity threshold of 0.4. Again, the results indicate a trade-off between precision and recall depending on the threshold selection. Increasing the threshold imposes stricter similarity criteria, thereby improving precision but reducing recall, as fewer triples are considered sufficiently similar to match with a campaign. Conversely, lowering the threshold leads to a higher recall by permitting a broader set of matches; however, this comes at the cost of reduced precision due to increasing false positives. The superior performance of SBERT over TF-IDF confirms the importance of capturing semantic rather than purely lexical similarity in fake news attribution. By leveraging word embeddings, SBERT effectively identifies connections between different phrasings of the same narrative, making it particularly robust against minor textual variations and paraphrasing strategies commonly used in disinformation campaigns. SBERT demonstrated several key advantages over lexical similarity methods. Its ability to capture meaning beyond direct word matching made it highly effective in detecting reworded but semantically identical disinformation content. By leveraging word embeddings, SBERT provided a context-aware approach, reducing the risk of missing campaign attributions due to superficial linguistic changes. Additionally, its performance gains in accuracy highlighted its suitability for fake news analysis, where threat actors frequently paraphrase misleading narratives.",
        "summarize_figure": "2505.03345v1_SBERT",
        "summarization": "The figure shows the evaluation results of the Sentence - BERT (SBERT) method in the attribution of fake news to disinformation campaigns. The same accuracy metric as in the lexical similarity analysis (the ratio of the number of correctly classified articles to the total number of fake news articles) is used for measurement. The results show that the SBERT method outperforms the Term Frequency - Inverse Document Frequency (TF-IDF) - based methods, achieving an accuracy of 67.5% when the similarity threshold is 0.4. Similar to other methods, the SBERT method also has a trade - off between precision and recall in threshold selection: increasing the threshold makes the similarity criteria stricter, improving precision but reducing recall; decreasing the threshold can increase recall but reduces precision due to more false positives. The SBERT method effectively captures semantic similarity rather than pure lexical similarity by leveraging word embeddings, and can identify connections between different phrasings of the same narrative, making it more robust in dealing with the common text variations and paraphrasing strategies in disinformation campaigns, and more suitable for fake news analysis."
    },
    "288": {
        "figure1": "2505.03346v1_Inexact_KLR_traj",
        "label1": "fig:VDP traj",
        "caption1": "Open-loop and closed-loop optimal control trajectories of the Van der Pol oscillator under different control methods.",
        "text": "Figure \\ref{fig:VDP traj} shows the trajectories from different initial conditions converging to the equilibrium point under different control methods over a simulation horizon of 1000 steps (0.02s for each step). The black curves represent the open-loop trajectories of the Van der Pol system. All methods successfully stabilize the system except KL-IO-DeePC, which heavily relies on the existence of an exact finite-dimensional Koopman realization. When such a realization cannot be obtained—which is the case in most practical scenarios—its performance degrades significantly. Notably, both the proposed method and KL-IO-DeePC use the same type of regularizer. As previously discussed, we attribute the improved performance of our approach to our DeePC formulation based on input/lifted state, which enables more effective use of the lifting function information. Furthermore, operating in a higher-dimensional lifted space allows the regularizer in our formulation to better mitigate the impact of Koopman approximation errors.",
        "summarize_figure": "2505.03346v1_Inexact_KLR_traj",
        "summarization": "The figure shows the open - loop and closed - loop optimal control trajectories of the Van der Pol oscillator under different control methods. The horizontal axis is x1 and the vertical axis is x2. The black curves represent the open - loop trajectories. The dashed lines of different colors represent the trajectories of control methods such as L-MPC, KL-MPC, KL-DeePC, and KL-IO-DeePC. Within a simulation time of 1000 steps (0.02 s per step), all methods except KL-IO-DeePC can stabilize the system to the equilibrium point. KL-IO-DeePC relies heavily on the exact finite - dimensional Koopman realization, and its performance degrades significantly when this cannot be achieved in most practical scenarios. The proposed method and KL-IO-DeePC use the same type of regularizer. The former, based on the input/lifted state DeePC formulation, can more effectively utilize the lifting function information. Moreover, in the high - dimensional lifted space, the regularizer can better mitigate the impact of Koopman approximation errors, resulting in better performance."
    },
    "289": {
        "figure1": "2505.03346v1_Inexact_KLR_conv",
        "label1": "fig:conv1",
        "caption1": "Closed-loop trajectories starting from $x(0)=[-0.8,0.4]$ under different methods after 400 steps (left) and 1000 steps (right).",
        "text": "Figure \\ref{fig:conv1} shows the trajectories of system under different methods with initial state  $x(0)=[-0.8,0.4]$ after 400 steps. The KL-DeePC converges faster in this case.",
        "summarize_figure": "2505.03346v1_Inexact_KLR_conv",
        "summarization": "This picture shows the closed - loop trajectories of a system starting from x(0)=[-0.8, 0.4] under L-MPC, KL-MPC, and KL-DeePC methods. The KL-DeePC method converges faster."
    },
    "290": {
        "figure1": "2505.03346v1_Inexact_KBR_state",
        "label1": "fig:Inexact KBR state",
        "caption1": "Comparison of state responses under different control methods.",
        "text": "Finally, we apply the proposed KB-DeePC to control system \\eqref{eq:inexact KBR system}, where neither an exact KBR nor KLR exists, and compare its performance with both KB-MPC and KL-DeePC. It can be seen from Figure \\ref{fig:Inexact KBR state} that the proposed KB-DeePC achieves similar performance as the KB-MPC based on EDMD. This is because even if we cannot obtain an exact KBR for system \\eqref{eq:inexact KBR system}, the bilinearization for this system based on a given basis is accurate enough, which is obvious from our motivating case in Section \\ref{sec:pre}.",
        "summarize_figure": "2505.03346v1_Inexact_KBR_state",
        "summarization": "This picture compares the state responses under different control methods. Three control methods are shown: KB - DeePC, KB - MPC, and KL - DeePC. The vertical axes are x2 and x1 respectively, and the horizontal axis is the time step. As the time step increases, the values of x2 and x1 under the three methods gradually decrease from higher values to approach 0. The performance of KB - DeePC is similar to that of KB - MPC."
    },
    "291": {
        "figure1": "2505.03353v1_reroute",
        "label1": "fig:reroute",
        "caption1": " An illustration of the rerouting argument from~\\cite{CyganMPP13}. Left: linkage $\\cal{Q}$ is given as the two black horizontal paths and  $\\cal{P}$ comprises the purple and the red path. When considered oriented towards the internal face $C_2$, $\\cal{P}$ winds four times clockwise around $C_2$, compared  to $\\cal{Q}$. Right: linkage $\\cal{P}'$ has the same endpoints as $\\cal{P}$ while its winding number is reduced. ",
        "text": "An illustration of the rerouting argument from~\\cite{CyganMPP13}. Left: linkage $\\cal{Q}$ is given as the two black horizontal paths and  $\\cal{P}$ comprises the purple and the red path. When considered oriented towards the internal face $C_2$, $\\cal{P}$ winds four times clockwise around $C_2$, compared  to $\\cal{Q}$. Right: linkage $\\cal{P}'$ has the same endpoints as $\\cal{P}$ while its winding number is reduced. }\\label{fig:reroute}\nThe main idea behind the $2^{\\Oh(k^2)}\\cdot n^{\\Oh(1)}$-time algorithm of~\\cite{LokshtanovMPSZ20} is that it suffices to inspect only $2^{\\Oh(k^2)}$ homology classes. This is challenging because a solution might ``wind'' around some terminal vertex, forming a deep ``spiral'', and solutions which differ in the depth of such a spiral are not homologous. Taking into account that there are $\\Omega(k)$ potential sources for spirals, we may a priori obtain $n^{\\Omega(k)}$ different homologies. Addressing this issue required incorporating two more techniques. First, Adler et al.~\\cite{AdlerKKLST17} had shown that one can reduce the treewidth $\\mathbf{tw}$ of input graph $G$ to $2^{\\Oh(k)}$ using the irrelevant vertex technique.  This bounds the size of a maximal grid appearing in $G$ and simplifies the analysis of spirals. The second ingredient originates from the fixed-parameter algorithm for {\\sc Directed Planar Disjoint Paths}, due to Cygan, Marx, Pilipczuk, and Pilipczuk~\\cite{CyganMPP13}. Consider a directed plane graph $D$ with outer face $C_1$ and a distinguished internal face $C_2$ and let ${\\cal Q}$ be a collection of disjoint paths connecting $C_1$ with $C_2$. Then for any other such collection of paths $\\pcal$ of the same size, one can reroute $\\pcal$ to almost match the {\\em winding number}\\footnote{On an intuitive level, the winding number of a curve measures how many times it goes around a certain object on the plane.} of~${\\cal Q}$, while preserving the endpoints of the paths in $\\pcal$~\\cite[Lemma 4.8]{CyganMPP13arxiv}, see \\Cref{fig:reroute}. Effectively, these two ideas reduce the problem to the case where each spinal path $Q$ of the considered Steiner tree $T$ has length $\\Oh(\\mathbf{tw})$, which in total gives the bound $\\mathbf{tw}^{\\Oh(k)} = 2^{\\Oh(k^2)}$ on the number of enumerated homology classes.",
        "summarize_figure": "2505.03353v1_reroute",
        "summarization": "The picture shows an illustration of the rerouting argument. In the left - hand figure, the linkage 𝒬 consists of two black horizontal paths, and 𝒫 is composed of purple and red paths. When oriented towards the internal face C2, 𝒫 winds four times clockwise around C2 compared to 𝒬. In the right - hand figure, the linkage 𝒫' has the same endpoints as 𝒫, but its winding number is reduced."
    },
    "292": {
        "figure1": "2505.03353v1_ring-outline",
        "label1": "fig:ring-outline",
        "caption1": " An illustration of the problematic scenario when the solution path $P$ crosses the geodesic spinal path $Q$ in the tree $T$ many times, because of the winding behavior. The fat segments of $P$ and $Q$ represent subpaths in $G$ of  the same length. By analyzing the shortest path structure in this part of the graph, we identify a region (in blue) where these structures coincide for all terminal pairs. ",
        "text": "An illustration of the problematic scenario when the solution path $P$ crosses the geodesic spinal path $Q$ in the tree $T$ many times, because of the winding behavior. The fat segments of $P$ and $Q$ represent subpaths in $G$ of  the same length. By analyzing the shortest path structure in this part of the graph, we identify a region (in blue) where these structures coincide for all terminal pairs.  }\\label{fig:ring-outline}\nTo mitigate this problem, we shall analyze the homology of a solution with respect to a Steiner tree $T$ in $G$ that is made of geodesics\\footnote{A {\\em{geodesic}} in a graph is a path that is a shortest path between its endpoints.}. That is, every spinal path $Q$ of $T$ must be a geodesic. Consider some path $P$ from a solution that winds around some terminal with respect to $T$; see \\Cref{fig:ring-outline}. This means that $P$ crosses some spinal path $Q$ of $T$ several times in the same direction. Since $P$ is also a geodesic, this gives rise to a rather peculiar subgraph of $G$. More formally, let $u,v \\in V(Q) \\cap V(P)$ be two consecutive crossing points of $Q$ and $P$. Then the distance between $u$ and $v$ along $Q$ must be equal to their distance along~$P$. Consequently, when the number of such crossings is large, then there are multiple non-equivalent (in the topological sense) ways to reach the interior of the spiral from the exterior via a shortest path: on each crossing one can choose to follow either $Q$ or $P$. Now comes the key observation:\nThe situation described above implies the existence of a ring-shaped region $R$  that covers almost the entire spiral and in which the structure of shortest $(s_i,t_i)$-paths is the same for each terminal pair $(s_i,t_i)$ separated by the spiral; see \\Cref{fig:ring-outline}.",
        "summarize_figure": "2505.03353v1_ring-outline",
        "summarization": "The picture shows an illustration of a problematic scenario. Due to the winding behavior, the solution path P crosses the geodesic spinal path Q in the tree T multiple times. The thick segments of P and Q represent sub - paths of the same length in graph G. By analyzing the shortest - path structure in this part of the graph, a blue region is identified where the shortest - path structures for all terminal pairs coincide."
    },
    "293": {
        "figure1": "2505.03353v1_decomposition-updated",
        "label1": "fig:decomposition",
        "caption1": " A refinement operation for a Steiner tree $T$ (in black) with respect to regions $R_1, \\dots, R_\\ell$ (blue). The purple path $Q$ belongs to a linkage $\\cal{Q}$ that connects the two sides of the region $R_1$. We want $T$ to wind within $R_1$ in the same fashion as $Q$ to match the behavior of a certain solution. We remove the subpaths of $T$ that traverse $R_1$ (the dotted segments) and replace them with $Q$. Next, we insert the red arcs to $T$ in order to maintain connectivity. These arcs may not correspond to any paths in the graph so we have to augment it with additional edges that are flagged as forbidden to be used by a solution.",
        "text": "In order to turn this idea into an algorithm, we need to engineer the definition of such a ``ring-shaped region'' $R$ (formalized later as a ``$\\mathsf{dag}$-$\\mathsf{ring}$'') in such a way that: and These properties ensure that a solution cannot form any large spirals outside the regions $R_1, \\dots, R_\\ell$. This enables us to extract the subgraphs where spiraling is possible and analyze them by treating their shortest paths structures as DAGs.  We exploit this decomposition to refine the Steiner tree $T$ to only use paths with ``correct'' winding numbers within each $R_i$. By property~(c), we can count the number $q$ of subpaths that traverse $R_i$ in any solution. Next, we compute an arbitrary linkage $\\mathcal{Q}$ of size $q$ that connects the two sides of $R_i$. The rerouting argument from the work of Cygan et al.~\\cite{CyganMPP13} ensures that  any solution can be tweaked to almost match (within constant additive error) the winding number of $\\mathcal{Q}$ within $R_i$. Therefore, if we tweak $T$ to go alongside $\\mathcal{Q}$, then some solution is guaranteed to have only few ``non-trivial'' crossings with $T$ in this area (\\Cref{fig:decomposition}).\nA refinement operation for a Steiner tree $T$ (in black) with respect to regions $R_1, \\dots, R_\\ell$ (blue). The purple path $Q$ belongs to a linkage $\\cal{Q}$ that connects the two sides of the region $R_1$.  We want $T$ to wind within $R_1$ in the same fashion as $Q$ to match the behavior of a certain solution. We remove the subpaths of $T$ that traverse $R_1$ (the dotted segments) and replace them with $Q$. Next, we insert the red arcs to $T$ in order to maintain connectivity. These arcs may not correspond to any paths in the graph so we have to augment it with additional edges that are flagged as forbidden to be used by a solution.}\\label{fig:decomposition}",
        "summarize_figure": "2505.03353v1_decomposition-updated",
        "summarization": "The picture shows a refinement operation for a Steiner tree T (in black) with respect to regions R1, …, Rℓ (blue). The purple path Q belongs to a linkage 𝒬 that connects the two sides of region R1. To make T wind within R1 in the same way as Q, matching the behavior of a certain solution, sub - paths of T traversing R1 (dotted segments) are removed and replaced with Q. Then, red arcs are inserted to maintain connectivity. These arcs may not correspond to any paths in the graph, so additional edges flagged as forbidden for the solution are added."
    },
    "294": {
        "figure1": "2505.03355v1_adia_pop_comp",
        "label1": "fig:adia_pop",
        "caption1": "(a) Population probabilities of the adiabatic singlet states in the Hz$\\cdots$H\\textsubscript{2}O complex as a function of \\textit{t} after the pump pulse. (b) Population probabilities of the adiabatic singlet states as a function of  \\textit{t} - \\textit{t}\\textsubscript{push} after reexcitation by the push pulse (see legend for the color code). The populations of higher-lying electronic states are displayed collectively and labeled as S\\textsubscript{\\textit{n}}. The data in (a) are taken from Ref.~\\cite{pios_hz_h2o_pumpprobe}.",
        "text": "The pump pulse centered at 4.29~eV with a width of 0.1~eV populates at \\textit{t} = 0~fs primarily the quasi-degenerate bright S\\textsubscript{5}/S\\textsubscript{6} electronic states of Hz (see Section S2.1 of the SI).  The S\\textsubscript{7} ($n\\pi^{*}$) state also acquires some population due to vibronic mixing with the S\\textsubscript{5}/S\\textsubscript{6} states. The time evolution of the populations of the adiabatic electronic states up to 100~fs is displayed in Figure~\\ref{fig:adia_pop}a.  The initial populations of the S\\textsubscript{5}, S\\textsubscript{6} and S\\textsubscript{7} states decay on an ultrafast time scale of about 10~fs, while the population of the S\\textsubscript{2} ($n\\pi^{*}$) state rises on the same time scale.  After a delay of about 15~fs, the population of the S\\textsubscript{1} ($\\pi\\pi^{*}$) state rises quickly and subsequently more slowly.  At \\textit{t} = 100~fs, more than 80\\% of the initially excited electronic population is in the S\\textsubscript{1} state, see Figure~\\ref{fig:adia_pop}a.  Upon close inspection of Figure~\\ref{fig:adia_pop}a one can see that about 5\\% of the population are in the S\\textsubscript{0} state (black curve) at \\textit{t} = 100~fs.\nThe simulated push pulse at t\\textsubscript{\\textit{push}} = 100~fs is centered at an energy of 2.8~eV with a width of $\\Delta E\\approx$ 3.7~eV, corresponding to a duration of $\\tau =$ 1~fs (see Section S2.3 of the SI).  It excites the ensemble of trajectories in the S\\textsubscript{1} state to higher excited singlet states S\\textsubscript{n} with probabilities given by the S\\textsubscript{1} - S\\textsubscript{n} oscillator strengths.  The resulting electronic populations and their time evolution are shown in Figure~\\ref{fig:adia_pop}b.  At \\textit{t} = \\textit{t}\\textsubscript{\\textit{push}}, more than 80\\% of the electronic population are in higher excited adiabatic electronic states denoted collectively as “S\\textsubscript{n}” in Figure~\\ref{fig:adia_pop}b.  Lower lying states, such as S\\textsubscript{6}, S\\textsubscript{7}, S\\textsubscript{9}, are populated with a few percent.  Figure~\\ref{fig:adia_pop}b reveals that the “S\\textsubscript{n}” population decays on a time scale of about 30~fs and that the S\\textsubscript{1} state is re-populated on a similar time scale.  Notably, the population of the S\\textsubscript{0} state has grown to about 20\\% at \\textit{t} – \\textit{t}\\textsubscript{\\textit{push}} = 100~fs.  The analysis of the nuclear geometries of the trajectories terminating in the S\\textsubscript{0} state reveals that 56\\% of these trajectories are nonreactive (that is, no proton transfer from H\\textsubscript{2}O to Hz takes place) and 44\\% are reactive, representing HzH$\\cdots$OH biradicals in their electronic ground states. The excitation of the S\\textsubscript{1} population by the push pulse thus increases the proton-transfer reaction probability by about an order of magnitude.  The molecular geometry of a representative non-reactive product in the S\\textsubscript{0} state is illustrated in Figure~\\ref{fig:rep_geos}a, while the molecular geometry of a representative reactive product is shown in Figure~\\ref{fig:rep_geos}b. The non-reactive product geometry exhibits strong in-plane deformations with only minor out-of-plane displacements. The reactive product geometry is similar to previously reported geometries\\cite{pios_hz_h2o_pumpprobe} with the HzH radical exhibiting both, in-plane and out-of-plane, deformations.",
        "summarize_figure": "2505.03355v1_adia_pop_comp",
        "summarization": "Figure a shows the population probabilities of adiabatic singlet states in the Hz∙∙∙H₂O complex as a function of time t after the pump pulse, with curves for S₀ - S₈ and Sₙ. S₅, S₆, and S₇ populations decay in about 10 fs, S₂ rises then, S₁ rises after 15 fs, and at 100 fs, over 80% is in S₁ and 5% in S₀. Figure b shows the probabilities as a function of t - t_push after re - excitation by the push pulse. At t = t_push, over 80% is in Sₙ. Sₙ decays in about 30 fs, S₁ is repopulated, and S₀ reaches about 20% at t - t_push = 100 fs."
    },
    "295": {
        "figure1": "2505.03355v1_rep_geo",
        "label1": "fig:rep_geos",
        "caption1": "(a) Top and side view of the structure of a non-reactive conical intersection. (b) Top and side view of the structure of a reactive conical intersection.",
        "text": "The simulated push pulse at t\\textsubscript{\\textit{push}} = 100~fs is centered at an energy of 2.8~eV with a width of $\\Delta E\\approx$ 3.7~eV, corresponding to a duration of $\\tau =$ 1~fs (see Section S2.3 of the SI).  It excites the ensemble of trajectories in the S\\textsubscript{1} state to higher excited singlet states S\\textsubscript{n} with probabilities given by the S\\textsubscript{1} - S\\textsubscript{n} oscillator strengths.  The resulting electronic populations and their time evolution are shown in Figure~\\ref{fig:adia_pop}b.  At \\textit{t} = \\textit{t}\\textsubscript{\\textit{push}}, more than 80\\% of the electronic population are in higher excited adiabatic electronic states denoted collectively as “S\\textsubscript{n}” in Figure~\\ref{fig:adia_pop}b.  Lower lying states, such as S\\textsubscript{6}, S\\textsubscript{7}, S\\textsubscript{9}, are populated with a few percent.  Figure~\\ref{fig:adia_pop}b reveals that the “S\\textsubscript{n}” population decays on a time scale of about 30~fs and that the S\\textsubscript{1} state is re-populated on a similar time scale.  Notably, the population of the S\\textsubscript{0} state has grown to about 20\\% at \\textit{t} – \\textit{t}\\textsubscript{\\textit{push}} = 100~fs.  The analysis of the nuclear geometries of the trajectories terminating in the S\\textsubscript{0} state reveals that 56\\% of these trajectories are nonreactive (that is, no proton transfer from H\\textsubscript{2}O to Hz takes place) and 44\\% are reactive, representing HzH$\\cdots$OH biradicals in their electronic ground states. The excitation of the S\\textsubscript{1} population by the push pulse thus increases the proton-transfer reaction probability by about an order of magnitude.  The molecular geometry of a representative non-reactive product in the S\\textsubscript{0} state is illustrated in Figure~\\ref{fig:rep_geos}a, while the molecular geometry of a representative reactive product is shown in Figure~\\ref{fig:rep_geos}b. The non-reactive product geometry exhibits strong in-plane deformations with only minor out-of-plane displacements. The reactive product geometry is similar to previously reported geometries\\cite{pios_hz_h2o_pumpprobe} with the HzH radical exhibiting both, in-plane and out-of-plane, deformations.",
        "summarize_figure": "2505.03355v1_rep_geo",
        "summarization": "Figure a shows top and side views of a non - reactive conical intersection structure with gray, blue, white atoms and a red - white small molecule, having strong in - plane and minor out - of - plane deformations. Figure b shows top and side views of a reactive conical intersection structure, with the HzH radical having in - plane and out - of - plane deformations."
    },
    "296": {
        "figure1": "2505.03355v1_dynamic_map_comp",
        "label1": "fig:dynamic_map1",
        "caption1": "(a) Dynamic map (projection of the trajectories onto the plane of the OH and ON distances) covering the first 100~fs following excitation by the pump pulse. The data are taken from Ref.~ \\cite{pios_hz_h2o_pumpprobe}. (b) Dynamic map covering the first 100~fs after re-excitation by the push pulse. The energy is given relative to the local  S\\textsubscript{1} minimum. The crosses denote terminations of trajectories at S\\textsubscript{1}-S\\textsubscript{0} CIs.",
        "text": "Figure~\\ref{fig:dynamic_map1}a displays the dynamic map of the trajectories initiated at \\textit{t} = 0~fs by the pump pulse.  The potential energy of the trajectories is in the range 2.0 – 4.0~eV.  The OH distance is confined to the interval 0.9~\\AA – 1.3~\\AA\\space and the ON distance varies from 2.7~\\AA\\space to 3.6~\\AA.  This part of the dynamic map represents radiationless electronic decay dynamics in the Hz chromophore from the bright S\\textsubscript{5}/S\\textsubscript{6} states to the S\\textsubscript{1} state.  The region R\\textsubscript{OH} $>$ 1.5~\\AA\\space represents trajectories which have undergone proton transfer.  Just a few trajectories enter the proton-transfer region.  Their dynamics is driven by the (diabatic) CT state and they terminate at an S\\textsubscript{1}-S\\textsubscript{0} conical intersection.  It is seen that the proton-transfer dynamics is strongly correlated with the donor-acceptor distance R\\textsubscript{ON}, which is typical for proton-transfer reactions.  The reduction of the ON distance lowers the barrier of the proton-transfer reaction.  Figure~\\ref{fig:dynamic_map1}a illustrates that the PCET reaction is a rare event in the Hz$\\cdots$H\\textsubscript{2}O complex due to the very efficient radiationless relaxation within the LE excited states of Hz and the rather high barrier for proton transfer ($\\approx$ 1.0~eV) on the S\\textsubscript{1} potential-energy surface.  Closer inspection reveals that the two conical intersections encountered at R\\textsubscript{OH} $<$ 1.0~\\AA\\space in Figure~\\ref{fig:dynamic_map1}a result from the transfer of the second proton of H\\textsubscript{2}O (which is not monitored in the dynamic map).  Thus all encounters of S\\textsubscript{1}-S\\textsubscript{0} conical intersections in Figure~\\ref{fig:dynamic_map1}a occur through a proton-transfer reaction and the branching ratio for proton transfer is rather low.  It should be kept in mind, however, that only proton transfer on the S\\textsubscript{1} surface is recorded.  Proton transfer processes occurring in higher electronic states are not recorded in the dynamic map of Figure~\\ref{fig:dynamic_map1}a.\nThe dynamic map shown in Figure~\\ref{fig:dynamic_map1}b represents the dynamics after re-excitation of the Hz chromophore by the push pulse. The overall change of color reflects the higher electronic potential energy available after the push pulse.  The range of the majority of trajectories in the OH distance is about the same as in Figure~\\ref{fig:dynamic_map1}a, while the range in the ON distance is increased, from 2.5~\\AA\\space to 4.0~\\AA.  It is obvious that the number of encounters of S\\textsubscript{1}-S\\textsubscript{0} conical intersections is strongly increased compared to Figure~\\ref{fig:dynamic_map1}a.  The electronic potential energy imparted by the push pulse is rapidly converted into nuclear kinetic energy.  The resulting large-amplitude nuclear motion of the Hz chromophore facilitates access to S\\textsubscript{1}-S\\textsubscript{0} conical intersections.  The conical intersections encountered at OH distances $<$ 1.3~\\AA\\space lead to direct internal conversion in the Hz chromophore.  The conical intersections encountered at OH distances $>$ 1.3~\\AA\\space represent PCET reactions leading to HzH and OH radicals.  The increased nuclear kinetic energy on the S\\textsubscript{1} energy surface also increases the probability of overcoming the barrier for proton transfer (H-atom tunneling is not taken into account in classical trajectory dynamics).  The proton-transfer yield is thus significantly enhanced by the push pulse at the cost of loss of electronic population from the reservoir of the S\\textsubscript{1} state by competing direct internal conversion to the S\\textsubscript{0} state.      The data are taken from Ref.~ \\cite{pios_hz_h2o_pumpprobe}. (b) Dynamic map covering the first 100~fs after re-excitation by the push pulse. The energy is given relative to the local  S\\textsubscript{1} minimum. The crosses denote terminations of trajectories at S\\textsubscript{1}-S\\textsubscript{0} CIs.}\nIt is eye-catching that the reactive conical intersections in Figure~\\ref{fig:dynamic_map1}a as well as in Figure~\\ref{fig:dynamic_map1}b appear to be approximately located along a line which is tilted in the OH-ON plane, in contrast to the nonreactive conical intersections in Figure~\\ref{fig:dynamic_map1}b, which are located in a rectangular region with OH distances around 1.0~\\AA\\space and ON distances between 2.8~\\AA\\space and 3.5~\\AA.  The clear correlation of the OH and ON distances for the reactive conical intersections suggests that they may occur at a fixed NH distance of the Hz$\\cdots$H\\textsubscript{2}O complex.  This expectation is confirmed by the dynamic maps constructed as functions of the NH and ON distances which are displayed in Figures~S3a and S3b.  It is seen that the reactive conical intersections are aligned on a vertical line close to 1.0~\\AA\\space in the NH-ON plane.  The nonreactive conical intersections, on the contrary, are broadly distributed in the NH distance from about 2.0~\\AA\\space to 4.0~\\AA.  The formation of the covalent NH bond of the HzH radical is thus critical for the occurrence of reactive S\\textsubscript{1}-S\\textsubscript{0} conical intersections in the Hz$\\cdots$H\\textsubscript{2}O complex.",
        "summarize_figure": "2505.03355v1_dynamic_map_comp",
        "summarization": "Figure a is a dynamic map within the first 100 fs after the pump - pulse excitation (projection of the trajectories onto the plane of the OH and ON distances). The horizontal axis is the OH distance, and the vertical axis is the ON distance. The color represents energy. The data points show that the OH distance is confined to 0.9 - 1.3 Å, and the ON distance varies from 2.7 - 3.6 Å. This region represents the radiationless electronic decay dynamics of the Hz chromophore from the bright S₅/S₆ states to the S₁ state. Only a few trajectories enter the proton - transfer region (OH distance > 1.5 Å). Proton - transfer dynamics is strongly correlated with the donor - acceptor distance ON, and the PCET reaction is a rare event in the Hz∙∙∙H₂O complex."
    },
    "297": {
        "figure1": "2505.03356v1_learning_curve_mujoco",
        "label1": "lc mujoco",
        "caption1": "Learning curves over four benchmark tasks.  Curves are uniformly smoothed for visual clarity. The shaded region represents the corresponding standard deviation.",
        "text": "The learning capability of the proposed method was illustrated by the learning curves in Fig.~\\ref{lc mujoco} and the average returns in Table~\\ref{table:performance} (the number in red indicated the best performance among all approaches). It is clearly demonstrate the significant advantages of the CSAC algorithm across multiple environments.",
        "summarize_figure": "2505.03356v1_learning_curve_mujoco",
        "summarization": "The picture shows learning curves for HalfCheetah-v4, Walker2d-v4, Ant-v4, and Hopper-v4 benchmark tasks. The x - axis is time steps (1e6), y - axis is average return. Five algorithms (CSAC(ours), SAC, PPO, TD3, SD3) are compared. Curves are smoothed, shaded areas show standard deviation. CSAC(ours) shows advantages across tasks."
    },
    "298": {
        "figure1": "2505.03356v1_diff_Walker2d-v4",
        "label1": "fig:diff",
        "caption1": "Learning curves of CSAC with different \\(\\tau\\) values in the Walker2d-v4 task. The shaded region represents the standard deviation of the average evaluation over five trials. Curves are smoothed uniformly for visual clarity.",
        "text": "The experimental results, illustrated in the accompanying Fig.\\ref{fig:diff}, present the variation in average returns within one million timesteps under different \\(\\tau\\) settings, ranging from 0.005 to 50.0.",
        "summarize_figure": "2505.03356v1_diff_Walker2d-v4",
        "summarization": "The picture shows the learning curves of the CSAC algorithm with different τ values in the Walker2d - v4 task. The horizontal axis is the time step (in units of 1e6), and the vertical axis is the average return. Different - colored curves represent τ values of 0.005, 0.05, 0.5 (ours), 5.0, and 50.0. The shaded area represents the standard deviation of the average evaluation over five trials, and the curves are uniformly smoothed. As the time step increases, the trends of the average return under different τ values vary, and the curve with τ = 0.5 (ours) shows a relatively significant increase in average return."
    },
    "299": {
        "figure1": "2505.03360v1_density_hybrid_0.150000",
        "label1": "fig_density_sim1",
        "caption1": "Time evolution of density for \\hyperref[subTestSod]{\\textbf{Test 1}}. In the first row the regime adaptation and the density at three different times are displayed. Red, green and black dots, identify, respectively, cells updated using Euler equations, Boltzmann equations with ES-BGK operator and Boltzmann equations with Boltzmann (Hard-Sphere) operator, using the new hybrid scheme. The cyan line is the analytical solution, the magenta one is the solution obtained using only the Euler equations, and the blue line is computed using only the Boltzmann equation. The same plots, but for the temperature, are showed in the second row. The time evolution of the density for the hybrid scheme in the 2D space (solution constant along the $y-$direction) is displayed in the last row.",
        "text": "A classical benchmark test for the 1D Euler equations is the so-called \\emph{Sod shock tube test problem} \\cite{tor-2009}: its solution consists of a left rarefaction, a contact and a right shock. In the 1D case, on a spatial domain $[0, 1]$ the initial condition is constituted by a left state $U_L=(\\rho_L, u_{xL}, P_L)=(1.0, 0.0, 1.0)$ for $x\\leq 0.5$ and by a right state $U_R=(\\rho_R, u_{xR}, P_R)=(0.125, 0.0, 0.1)$ for $x>0.5$. In this test we simulate these initial conditions, considering also the $y-$direction, along which the solution is always uniform in space. Also the velocity along the $y-$direction is null. In Figure \\ref{fig_density_sim1} we compare the density and the temperature obtained using the hybrid scheme, with the Euler solution, with the Hard-Sphere solution and the analytic one (associated to the Euler equations). We discretize the spatial domain with $100$ points in the $x-$ direction, and $16$ in the $y-$ direction, imposing $\\Delta x = \\Delta y$. The velocity domain is discretized using 32 points in the three directions, and we considered a cut-off domain $\\Omega_v=[-8, 8]^3$. The value of $R$ has been taken approximately equal to 3.3, and both numbers of points $A_1$ and $A_2$ discretizing the spherical integration of the Boltzmann operator (see Section \\ref{subBoltzOp}) have been taken equal to 4.         The distribution function is initialized everywhere as a Maxwellian, whose moments, correspond to the ones of the Euler equations (i.e. $U_L$ and $U_R$), and null velocity in the other directions.         We consider a Knudsen number $\\epsilon=10^{-6}$ and a time step $\\Delta t=0.1 dx$. It is remarkable that the usage of an IMEX approach for the integration of the ES-BGK equation and of the full Boltzmann one allowed us to use the timestep associated to the transport part of the Boltzmann equation, instead of the one constrained by the collision operator, which in that case, would be much smaller than the one implemented thanks to the IMEX approach. This property significantly accelerates the code execution.         The thresholds used are: $\\eta_0=1\\times 10^{-5}$, $\\eta_1=3.5\\times 10^{-10}$, $\\delta_0=10^{-3}$.         This test allows us to check that the coupling and the transitions between different regimes are executed correctly and that all the numerical schemes are working properly and converging towards the right solution. Indeed, since this test is performed in a very stiff regime (i.e. $\\epsilon << 1$) then the solution of the kinetic levels must converge towards the hydrodynamic limit (Euler equations).         It is clear that the numerical solution is strongly consistent with the analytical one.         For this specific test case, the hybrid solution is approximately 1.8 times faster then the full kinetic one.",
        "summarize_figure": "2505.03360v1_density_hybrid_0.150000",
        "summarization": "The picture shows the time evolution of density in Test 1 (Sod shock tube test) at time = 0.150 under a hybrid scheme. Contour lines of different colors show the density distribution, with a color bar for density values. Conducted on [0, 1] with specific initial left and right states, the hybrid scheme combines Euler and Boltzmann equations. Comparing with analytical, Euler - only, and Boltzmann - only solutions validates the scheme and numerical convergence. The hybrid solution is about 1.8 times faster than the full kinetic one."
    },
    "300": {
        "figure1": "2505.03360v1_density_bol_0.225000",
        "label1": "fig_density_sim2",
        "caption1": "Time evolution of density for \\hyperref[subTestFluidFixedRectObs]{\\textbf{Test 2}}.  The first row displays the regime adaptation at three different times. Red, green and black dots, identify, respectively, cells updated using Euler equations, ES-BGK equation and Boltzmann equation. In the second, third and fourth row are displayed, respectively, the time evolution of density computed using hybrid scheme, Euler equations and Boltzmann equation.",
        "text": "To simulate the presence of a rectangular object, we have defined a rectangular domain (white cells in Figure \\ref{fig_density_sim2} and Figure \\ref{fig_temperature_sim2}) in which the integrators are never executed. These cells, are part of the stencils of the closest cells of the integration domain, thus we have assigned to them a constant and uniform value of temperature, pressure, density and velocity, given by: $P_*=1.5, \\rho_*=1.0, T_*=P_* / \\rho_*$ and $\\vec{u}_*=\\vec{0}$. This rectangular shape domain, is surrounded by a small region (2 cells thick) in which the integrator is always the one associated to the full Boltzmann equation with Boltzmann operator.\\\\ The fluid in the integration domain is initialized with the following parameters: $P=1.0, \\rho=1.0, \\vec{u}=(3.0, 0.0, 0.0)$. The distribution functions are initialized everywhere as a Maxwellian whose moments corresponds to the initial values of the Euler equations.\nIn Figures \\ref{fig_density_sim2} and \\ref{fig_temperature_sim2}, are showed, respectively, the density and temperature profiles, with $\\epsilon = 10^{-4}$, at time $t=0.075,0.15, 0.225$ for the hybrid scheme, the full Euler integrator and for the Boltzmann operator (computed using CUDA GPU parallelization), as well as the domain indicators for the hybrid scheme (red, green and black dots indicate cells computed using, respectively, Euler equations, ES-BGK operator and Boltzmann operator). In this simulation we considered $\\Xi=5/3$ as specific heat ratio. We estimated, as explained in Section \\ref{subNumSim}, the total duration of the numerical integration performed using just the full Boltzmann operator on CPU over the entire domain, and the ratio between the execution time for the full Boltzmann and the hybrid scheme is equal to 3. The thresholds used are: $\\eta_0=1\\times 10^{-3}$, $\\eta_1=2.8\\times10^{-5}$, $\\delta_0=10^{-4}$. The time step is equal to $0.1 dx$.",
        "summarize_figure": "2505.03360v1_density_bol_0.225000",
        "summarization": "The picture shows the time evolution of density in “Test 2” at time = 0.225. Different - colored contour lines represent the density distribution, and the color bar indicates the range of density values. To simulate the presence of a rectangular object, a rectangular region (white part) is defined where the integrator does not execute, surrounded by a small region where the integrator of the full Boltzmann equation with the Boltzmann operator is always used. The fluid in the integration domain has initial parameters, and the distribution function is initialized as a Maxwellian. This figure compares the density distributions computed by the hybrid scheme, the Euler equations, and the Boltzmann equation. In the hybrid scheme, red, green, and black dots represent cells updated using the Euler equations, the ES - BGK equation, and the Boltzmann equation, respectively, verifying the effectiveness of the scheme. The ratio of the execution time of the full Boltzmann operator on the CPU to that of the hybrid scheme is 3."
    },
    "301": {
        "figure1": "2505.03360v1_temperature_bol_0.225000",
        "label1": "fig_temperature_sim2",
        "caption1": "Time evolution of temperature for \\hyperref[subTestFluidFixedRectObs]{\\textbf{Test 2}}.  In the first, second and third row are displayed, respectively, the time evolution of temperature computed using hybrid scheme, Euler equations and Boltzmann equation.",
        "text": "To simulate the presence of a rectangular object, we have defined a rectangular domain (white cells in Figure \\ref{fig_density_sim2} and Figure \\ref{fig_temperature_sim2}) in which the integrators are never executed. These cells, are part of the stencils of the closest cells of the integration domain, thus we have assigned to them a constant and uniform value of temperature, pressure, density and velocity, given by: $P_*=1.5, \\rho_*=1.0, T_*=P_* / \\rho_*$ and $\\vec{u}_*=\\vec{0}$. This rectangular shape domain, is surrounded by a small region (2 cells thick) in which the integrator is always the one associated to the full Boltzmann equation with Boltzmann operator.\\\\ The fluid in the integration domain is initialized with the following parameters: $P=1.0, \\rho=1.0, \\vec{u}=(3.0, 0.0, 0.0)$. The distribution functions are initialized everywhere as a Maxwellian whose moments corresponds to the initial values of the Euler equations.\nIn Figures \\ref{fig_density_sim2} and \\ref{fig_temperature_sim2}, are showed, respectively, the density and temperature profiles, with $\\epsilon = 10^{-4}$, at time $t=0.075,0.15, 0.225$ for the hybrid scheme, the full Euler integrator and for the Boltzmann operator (computed using CUDA GPU parallelization), as well as the domain indicators for the hybrid scheme (red, green and black dots indicate cells computed using, respectively, Euler equations, ES-BGK operator and Boltzmann operator). In this simulation we considered $\\Xi=5/3$ as specific heat ratio. We estimated, as explained in Section \\ref{subNumSim}, the total duration of the numerical integration performed using just the full Boltzmann operator on CPU over the entire domain, and the ratio between the execution time for the full Boltzmann and the hybrid scheme is equal to 3. The thresholds used are: $\\eta_0=1\\times 10^{-3}$, $\\eta_1=2.8\\times10^{-5}$, $\\delta_0=10^{-4}$. The time step is equal to $0.1 dx$.",
        "summarize_figure": "2505.03360v1_temperature_bol_0.225000",
        "summarization": "The picture shows the time evolution of temperature in “Test 2” at time = 0.225. Different - colored contour lines represent the temperature distribution, and the color bar indicates the range of temperature values. To simulate a rectangular object, a rectangular region (white part) is defined where the integrator does not execute, and the surrounding small region uses the integrator of the full Boltzmann equation with the Boltzmann operator. The fluid in the integration domain has initial parameters, and the distribution function is initialized as a Maxwellian. This figure compares the temperature distributions computed by the hybrid scheme, the Euler equations, and the Boltzmann equation. In the hybrid scheme, red, green, and black dots represent cells updated using the Euler equations, the ES - BGK equation, and the Boltzmann equation, respectively. It also presents relevant parameter settings and the execution - time ratio of 3 between the full Boltzmann operator and the hybrid scheme."
    },
    "302": {
        "figure1": "2505.03364v1_1-Sensemaking_loop",
        "label1": "fig:fs-loop",
        "caption1": "setup{justification=justified, singlelinecheck=false} \\Description[Foraging-Sensemaking Loop]{Foraging-Sensemaking Loop} \\centering \\includegraphics[width=1\\linewidth]{figures/1-Sensemaking_loop.pdf} \\setlength{\\abovecaptionskip}{-2mm} \\caption{Foraging-Sensemaking Loop~\\cite{Pirolli2005} (top) can be divided into two main loops: the foraging loop and the sensemaking loop. The automated workflow of DroidRetriever (bottom) aligns with the Foraging-Sensemaking Loop.}  \\vspace{-3mm",
        "text": "Mobile applications have become an integral part of daily life, serving as essential tools for fulfilling diverse information needs in both routine and dynamic contexts. Whether planning a trip, tracking deliveries, or deciding where to eat, users rely on mobile apps to gather, compare, and evaluate information in real time. This constant engagement with fragmented data across multiple apps reflects a deeper cognitive process known as sensemaking—the active interpretation and integration of information to understand situations and make informed decisions. When this process occurs on-the-go, mediated through smartphones or other mobile devices, it is referred to as mobile sensemaking. As users shift between apps, they engage in iterative cycles of foraging for information and synthesizing it—a pattern researchers term the forage-sensemaking loop ~\\cite{Pirolli1995, Pirolli2005}, as illustrated in Fig.~\\ref{fig:fs-loop}. This mobile sensemaking process is increasingly critical in today’s fast-paced digital ecosystem, where timely, informed action depends on the ability to navigate complex information landscapes across multiple interfaces.\nThe sensemaking process typically involves two intertwined phases: foraging and structuring~\\cite{Pirolli1995, Pirolli2005}. During foraging, individuals gather information from diverse sources such as articles, blogs, and videos, while structuring involves schematizing this information into coherent formats like comparison tables or decision trees~\\cite{liu2023tool}. These phases form an iterative Foraging-Sensemaking Loop (Fig.~\\ref{fig:fs-loop}), which can be cognitively demanding, especially when processing large amounts of data~\\cite{sensemaking-cost-1993}.",
        "summarize_figure": "2505.03364v1_1-Sensemaking_loop",
        "summarization": "The picture shows the Foraging - Sensemaking Loop, which can be divided into the foraging loop and the sensemaking loop. The foraging loop includes Search (locating target pages in relevant apps), Read (reading information on each page), and Organize (extracting and semantically grouping useful information). The sensemaking loop includes Schematize (ideating schema and structuring information), Refine (refining the schema), and Present (using the collected information for specific activities). It also shows the automated workflow of DroidRetriever corresponding to this loop, including planning the search and auto - navigating on mobile, reading and extracting relevant information from scrolling screenshots, etc."
    },
    "303": {
        "figure1": "2505.03368v1_img_probing-and-sae_v1-0",
        "label1": "fig:framework",
        "caption1": " An example illustrating the extraction of the activations from an LLM (top); the use of the activations in a linear probe to predict the latitude and longitude of the place mentioned in the input (bottom-left) and a sparse autoencoder (bottom-right); and the use of spatial autocorrelation to analyse the activations and the sparse features (centre). Our approach encompasses the latter two components. ",
        "text": "To clarify the terminology used in the rest of the chapter -- a neural network can be thought of as composed of \\emph{neurons} laid out in subsequent \\emph{layers} with \\emph{edges} connecting neurons at one layer to neurons at the next layer \\cite{lecun2015deep}. Each edge contains one of the ``levers'' mentioned above, which are called \\emph{parameters} (or \\emph{weights}) and control the flow of information through the network. Each neuron of the first layer corresponds to a piece of the input (e.g., a pixel of an image, a column of a table, or a word in a sentence). Each piece of information of the input is pushed forward from its neuron at the first layer through the edges towards the second layer. In transit, each edge uses its own parameter to modify the input into a new value, which arrives at the neuron at the second layer. That neuron combines together all the values from all its incoming edges and applies an \\emph{activation function} (usually a non-linear function such as a sigmoid or a ReLU \\cite{nair2010rectified}) to produce a value commonly referred to as \\emph{activation}. The \\emph{transformer layers} \\cite{vaswani2023attentionneed}, of which LLMs are composed, are sophisticated blocks composed of several layers, including a mechanism called \\emph{attention}, which allows LLMs to better understand how different parts of a text influence each other's meaning. The analyses in this chapter focus on the activations extracted after the attention mechanism, before the \\emph{Multi-Layer Perceptron (MLP)} component of the transformer layer, as illustrated in Figure~\\ref{fig:framework}. A set of activations from the same layer is called an internal \\emph{representation} \\cite{6472238} of the input at that specific layer, and if it is identified as something humanly interpretable -- such as high output values when the input  references to the Golden Gate Bridge in a language model \\cite{templeton2024scaling} -- it is called a \\emph{feature} of the input.\nProbing techniques often employ targeted tasks that reveal how well the internal representations capture specific linguistic aspects or concepts. A typical probing process involves extracting the model's internal representations while the model is processing a specifically selected set of inputs, as illustrated in Figure~\\ref{fig:framework}. A lightweight model (e.g., a linear regression) is then trained to perform a specific task, such as estimating the latitude and longitude of the place mentioned in the input sentence based on the internal representation extracted from the model. If the lightweight model performs well on the task, it suggests that the LLM's internal representations contain the relevant information. For example, if a linear regression can reliably predict the latitude and longitude of a place based on the internal representations extracted from the LLM while it processes a sentence about that place, we can infer that the LLM has encoded some form of geographical awareness in its internal parameters. These types of probes provide insights into how well the LLM has learned to encode fundamental properties, thus offering explainability about both its strengths and limitations in handling basic linguistic properties.\nIn this section, we explore how spatial autocorrelation \\cite{getis2009spatial} can be used to identify interpretable geospatial patterns in internal representations extracted from LLMs. Our methodology involves using placenames alongside their geographical areas as prompts to generate internal representations, which are then correlated with those of nearby placenames using spatial autocorrelation to assess how well the LLM internalises geographical patterns, as illustrated in the bottom-centre component of Figure \\ref{fig:framework}.\nIn this section, we describe our approach to extracting geospatially interpretable features using a sparse autoencoder \\cite{ng2011sparse} and spatial autocorrelation \\cite{osullivan2010geographic,getis2009spatial}. That is illustrated in Figure \\ref{fig:framework}, where the features obtained from the sparse autoencoder in the bottom-right component are used as input for a spatial autocorrelation analysis in the bottom-centre component. The primary goal is to understand how LLMs encode geographical information and whether these internal representations display geospatial patterns.",
        "summarize_figure": "2505.03368v1_img_probing-and-sae_v1-0",
        "summarization": "The picture shows an example of extracting activations from a Large Language Model (LLM). The top part is the LLM structure, including the Transformer module, which consists of the Attention mechanism and the Multi-Layer Perceptron (MLP). The bottom part shows the applications of activations: the bottom - left is a Linear probe, using activations to predict the latitude and longitude of a place in the input; the center is Spatial autocorrelation, used to analyze activations and sparse features; the bottom - right is a Sparse autoencoder, reconstructing attention values. This approach encompasses the latter two components, aiming to understand how LLMs encode geographical information and whether their internal representations exhibit geospatial patterns."
    },
    "304": {
        "figure1": "2505.03368v1_img_results-probing_v1-0",
        "label1": "fig:results-map-activations",
        "caption1": "Activations captured for the input placenames at different layers of the LLM (left for each region) and their local spatial autocorrelation (local Moran's $I$ clusters, $p<.01$, right for each region), illustrating the polysemantic nature of its internal representations. Two neurons at layers 7 (a) and 15 (b) show high values for the State of New York and Northern Ireland and very low values for northern Italy. A neuron at layer (c) 15 shows high values for several UK cities. A neuron at layer 31 (d) shows high values for the State of New York and Northern Ireland and diverse values for provinces in Italy.",
        "text": "Our results illustrate a glimpse of all the aspects mentioned above. While discussing all the 1,841 (14.98\\%) neurons across layers 7, 15, and 31 that display a significant autocorrelation ($p<.01$ and Moran's $I \\geq 0.3$) is beyond the scope of this chapter, some examples are reported in Figure~\\ref{fig:results-map-activations}.\nFor instance, Figure~\\ref{fig:results-map-activations}(a) illustrates the internal representations of the neuron with the second highest spatial autocorrelation at layer 7. Very high values highlight areas whose name was used as part of the prompts (e.g., ``\\texttt{New York}'' as \\texttt{[state]} in the input for placenames in the State of New York or ``\\texttt{Northern Ireland}'' as \\texttt{[country]} for placenames in Northern Ireland, as outlined in Section~\\ref{sec:core-methods-data}). At the same time, a clear pattern of increasing values can be seen by moving from the north of Italy to the south, despite prompts not including explicit references to southern, central or northern Italy, only the name of the place and province. This indicates that the LLM might be encoding broader regions or geographical locations as part of the internal representation of place and province names, which could be linked to LLMs' ability to generate latitude and longitude values for input placenames \\cite{mai2023opportunitieschallengesfoundationmodels, 10.1145/3589132.3625625}. A very similar pattern is also displayed by the neuron with the sixth-highest spatial autocorrelation at layer 15, as shown in Figure~\\ref{fig:results-map-activations}(b), which indicates some form of consistency across layers. However, the neuron with the second highest spatial autocorrelation at layer 15 might be introducing some level of complexity that goes beyond the linearity of cartographic coordinates or named areas. As shown in Figure~\\ref{fig:results-map-activations}(c), that particular neuron seems to reveal particularly high values in East Anglia and the East Midlands, with lower values in larger cities such as London, Birmingham, Liverpool and Manchester in the UK, as well as New York City and Philadelphia in the US. At the same time, the neuron also seems to display a gradual change from the north of Italy to the south, as in Figure~\\ref{fig:results-map-activations}(a) and (b), without a clear pattern related to major cities. Figure~\\ref{fig:results-map-activations}(b) and (c) illustrate how different neurons represent different information within the same layer. Interestingly, the final layer seems to focus back to local or broad-stroke geographies, as illustrated in Figure~\\ref{fig:results-map-activations}(d) by the neuron with the highest spatial autocorrelation at layer 31, which shows low values for Pennsylvania and high values for the State of New York, generally low values for Great Britain and high in Northern Ireland, and a patchwork of high and low values across Italy.\nImportantly, the neurons showing high spatial autocorrelation do not seem to behave in a monosemantic (one-meaning) manner, where they would output high values for one area or concept only. Rather, they can behave in a polysemantic (multiple-meaning) manner, where the same neuron (same row in Figure~\\ref{fig:results-map-activations}) outputs high values for different areas or concepts. That seems to indicate that the superposition hypothesis (introduced below) might hold for geographical information. Therefore, in the next section, we explore a mechanistic interpretability approach aimed at addressing this limitation by decomposing these polysemantic structures into more interpretable, monosemantic features. In particular, we explore the use of sparse autoencoders to identify specific directions in the LLM's internal representation space that correspond to distinct geographical entities and concepts, allowing us to better understand how LLMs process geographical information.",
        "summarize_figure": "2505.03368v1_img_results-probing_v1-0",
        "summarization": "The picture shows the activations captured for input placenames (left) and their local spatial autocorrelation (local Moran's I clusters, p < 0.01, right) in different layers of the Large Language Model (LLM), illustrating the polysemantic nature of its internal representations. For example, two neurons at layer 7 (a) and layer 15 (b) show high values for the State of New York and Northern Ireland, and very low values for northern Italy. A neuron at layer 15 (c) shows high values for several UK cities. A neuron at layer 31 (d) shows high values for the State of New York and Northern Ireland, and diverse values for provinces in Italy."
    },
    "305": {
        "figure1": "2505.03368v1_img_results-sparse-autoencoder_v1-0",
        "label1": "fig:results-map-sae-l15",
        "caption1": "Features extracted from layer 15 through a sparse autoencoder (left for each region) and their local spatial autocorrelation (local Moran's $I$ clusters, $p<.01$, right for each region): (a) Wales as a region part of prompt; (b) south of Italy as a region activating a seemingly monosemantic feature; (c) north-east of Italy and north-west of England as regions activating a seemingly polysemantic feature; and (d) a representation of ``city'' highlighting New York City and London, amongst others.",
        "text": "For instance, Figure~\\ref{fig:results-map-sae-l15}(a) illustrates an example of a feature encoding a region that was part of the prompt, in this case, Wales. Figure~\\ref{fig:results-map-sae-l15}(b) and (c) are, instead, examples of features displaying strong spatial autocorrelation and high values across areas that were not explicitly part of input prompts. While Italian provinces were used in the prompts for Italian placenames, the feature in Figure~\\ref{fig:results-map-sae-l15}(b) encodes the whole of southern Italy, and the feature in Figure~\\ref{fig:results-map-sae-l15}(c) encodes the north-east areas of the country. At the same time, while the feature illustrated in \\ref{fig:results-map-sae-l15}(b) seems to be strongly monosemantic -- with very high values in the south of Italy and low values in the centre and north of Italy, as well as across the UK and the US states included in the analysis -- the feature illustrated in \\ref{fig:results-map-sae-l15}(c) displays high values for both the north-east of Italy (especially around Venice) and the north-west of England (especially around Liverpool). A connection might be drawn around the historical importance of Liverpool and Venice as ports and areas of industrial development. However, that would exclude other places with comparable port and industrial histories that are part of the maps but do not display high values, such as Genova. A clearer display of an important geographical concept (i.e., ``urban'') is instead illustrated in \\ref{fig:results-map-sae-l15}(d), where high values are visible for several large cities, including New York City, Philadelphia, Pittsburg, London, Manchester and Milano.",
        "summarize_figure": "2505.03368v1_img_results-sparse-autoencoder_v1-0",
        "summarization": "The picture shows the features extracted from layer 15 of the Large Language Model through a sparse autoencoder (left) and their local spatial autocorrelation (local Moran's I clusters, p < 0.01, right). For example, (a) shows the feature encoding of Wales as part of the prompt region; (b) shows the south of Italy activating a seemingly monosemantic feature; (c) shows the north - east of Italy and the north - west of England activating a seemingly polysemantic feature; (d) highlights the concept of “city,” such as New York City and London."
    },
    "306": {
        "figure1": "2505.03372v1_hist_comp",
        "label1": "fig:hist_comp",
        "caption1": "Comparison of CUB's histogram implementation with my own, which is either launched with as many threads as the size of the text (large grid) or as many threads as necessary to fully occupy the GPU (small grid). Texts are uniformly randomly generated. Run on an NVIDIA RTX 3090.",
        "text": "Another important optimization was finding the optimal total number of threads to launch. It is often recommended to launch as many threads as units of work (in this case the text size), but I found this to severely worsen performance in comparison to a persistent-thread approach, where I launch just enough threads in order to fully occupy the GPU. Figure \\ref{fig:hist_comp} shows the difference between the different total numbers of threads for uniformly randomly distributed texts with a variety of alphabet sizes. This makes sense since the more threads you launch, and therefore blocks, the more atomic writes you need to make to the global histogram.\nSurprisingly, this implementation performed significantly better than CUB's. Figure \\ref{fig:hist_comp} also compares their performance for randomly distributed texts with a variety of alphabet sizes. The jump in execution time seen in my implementation corresponds to when the histogram becomes too large to fit in shared memory.",
        "summarize_figure": "2505.03372v1_hist_comp",
        "summarization": "The picture compares the time performance of CUB's histogram implementation (CUB) and custom implementations (Custom small grid and Custom large grid) under different alphabet sizes. The horizontal axis is the alphabet size, and the vertical axis is the time (in ms). The data points show the time changes of the three methods when the number of elements is 1G. The Custom small grid method has the lowest and most stable time consumption. The Custom large grid method has time fluctuations as the alphabet size changes. The CUB method has a sharp increase in time as the alphabet size increases, which is related to whether the histogram exceeds the shared memory."
    },
    "307": {
        "figure1": "2505.03372v1_flk_comp",
        "label1": "fig:flk_comp",
        "caption1": "Comparison of the filling of a level of the wavelet tree using method one (no shared memory per thread) or method two (shared memory per thread). Texts are uniformly randomly generated with an alphabet size of 4. Run on an NVIDIA RTX 3090.",
        "text": "In the second possibility, each thread has an array in shared memory where it can store 32 symbols. The first step is to load a slice of the text into shared memory so that each thread has its 32 symbols in its shared memory array. Between arrays there is padding in order to avoid bank conflicts, since inside of a warp the threads will be accessing the same element in their array. Once each thread has its 32 symbols in shared memory, it accesses them sequentially and extracts the desired bit from each. It then writes the 32-bit word to the bit array. This results in fully coalesced accesses to global memory and bank-conflict free accesses to shared memory. The advantage of this method over the first is that the parallelism is at thread-level and not warp-level. Figure \\ref{fig:flk_comp} shows a performance comparison of both methods.",
        "summarize_figure": "2505.03372v1_flk_comp",
        "summarization": "The picture compares the performance of two methods when filling a level of the wavelet tree. The horizontal axis is the data size, and the vertical axis is the time (in ms). The red curve represents the “No shared memory per thread” method, and the blue curve represents the “Shared memory per thread” method. As the data size increases from 2 to the 29th power to 2 to the 33rd power, the time of both methods increases, but the “Shared memory per thread” method has a relatively slower time increase and better performance."
    },
    "308": {
        "figure1": "2505.03375v1_overview",
        "label1": "fig:scenario",
        "caption1": "Overview of the workflow proposed for efficient \\wifi sensing in \\gls{iot} forensics applications. Collected \\acrshort{csi} data is stored in a compressed form and retrieved after a potentially very long time. Lossy compression algorithms can be used to minimize the \\acrshort{csi} storage requirements if they are proven to retain good sensing accuracy.",
        "text": "In this work, we investigate the impact of lossy compression on \\wifi sensing in forensic \\gls{iot} contexts. We evaluate both traditional compression schemes (using scalar and vector quantization) and \\gls{dl} approaches based on recent works in this field~\\cite{yang2022efficientfi, cominelli2023fusion}. The pipeline we propose in \\cref{fig:scenario} compresses \\gls{csi} data at the time of collection, enabling efficient long-term storage and retrieval for forensic analysis. Our goal is to characterize the tradeoff between compression ratio and classification accuracy for two important sensing tasks---presence detection and activity recognition---identifying strategies that preserve sensing capabilities while significantly reducing storage requirements. To this end, we present and discuss empirical results obtained on real-world datasets showing that traditional compression techniques can already achieve outstanding performance with a simpler and more explainable architecture. Our analysis ultimately aims to simplify the design of data-efficient and forensic-ready \\wifi sensing systems.",
        "summarize_figure": "2505.03375v1_overview",
        "summarization": "The picture shows the workflow for efficient Wi - Fi sensing in IoT forensic applications. First, Wi - Fi sensing data is collected through Wi - Fi transmit (Wi - Fi TX) and receive (Wi - Fi RX) devices to obtain full Channel State Information (CSI) data. Then, the full CSI data is fed into the lossy compression module, which includes methods such as Principal Component Analysis (PCA), Scalar Quantization (Scalar Quant.), Vector Quantization (Vector Quant.), and Deep Learning (Deep Learning) to compress the data into compressed CSI data. Finally, the compressed data is stored and retrieved for forensic tasks such as presence detection and activity recognition. This workflow aims to balance the compression ratio and classification accuracy, maintaining sensing capabilities while reducing storage requirements."
    },
    "309": {
        "figure1": "2505.03375v1_model2",
        "label1": "fig:methodology",
        "caption1": "Methodology for evaluating the tradeoff between the compression ratio and sensing accuracy. In the compression stage, we consider several different algorithms; the sensing accuracy is evaluated in two target application scenarios, namely \\textit{presence detection} and \\textit{activity recognition}.",
        "text": "The diagram in \\cref{fig:methodology} shows the processing pipeline used to analyze the tradeoff between the \\gls{csi} data compression ratio and the sensing accuracy. After data collection, the Compression block applies the lossy compression techniques described in \\cref{ssec:lossy-compression}. The uncompressed CSI data is split into two disjoint subsets for training and evaluating the models. The trained models are then evaluated on the compressed data. Instead of measuring the absolute sensing accuracy (which depends on the sensing model), we are more interested in the performance losses due to the data reconstruction errors from lossy compression. Therefore, we measure the performance loss as the difference between the compressed vs. uncompressed sensing performance to analyze the tradeoff between storage and sensing capabilities.\nFor the threshold-based classifier, we consider the following techniques to compress the original data:     Classification is performed on the \\gls{csi} amplitude where each subcarrier is quantized. The performance is expressed as a function of the quantization level (up to 8 bits per subcarrier).     Classification is performed on the \\gls{csi} amplitude where the entire \\gls{csi} is vectorially quantized. The performance is expressed as a function of the quantization level (up to 8 bits per frame).     first, we apply the \\gls{pca} transform learned from training data and store each component as a 32-bit floating-point value. Then, classification is performed on the resulting frame where each component is quantized (up to 8 bits per component). We start with just one principal component and then consider more components.     first, we apply the \\gls{pca} transform learned from training data and store each component as a 32-bit floating-point value.     Then, classification is performed on the vector quantization of the resulting frame (up to 8 bits per frame). We start with just one principal component and then consider more components. Hence, the output of the Compression stage (cf. \\cref{fig:methodology}) is a compressed version of the \\gls{csi} amplitude dataset, containing time windows of \\wifi frames. We summarize the content of each time window using a single feature that quantifies the \\textit{amount of variation} of the \\gls{csi} data in that window. Specifically, since each time window $k$ contains several \\wifi frames, we first compute the sample standard deviation $\\sigma^i_{k}$ of the \\gls{csi} amplitude over each subcarrier $i$. Then, we compute the average standard deviation over all the subcarriers in the time window $k$ to describe the window content through the single-valued feature:     A^*_{k} = \\mathbb{E}\\left\\{\\sigma_{k}^{i}\\right\\} \\quad which is used to feed a binary threshold classifier.\nIn the \\gls{dl}-based approach, we replace the manual feature engineering with an automatic learning process. Instead of relying on the threshold classifier, we feed the compressed dataset to a \\gls{mlp} that has been trained to perform a binary classification of the current state of the room: presence or empty room. In addition to the traditional compression schemes described before, we introduce in the compression stage (cf., \\cref{fig:methodology}) a new technique based on a \\gls{vae}. The procedure based on the \\gls{vae} and \\gls{mlp} is sketched in \\cref{fig:vae_arch}. We highlight that the \\gls{vae} could not be used with the manual feature engineering described before because the latent space of the \\gls{vae} does not provide a convenient and explainable interpretation of its values (as opposed to $A^*$). However, the latent space variables of the \\gls{csi} can still be quantized with different levels of granularity, up to a maximum of 256 bits. For this reason, we can define also in this case two possible compression techniques: \\textbf{VAE\\_SQ} and \\textbf{VAE\\_VQ}, based respectively on the scalar and vector quantization of the \\gls{vae}'s latent space.",
        "summarize_figure": "2505.03375v1_model2",
        "summarization": "The picture shows the methodology for evaluating the tradeoff between the compression ratio and sensing accuracy. The original dataset is divided into training and test sets. The original dataset is compressed to obtain a compressed dataset. The training set is used for model training, and the original and compressed test sets are used for model evaluation respectively. The sensing performance loss is obtained by comparing the performance of the two. Several algorithms are considered in the compression stage, and the sensing accuracy is evaluated in two target application scenarios, namely presence detection and activity recognition, to analyze the tradeoff between storage and sensing capabilities."
    },
    "310": {
        "figure1": "2505.03375v1_vae_arch",
        "label1": "fig:vae_arch",
        "caption1": "Sketch of the pipeline for the \\acrshort{dl}-based classification approach when using the \\acrshort{vae} to compress the dataset.",
        "text": "In the \\gls{dl}-based approach, we replace the manual feature engineering with an automatic learning process. Instead of relying on the threshold classifier, we feed the compressed dataset to a \\gls{mlp} that has been trained to perform a binary classification of the current state of the room: presence or empty room. In addition to the traditional compression schemes described before, we introduce in the compression stage (cf., \\cref{fig:methodology}) a new technique based on a \\gls{vae}. The procedure based on the \\gls{vae} and \\gls{mlp} is sketched in \\cref{fig:vae_arch}. We highlight that the \\gls{vae} could not be used with the manual feature engineering described before because the latent space of the \\gls{vae} does not provide a convenient and explainable interpretation of its values (as opposed to $A^*$). However, the latent space variables of the \\gls{csi} can still be quantized with different levels of granularity, up to a maximum of 256 bits. For this reason, we can define also in this case two possible compression techniques: \\textbf{VAE\\_SQ} and \\textbf{VAE\\_VQ}, based respectively on the scalar and vector quantization of the \\gls{vae}'s latent space.",
        "summarize_figure": "2505.03375v1_vae_arch",
        "summarization": "The picture shows the pipeline framework for using a Variational Autoencoder (VAE) to compress the dataset in a Deep Learning (DL)-based classification approach. The 3D data on the left represents subcarrier data within a time window. It is processed by the Encoder to obtain mean (μ) and standard deviation (σ) parameters, and the latent variable Z is obtained through Sampling. Z is fed into both the Decoder and the Multi-Layer Perceptron (MLP) for classification to determine the activity in the room (Activity 1, Activity 2, Activity 3), thus enabling the processing and application of compressed data."
    },
    "311": {
        "figure1": "2505.03377v1_length_dists",
        "label1": "length-dist-fig",
        "caption1": "Learned embeddings capture length distributions of more accurately} \\vskip -0.16in",
        "text": "While the graph structure of the CRF ensures that the only meaningful hidden state and output label sequences are produced, the deep feature model attends to the task of learning as rich as possible a representation of the hidden state as possible. This enables the model to learn proper length distributions over the different intron and exons segments as seen in Figure \\ref{length-dist-fig}.\nWe find that our model readily learns these signals directly from data (Figure (\\ref{length-dist-fig})). The latent CRF without learned embedding is not sufficient to reproduce the true lengths resulting in a flattened distribution similar to \\citet{stanke_gene_2003}. However, with the learned embeddings, GeneDecoder captures the length distributions faithfully, especially for exons. It is very promising that the pre-trained embeddings GPN model is also able to capture the length distributions accurately. The GPN embeddings were not fine-tuned in this case. This further cements that incorporating deep learning, and particularly unsupervised pre-trained embeddings, is a promising direction for GeneDecoder and genefinders in general. Especially when expanding the model to perform cross-species prediction. However, DNA language modelling is still a nascent field and there are currently many unanswered questions on how to transfer the masked modelling concept to DNA sequences as compared to proteins. These concern both the sparsity of information, the longer sequences and the much longer range signal that likely needs to be captured for high performance in many downstream tasks. The GPN model is trained on DNA segments of 512 nucleotides which might exclude capturing many longer range signals and affect performance.",
        "summarize_figure": "2505.03377v1_length_dists",
        "summarization": "The picture shows the intron length distribution (top) and exon length distribution (bottom). Different colored curves in the figure represent the true distribution (True), linear model (Linear), GeneDecoder model, and GPN model. In both length distributions, the GeneDecoder model and GPN model can capture the true distribution well. Especially, the GPN model can accurately reflect it without fine - tuning, while the linear model performs poorly with a flatter distribution. This indicates that deep learning and pre - trained embeddings have promising applications in gene - related models."
    },
    "312": {
        "figure1": "2505.03382v1_Bestel_ODE",
        "label1": "fig:bestel_ode",
        "caption1": "Time-evolution of the Bestel-Clément-Sorine model with different values of the maximal active stiffness $\\sigma_0>0$.",
        "text": "In this work, we adopt the PINN framework to estimate active stress parameters in three-dimensional, time-dependent  cardiac biomechanical models.  The starting point is Cauchy's equation in the time-dependent formulation, where $\\bu(\\bX, t) \\in \\R^3$ describes the displacement at the point $\\bX \\in \\R^3$ and time $t>0$ in Lagrangian formulation, $\\bF=\\bI + \\nabla \\bu$ is the deformation tensor, $J = \\det(F)$ is its Jacobian, and $\\bP(\\bu)$ the first Piola-Kirchhoff stress tensor. In this work we consider zero body forces. $\\Gamma_N$ represents the portion of the boundary where Neumann boundary conditions (BCs) are imposed, whereas Dirichlet BCs are imposed on $\\Gamma_D$.   The unit vector $\\bn$ denotes the outward normal vector on $\\Gamma_N$ and $p$ is a given pressure. The vectors $\\bu_0$ and $\\bv_0$ represent the initial condition for the displacement and velocity, respectively. We consider an active stress formulation of $\\bP(\\bu)$ in the form where $\\bP_{pas}(\\bu)$ models passive behaviour and $\\bP_{act}(\\bu)$ active stress triggered by electrophysiological activation via excitation-contraction coupling mechanisms. According to standard assumptions in soft tissue modelling, particularly in the cardiac setting~\\citep{Holzapfel2009Constitutive}, the passive tissue behaviour is modelled as a nearly incompressible~\\citep{Flory1961}, hyperelastic material.  Its mechanical response is derived from an associated strain energy function $\\mathcal{W}=\\mathcal{W}(\\bF)$: We consider the transverse-isotropic Guccione material model~\\citep{Guccione1991Passive} where & b_{\\mathrm{t}}\\left[\\left(\\mathbf{s}_0 \\cdot \\overline{\\mathbf{E}} \\mathbf{s}_0\\right)^2+\\left(\\mathbf{n}_0 \\cdot \\overline{\\mathbf{E}} \\mathbf{n}_0\\right)^2+2\\left(\\mathbf{s}_0 \\cdot \\overline{\\mathbf{E}} \\mathbf{n}_0\\right)^2\\right]+\\\\ & 2 b_{\\mathrm{fs}}\\left[\\left(\\mathbf{f}_0 \\cdot \\overline{\\mathbf{E}} \\mathbf{s}_0\\right)^2+\\left(\\mathbf{f}_0 \\cdot \\overline{\\mathbf{E}} \\mathbf{n}_0\\right)^2\\right], with $\\bmf_0$ the myocyte fibre orientation; $\\bs_0$ the sheet orientation; $\\bn_0$ the sheet-normal orientation. Moreover, $\\overline{\\mathbf{E}}=\\frac{1}{2} \\left( \\overline{\\bC} - \\bI\\right)$ denotes the isochoric Green–Lagrange strain tensor, where $\\overline{\\mathbf{C}}:=J^{-2 / 3} \\bF^T\\bF$ is the isochoric right Cauchy-Green deformation tensor.  The parameter $\\alpha_P$ denotes the passive stiffness and is set to $\\SI{0.8}{\\kPa}$, whereas the bulk modulus $\\kappa$ is set to $\\SI{650}{\\kPa}$. Default values of $b_\\mathrm{f} = 18.48$, $b_\\mathrm{t} = 3.58$, and $b_\\mathrm{fs} = 1.627$ are used. For the active stress part, we consider the model~\\citep{ambrosi2012active} which acts along the fibre direction $\\bmf_0$. In this work we consider a combination of Dirichlet (BCD) and Neumann (BCN)  boundary conditions, as detailed in~\\Cref{sec:Results}.\\\\  The time-dependent active stress parameter $S_a(t)$ translates the electrophysiological signal into mechanical stress. It is modelled by the Bestel-Clément-Sorine model~\\citep{bestel2001biomechanical} S_a(0)=0, where the additional control variable $a(t)$ is given by~\\citep{AROSTICA2025117485} |a(t)|_{+} & =\\max \\{a(t), 0\\} \\\\ a(t) & =\\alpha_{\\max } f(t)+\\alpha_{\\min } \\cdot(1-f(t)) \\\\ f(t) & \\left.=S^{+}\\left(t-t_{\\text {sys }}\\right) \\cdot S^{-}\\left(t-t_{\\text {dias }}\\right)\\right) \\\\ S^{ \\pm}(\\Delta t) & =\\frac{1}{2}\\left( \\pm \\tanh \\left(\\frac{\\Delta t}{\\gamma}\\right)\\right). The parameters $\\alpha_{max}, \\alpha_{min}, t_{sys}, t_{dias}$ are related to the cardiac cycle. Physiological values $\\alpha_{min}=-30$, $\\alpha_{max}=5$, $t_{sys}=\\SI{0.161}{\\s}$, $t_{dias}=\\SI{0.484}{\\s} $~\\citep{AROSTICA2025117485} are used. From a biomechanical point of view, the maximum active stiffness $\\sigma_0 \\geq 0$ gives most information about the biomechanical properties of the tissue. Also, if $S_a^1(t)$ denotes the solution to the initial-value problem \\eqref{eqn:bestel-clement-sorine} with $\\sigma_0=1$, then the rescaled version $S_a^\\sigma(t) \\coloneqq \\sigma S_a^1(t)$ solves the same initial-value problem \\eqref{eqn:bestel-clement-sorine} with parameter $\\sigma_0=\\sigma$, as depicted in~\\Cref{fig:bestel_ode}.  In this work, we consider both a quasi-static approach and the time-dependent problem. In the first setting, for a fixed time point $t^* \\in (0,T]$, we denote the corresponding displacement $u^*(\\bX) \\in \\R^3$ as the solution to the quasi-static equation - \\nabla \\cdot \\bP_*(\\bu^*) = \\bmf, where Provided that we can compute $S_a^1(t^*)$, if we know $S_a(t^*)$, we can immediately reconstruct  For cardiac tissue, $\\sigma_0 >0$ is expected to vary in space depending on the health state of the tissue. This makes the solution of system \\eqref{eqn:bestel-clement-sorine} for the time-trajectory of $S_a(t)$ also space-dependent. Therefore, in general, we write $\\sigma_0(\\bx)$ and $S_a(t; \\bx)$. The PINN framework enables to approximate a solution to a given PDE and identify unknown parameters encoding the fundamental physical law governing a phenomenon partially measured with sensors.  In our in-silico test cases, the observation data employed to train the neural network is the FEM numerical solution of the test case considered, uniformly sampled in the domain to obtain $\\bu_i^{\\text{obs}}$, for $i = 1,2,\\ldots, N_\\mathrm{obs}$.  We mimic the presence of measurement error by corrupting this data with additive white noise, i.e. zero-mean Gaussian noise with a given standard deviation $\\sigma$: As in~\\citep{caforio2024physics}, we consider a metric defined as limiting dispersion (LD), defined as: Specifically, training the network involves minimising a carefully designed cost function that considers the residuals of the governing PDE, the initial and boundary conditions, and the data discrepancy term.  We refer to~\\citep{caforio2024physics} for further detail on the basic principles of PINNs and our implementation for applications in soft tissue mechanics. First, we consider the case of a homogeneous parameter field $\\sigma_0(\\bx)=\\sigma_0$ in the domain, i.e., a constant parameter.  For the quasi-static test case, this means that we want to reconstruct $S_a(t^*; \\bx) = S_a(t^*) \\eqqcolon S_a$. We represent the displacement field through a NN taking as input the three spatial coordinates and outputs the three components of the displacement field on that point.  The resulting problem is the following optimisation problem:     &\\hspace{-6.5ex} \\frac{\\lambda_{\\text{BCD}}}{N_\\mathrm{bc}}\\sum_{i = 1}^{N_{\\text{bcd}} }\\norm{\\bu_{\\Gamma_D} (\\mathbf{x}^{\\text{bcd}}_{i}) - \\mathrm{NN}_{\\bu}(\\mathbf{x}^{\\text{bcd}}_i; \\bw)}^2,\\\\     &\\hspace{-6.5ex} \\frac{\\lambda_{\\text{BCN}}}{N_\\mathrm{bcn}}\\sum_{i = 1}^{N_{\\text{bcn}} }\\norm{ $\\{\\bx_i^\\text{pde}\\}_{i=1}^{N_\\text{pde}}$, $\\{\\bx_i^\\text{bcd}\\}_{i=1}^{N_\\text{bcd}}$, and $\\{\\bx_i^\\text{bcn}\\}_{i=1}^{N_\\text{bcn}}$ denoting the collocation points for the PDE residual loss and the BCD and BCN loss terms, respectively.} The Neumann data $\\mathbf p$ is a pre-described pressure on the respective boundaries. To ensure clarity, we avoid using subscript 2 when referring to the $l^2$-norm. Hyperparameters $\\lambda_i$, for $i\\in\\{\\text{OBS},\\text{PDE},\\text{BCD},\\text{BCN}, w$\\}, play a role in non-dimensionalising each loss term and in weighting the contribution of each term to the overall loss function. Given that the parameter $S_a$ can also be defined as trainable parameters, the framework inherently enables us to conduct parameter estimation (model inversion) \\citep{Raissi2019,Haghighat2021a}.",
        "summarize_figure": "2505.03382v1_Bestel_ODE",
        "summarization": "This figure shows the time - evolution of the active stress parameter Sa in the Bestel - Clément - Sorine model for different values of the maximal active stiffness σ0 (values include 100, 112.5, 125, etc.). The horizontal axis represents time (in seconds), and the left vertical axis is the active stress parameter Sa (in kPa), while the right vertical axis is the control variable a. As time progresses, Sa first rises rapidly, reaches a peak, and then drops rapidly. Different σ0 values correspond to different curves. The larger the σ0 value, the higher the peak of Sa. The overall trend of the curves over time is similar, but the specific values differ."
    },
    "313": {
        "figure1": "2505.03385v1_flowchart1.2",
        "label1": "fig:Figure 7",
        "caption1": "Visualization of models' training an evaluation method}  \\end{flushright",
        "text": "The dataset utilized, created by \\citet{liu2017dataset}, exhibits an imbalance in the number of AR samples, with 552 C-class samples, 23 X-class samples, 142 M-class samples, and 128 B-class samples. To address this class imbalance, which poses a significant challenge in machine learning (e.g., \\citet{japkowicz2002}), the approach suggested by \\citet{liu2017} was adopted. From the total 552 C-class samples, 142 unique samples were randomly selected to create a balanced dataset. To ensure robustness and avoid bias, this random selection was repeated 100 times, producing 100 datasets for multi-class classification. Each dataset contains 128 B-class, 142 C-class, 142 M-class, and 23 X-class AR samples, as illustrated in the models' training method in \\hyperref[fig:Figure 7]{Figure~\\ref{fig:Figure 7}}.",
        "summarize_figure": "2505.03385v1_flowchart1.2",
        "summarization": "The figure shows the workflow of model training and evaluation methods. To handle data imbalance, the data is split 100 times to obtain a balanced multiclass dataset (with B, C, M, and X - class samples) and a binary classification dataset (with B/C - class and M/X - class samples). Then, different model parameter values are chosen from the hyperparameter grid, and 10 - fold cross - validation is performed. Each fold is divided into a test set (10%) and a training set (90%). Performance metrics for each fold are calculated, and the best metrics for each fold are found using GridSearch. Finally, the overall best performance metrics across folds and datasets are averaged to evaluate the model and compare the performance of different models."
    },
    "314": {
        "figure1": "2505.03385v1_modelmetrics8PC",
        "label1": "fig:Figure 11",
        "caption1": "Model performance Comparison across varying classifications (PC=8) \\textit{Note}: MC: Multiclass Classification ; BC: Binary Classification ",
        "text": "The results from \\autoref{table:Table 4} regarding \\textit{varying principal components}, suggest that Random Forest and XGBoost effectively leverage the additional features provided by 100 PC (capturing 97.5\\% of the variance of the original data set) to capture complex patterns and interactions in the data, resulting in better overall performance compared to the 8 PC approach (capturing 95\\% of data set variance). The improvements in accuracy, ROC AUC, and F1 scores indicate that these models benefit from the increased dimensionality, allowing them to make more accurate predictions. number of features, highlghted in \\hyperref[fig:Figure 11]{Figure~\\ref{fig:Figure 11}} and \\hyperref[fig:Figure 12]{Figure~\\ref{fig:Figure 12}}. While it shows some improvement in handling multiclass classification with more principal components, the decline in binary classification performance suggests that KNN may struggle with the higher dimensionality, potentially due to overfitting or difficulty in managing the increased complexity of the data. interaction features derived from PCA to optimize each model's performance.\nacross models highlights key insights into the performance of Random Forest, KNN, and XGBoost models using 8 principal components (PC) for both binary and multiclass classification tasks, visualized in \\hyperref[fig:Figure 11]{Figure~\\ref{fig:Figure 11}}. Random Forest and XGBoost exhibited solid performance in both classification tasks, maintaining high accuracy, ROC AUC, and F1 scores, indicating their ability to effectively leverage the essential features captured by the principal components. \\par In contrast, the KNN model showed moderate performance, with reasonable results in binary classification but slightly lower metrics compared to Random Forest and XGBoost. In multiclass classification, KNN exhibited modest improvement but still lagged behind the other models.",
        "summarize_figure": "2505.03385v1_modelmetrics8PC",
        "summarization": "The figure compares performance metrics (Accuracy, ROC_AUC, PR_AUC, F1_Score) of Random Forest, KNN, and XGBOOST models for 8 PC in multiclass (MC) and binary (BC) classification. In MC, Random Forest and XGBOOST have relatively high ROC_AUC; KNN's metrics are more balanced. In BC, Random Forest and XGBOOST maintain good performance, while KNN lags."
    },
    "315": {
        "figure1": "2505.03385v1_modelmetrics100PC",
        "label1": "fig:Figure 12",
        "caption1": "Model performance Comparison across varying classifications (PC=100) \\textit{Note}: MC: Multiclass Classification ; BC: Binary Classification ",
        "text": "The results from \\autoref{table:Table 4} regarding \\textit{varying principal components}, suggest that Random Forest and XGBoost effectively leverage the additional features provided by 100 PC (capturing 97.5\\% of the variance of the original data set) to capture complex patterns and interactions in the data, resulting in better overall performance compared to the 8 PC approach (capturing 95\\% of data set variance). The improvements in accuracy, ROC AUC, and F1 scores indicate that these models benefit from the increased dimensionality, allowing them to make more accurate predictions. number of features, highlghted in \\hyperref[fig:Figure 11]{Figure~\\ref{fig:Figure 11}} and \\hyperref[fig:Figure 12]{Figure~\\ref{fig:Figure 12}}. While it shows some improvement in handling multiclass classification with more principal components, the decline in binary classification performance suggests that KNN may struggle with the higher dimensionality, potentially due to overfitting or difficulty in managing the increased complexity of the data. interaction features derived from PCA to optimize each model's performance.",
        "summarize_figure": "2505.03385v1_modelmetrics100PC",
        "summarization": "The chart compares performance metrics (Accuracy, ROC_AUC, PR_AUC, F1_Score) of Random Forest, KNN, and XGBOOST models for 100 PC in multiclass (MC) and binary (BC) classification. In MC, Random Forest has high ROC_AUC; PR_AUC values are close, with little difference in Accuracy and F1_Score. In BC, KNN's performance drops; Random Forest and XGBOOST maintain better performance."
    },
    "316": {
        "figure1": "2505.03387v1_bootstrap_analysis_schema",
        "label1": "bootstrap_analysis_schema",
        "caption1": "Experimental Procedure for the Bootstrap Analysis. The preprocessed dataset is divided into six binary classification scenarios. For each scenario, training set sampling is performed at different sizes, while the remaining samples constitute the test set. Three classification approaches are evaluated: (1) the L1-KSVM Framework with augmentation, where 200 synthetic samples per class are generated using a Gaussian noise-based augmentation technique, (2) the L1-KSVM Framework without augmentation, which follows the same feature selection and classification procedure without synthetic data, and (3) the Baseline LASSO regression model, where features are retained if their L1-coefficient is greater than zero. Features collection and classification is then performed. Resulting data are collected to conduct performance analysis.",
        "text": "The experimental procedure is designed to evaluate the effect of sample size on classification performance and the interpretability of feature selection. A visual representation of the process is provided in Figure \\ref{bootstrap_analysis_schema}. We conducted a bootstrap analysis on six binary classification scenarios.  For each predefined classification scenario, we assessed different training set sizes by randomly selecting subsets from the balanced dataset, which consists of 500 samples per class. The training set sizes tested were: 350 (70\\%), 300 (60\\%), 250 (50\\%), 150 (30\\%), 100 (20\\%), 25 (5\\%), and 10 (2\\%). All remaining samples constituted the test set. Each experiment was repeated 100 times per sample size, with new bootstrap samples drawn in each iteration. Three classification approaches were evaluated: The trained models were tested on the test set, and classification performance is measured using accuracy and confusion matrices. Counts of the microRNAs (features) selected by the regularization method are collected for each sample size across the 6 classification scenarios. The final results are analyzed to determine the effectiveness of proposed methodology.  To avoid overestimating the performance of the proposed framework, we also conducted a cross-validation analysis for each sample size and computed the average accuracy across the six classification scenarios. This approach emulates a real-world scenario and permits determining if the accuracy is conserved when the trained classifier is applied to a larger dataset.",
        "summarize_figure": "2505.03387v1_bootstrap_analysis_schema",
        "summarization": "The figure shows the experimental procedure for bootstrap analysis. The preprocessed dataset is divided into six binary - classification scenarios. For each scenario, subsets of different sizes (350, 300, 250, 150, 100, 25, 10) are randomly selected from the balanced dataset as the training set, with the remaining samples forming the test set. Three classification methods are evaluated: (1) the L1 - KSVM framework with augmentation, generating 200 synthetic samples per class using a Gaussian - noise - based augmentation technique; (2) the L1 - KSVM framework without augmentation, following the same feature - selection and classification process but without synthetic data; (3) the baseline LASSO regression model, retaining features with an L1 - coefficient greater than zero. Trained models are tested on the test set, and classification performance is measured using accuracy and confusion matrices. The number of microRNAs (features) selected by the regularization method for different sample sizes is collected to analyze the effectiveness of the methodology."
    },
    "317": {
        "figure1": "2505.03387v1_all_models_baseline_too_bootstrap_analysis_results",
        "label1": "accuracy_analysis",
        "caption1": "Each subplot corresponds to a binary classification scenario: Scenario 1 (Healthy Controls vs. LCa), Scenario 2 (Healthy Controls vs. NTLD), Scenario 3 (Healthy Controls vs. OD), Scenario 4 (LCa vs. NTLD), Scenario 5 (LCa vs. OD), and Scenario 6 (NTLD vs. OD). The x-axis represents the training sample size, while the y-axis shows the classification accuracy. The plots compare three approaches: the L1-KSVM Framework with Data Augmentation (blue solid line), incorporating 200 synthetic samples per class; the L1-KSVM Framework without Augmentation (red solid line), applying the same feature selection and classification process but without synthetic data; and the Baseline LASSO Regression Model (green dashed line), which retains all features with a nonzero L1 coefficient. Shaded areas indicate standard deviation across 100 bootstrap iterations per sample size.",
        "text": "Figure \\ref{accuracy_analysis} illustrates the classification accuracy across six binary classification scenarios for different training set sizes and classification methods. As expected, an increase in training sample size generally improves classification performance. Table \\ref{performance_metrics} provides a detailed breakdown of performance metrics, including accuracy (Acc. \\%), true positives (TP \\%), true negatives (TN \\%), false positives (FP\\%), false negatives (FN \\%) and the accuracy collected from the cross validation analysis (Cross-Val Acc. \\%)\nThe results of this study demonstrate the effectiveness of integrating feature selection and synthetic data generation in omics-based classification tasks. The baseline LASSO regression model exhibited relatively better classification accuracy across multiple scenarios, as shown in Figure \\ref{accuracy_analysis} and Table \\ref{performance_metrics}. However, this came at the critical cost of retaining a significantly larger number of features (Table \\ref{features_selection}), which can reduce model interpretability and increase the risk of overfitting, particularly in high-dimensional datasets. The proposed L1-KSVM framework offered an optimal balanced trade-off between high accuracy and feature selection. The use of data augmentation significantly enhances classification performance compared to its non-augmented counterpart, particularly in low sample conditions, where the model benefits from the additional synthetic training examples, improving its generalization ability. Moreover, the L1-KSVM framework achieved comparable accuracy to the baseline model while drastically reducing the number of selected features, making it a more interpretable and explainable solution.  We decided to include in the framework the L1-KSVM architecture to evaluate a more expressive model capable of capturing non-linear patterns through kernel-based projections while maintaining feature selection through L1 regularization. This combination aimed to balance model complexity and interpretability. The use of synthetic data via Gaussian noise further tested whether data augmentation could improve robustness in low-sample settings. We demonstrate that the proposed frameworks achieve consistent cross-validated performance on small datasets that emulate real-world scenarios. The accuracy remains stable when the trained classifier is applied to a larger test set, highlighting its robustness and generalizability. Reducing high dimensionality by selecting relevant features, which enables accurate classification of patient conditions, provides a significant advantage in understanding underlying biological insights and facilitates more precise statistical and bioinformatics analyses. This study advances the integration of synthetic data into omics-based analysis while improving decision interpretability through targeted feature selection. Future research will aim to validate the framework across multiple omics datasets and explore more sophisticated data augmentation techniques, such as generative deep learning models (like generative adversarial networks (GANs) and variational autoencoders (VAE)) to design more biologically meaningful synthetic samples. This approach will be crucial in overcoming the study's limitation, as Gaussian noise-based augmentation may not fully capture the complex biological variability present in real-world omics datasets.",
        "summarize_figure": "2505.03387v1_all_models_baseline_too_bootstrap_analysis_results",
        "summarization": "The chart shows the classification accuracy for six binary - classification scenarios (comparisons between healthy controls and various diseases) under different training sample sizes, comparing three methods: the L1 - KSVM framework with data augmentation (blue solid line, with 200 synthetic samples per class), the L1 - KSVM framework without augmentation (red solid line), and the baseline LASSO regression model (green dashed line). The shaded areas represent the standard deviation of 100 bootstrap iterations for each sample size. As the training sample size increases, the classification performance generally improves. The baseline LASSO regression model has relatively high accuracy in multiple scenarios but retains many features, potentially reducing model interpretability and increasing overfitting risk. The L1 - KSVM framework with data augmentation ensures high accuracy while significantly reducing the number of selected features, and data augmentation significantly improves classification performance, especially under low - sample conditions."
    },
    "318": {
        "figure1": "2505.03390v1_all",
        "label1": "fig1",
        "caption1": "CFSRAG clustering performance changes on D1, D2, D3 and D4, respectively, when $\\alpha$, $\\beta$ and $\\lambda$ change.",
        "text": "The hyperparameter tuning of the CFSRAG model has a significant impact on its overall performance. In this section, we conducted sensitivity experiments on three key hyperparameters of the CFSRAG model: $\\alpha$, $\\beta$ and $\\lambda$. We evaluated the model's performance under various combinations of these hyperparameters and plotted 3D graphs, as shown in Fig.~\\ref{fig1}. For brevity, we report only the influence on NMI values. It can be seen from the Fig.~\\ref{fig1} that our model is very sensitive to the values of the hyperparameters    $\\alpha$, $\\beta$ and $\\lambda$, and the performance of the model is affected by different parameter combinations. Therefore, it is particularly important to choose different parameter combinations for different data.",
        "summarize_figure": "2505.03390v1_all",
        "summarization": "The figure shows the changes in the clustering performance of the CFSRAG model, measured by the Normalized Mutual Information (NMI), on datasets D1, D2, D3, and D4 as the hyperparameters α, β, and λ vary. For dataset D1, different curves (representing M3, M6, M5, M8, and CFSRAG) show different trends with the change of parameter p. For example, M5 first increases and then decreases, while CFSRAG is relatively stable with higher values. This indicates that the CFSRAG model is sensitive to hyperparameter values, and different parameter combinations affect its performance. It is crucial to select appropriate parameter combinations for different datasets."
    },
    "319": {
        "figure1": "2505.03393v1_Introduction_figure_cameraready",
        "label1": "fig:intro_example",
        "caption1": "Two decision trees built to diagnose cognitive impairment (CI) in adult patients. The left side illustrates a regular decision tree fit solely for accuracy, while the right shows a missingness-avoiding (MA) tree, which incorporates a regularization parameter $\\alpha$ to reduce reliance on missing values. The regular tree initially splits on a positive MRI scan outcome, determined by hippocampal volume ($V_h$). However, this feature is missing for many patients who did not undergo a scan, resulting in high reliance on missing values (indicated by a large $\\rho$). In contrast, the MA tree achieves a comparable AUC while avoiding reliance on missing data entirely ($\\rho = 0$). Orange nodes indicate missing values along the decision path, while blue denotes no missingness reliance. Note that zero imputation is used, resulting in complete observations in the right branch of the regular tree.",
        "text": "Suppose we want to fit a decision tree to the healthcare data to predict whether a patient suffers from cognitive impairment. Due to the ODDC structure, we can construct an accurate tree with zero missingness reliance by first splitting on the patient’s age ($x_1$), followed by cognitive test results ($x_2$) for patients older than 65, as illustrated in \\cref{fig:intro_example}. For patients with $x_1 > 65$ and $x_2=1$, the MRI scan outcome---the most predictive variable---is always available and can be used for prediction.",
        "summarize_figure": "2505.03393v1_Introduction_figure_cameraready",
        "summarization": "The figure shows two decision trees for diagnosing cognitive impairment (CI) in adult patients. The left one is a regular decision tree built solely for accuracy, splitting first based on whether the MRI scan result is positive according to the hippocampal volume (Vh). However, this feature is missing for many patients without a scan, leading to a high reliance on missing values (large ρ value). The right one is a missingness - avoiding (MA) decision tree, which introduces a regularization parameter α to reduce reliance on missing values. The MA decision tree first checks if the age is greater than 65, and then further splits based on the Mini - Mental State Examination (MMSE) score, etc. The AUC of the MA decision tree is comparable to that of the regular one, while completely avoiding reliance on missing data (ρ = 0). Orange nodes indicate missing values in the decision path, and blue nodes indicate no reliance on missing values."
    },
    "320": {
        "figure1": "2505.03393v1_illustration",
        "label1": "fig:intro",
        "caption1": "Missing values can be avoided in several ways. Sparse models (left) can be trained to not use features that are frequently missing. Disjunctive rule models (middle) can be fit to include rules that exploit redundancy in the variable set. Trees (right) can be fit so that missing values rarely occur on the decision paths.",
        "text": "Our goal is to learn hypotheses $h$ that are \\emph{missingness avoiding}, that is, they are unlikely to require the value of a missing variable at test time. Let $a_h(\\bx,j)=1$ denote the event that computing $h(\\bx)$ requires access to the value of $x_j$ (imputed or observed) and $a_h(\\bx,j)=0$ otherwise. For example, computing the prediction of a linear model with imputed inputs,  $h_\\theta(\\bx^I) = \\theta^\\top \\bx^I$, requires access to $x^I_j$ whenever $\\theta_j \\neq 0$. A decision tree requires access to $x^I_j$ if feature $j$ appears on the prediction path from root to leaf for the input $\\bx$. A rule model requires access to $x^I_j$ if the truth values of its rules are contingent on $x^I_j$. \\Cref{fig:intro} illustrates how different models may avoid relying on missing values.",
        "summarize_figure": "2505.03393v1_illustration",
        "summarization": "The figure shows different ways to avoid missing values in terms of Sparsity, Redundancy, and Context. In the left sparse model, feature C is frequently missing, and during training, it can be made not to use this feature (corresponding to the value of C in θ being 0). The middle disjunctive rule model exploits the redundancy of the variable set. For example, as long as one of A, B, and C is true, the result is true, so even if C is missing, it does not affect the judgment. The right decision tree model constructs an appropriate structure so that missing values rarely appear on the decision path (red arrows indicate the path). These methods reflect the strategies of different models to avoid relying on missing values."
    },
    "321": {
        "figure1": "2505.03393v1_madt_auc_minus_rho_vs_depth_life",
        "label1": "fig:depth_aucrho_alpha1",
        "caption1": "LIFE: $\\mathrm{AUROC}-\\hat{\\rho}$ vs. max. tree depth.",
        "text": "We further examine the impact of increasing missingness regularization when fitting {\\tt MA-DT} with varying values for the \\verb|max_depth| parameter. \\cref{fig:depth} and ~\\cref{fig:depth_aucrho_alpha1} in \\cref{app:additional_results} illustrate the trends in AUROC and missingness reliance for LIFE. When $\\alpha = 0$, {\\tt MA-DT} behaves as a standard decision tree without missingness reliance regularization. For $\\alpha \\in \\{0, 0.1, 1\\}$, a depth of 3–4 captures the data's complexity without overfitting. On the other hand, $\\hat{\\rho}$ varies from 0.1 ($\\alpha = 1$) to 0.3 ($\\alpha = 0$), demonstrating a clear case where missingness regularization can be applied without sacrificing performance. Notably, reducing the tree’s maximum depth is a less effective way to control missingness reliance, as it negatively impacts AUROC.\nIn this section, we provide additional results from our experiments. \\Cref{tab:results_table_mice} presents the same results as \\cref{tab:results_table}, but using MICE imputation instead of zero imputation. \\Cref{tab:results_table_breast_p} shows results using both imputation techniques on two additional datasets: Breast Cancer and Pharyngitis. \\Cref{tab:results_table_alphas} and \\cref{tab:results_table_breast_alphas} report the performance of MA models under different settings of the missingness reliance parameter~$\\alpha$. \\Cref{fig:depth_aucrho_alpha1} illustrates the trade-off between AUROC and missingness reliance ($\\hat{\\rho}$) when varying the maximum allowed tree depth in LIFE, across different $\\alpha$ values. Finally, \\cref{fig:life_examples} shows example LIFE trees under different $\\alpha$ settings.",
        "summarize_figure": "2505.03393v1_madt_auc_minus_rho_vs_depth_life",
        "summarization": "The chart shows the relationship between the maximum tree depth and AUROC - ρ̂ in the LIFE dataset for different α values (0.0, 0.1, 1.0, 10.0). When α = 0, MA - DT is similar to a standard decision tree without missingness reliance regularization. When α is between 0 and 1, a tree depth of 3 - 4 can capture the data's complexity without overfitting. The ρ̂ value is 0.1 when α = 1 and 0.3 when α = 0, indicating that missingness regularization can be applied without sacrificing performance. Also, reducing the maximum tree depth is an ineffective way to control missingness reliance and negatively impacts AUROC."
    },
    "322": {
        "figure1": "2505.03397v1_schematic",
        "label1": "fig:schematic",
        "caption1": "The diagram illustrates an iterative process for characterising and learning noise properties in quantum systems. Starting with a power spectral density (PSD) (top), noise realisations are generated (right) and combined with control pulses in a quantum simulation. The outcomes of these simulations are mapped onto a quantum feature space (bottom), where data points encode the effects of the noise. This feature space is used by regression models such as decision trees to infer key noise properties, including type and stationarity (left). These inferred properties refine the PSD model used in subsequent iterations. This closed-loop framework enables the classification and clustering of noise processes. It facilitates tailored noise mitigation strategies, leveraging machine learning techniques like decision trees, k-means clustering, and neural networks for enhanced quantum control.",
        "text": "We introduce in this paper \\textit{quantum feature spaces}, which encode the bath's influence on system dynamics while making minimal assumptions about the bath such that it can be used beyond toy problems and have utility for a broad range of physically realised qubits. The quantum feature space is deduced from observable expectations using efficient linear regression models. We then show that no expensive algorithms are needed to classify the noise affecting a qubit; instead, we use the Euclidean distance within the quantum feature space for classification.~\\cref{fig:schematic} demonstrates further uses for the quantum feature space. The feature space can be used as the input space to simple machine learning algorithms for noise classification, or the space can enable control pulse optimisation via visualising how control pulse parameters manifest themselves in the quantum feature space.",
        "summarize_figure": "2505.03397v1_schematic",
        "summarization": "The diagram shows an iterative process for characterising and learning noise properties in quantum systems. It begins with power spectral density (PSD). Noise realisations are generated and combined with control pulses in quantum simulations. The outcomes are mapped to a quantum feature space. Regression models, like decision trees, use this space to infer key noise properties. The inferred properties refine the PSD model for subsequent iterations. This closed - loop framework enables noise process classification and clustering. Leveraging machine - learning techniques, it promotes customised noise mitigation strategies for better quantum control."
    },
    "323": {
        "figure1": "2505.03397v1_control_pulses",
        "label1": "fig:control_pulses_visualisation",
        "caption1": "Control pulses used in the simulations. The top row shows the ideal CPMG pulses and the bottom row shows the realistic CPMG pulses.",
        "text": "To simulate ideal CPMG pulses, we set $\\lambda = \\frac{1}{96}$, $\\delta_{\\tau} = 0$, $n_{\\max} = 5$, and $A_n = \\pi$. These values were chosen to approximate delta function pulses, shown in the top panel of~\\cref{fig:control_pulses_visualisation}. For the realistic CPMG pulses we set $\\lambda = \\frac{1}{24}$, $\\delta_{\\tau}$ was chosen from a uniform random distribution spanning the interval $[-\\frac{24T}{M}, \\frac{24T}{M}]$, and $A_n = \\pi + \\epsilon$ where $\\epsilon$ was drawn from a uniform random distribution spanning the interval $[-\\frac{\\pi}{5}, \\frac{\\pi}{5}]$. This is shown in the bottom panel of~\\cref{fig:control_pulses_visualisation}. Fifty such unique realistic control pulse sequences were generated with ensemble averaging over 2000 noise process realisations for each control pulse, resulting in a cluster of 50 unique points in the quantum feature space.",
        "summarize_figure": "2505.03397v1_control_pulses",
        "summarization": "The picture shows control pulses used in simulations. The top graph, \"Ideal Control Pulse Over Time\", has pulses of value π at specific time steps and 0 otherwise, set by parameters like λ = 1/96, δτ = 0, nmax = 5, An = π to approximate delta - function pulses. The bottom graph, \"Realistic Control Pulse Over Time\", has more spread - out pulses, with parameters λ = 1/24, δτ from a uniform random distribution, and An = π + ε (ε from a uniform random distribution). 50 unique realistic sequences were generated with ensemble averaging."
    },
    "324": {
        "figure1": "2505.03397v1_vis_increasing_noise_str",
        "label1": "fig:vo_space_with_changing_parameters",
        "caption1": "(a) Visualisation of the quantum feature space with widening Gaussian-shaped control pulses, with the fraction indicating the scale factor. (b) Visualisation of the quantum feature space with interpolation between noise processes implemented by utilising a linear combination of the `$1/f$ + bump' and coloured Gaussian noise processes. (c) Visualisation of the quantum parameter space with two different noise processes, `$1/f$ + bump' and coloured Gaussian, with colour saturation representing increasing energy.",
        "text": "We first explored the impact of widening control pulse widths on the quantum feature space. The control pulse width was varied by changing the value of $\\lambda$ in~\\cref{eq:gaussian_pulse}. We set $\\lambda = \\frac{1}{96}$, $\\frac{1}{48}$, $\\frac{1}{24}$, $\\frac{1}{12}$, $\\frac{1}{6}$, and $\\frac{1}{3}$. For these simulations, we used the `$1/f$ + bump' with a bump in the PSD at the $200^\\mathrm{th}$ bin (the unknown noise process from~\\cref{subsec:black_box_noise_spectroscopy}).~\\cref{fig:vo_space_with_changing_parameters}(a) illustrates the impact of widening control pulse widths. The figure shows the interpolation between data points arising from widening control pulses is smooth and well-ordered, providing evidence that the quantum feature space is well-behaved, where it is known that smooth interpolation in a feature space is a highly desirable characteristic~\\cite{bengio2013representation,radford2015unsupervised,guo2024smooth,higgins2017beta}.\nThe results in~\\cref{fig:vo_space_with_changing_parameters}(b) are similar and show a smooth interpolation between the $1/f$ noise process with a bump and the coloured Gaussian noise process. These interpolations were achieved by creating various linear combinations of the noise processes at different ratios, progressively transitioning from one to the other.\nResults for increasing noise strengthen experiments are shown in~\\cref{fig:vo_space_with_changing_parameters}(c), where increasing colour saturation represents increasing energy. For the coloured noise process and `$1/f$ + bump', as the noise strength increases, all points tend toward the centre of the feature space, where $\\alpha_O$, $\\beta_O$, and $\\gamma_O$ are all equal to zero. Observe that when $\\alpha_O$, $\\beta_O$, and $\\gamma_O = 0$, $Q_OD_OQ_O^{\\dagger}$ becomes a zero matrix, and looking to~\\cref{eq:expectationWithNoise3}, we find that the expectation of the observable is zero. This indicates that as the noise process increases in energy it saturates the system, and as such all information about the noise process and control is lost. Conversely, as the noise strength decreases, points move toward the associated identity in the quantum feature space and therefore result in the system dynamics being dominated by the control pulses.\nHowever, the `$1/f$ + bump' noise process shows a more complex behaviour in~\\cref{fig:vo_space_with_changing_parameters}(c) when compared to the coloured noise process. The red dots, which represent the `$1/f$ + bump' noise process, rotate around $Q_X$ and $Q_Y$ as the noise strength increases. This rotation is likely due to the spectral bump in the noise profile, which introduces off-diagonal components into the noise operator. In a quantum system under control pulses, such off-diagonal elements can lead to phase shifts or rotations in the operator representation. Essentially, the result shows that the `$1/f$ + bump' noise process does not simply scale the operator uniformly; it alters its orientation in the feature space.",
        "summarize_figure": "2505.03397v1_vis_increasing_noise_str",
        "summarization": "The picture shows the quantum parameter space QO with two different noise processes: \"1/f + bump\" (in red) and coloured Gaussian noise (in blue), with green representing Identity. The horizontal axis is α, the vertical axis is β, and the vertical axis is γ. As the noise strength increases (increasing color saturation represents increasing energy), points of both the coloured noise process and the \"1/f + bump\" noise process tend towards the centre of the feature space (where α, β, and γ are all equal to 0), resulting in a zero expectation value of the observable, indicating loss of noise and control information. However, the \"1/f + bump\" noise process behaves more complexly, with its representative points (red dots) rotating around QX and QY as noise strength increases, likely due to non - diagonal components introduced by the spectral bump in the noise profile, causing phase shifts or rotations in the operator representation under control pulses."
    },
    "325": {
        "figure1": "2505.03405v1_cdf_attrition_pcexpdr_combine",
        "label1": "Per_capita_expenditure_dominance",
        "caption1": "Cumulative distribution functions of predicted values for per capita expenditure, first visit} \\begin{center} \\includegraphics[scale=0.45]{cdf_attrition_pcexpdr_combine} Source: Authors' estimations based on GHS Data \\end{center",
        "text": "However, attrition could affect the prediction capacity of these variables when used together in a multivariate form in models that include our key variables: expenditure per capita and risk aversion. The variables considered in Table \\ref{T_test} enter the models used to predict expenditure (equations \\ref{ols} and  \\ref{pols}), and to study the association between migration and risk aversion (equation \\ref{migration}) in a multivariate form. To test for non random attrition  on per capita expenditure and risk-aversion respectively, we compare the Cumulative Distribution Functions (CDFs) of the predicted values of these variables (based on the predictors listed in Table \\ref{T_test}) and test for stochastic dominance of first degree between the samples with and without attrition. This is a rather simple but effective test. We want to make sure that predictors are stable with respect to these variables whether we have attrition or not. If the predictions generated by the sample with attrition are different from those generated by the sample without attrition, our results would be biased. For per capita expenditure, we observe this variable all along the panel. We can therefore use the 2010 full sample to make predictions and then compare the two distributions of predicted values for the sample with and without attrition. Results are shown in Table \\ref{pcexpdr_attrition} and Figure \\ref{Per_capita_expenditure_dominance} and show that there is no difference in the distributions of predicted values between the two samples. For risk-aversion, we observe this variable only in 2016. We assume that risk-aversion is time-invariant and we use individual characteristics in 2010 to predict the probability of being risk-tolerant for the full panel based on individuals left in the 2016 panel sample with attrition. Results are shown in Table \\ref{risk_love_logit} and Figure \\ref{Risk_love_dominance} and show again that there is no difference in the distribution of predicted values of risk-aversion between the overall sample and the sample with attrition. We can therefore conclude that attrition does not bias our results with respect to per capita expenditure and risk-aversion.",
        "summarize_figure": "2505.03405v1_cdf_attrition_pcexpdr_combine",
        "summarization": "The picture shows the Cumulative Distribution Functions (CDF) of predicted values for per capita expenditure, comparing two groups of data: \"Last Visit\" (in blue) and \"Overall Sample\" (in brown). The horizontal axis is per capita expenditure, and the vertical axis is the CDF value. The curves show that as per capita expenditure increases, the CDF value rises. The two curves have similar trends, indicating that there is no difference in the distribution of predicted per capita expenditure values between the samples with and without attrition, suggesting that attrition does not bias the prediction results for per capita expenditure."
    },
    "326": {
        "figure1": "2505.03405v1_cdf_attrition_risk_combine",
        "label1": "Risk_love_dominance",
        "caption1": "Cumulative distribution functions of predicted values for risk-tolerance} \\begin{center} \\includegraphics[scale=0.45]{cdf_attrition_risk_combine} Source: Authors' estimations based on GHS Data \\end{center",
        "text": "However, attrition could affect the prediction capacity of these variables when used together in a multivariate form in models that include our key variables: expenditure per capita and risk aversion. The variables considered in Table \\ref{T_test} enter the models used to predict expenditure (equations \\ref{ols} and  \\ref{pols}), and to study the association between migration and risk aversion (equation \\ref{migration}) in a multivariate form. To test for non random attrition  on per capita expenditure and risk-aversion respectively, we compare the Cumulative Distribution Functions (CDFs) of the predicted values of these variables (based on the predictors listed in Table \\ref{T_test}) and test for stochastic dominance of first degree between the samples with and without attrition. This is a rather simple but effective test. We want to make sure that predictors are stable with respect to these variables whether we have attrition or not. If the predictions generated by the sample with attrition are different from those generated by the sample without attrition, our results would be biased. For per capita expenditure, we observe this variable all along the panel. We can therefore use the 2010 full sample to make predictions and then compare the two distributions of predicted values for the sample with and without attrition. Results are shown in Table \\ref{pcexpdr_attrition} and Figure \\ref{Per_capita_expenditure_dominance} and show that there is no difference in the distributions of predicted values between the two samples. For risk-aversion, we observe this variable only in 2016. We assume that risk-aversion is time-invariant and we use individual characteristics in 2010 to predict the probability of being risk-tolerant for the full panel based on individuals left in the 2016 panel sample with attrition. Results are shown in Table \\ref{risk_love_logit} and Figure \\ref{Risk_love_dominance} and show again that there is no difference in the distribution of predicted values of risk-aversion between the overall sample and the sample with attrition. We can therefore conclude that attrition does not bias our results with respect to per capita expenditure and risk-aversion.",
        "summarize_figure": "2505.03405v1_cdf_attrition_risk_combine",
        "summarization": "The picture shows the Cumulative Distribution Functions (CDF) of predicted values for risk - tolerance, comparing two groups of data: \"Last Visit\" (blue curve) and \"Overall Sample\" (brown curve). The horizontal axis is Risk Love, and the vertical axis is the CDF value. As the Risk Love value increases, the CDF value rises. The two curves have similar trends, indicating that there is no difference in the distribution of predicted risk - tolerance values between the samples with and without attrition, suggesting that attrition does not bias the prediction results for risk - tolerance."
    },
    "327": {
        "figure1": "2505.03405v1_Acled_Fatalities",
        "label1": "Acled",
        "caption1": "Incidence of Fatalities by Local Government Areas, 2011-2016} \\includegraphics[scale=1]{Acled_Fatalities} Source: Authors' elaboration based on ACLED Data (http://www.acleddata.com/data/)",
        "text": "The second data set we use is the the Armed Conflict Location \\& Event Data (ACLED). These data cover daily conflicts episodes in the country since 1997 and include information on the cause of conflict, perpetrators, casualties and fatalities. We will use the number of fatalities by LGA and match these data with the GHS panel data for the period 2010-2016. We then subdivide LGAs in areas that had no conflict (in any of the years considered), areas with some conflict (defined as areas with fatalities due to conflict in at least one of the years considered but less than six years) and areas always in conflict (defined as areas with fatalities due to conflict in all six years considered).\\footnote{Data are available on-line and include codebooks, user guides, questionnaires and other relevant material (see http://www.acleddata.com/data/).} The ACLED data set is not perfect as many episodes of violence are not reported or detected but the map of casualties constructed with these data shows correctly all the areas where the conflict was known to be intense (see Figure \\ref{Acled}).",
        "summarize_figure": "2505.03405v1_Acled_Fatalities",
        "summarization": "The picture shows the incidence of fatalities in local government areas from 2011 to 2016. It is drawn based on ACLED data, which cover daily conflict events in the country since 1997, including information like conflict causes, perpetrators, and casualties. From the picture, differences in the occurrence of fatalities due to conflict in different local government areas during these six years can be seen. Some areas may have no fatalities from conflict over the years, some have in some years, and some have every year, helping to understand the extent and distribution of areas affected by conflict - related fatalities."
    },
    "328": {
        "figure1": "2505.03415v1_Workflow",
        "label1": "fig:Overview",
        "caption1": "Overview of the workflow used here: (a) A dataset $\\dataset$ consisting of structure parameters $\\strpar^\\alpha$ and corresponding effective elasticity tensors $\\eltens^\\alpha$ is generated. Points in the four-dimensional parameter space $\\strparspc$ are sampled appropriately, the corresponding geometries $\\geo^\\alpha$ are created, and the effective elasticity tensor is determined computationally. Geometry generation and simulation are abbreviated as $\\fwdmod : \\strpar\\mapsto\\eltens$. (b) The generated dataset is used to calibrate the proposed surrogate model $\\surmod$, which replaces $\\fwdmod$. (c) Inverse design is formulated as the minimization of an objective function $\\ell$ with respect to the structure parameters $\\strpar$ and additionally rotations $\\rottens$, which can be solved efficiently using the surrogate model.",
        "text": "The present workflow is shown in \\reff{fig:Overview}. First, \\refs{sec:spinodoids} provides an overview of how to generate spinodoid structures, followed by a specific neural network architecture in \\refs{sec:penn}, which is used in the formulation of the surrogate model described in  \\refs{sec:surrogate_model}. The inverse design framework is then presented in \\refs{sec:inverse_design}. The datasets used for training the surrogate are generated according to the approach presented in \\refs{sec:data_generation}. In \\refs{sec:applications}, we analyze which dataset sizes yield sufficiently accurate surrogates for three inverse design tasks of varying complexity.",
        "summarize_figure": "2505.03415v1_Workflow",
        "summarization": "The picture shows an overview of a workflow. In part (a), data generation samples from a four - dimensional parameter space S to get Sα, generates geometries Gα, determines the effective elasticity tensor computationally via f : S→Ĉ, creating dataset D. In part (b), this dataset calibrates a surrogate model. In part (c), inverse design formulates minimizing an objective function with respect to structure parameters and rotations, solvable with the surrogate model."
    },
    "329": {
        "figure1": "2505.03415v1_Spinodoid_overview",
        "label1": "fig:Spinodoid",
        "caption1": "Generation of spinodoid structures using a 2d example: (a) The allowed sampling space of wave vectors is constrained by the half-angles $\\mangle_i$. (b) A maximum number of cosine waves is sampled within the allowed region. (c) These cosine waves are superimposed to form a Gaussian random field. (d) Another parameter $\\rho$ determines how much of the resulting structure is occupied by Material 1. Based on this parameter, the Gaussian random field is thresholded at a specific function value, and (e) everything below this threshold is interpreted as Material 1 (blue), while everything above is interpreted as Material 2 (red).",
        "text": "Spinodoid structures are a special class of metamaterials~\\cite{kumar_inverse-designed_2020}, characterized in particular by their smooth geometrical features  and versatile and adjustable properties despite their low-dimensional parameter space. The name ``spinodoid'' is derived from ``spinodal decomposition'', a specific form of phase separation in which a homogeneous initial phase splits into multiple distinct phases. The suffix ``-oid'' suggests that spinodoids are not exactly the topologies resulting from spinodal decomposition in a strict sense, but rather an approximation with similar-looking structures inspired by it. Spinodoids are generated by a Gaussian random field $\\GRF$ of the form where $\\tensorone{x}\\in\\realnums^3$ is the position in 3d space, $\\wavevector_i\\in\\realnums^3$ with $|\\wavevector_i|=1$ denotes the direction of the $i$th of $\\Nwaves$ superimposed cosine waves and $\\phaseshift_i$ is its phase angle. For a graphical representation of an equivalent 2d example, see \\reff{fig:Spinodoid}. The directions $\\wavevector_i$ are sampled uniformly from six (four in the 2d case) not necessarily connected subregions of the unit sphere. These subregions are determined by three angles (two for the 2d case) $\\mangle_1, \\mangle_2, \\mangle_3$ that define the maximum allowed angles between the position vector of a point on the unit sphere $S^2$ and each coordinate axis. Formally, $\\wavevector_i$ is sampled uniformly within and $\\phaseshift_i$ is sampled uniformly from $[0,2\\pi)$. Based on the Gaussian random field $\\GRF$ described in \\refe{eq:GRF}, a two-phase spinodoid metamaterial is defined using the indicator function             0\\,,\\quad \\text{if } \\GRF(\\tensorone{x}) \\leq \\GRF_0 \\\\             1\\,,\\quad \\text{if } \\GRF(\\tensorone{x}) > \\GRF_0 \\quad , where $\\GRF_0=\\sqrt{2}\\erf^{-1}(2\\rho-1)$ splits the domain into two parts, such that the portion of points with $\\delta(\\tensorone{x})=0$ equals the specified volume fraction $\\rho$. All points with $\\delta(\\tensorone{x})=0$ are assigned material $M_1$ and all other points with $\\delta(\\tensorone{x})=1$ are assigned another material $M_2$.",
        "summarize_figure": "2505.03415v1_Spinodoid_overview",
        "summarization": "The picture shows the generation process of spinodoid structures using a 2D example. Part (a) shows that the allowed sampling space of wave vectors is constrained by the half - angles θi. In part (b), the maximum number of cosine waves is sampled within the allowed region. In part (c), these cosine waves are superimposed to form a Gaussian random field. In part (d), the parameter ρ determines the proportion of Material 1 in the generated structure, and a specific function - value threshold is set for the Gaussian random field based on this parameter. In part (e), the region below the threshold is regarded as Material 1 (blue), and the region above is regarded as Material 2 (red)."
    },
    "330": {
        "figure1": "2505.03415v1_PENN",
        "label1": "fig:PENN",
        "caption1": "(a) A permutation equivariant layer, that maps three first order matrices $\\tex^i$, i.e., $\\rankx=1$, to three matrices of (b) rank one, i.e., $\\ranky=1$ or (c) rank two, i.e., $\\ranky=2$. The weights and biases of the layer are summarized in $\\WeightsAndBiases$. Each connection $(i,j)$ is independent of the others, but is restricted internally to enforce the equivariance condition. More precisely, the weight matrix $\\weightMatrix^{22}$ is constructed such that every orbit of input-output index pairs $\\indexsetInput\\times\\indexsetOutput$ receives an independent weight. In case of rank one output matrices (b), this results in only two independent weights, one for horizontal connections and one for the diagonal ones. For a rank two output matrix (c), there are five weights. The same idea applies to any ranks $\\rankx$ and $\\ranky$.",
        "text": "Input $\\tex$ and output $\\tey$ both share the same set of possible (one-valued) index tuples $\\indexsetInput=\\indexsetOutput=\\left\\{\\idxtuple{1},\\idxtuple{2},\\idxtuple{3}\\right\\}$. In order to find the symmetries of $\\weightMatrix$ such that the resulting layer is $\\groupG$-equivariant, $\\indexsetInput\\times\\indexsetOutput$ is partitioned into orbits under $\\groupG$-action. For the edge $\\idxtuplepair{1}{1}$, the orbit contains the three elements $\\groupG\\circ\\idxtuplepair{1}{1}=\\bigl\\{ \\idxtuplepair{1}{1}, \\idxtuplepair{2}{2}, \\idxtuplepair{3}{3} \\bigr\\}$. Consequently, all edges in $\\groupG\\circ\\idxtuplepair{1}{1}$ must receive the same weight indicated with the blue dashed lines in \\reff{fig:PENN}(b). There are six edges, that do not have a color yet, e.g., the edge $\\idxtuplepair{1}{2}$. The orbit $\\groupG\\circ\\idxtuplepair{1}{2}$ of this edge  contains all of the remaining edges. Again, all edges in $\\groupG\\circ\\idxtuplepair{1}{2}$ receive the same color which can be different from the color assigned to the other orbit $\\groupG\\circ\\idxtuplepair{1}{1}$. Since $\\groupG\\circ\\idxtuplepair{1}{1}\\cup\\groupG\\circ\\idxtuplepair{1}{2}=\\indexsetInput\\times\\indexsetOutput$, there cannot exist another orbit. Thus, there are only two distinct orbits and consequently two independent weights, indicated with two different colors.",
        "summarize_figure": "2505.03415v1_PENN",
        "summarization": "The picture (a) shows a permutation equivariant layer that maps three first - order matrices xi to other matrices. Part (b) shows the case of mapping to three first - order matrices yi, and part (c) shows the case of mapping to three second - order matrices yi. The weights and biases of the layer are summarized in WeightsAndBiases. Each connection (i, j) is independent, but restricted internally to satisfy the equivariance condition. Specifically, the weight matrix weightMatrix22 is constructed so that each orbit of input - output index pairs has an independent weight. When the output is a first - order matrix (b), there are only two independent weights corresponding to horizontal and diagonal connections. When the output is a second - order matrix (c), there are five weights. This principle applies to input and output matrices of any order."
    },
    "331": {
        "figure1": "2505.03418v1_case-v1",
        "label1": "fig:case",
        "caption1": "Case Study: Example from the Field of Machine Learning",
        "text": "Let's use machine learning tasks as an example (Figure \\ref{fig:case}). Developing a high-quality machine learning model can be framed as a problem-solving process, where we seek a feasible solution trace $T(\\pi) \\in \\mathcal{T}_{feasible}$. Each step in this process corresponds to a state transition $O_i \\to O_{i+1}$  , driven by reasoning, domain knowledge, and iterative evaluation. Initially, we define the problem by identifying the task and structuring it into a machine learning formulation. Then, we transition through intermediate states by analyzing data, applying preprocessing techniques, and performing feature engineering. Once the data is processed, we select suitable modeling techniques and develop the model for training. To refine these transitions, domain knowledge plays a crucial role, guiding the selection of appropriate models and training strategies. Knowledge may originate from historical approaches, theoretical research, or expert intuition, shaping the feasible state space $\\mathcal{T}_{feasible}$. Developing effective machine learning models requires multiple rounds of evaluation for each method, including both human assessments and experimental evaluations. Since machine learning models rely on learning data distributions from training data to make predictions, assessing the quality of a solution solely by examining it is challenging. Instead, empirical validation through human assessments and experimental testing determines the effectiveness of a model before convergence to an optimal solution $Y$.",
        "summarize_figure": "2505.03418v1_case-v1",
        "summarization": "The picture shows a case study process in the field of machine learning. User inputs include machine learning questions, training data, and evaluation data. From the perspective of a large language model, background understanding is first carried out, followed by data preprocessing to obtain cleaned data. Selected features are obtained through feature engineering, and then code is generated via feature modeling for model training. The trained model is evaluated. If the result meets expectations, the process ends; otherwise, rethinking and adjustment are needed. Knowledge (such as historical solutions, academic papers, etc.) and the environment (such as Python, Matlab, etc.) provide support, and software engineers, data engineers, and other personnel are involved."
    },
    "332": {
        "figure1": "2505.03418v1_domain-v3",
        "label1": "fig:domains",
        "caption1": "Some scenarios for complex problem solving.",
        "text": "The scope of complex problem solving spans a wide range of domains, encompassing challenges that touch virtually every aspect of human society (Figure \\ref{fig:domains}). For instance, designing robust software system architectures requires balancing scalability, reliability, and user needs, while proving mathematical theorems demands rigorous logical reasoning and abstraction. In the realm of data science, building accurate models to interpret vast datasets is essential for informed decision-making. Similarly, drug discovery involves navigating intricate molecular interactions to identify effective therapies, and constructing physical models enables us to simulate and understand natural phenomena. These examples highlight the diversity of complex problems humanity strives to solve, each requiring a blend of domain expertise, reasoning, and creativity.\nDespite the significant advancements in LLMs for complex problem solving, each domain presents its own unique challenges when applying LLMs to practical applications. Take some domains in Figure \\ref{fig:domains} as examples. In software engineering, LLMs are tasked with generating or modifying code within large code repositories for bug fixings and new feature implementations. This requires them not only to reason about code generation but also to have a comprehensive understanding of the entire codebase and project requirements \\cite{yang2024swe}. Furthermore, software development demands not just code correctness but also optimization in terms of computational efficiency and memory usage \\cite{shypula2023learning}, adding an additional layer of complexity to the evaluation process. Mathematics encompasses two primary types of tasks: calculations and provings. While extensive data are available for basic arithmetic and computational tasks, data scarcity remains a significant challenge in advanced mathematics, particularly in higher education and research \\cite{glazer2024frontiermath}. To address this limitation, it is essential to leverage domain knowledge more effectively for data synthesis to mitigate the impact of data scarcity and utilize existing mathematical knowledge, such as theorems, to improve mathematical proving. In addition, mathematical theorem proving usually lack an effective way to verify the proving solution, making it difficult to train LLM models to generate rigorously correct mathematical reasoning solutions. Data science involves working with large datasets, yet task descriptions often lack sufficient details about the distribution of input data, making it challenging for LLMs to generate the most suitable solutions to model the large datasets well \\cite{arxiv24_mlebench}. This also complicates the evaluation of LLM-generated outputs, necessitating multi-level assessments. Furthermore, leveraging a comprehensive knowledge base of data modeling techniques is crucial for developing more effective methods to tackle complex data science problems. Scientific research often involves open-ended problems, which prevents us from training LLMs to solve scientific problems directly. One potential solution is to involve humans in the process (human-LLM collaboration), allowing for iterative collaboration between humans and LLMs to explore existing scientific literature and human knowledge \\citep{scherbakov2024emergence,susnjak2024automating, beltagy2019scibert,jin2023pubmed,lo2019s2orc,luo2024large}, generate novel ideas \\cite{si2024llms, wang2024scipip, wang2024scimon, baek2024researchagent} and automate entire research pipelines \\cite{lu2024aiscientist}. These challenges highlight the need for further research into complex problem-solving that go beyond the current reasoning LLMs.",
        "summarize_figure": "2505.03418v1_domain-v3",
        "summarization": "The picture shows several scenarios for complex problem - solving. Medical diagnosis requires knowledge of anatomy, physiology, pathophysiology, etc. Data analysis relies on programming, data visualization, statistical methods, etc. Mathematics solving involves math theorems, numerical methods, algebra and calculus, etc. Software engineering covers programming, system design, software development lifecycle, etc. Scientific research needs data analysis, research methodology, scientific principles, etc. Legal judgments involve case analysis, legal principles, ethical standards, etc. Different fields have corresponding demands for specialized knowledge."
    },
    "333": {
        "figure1": "2505.03418v1_loop-v3",
        "label1": "fig:cps",
        "caption1": "The loops of complex problem solving.",
        "text": "Figure \\ref{fig:cps} illustrates LLM-based techniques for complex problem solving.  Current Chain-of-Thought (CoT) LLMs are trained through data synthesis. The process begins with generating CoT data, followed by selecting the correct CoT samples using a verifier for model training. During inference, the LLM generates multiple CoT solutions, and a verifier is used to identify the correct one for the given task. There are multiple approaches to synthesizing data. One method involves having the LLM generate CoT data autonomously, which requires the base model to be well-trained. For applications with limited training data, knowledge mining can be conducted on existing datasets to synthesize data, while human expertise can also be incorporated. Additionally, the mined knowledge can be injected into the LLM during inference rather than being solely used for training. Certain applications produce results that are difficult to verify, such as machine learning tasks. In such cases, multiple verification methods can be employed. Besides using an LLM-based verifier, symbolic verification and experimental evaluations can be conducted.  Additionally, human experts may also be involved in the verification process.\nWe can utilize the architecture depicted in Figure \\ref{fig:cps} to improve chain-of-thought reasoning for tackling complex problems. When a question is posed, a \\textit{generator} powered by a large language model (LLM) produces multiple reasoning paths. These paths are then evaluated by a \\textit{verifier} to determine their accuracy. If some of the reasoning paths are validated as correct, they are used to formulate an answer to the question. However, if none of the paths are deemed correct, a \\textit{corrector} is employed to create new reasoning paths by modifying the incorrect ones and incorporating additional feedback from the verifier. In this approach, enhancing the likelihood of getting a correct solution for any given question requires improving two key metrics:     at least one of the generated reasoning paths,     all generated paths. To enhance precision, the \\textit{verifier} must be fine-tuned to more accurately identify the correct path.",
        "summarize_figure": "2505.03418v1_loop-v3",
        "summarization": "The picture shows the loop of complex problem solving based on Large Language Model (LLM). The LLM generates solutions through multi - step reasoning, involving knowledge discovery, domain knowledge, and procedure knowledge. The reasoning results are checked in the verification stage, which can be done through an LLM verifier, a symbolic verifier, and an experiment verifier, and can also incorporate Human - Computer Interaction (HCI). If the reasoning results are incorrect, they can be fed back to the LLM for correction. Through continuous iteration of this process, complex problems can be solved."
    },
    "334": {
        "figure1": "2505.03419v1_running-time",
        "label1": "fig:time",
        "caption1": "% \\ \\small Running time dependence on $2$-admissibility. Marker sizes represent the number of edges. The dotted lines and percentages indicate on how many networks the algorithm finished below the specified time. ",
        "text": "For the running time (Figure~\\ref{fig:time}) we find that the algorithm performs well for networks whose $2$-admissibility lies below $\\sim$300, in this range the network size is the dominating factor. This indicates that the worst-case of~$O(p^4 |G|)$ is quite pessimistic and therefore that the `optimistic' algorithm design paid off. The computation took less than ten minutes on almost all networks (96\\%), including some with millions of edges, and finished in less than a second for most smaller networks (71\\%).",
        "summarize_figure": "2505.03419v1_running-time",
        "summarization": "The picture shows the relationship between the running time of the algorithm and 2 - admissibility. The horizontal axis is 2 - admissibility, the vertical axis is running time. The size of the dots represents the number of edges, and dots of different colors correspond to 10K, 100K, and 1M edges respectively. The dotted lines and percentages indicate the proportion of networks for which the algorithm finishes running within a specific time. The algorithm performs well for networks with 2 - admissibility below approximately 300, where the network size is the dominant factor. Nearly 96% of the networks (including those with millions of edges) take less than ten minutes for computation, and most smaller networks (71%) finish within one second."
    },
    "335": {
        "figure1": "2505.03419v1_adm-vs-memory",
        "label1": "fig:memory",
        "caption1": "% \\ \\small Peak memory consumption dependence on $2$-admissibility. Marker sizes represent the number of edges. The dotted lines and percentages indicate on how many networks the algorithm used less than the indicate amount of memory. On average, a third of the memory consumption is due to storing the network. ",
        "text": "Memory consumption was, as predicted by theory, modest (Figure~\\ref{fig:memory}). None of our experiments used more than 1.34 GB of RAM. Note that already simply storing the networks in-memory accounts for a good fraction of the total memory usage: Among the 44~networks with noticable memory footprint (> 10MB), on average 30\\% of memory accounts for storing the network itself and 70\\% is used by the algorithm's data structures.",
        "summarize_figure": "2505.03419v1_adm-vs-memory",
        "summarization": "The picture shows the relationship between peak memory usage and 2 - admissibility. The horizontal axis is 2 - admissibility, the vertical axis is peak memory usage. The size of the dots represents the number of edges, and dots of different colors correspond to 10K, 100K, and 1M edges respectively. The dotted lines and percentages indicate the proportion of networks for which the algorithm uses less than a specific amount of memory. As predicted by theory, memory consumption is modest. None of the experiments used more than 1.34GB of memory. Among 44 networks with significant memory footprint (> 10MB), on average, 30% of memory is used to store the network itself, and 70% is used for the algorithm's data structures."
    },
    "336": {
        "figure1": "2505.03419v1_adm-vs-deg",
        "label1": "fig:degen",
        "caption1": "%  \\small Comparison of $2$-admissibility and degeneracy. Marker sizes represent the number of edges. The line represents a best-fit polynomial $\\adm_2 \\approx \\deg^{1.25}$, the red markers are outliers whose residual has an absolute z-score $\\geq 3$. ",
        "text": "Slightly more complicated functions (like $\\alpha d^\\beta + \\gamma$) did not result in a significantly better fit and we chose to keep the modelling  simple. Hence, we propose that the $2$-admissibility grows roughly as $d^{1.25}$ (\\cf Figure~\\ref{fig:degen}), as such we expect most networks with low degeneracy to also have quite low $2$-admissibility",
        "summarize_figure": "2505.03419v1_adm-vs-deg",
        "summarization": "This picture shows the relationship between 2 - admissibility and degeneracy. The sizes of the marker points represent the number of edges, including 10K, 100K, and 1M edges. The gray line represents the best - fit polynomial, where 2 - admissibility is approximately equal to degeneracy to the power of 1.25. The red markers are outliers with an absolute z - score of the residual greater than or equal to 3. As degeneracy increases, 2 - admissibility generally shows an upward trend."
    },
    "337": {
        "figure1": "2505.03425v1_motivation1",
        "label1": "fig:motivation1",
        "caption1": "A motivating example from CVE-2017-2897, where the red blocks indicate a reachable path to the vulnerable target function \\texttt{read\\_MAST}.",
        "text": "To address this issue, our core idea is to leverage the semantic understanding and code generation capabilities of LLM. We transform the complex path constraint problem into a code generation task. Specifically, \\textbf{we collect the call chains of the target function within the target library, use LLM to analyze these call chains, and generate an executable harness that explicitly constrains the execution path to the target}. This approach avoids complex path exploration. As shown in~\\autoref{fig:motivation1}, CVE-2017-2897 is an out-of-bounds write vulnerability in libxls 1.4.0. The entry program contains numerous complex execution paths. Traditional directed fuzzers require extensive path exploration before reaching the target, but most of these paths are irrelevant to the vulnerable function \\texttt{read\\_MAST}. By analyzing the call relations of \\texttt{read\\_MAST} in the library, we can generate a harness program to constrain the execution path explicitly, significantly simplifying the path exploration process.",
        "summarize_figure": "2505.03425v1_motivation1",
        "summarization": "The picture is an example from CVE - 2017 - 2897, showing function call relationships. The red - colored modules (such as xls_open(), ole2_open(), read_MAST()) indicate the reachable paths to the vulnerable target function read_MAST. The whole chart presents numerous complex execution paths in the entry program. It reflects that traditional directed fuzzers need extensive path exploration before reaching the target function, and most of these paths are irrelevant to the vulnerable target function."
    },
    "338": {
        "figure1": "2505.03425v1_overview",
        "label1": "fig:workflow",
        "caption1": "Overview of \\approach\\!.",
        "text": "Our approach aims to improve the efficiency of directed fuzzing by leveraging the reasoning and code generation capabilities of LLM. As shown in~\\autoref{fig:workflow}, \\approach{} consists of multiple phases to systematically guide the fuzzing process toward specific target functions. First, it uses static analysis to identify all potential call chains of the target function and filters out the feasible ones~(\\textit{\\autoref{sec:3.1}}). Then, LLM analyze these call chains to infer the execution conditions required to trigger the target function~(\\textit{\\autoref{sec:3.2}}). Based on this information, \\approach{} generates executable target harness and initial input seeds to guide execution paths toward the target~(\\textit{\\autoref{sec:3.3}},~\\textit{\\autoref{sec:3.4}}). Finally, \\approach{} constructs target-specific mutators using target execution reports to optimize input mutation, reducing the time required to reach the target function and enhancing the effectiveness of the fuzzing process~(\\textit{\\autoref{sec:3.5}}).",
        "summarize_figure": "2505.03425v1_overview",
        "summarization": "The picture shows a workflow to improve the efficiency of directed fuzzing. First, through call chain analysis, it identifies target call chains from the library and target function and filters out the available ones. Then, it conducts execution condition analysis to infer execution conditions from source code. Next, it uses source code and execution conditions to generate a target harness, and gets an executable target harness based on whether the test passes. After that, it infers a custom mutator according to the target description. Finally, it combines execution conditions and the executable harness to infer input seeds, identifies reachable inputs, and runs fuzzing."
    },
    "339": {
        "figure1": "2505.03428v1_threshold-rho-square-fig",
        "label1": "fig:dynamics-time",
        "caption1": "On the left, larger costs $\\alpha$ increase the hitting time  (100 repetitions 95\\% confidence). On the right, larger  rewards values $\\rho$ help to maintain the dynamics above the threshold once it is reached.",
        "text": "We observe experimentally (Figure~\\ref{fig:dynamics-time}) that lower costs $\\alpha$ \\emph{accelerates}  convergence to the desired ``high value'' region, while increasing  rewards $\\rho$  helps to \\emph{maintain} the desired equilibrium (but it does \\emph{not} accelerate  convergence). Intuitively,  the dynamics converge quickly to an ``average''  contribution level $\\ell^*$ which depends \\emph{only} on $\\alpha\\beta$:\\begin{align}\\label{eq:equilibrium-alphabeta}",
        "summarize_figure": "2505.03428v1_threshold-rho-square-fig",
        "summarization": "The chart shows the level values over time for different rho values (0.01 and 0.1) under the conditions of n = 100, τ = 50, α = 0.1, and β = 1. The blue line represents rho = 0.01, and the orange line represents rho = 0.1. Initially, both level values rise rapidly and then fluctuate within a certain range. The orange line (rho = 0.1) on the right is more stable during the fluctuations and shows a significant increase near the time point of 1000. This indicates that a larger reward value rho helps maintain the dynamic level after a certain state is reached."
    },
    "340": {
        "figure1": "2505.03428v1_hitting-alpha",
        "label1": "fig:hitting-revisited",
        "caption1": "Hitting time for threshold technologies with increasing costs $\\alpha$ (same as Fig.~\\ref{fig:dynamics-time}(left) in main text reproduced here for convenience).",
        "text": "At the lowest end of the cost spectrum, governance projects tend to be cheap, corresponding to a low $\\alpha$ in our model. Note that a low $\\alpha$ makes $\\ell^*$ higher \\eqref{eq:equilibrium-alphabeta}, which makes convergence to $\\ell^*$ faster. Revisiting Figure~\\ref{fig:hitting-revisited}, a low $\\alpha$ is likely to place the system in the fast hitting region (the interval from 0 to approximately $0.5$ in the graph) where the hitting time is growing approximately linearly, with a low slope, in the cost. The implication is that it is easier for a system designer to make such projects work. Conventional wisdom in the field is in line with this result, as governance projects are perceived to be easier to succeed.",
        "summarize_figure": "2505.03428v1_hitting-alpha",
        "summarization": "The chart shows the hitting time for threshold technologies as the cost α increases under the conditions of n = 100, τ = 50, β = 1, ρ = 0.1, and 50 repetitions. When the α value is small (from 0 to approximately 0.5), the hitting time increases slowly, showing an approximately linear relationship with a small slope. When the α value exceeds 0.5, the hitting time rises sharply. This indicates that a lower α value allows the system to reach equilibrium faster, and governance projects are more likely to succeed."
    },
    "341": {
        "figure1": "2505.03428v1_linear-lam-rep10",
        "label1": "fig:linear-lam-reps",
        "caption1": "Linear technologies for different $\\lambda_V\\in\\{10,20,100\\}$ ($\\lambda_V = 1/\\tau$, 10 repetitions and 95\\% accuracy).",
        "text": "Here the intuition is analogous to the threshold technology: The dynamics converge quickly to some average level $\\ell^* = \\frac{n}{p_{-\\Gamma \\beta}}= \\frac{n}{\\exp(-\\Gamma \\beta)}$ and from there it takes exponential time to reach higher values. In particular, increasing the steepness $\\lambda_V$ increases $\\ell^*$ (see Figure~\\ref{fig:linear-lam-reps}).",
        "summarize_figure": "2505.03428v1_linear-lam-rep10",
        "summarization": "The chart shows the change of level values over time for different tau values (0.01, 0.05, 0.1) under the conditions of n = 100, α = 0.1, β = 1, ρ = 0.5, and 10 repetitions. All three curves rise rapidly at first and then tend to fluctuate steadily. The smaller the tau value (such as 0.01, corresponding to the blue line), the relatively higher the level value reached and the larger the fluctuation range. This indicates that increasing λ_V (λ_V = 1/tau) will increase the average level value ℓ*."
    },
    "342": {
        "figure1": "2505.03437v1_Fig1-EOS-T-NL3w03",
        "label1": "EOS-NL3w03",
        "caption1": " (Color online) Nuclear matter equation of state on isotherms with the NL3w03.  The black dotted line represents the causal limit, and the other curves represent the results at various temperatures as labelled.",
        "text": "The EOSs for symmetric nuclear matter with parameter set NL3w03 at different temperatures are shown in Fig. ~\\ref{EOS-NL3w03}.  Consistent with the result in the literature~\\cite{serot1992relativistic}, the finite temperature  has  considerable effects on the EOS at low densities, instead of  a minor impact at high densities.  Taking the temperature T=40 MeV as an example, we illustrate the EOSs of the four   models in  Fig.~\\ref{EOS-T}. It shows that the EOSs of the TM1w02 and FSUGarnet models are softer than those of the NL3w03 and GM1 models. Here, the softening is mainly due to the inclusion of the nonlinear self-interaction term of the $\\omega$ meson ($\\sim c_3$ $\\omega_0^4$, see eq.~(\\ref{eq-L})).",
        "summarize_figure": "2505.03437v1_Fig1-EOS-T-NL3w03",
        "summarization": "The chart shows the nuclear matter equation of state based on the NL3w03 parameter set on isotherms. The horizontal axis represents the energy density ε (in MeV fm⁻³), and the vertical axis represents the pressure P (in MeV fm⁻³). The black dotted line represents the causal limit. The other curves in different colors correspond to the results at the labeled different temperatures (such as 10, 20, 40, 80). It can be seen that the finite temperature has a significant effect on the equation of state at low densities, and a minor impact at high densities."
    },
    "343": {
        "figure1": "2505.03437v1_Fig2-EOS-T",
        "label1": "EOS-T",
        "caption1": " (Color online) The equation of state of symmetric nuclear matter with various RMF parameter sets, NL3w03,  GM1, TM1w02, and FSUGarnet {at temperature T=40 MeV.\t} ",
        "text": "The EOSs for symmetric nuclear matter with parameter set NL3w03 at different temperatures are shown in Fig. ~\\ref{EOS-NL3w03}.  Consistent with the result in the literature~\\cite{serot1992relativistic}, the finite temperature  has  considerable effects on the EOS at low densities, instead of  a minor impact at high densities.  Taking the temperature T=40 MeV as an example, we illustrate the EOSs of the four   models in  Fig.~\\ref{EOS-T}. It shows that the EOSs of the TM1w02 and FSUGarnet models are softer than those of the NL3w03 and GM1 models. Here, the softening is mainly due to the inclusion of the nonlinear self-interaction term of the $\\omega$ meson ($\\sim c_3$ $\\omega_0^4$, see eq.~(\\ref{eq-L})).",
        "summarize_figure": "2505.03437v1_Fig2-EOS-T",
        "summarization": "The chart shows the equation of state of symmetric nuclear matter at a temperature of T = 40 MeV for different relativistic mean - field (RMF) parameter sets (NL3w03, GM1, TM1w02, FSUGarnet). The horizontal axis is the energy density ε (in MeV fm⁻³), and the vertical axis is the pressure P (in MeV fm⁻³). The equations of state of the NL3w03 and GM1 models are stiffer, while those of the TM1w02 and FSUGarnet models are softer. This softening is mainly due to the inclusion of the nonlinear self - interaction term of the ω meson."
    },
    "344": {
        "figure1": "2505.03440v1_spine_track__8_iter0-8",
        "label1": "fig:gaze-rays",
        "caption1": "A collection of gaze rays across time, collected by following a moving cell with one's eyes. We slide a simple Gauss kernel in the shape of $[0.25, 0.5, 0.25]$ along each ray to smooth the signal and extract local maxima more effectively. Here we show the effect of various iterations of Gauss smoothing. It can be seen that no or low amounts of smoothing can lead to incorrectly extracted tracks, and starting from 4 iterations onward we are able to extract the correct cell track.",
        "text": "To avoid the Midas touch problem \\cite{jacob_eye_1995}, it is important to remove any visual distractions that could lead the user to unintentionally look away from their target cell. For this reason, we implemented a dual input approach for cell tracking by eye tracking: as soon as the user is ready to start tracking a cell with their eyes, they press the left trigger button. This starts playback of the dataset and the continuous collection of gaze directions and volume density samples along each ray. Once the user interrupts the tracking with the trigger button, or if the first time point is reached, the collected gaze rays and their sampled values are analyzed and subsequently sent to Mastodon. A collection of rays is plotted in \\cref{fig:gaze-rays}, where each ray originates from the positive Y axis and extends downwards. Changes over time are plotted along the X axis. We apply a subtle Gaussian smoothing to extract local maxima more robustly.",
        "summarize_figure": "2505.03440v1_spine_track__8_iter0-8",
        "summarization": "The chart shows a collection of gaze rays over time. A Gaussian kernel in the shape of [0.25, 0.5, 0.25] is slid along each ray to smooth the signal and extract local maxima. Points of different colors represent different Gaussian smoothing iterations (0, 2, 4, 6, 8). It can be seen that 0 or few smoothing iterations lead to incorrect track extraction, and correct cell track extraction starts from 4 iterations."
    },
    "345": {
        "figure1": "2505.03441v1_fig3_arx",
        "label1": "fig:toy_network",
        "caption1": "Toy example of the group structure of nodes in a multiplex network with $L=3$ layers. There are two global groups, indicated by the shape of the nodes, and three layer groups, indicated by the colours. Probabilities of layer group assignments given the global groups are indicated in the row vectors.",
        "text": "and we write $\\gamma = \\{\\gamma_{ks}\\}$ and $\\gamma^{\\prime} = \\{\\gamma^{\\prime}_{ks}\\}$ for the matrices of storing parameters. Additionally, we assume that a vector $\\boldsymbol{x}_i \\in \\mathbb{R}^P$ of covariates, which is assumed to be completely observed, is associated to each node $i \\in \\mathcal{V}$. We use the covariates $\\boldsymbol{x}_i$ to construct a the node-specific probability vectors $\\boldsymbol{\\tau}_i$ in \\eqref{eq:wi} for each $i \\in \\mathcal{V}$. To allow for learning of an unbounded number of communities, we utilise a stick-breaking representation \\citep{sethuraman1994} for each $\\boldsymbol{\\tau}_i$, which ensures that $\\sum_{k=1}^\\infty \\tau_{ik} = 1$, where $\\tau_{ik}$ is the probability that node $i$ belongs to global group $k$. %A stick-breaking decomposition is particularly useful for variational approximations, following \\cite{blei2006}. W In particular, we employ the probit stick-breaking construction of \\cite{chung_2009} and \\cite{rodriguez_2011}. In this model, $\\boldsymbol{\\tau}_i = (\\tau_{i1}, \\tau_{i2},\\dots)^{\\intercal}$ is obtained as  where $\\Phi\\{\\cdot\\}$ is the cumulative density of a standard normal random variable and $\\boldsymbol{\\varphi}_k \\sim \\text{Normal}(\\boldsymbol{\\varphi}_k^0, \\sigma_k^2I_P)$, with $I_P$ the $P \\times P$ identity matrix. This approach is similar to \\cite{kim2012}, where the authors use a logistic stick-breaking process \\citep{ren2011}. We opt for the probit stick-breaking process instead as \\cite{rodriguez_2011} prove that an $M$-dimensional truncation of this model converges in total variation norm, and therefore in distribution, to the infinite process. This is particularly useful for Bayesian inference via variational approximations as it demonstrates that truncations can be chosen for arbitrarily accurate inference. We place an independent $\\text{Normal}(\\boldsymbol{\\mu}, I)$ prior distribution on each $\\boldsymbol{\\varphi}_k^0$, and independent $\\text{Inverse\\text{-}Gamma}(\\nu_0,\\omega_0)$ prior distributions on the $\\sigma_k^2$, both selected for conjugacy. Write $\\varphi = \\{\\varphi_{kp}\\}$, $\\varphi^0 = \\{\\varphi_{kp}^{0}\\}$, and $\\boldsymbol{\\sigma}^2 = (\\sigma_1^2,\\sigma_2^2,\\dots)^\\intercal$ to store these parameters as matrices and a vector. In summary, the full model is expressed as follows: \tA_{\\ell i j} \\mid z_{\\ell i}, z_{\\ell j}, \\rho_{z_{\\ell i}z_{\\ell j}} &\\sim \\text{Bernoulli}(\\rho_{z_{\\ell i}z_{\\ell j}}), & & \\text{for all } i, j \\in \\mathcal{V}, \\; i \\neq j, \\; \\ell \\in [L], \\label{eqn:model_adjacency} \\\\ \tz_{\\ell i} \\mid w_i, \\boldsymbol{\\gamma}_{w_i} &\\sim \\text{Categorical}(\\boldsymbol{\\gamma}_{w_i}), & & \\text{for all } i \\in \\mathcal{V}, \\; \\ell \\in [L], \\label{eqn:model_z_elli}\\\\ \tw_i \\mid \\boldsymbol\\tau_i &\\sim \\text{Categorical}(\\boldsymbol\\tau_i), & & \\text{for all } i \\in \\mathcal{V}, \\label{eqn:model_w_i}\\\\  We call the proposed model \\emph{hierarchical multiplex stochastic blockmodel (HMPSBM)}. Figure \\ref{fig:model_schematic} provides an illustration of the parameters in the model, and Figure \\ref{fig:model_DAG} provides a representation of the HMPSBM as a directed acyclic graph (DAG). Figure \\ref{fig:toy_network} shows an illustration of the global and layer-level groups in a small network with 3 layers.\\\\",
        "summarize_figure": "2505.03441v1_fig3_arx",
        "summarization": "The chart shows a simple example of the node group structure in a multiplex network with L = 3 layers. There are two global groups, indicated by the shape of the nodes (circles and diamonds), and three layer groups, indicated by the colors of the nodes (green, red, blue). The row vectors above show the probabilities of nodes being assigned to layer groups given the global groups. The connection and grouping of nodes in different layers reflect the structural characteristics of the multiplex network."
    },
    "346": {
        "figure1": "2505.03441v1_world_map",
        "label1": "fig:world_map_FAO",
        "caption1": "Map of the world with countries present in the processed network coloured according to their inferred global group.",
        "text": "In Figure \\ref{fig:world_map_FAO} is a world map with countries that were present in the network coloured by their inferred global group. Group 4 (dark green) consists of France, China (main land), the United States of America, the United Kingdom, Italy, Spain, the Netherlands and Germany. This is an intuitive community as they represent some of the largest global economies and trading powers at the time. Group 3 (light green) consists of many emerging economies and regional powers (Brazil, Russia, Argentina) plus some smaller EU economies (Portugal, Greece) and Asian trading centers (Hong Kong, Malaysia). Group 2 seems to sit between these two groups, consisting of a mixture of advanced economies (Japan, Switzerland, Denmark, Sweden, Canada, South Korea) and emerging economies with major agricultural output (India, Turkey, Mexico, South Africa). A particularly interesting community is the group 8 (dark orange) which is made-up of only Syria and Iran. The dataset in question is from the year before the Syrian Revolution of 2011. Iran was one of the largest financial supporters of then President Bashar al-Assad and so it is interesting to see that our inference procedure has clustered these countries together.",
        "summarize_figure": "2505.03441v1_world_map",
        "summarization": "The picture is a world map, where countries are coloured according to their inferred global groups in the processed network. Different colours represent different groups. For example, Group 4 (dark green) includes France, China (mainland), the United States, etc., which are large global economies and trading powers. Group 3 (light green) includes emerging economies and regional powers like Brazil and Russia. Group 2 includes developed economies such as Japan and South Korea, and emerging economies like India and Turkey. Notably, Group 8 (dark orange) consists only of Syria and Iran."
    },
    "347": {
        "figure1": "2505.03448v1_traj1",
        "label1": "fig2",
        "caption1": "setup{justification=raggedright,singlelinecheck=false} \\caption{Estimated and groundtruth (GT) trajectories of 2 sample sequences.",
        "text": "As shown in Table \\ref{tab3}, test results based on the easy class sequences (S01-S05) indicate that the pose estimation accuracy of the VIO system (VINS-Stereo) is significantly higher than that of the VO system (ORB-SLAM2). This is because the presence of water waves and buoyancy causes high-frequency rapid jitter of the data collection platform during turns. On one hand, this violates the constant velocity motion model assumption relied upon by ORB-SLAM2, making pose estimation no longer accurate. On the other hand, under the premise of similar features in underwater environment and lower camera resolution, small-range rapid movements can cause degradation in visual pose estimation. In fact, based on what was mentioned in SVin2 \\cite{8967703}, this paper also applied CLAHE histogram equalization \\cite{pizer1987adaptive} to enhance visual features in the input images, but still did not achieve better results. VINS-Stereo, with the assistance of IMU data, can effectively compensate for the deficiencies in visual information. The IMU can provide high-frequency attitude and acceleration measurements, maintaining accurate pose estimation in the short term even when visual features are scarce. For data sequences with smoother motion and without rapid turns (S03), both ORB-SLAM2 and VINS can run relatively well, as shown in Fig. \\ref{fig2}(a).\nIn experiments based on hard class sequences, both Vins-Stereo and ORB-SLAM2 exhibited different degrees of failure. Under low-light conditions (S06-S07), ORB-SLAM2 struggled to extract effective ORB feature points, making it impossible to estimate trajectories. VINS-Stereo, on the other hand, uses a combination of Shi-Tomasi corner detection with optical flow tracking for matching, which can detect more reliable corners in low-contrast images, as shown in Fig. \\ref{fig2}(b). Additionally, optical flow tracking utilizes inter-frame continuity rather than re-detecting and matching features, reducing dependency on single-frame image quality, thus enabling more continuous trajectories. However, in HDR environment (S08), only a small area is illuminated, causing the scene to be in a state of constant abrupt changes, which can cause optical flow tracking to fail and affect VINS's performance. For blurry environment (S09), although ORB-SLAM2 had positioning errors of around 0.15m, it could not operate stably throughout the entire sequence, experiencing tracking loss during blurry frames. It was only through its effective relocalization capability that it could obtain pose estimation results for clear frames.",
        "summarize_figure": "2505.03448v1_traj1",
        "summarization": "The picture shows the estimated and groundtruth (GT) trajectories of two sample sequences. The black line represents the GT trajectory, the blue line represents the Vins algorithm trajectory, and the orange line represents the ORB algorithm trajectory. In the left - hand figure, the trajectories of different algorithms are intertwined, showing the deviation of each algorithm's trajectory from the GT trajectory in this sample sequence. In the right - hand figure, the Vins algorithm trajectory (blue) and the GT trajectory (black) show different trends, reflecting the performance differences of the algorithms in this sequence. As can be seen from the text, under different environmental conditions, the Vins and ORB algorithms perform differently in terms of pose estimation accuracy due to their different principles."
    },
    "348": {
        "figure1": "2505.03455v1_framework",
        "label1": "fig:framework",
        "caption1": "Overview of our eight-step procedure, from the attack implementation to the development of a unified defense framework against BTA and TDPA in VAS. The process begins with \\textbf{Step 1}, where raw user audio files are processed, and HFHPS triggers are embedded into a subset of the recordings. In \\textbf{Step 2}, targeted data poisoning is introduced by replacing a portion of user audio with attacker-supplied samples. \\textbf{Step 3} applies frequency-based analysis and weighted scoring to detect HFHPS triggers, followed by \\textbf{Step 4}, where detected triggered samples are labeled accordingly. In \\textbf{Step 5}, all labeled audio files are transformed into embeddings, which serve as input for \\textbf{Step 6}, where a convolutional neural network (CNN) is trained to distinguish between legitimate, attacked, and triggered samples. The trained model is then evaluated in \\textbf{Step 7} on an unseen dataset to identify both TDPA and BTA cases. Finally, in \\textbf{Step 8}, a voting aggregation mechanism integrates sample-level classifications to make a final user-level decision. This multi-stage approach enhances the detection of backdoor triggers and data poisoning while minimizing false positives, ensuring reliable authentication in VAS.} \\par",
        "text": "The following subsections describe the attack design, the subsequent detection and classification strategies, the generation of discriminative embeddings, and the CNN model training process.  Figure~\\ref{fig:framework} gives an overview of the entire process from staging our proposed threat model to implementing our defense framework. First, a portion of user audio recordings (each standardized to about three seconds) are embedded with HFHPS triggers and another portion is poisoned by attacker-supplied segments. Next, the audio files go through our PBSM detection mechanism and this layer labels the backdoor triggered files. The labeled audio files are then transformed into feature embeddings to train a convolutional neural network (CNN) capable of distinguishing poisoned files. To finalize decisions at the user level, a majority-vote mechanism aggregates classifications. By combining frequency-based detection, CNN-based classification, and voting-based user assignment, our framework robustly captures both subtle pitch-based triggers and malicious sample replacements.  Figure~\\ref{fig:framework} provides an overview of the entire process, from staging the proposed threat model to implementing our defense framework.   Initially, a subset of user audio recordings (each trimmed down to three seconds) is embedded with HFHPS triggers, while another portion is poisoned with attacker-supplied segments. These manipulated audio files then pass through our PBSM detection mechanism, which identifies and labels backdoor-triggered samples. Next, the labeled audio is transformed into feature embeddings, serving as input for our proposed CNN trained to differentiate between legitimate, poisoned, and triggered files.   Finally, a majority-vote mechanism aggregates classifications at the user level, determining whether an account is \\textit{Triggered}, \\textit{Attacked}, or \\textit{Legitimate}.  In order to unify the mathematical expressions used across this paper, Table \\ref{tab:notation_table} defines the symbols frequently referenced in this work.",
        "summarize_figure": "2505.03455v1_framework",
        "summarization": "The chart shows an eight - step protection process against Backdoor Trigger Attack (BTA) and Targeted Data Poisoning Attack (TDPA) in Voice Authentication Systems (VAS). It starts with processing raw user audio files and embedding High - Frequency Human - inaudible Pure - tone Signals (HFHPS) triggers, followed by introducing targeted data poisoning. Then, it detects HFHPS triggers through frequency - based analysis and weighted scoring and labels them. The labeled audio is converted into embedding vectors to train a Convolutional Neural Network (CNN) to distinguish between legitimate, attacked, and triggered samples. The trained model is evaluated on an unseen dataset to identify attacks, and finally, a voting aggregation mechanism makes a final user - level determination. This process can effectively detect attacks and reduce false positives."
    },
    "349": {
        "figure1": "2505.03460v1_dataset",
        "label1": "dataset",
        "caption1": "Statistics of VLD Dataset: (a) target categories, (b) building types, (c) floor number distribution, and (d) difficulty distribution. ",
        "text": "Based on both the predefined buildings and objects in CARLA maps and additional high-fidelity models manually added, we create 300 VLD tasks distributed across 22 different buildings. As shown in Fig.~\\ref{dataset}(a), the target objects \\(D_{tar}\\) cover a wide range of categories, including tools, containers, household items, food, furniture, 2D posters, toys, and ornaments. The types of buildings include low-rise residential buildings, high-rise structures, small villas, and culturally themed architectures, which are depicted in Fig.~\\ref{dataset}(b).\nBesides the rich variety in scenes, we also ensure broad coverage of task difficulty levels and target floor numbers \\(F_{tar}\\), as shown in Fig~\\ref{dataset}(c) and (d). The task difficulty is defined by the minimum number of drone turns required to detect the \\(D_{tar}\\). We classify tasks with fewer than 2 turns as 'Easy', those requiring 2 to 3 turns as 'Moderate', and those with more than 3 turns as 'Hard'.",
        "summarize_figure": "2505.03460v1_dataset",
        "summarization": "The picture shows the statistical information of the VLD dataset. In figure (a), the target categories are diverse, covering tools, containers, household items, food, furniture, 2D posters, toys, and ornaments. Figure (b) shows that building types include low - rise residential buildings (41%), high - rise structures (27%), villas (14%), and culturally themed architectures (18%). Figure (c) presents the floor number distribution, with more low - floor numbers and fewer high - floor numbers. Figure (d) indicates that in the task difficulty distribution, medium difficulty accounts for 49%, hard difficulty accounts for 32%, and easy difficulty accounts for 19%."
    },
    "350": {
        "figure1": "2505.03464v1_fig2",
        "label1": "fig:2",
        "caption1": "(a1)(b1) The influence of particle velocity and particle density on the critical opening size, respectively. In (a1), the minimal value of $O_{c}$ is observed when the particle velocity $v_{0}\\Delta t=0.025L$. \\protect (a2) The mechanism of velocity - regulated critical opening size: Particles with higher velocities are more likely to penetrate deeper into the adversarial particle cluster, reducing their chance of maintaining their original opinions at $t+\\Delta t$. This explains the interval over which $O_{c}$\\LyXZeroWidthSpace{} increases with particle velocity. (b2) The mechanism of density - regulated critical opening size: Increased particle density reduces relative density fluctuations within a particle's view field. Consequently, a particle crossing the opening perceives smaller density differences between the two opinion camps, which is unfavorable to filling the gap between its view angles regarding the two camps. Therefore, a particle inevitably switch its opinion at $t+\\Delta t$.",
        "text": "Opinions are embedded in each particle and thus faster particle movement is equivalent to faster opinion dissemination in this sense. But numerically tuning $v_{0}\\Delta t$ from $0.001L$ to $0.1L$, the corresponding $O_{c}$ undergoes a interesting V-shaped change {[}See Fig.~\\ref{fig:2}(a){]}, reaching its minimum around $v_{0}\\Delta t=0.025L$. We need to tackle this problem piece by piece. For the regions where $O_{c}$ increases monotonically (from $0.025L$ to $0.1L$), the mobility is adjusted from a moderate value to an excessively large one. When the speed is too high, the particle at time $t+1$ will enter a deeper opinion region of the opposing side. Since it is isolated ($\\gamma\\sim0$), it immediately flips its opinion, and true opinion propagation cannot be formed. On the contrary, when the speed drops from a moderate to extremely low value (from $0.025L$ to $0.001L$), a particle linger longer in the small area near the opening where two kinds of opinions coexist. This increases the probability of opinion reversal of a particle, preventing enough particles with the original opinion from crossing to the opposite side of the barrier. To compensate, the opening must widen to allow more particles to have the freedom to walk to the other side. Accordingly, we see the increase of the critical opening size.\nAs discussed in Sec.~\\ref{subsec:Barrier-induced-stalemate-concen}, local particle density fluctuation lifting opinion environment factor to $\\gamma$=1 is key to opinion spread. By arising the particle density (equivalent to particle number $N$ for a fixed system size), $O_{c}$ exhibits a monotonic increase {[}See Fig.~\\ref{fig:2}(b){]}, which, if understanding it from the single-particle picture, indicates that the ability of the fluctuation to lift $\\gamma$ is weakened. In other words, the relative density fluctuations are reduced. Hence the interpretation is qualitatively matched with the basic principle of statistic physics: as particle number $N\\rightarrow\\infty$, the relative density fluctuations are suppressed. In simulation, we find that the proportion of $\\gamma<1$ monotonically increases with the increase of particle density, which is another evidence for the suppression of opinion merging if considering our single-particle picture {[}See Fig.~\\ref{fig:3}(b){]}.",
        "summarize_figure": "2505.03464v1_fig2",
        "summarization": "The picture shows the influence of particle velocity and density on the critical opening size. In (a1), the critical opening size Oc reaches its minimum when v₀Δt = 0.025L. Faster - moving particles are more likely to penetrate deeper into the opposing particle cluster at t + Δt, reducing their chance of maintaining their original opinion, which explains the increase of Oc with particle velocity in certain intervals. In (b1), as particle density increases, Oc also increases. This is because higher particle density reduces relative density fluctuations within a particle's view field. A particle crossing the opening perceives smaller density differences between the two opinion camps, making it more likely to change its opinion at t + Δt."
    },
    "351": {
        "figure1": "2505.03464v1_fig3",
        "label1": "fig:3",
        "caption1": "Numerical results showing the distribution of the opinion environment factor $\\gamma.$ (a) As the view field radius R increases, the proportion of $\\gamma$ values below $1$ also increases. (b) Similarly, as the number of number of particles $N$ increases, the proportion of $\\gamma$ values below $1$ increases.\\protect. Here, we fix $O=0.25L$.",
        "text": "One may notice in Fig.~\\ref{fig:1}(b2), the upper bound of $\\gamma$ is below $1$ no matter how the opening is enlarged in the single-particle picture and thus deem a particle after crossing the opening will always immediately flip its opinion, not causing opinion transmission. In fact, the key is that the local particle density fluctuation beyond the mean field picture may fill the crucial gap of $\\gamma$ from $\\lesssim1$ to $1$. In simulation, statistic results show that with the opening size squeezed the proportion of $\\gamma<1$ increasing while there still exists a small proportion of $\\gamma$ is above $1$ {[}See Fig.~\\ref{fig:3}(a){]}.\nfactor $\\gamma.$ (a) As the view field radius R increases, the proportion of $\\gamma$ values below $1$ also increases. (b) Similarly, as the number of number of particles $N$ increases, the proportion of $\\gamma$ values below $1$ increases.\\protect\\label{fig:3}. Here, we fix $O=0.25L$.}\nAs discussed in Sec.~\\ref{subsec:Barrier-induced-stalemate-concen}, local particle density fluctuation lifting opinion environment factor to $\\gamma$=1 is key to opinion spread. By arising the particle density (equivalent to particle number $N$ for a fixed system size), $O_{c}$ exhibits a monotonic increase {[}See Fig.~\\ref{fig:2}(b){]}, which, if understanding it from the single-particle picture, indicates that the ability of the fluctuation to lift $\\gamma$ is weakened. In other words, the relative density fluctuations are reduced. Hence the interpretation is qualitatively matched with the basic principle of statistic physics: as particle number $N\\rightarrow\\infty$, the relative density fluctuations are suppressed. In simulation, we find that the proportion of $\\gamma<1$ monotonically increases with the increase of particle density, which is another evidence for the suppression of opinion merging if considering our single-particle picture {[}See Fig.~\\ref{fig:3}(b){]}.",
        "summarize_figure": "2505.03464v1_fig3",
        "summarization": "The picture shows the numerical results of the distribution of the opinion - environment factor γ. In sub - graph (a), with the opening size O fixed at 0.25L, as the view - field radius R increases from 0.05L to 0.30L, the proportion of γ values less than 1 increases. This indicates that a larger view - field radius makes the opinion - environment change more likely to cause particles to change their opinions. In sub - graph (b), with O still fixed at 0.25L, as the number of particles N increases from 200 to 3200, the proportion of γ values less than 1 also gradually increases. This shows that an increase in the number of particles reduces relative density fluctuations, which is unfavorable for particles to maintain their original opinions and affects opinion dissemination and merging."
    },
    "352": {
        "figure1": "2505.03465v1_sigma_n",
        "label1": "sig",
        "caption1": "$\\sigma_{n}$ and ${d_{k}^{n}}$",
        "text": "Let $d^{n}_{k}$ be a linear map from $V^{\\otimes n}$ to $V^{\\otimes n}$ defined as below \\footnote{$d_{k}^{n}$ is different from $d_{k,n}$ defined in Definition \\ref{Definition 4.2}. Actually, $d_{k,n}=(R_{M}\\otimes {\\rm id}_{V^{\\otimes n-1}})\\circ d_{k}^{n}$. }{\\rm :}     When $k=1$, $d_{1}^{n}:=\\mathrm{id}_{V^{\\otimes n}}$ and when $k>1$    $$d^{n}_{k}:=( R \\otimes \\mathrm{id}_{V^{\\otimes n-2}})\\circ (\\mathrm{id}_{V}\\otimes R\\otimes \\mathrm{id}_{V^{\\otimes n-3}})\\circ \\cdots \\circ (\\mathrm{id}_{V^{\\otimes k-2}}\\otimes R\\otimes \\mathrm{id}_{V^{\\otimes n-k}}).$$     We define a linear map $\\sigma_{n}:V^{\\otimes n}\\to V^{\\otimes n}$ by     $\\sigma_{n}=\\sum_{i=1}^{n}(-1)^{i-1}d^{n}_{i}.$    A graphical illustration of them is shown in Figure \\ref{sig} \\rm{:}",
        "summarize_figure": "2505.03465v1_sigma_n",
        "summarization": "The picture shows the graphical representations of σₙ and dᵏₙ. σₙ is a linear mapping from the n - fold tensor product space V⊗ⁿ of V to itself, defined as σₙ = ∑ᵢ₌₁ⁿ (-1)ⁱ⁻¹dⁱₙ, consisting of terms with different connection patterns and alternating plus - minus signs. dᵏₙ is also a linear mapping from V⊗ⁿ to V⊗ⁿ. When k = 1, d₁ⁿ is the identity mapping on V⊗ⁿ; when k > 1, dᵏₙ is composed of a series of tensor product composites of R and the identity mapping id. Different k values correspond to different line - connection forms. These mappings play important roles in the study of relevant algebraic structures."
    },
    "353": {
        "figure1": "2505.03468v1_Stackelberg_Classes_3",
        "label1": "fig:classes",
        "caption1": "Different hierarchical strategic interactions illustrating the four Stackelberg game classes (Squares represent leaders and circles represent followers).",
        "text": "As we can consider multiple strategic interactions at each layer of the hierarchical Stackelberg scheme, we have multiple classes for the Stackelberg game as summarized in Table \\ref{tab:stackelberg_classes}. Each class is also presented in Fig. \\ref{fig:classes}.\nThis subsection introduces the possible strategic interactions at each layer, which establish a Stackelberg game class. Let us consider the case in which the leaders $\\mathcal{L}$ do not cooperate, i.e., there are $L$ designer entities making decisions following an independent interest. Also, assume that once the design is determined, the followers $\\mathcal{M}$ design a control action independently, i.e., there are $M$ entities deciding control actions following different non-cooperative costs. The Stackelberg equilibrium corresponding to this scenario is formally presented in Definition \\ref{def:class_I} below (see Fig. \\ref{fig:classes}).\nLet us now assume that all the leaders $\\mathcal{L}$ coordinate each other and agree to jointly perform the design of the system. This cooperation leads to an optimization that is jointly performed by the $L$ leaders. Then, once the system design is established, the followers $\\mathcal{M}$ cooperatively design the control actions to operate the system. This cooperative followers' behavior leads to a centralized control design. Definition~\\ref{def:class_II} formally presents the Stackelberg equilibrium under this scenario (see Fig. \\ref{fig:classes}).\nThe aforementioned Stackelberg game classes have considered a homogeneous behavior for both the leader and followers, i.e. both layers cooperating or both layers acting independently in a non-cooperative framework. However, layers may exhibit heterogeneous strategic behavior. First, let us consider the case in which the leaders $\\mathcal{L}$ do not cooperate for the system design, whereas the followers $\\mathcal{M}$ cooperatively react to the system design by solving jointly a centralized optimal control problem. The emerging Stackelberg equilibrium for this scenario is formally presented next in Definition \\ref{def:class_III} (see Fig. \\ref{fig:classes}).\nFinally, let us assume that the leaders $\\mathcal{L}$ jointly optimize the system design. Once the system is designed, the followers react to this leader's strategic action by designing a selfish control input, i.e., the followers $\\mathcal{M}$ play a dynamic game. Definition \\ref{def:class_IV} shows he resulting Stackelberg game equilibrium for this combination of behavior across the layers (see Fig. \\ref{fig:classes}).\nThis section formally presents the procedure to compute each one of the Stackelberg equilibria corresponding to the four different classes introduced above (see Fig. \\ref{fig:classes}). We present a qualitative description of the steps that comprise each one of the computations together with its corresponding algorithm.\nIf the followers decide to cooperate in order to define the appropriate control inputs, then the problem becomes a traditional optimal control problem. The cooperative game problem is as follows: &\\min_{(\\mathbf{u}_1,\\dots,\\mathbf{u}_M) \\in \\prod_{i \\in \\mathcal{M}} \\mathcal{U}_i} \\sum_{i \\in \\mathcal{M}} V_i(x_0,\\mathbf{u}),\\\\ &\\text{s.~t.}~     {x}_{k+1} = A {x}_{k} +  B {u}_{k} + B_l {d}_k,\\\\     0 = E {u}_{k} + E_d {d}_{k},\\\\     {u}_{i,k} \\in \\mathbb{U}_i,~i \\in \\mathcal{M},\\\\     {x}_{k} \\in \\mathbb{X}(\\mathbf{a}), where      $B = [B_1 \\quad \\dots \\quad B_M]$,      $E = [E_1 \\quad \\dots \\quad E_M]$,  and     ${u}_{k} = [{u}_{1,k}^\\top \\quad \\dots \\quad {u}_{M,k}^\\top]^\\top$. %\\\\     We compute and test each one of the Stackelberg game classes presented in Fig. \\ref{fig:classes} by combining the aforementioned game problems. The results are presented and discussed in the coming section, where we present the Stackelberg equilibrium for each class and we also analyze the price of anarchy at each layer (leader and follower layer).\nFig.~\\ref{fig:x} illustrates the volume evolutions of selected tanks across four subsystems, each corresponding to one of the four classes. The observed daily pattern in tank volumes mirrors the water demand cycle. In Fig. \\ref{fig:x-s3}, the volume evolutions of tank $x_{14}$ are similar to the four classes, as the designed tank sizes are the same based on the leader solutions, which is consistent with the follower solutions shown in Fig. \\ref{fig:u-s3}. From the game theory perspective, the trajectories presented in Fig. \\ref{fig:u-s1} and Fig. \\ref{fig:u-s4} correspond to the Nash equilibrium for the followers when they strategically interact in a dynamic game (Classes I and IV). In contrast, the trajectories in Fig. \\ref{fig:u-s2} and Fig. \\ref{fig:u-s3} present the optimal control inputs corresponding to a cooperative dynamic game (Classes II and III). The reader may compare the followers' strategic interactions in Fig. \\ref{fig:classes}.",
        "summarize_figure": "2505.03468v1_Stackelberg_Classes_3",
        "summarization": "The picture shows four Stackelberg game classes that illustrate different hierarchical strategic interactions. Squares represent leaders and circles represent followers. In Class I, leaders aⱼ* (j in the leader set L ) do not cooperate, and followers uᵢ* (i in the follower set M ) act independently. In Class II, leaders ā* coordinate with each other, and followersӯ* jointly design control actions. In Class III, leaders aⱼ* do not cooperate, while followersӯ* cooperatively respond to the system design. In Class IV, leaders ā* jointly optimize the design, and followers uᵢ* take selfish control inputs in a dynamic game. These classes reflect different combinations of strategic behaviors."
    },
    "354": {
        "figure1": "2505.03477v1_fig_fwfs",
        "label1": "fig_fwfs",
        "caption1": "Sketch of a Fourier-type WFS~\\cite{HuNeuSha_2023}.",
        "text": "Non-linear Fourier-type WFSs~\\cite{Fauv16} have become particularly interesting for measuring wavefronts in AO due to their high sensitivity. They use optical Fourier filtering with a suitable optical element (e.g., a multi-facet glass prism) located in the focal plane (see fig.~\\ref{fig_fwfs}). The optical element splits the electromagnetic field into several beams, each of which produces a differently filtered image of the entrance pupil in the subsequent pupil plane. The intensity patterns are then measured by a camera. The underlying mathematical model for the intensity $I$ is given by  (I(\\phi))(x,y)=\\abs{F^{-1}\\kl{OTF\\cdot F\\kl{\\chi_\\Omega e^{-i\\phi}}}(x,y)}^2 for $F$ representing the Fourier transform, $\\phi$ the incoming wavefront and $\\chi_\\Omega$ the characteristic function of the pupil $\\Omega$. The optical transfer function $OTF$ describes the optical element that divides the light in the focal plane. Please note that in equ.~\\req{i1} we consider a simplified version of the actual incoming electromagnetic field $$\\sqrt{n}\\chi_\\Omega e^{-i\\phi},$$ where $n$ is the spatial average incoming flux. As $n$ is proportional to the total flux on the detector, this normalisation can always be done in post-processing. Furthermore, it is assumed that the incoming light is monochromatic and $$\\phi = \\frac{2\\pi}{\\lambda}\\Delta$$ is the perturbed phase at the considered wavelength $\\lambda$. The optical path difference $\\Delta$ was created by atmospheric turbulence or another source of perturbation.",
        "summarize_figure": "2505.03477v1_fig_fwfs",
        "summarization": "The picture is a sketch of a Fourier - type wavefront sensor (WFS). It shows the propagation path of the incident phase light, passing through the pupil plane, focal plane, and conjugate pupil plane in sequence, and finally reaching the detector. Fourier filtering is performed at the focal plane. A suitable optical element (such as a multi - facet glass prism) is used to split the electromagnetic field into multiple beams, and each beam generates a differently filtered image of the entrance pupil in the subsequent pupil plane, whose intensity patterns are measured by a camera. This sensor is used to measure wavefronts in adaptive optics and has high sensitivity. Its intensity mathematical model is based on concepts related to the Fourier transform."
    },
    "355": {
        "figure1": "2505.03477v1_fourier_detector",
        "label1": "fig_otf",
        "caption1": "Transfer function of different Fourier-type WFSs (top) and corresponding detector intensities for an example wavefront (bottom). The first three columns represent a PWFS but with different apex angles. The third column is a flattened PWFS where all four intensity pupil images are overlapping.",
        "text": "The constant $c>0$ refers to the angle of apex of the pyramidal prism, which influences the separation distance between the four intensity patterns in the conjugate pupil plane. The PWFS $OTF$ and its corresponding intensity pattern for an example wavefront are visualised in fig.~\\ref{fig_otf} where the first three columns represent PWFSs with different angles $c$. The WFSs can make a measurement by considering only the light on the pupils or the full frame, i.e., taking into account the interference effects and the light between the pupils. Although the full-frame approach can be more accurate than the pupils-only approach, its computational load can be challenging for ELT-scaled systems running in real-time~\\cite{FaHuLAM_ao4elt6_poster}.\nFourier-type WFSs are a general class of WFSs in the sense that different optical elements can be used to split the light in the focal plane. Possible choices are $n$-sided pyramidal prisms for $n \\in \\N$ as is the case for the 2-sided roof WFS or the 3-sided pyramid WFS~\\cite{Clare_ao4elt5,engler2017,Veri04}. The cone WFS is the extension of the idea to an infinite number of faces~\\cite{ClareCone2020}. The Zernike WFS has a small circular hole with depth $\\delta$ and diameter $p$ in the center of the mask of the optical element~\\cite{Zernike1934} and the iQuad WFS shows a focal plane that is divided into 4-quadrants around the origin, i.e., it has a Cartesian structure~\\cite{FaHuShaRaLAM19_AO4ELTproc}. Other Fourier-type WFSs are the bi-orthogonal Foucault knife-edge sensors (sharp and grey Bi-O edge WFSs). They follow the principal idea of roof WFSs and are therefore also motivated by the Foucault knife-edge test~\\cite{Veri2024}. The optical transfer functions of the above WFSs are all described by  OTF\\left(\\xi,\\eta\\right) = e^{i\\psi\\left(\\xi,\\eta\\right)} where the corresponding shape functions $\\psi$ are listed in tbl.~\\ref{table_otf} and visualised in fig.~\\ref{fig_otf} with their corresponding intensity images for an example wavefront.\nAs we wish to send DM actuator commands to the AO system, the number of active actuators $n_a$ denotes the number of unknowns to be found. NOPE is an algorithm that follows the idea of a full-frame approach. This means that WFS measurements on the full detector image are considered. Depending on the variant of the Fourier-type WFS, many of the detector pixels will be zero because they are exposed to a relatively low level of light (see fig.~\\ref{fig_otf}, bottom). Let $n\\sim 4n_a$ indicate the number of nonzero detector pixels. NOPE consists of pointwise multiplications and summations which have computational complexities of $\\mathcal{O}\\left(n_a\\right)$ or $\\mathcal{O}\\left(n\\right)$. These are combined with Fourier transforms, which attribute a significant portion of the computational overhead. They are carried out in the focal and detector plane where full frame images of the size $n$ are considered. Hence, the Fourier transforms have a complexity of $\\mathcal{O}\\left(n \\log n\\right)$. Thus, the overall computational effort of NOPE is $\\mathcal{O}\\left(n \\log n\\right)$.",
        "summarize_figure": "2505.03477v1_fourier_detector",
        "summarization": "The picture shows the transfer functions (top row) of different Fourier - type wavefront sensors (WFSs) and the corresponding detector intensities for an example wavefront (bottom row). The first three transfer functions in the top row represent Pyramidal Wavefront Sensors (PWFSs) with different apex angles, and the third column is a flattened PWFS where the four intensity pupil images overlap. The bottom row shows the detector intensity distributions for the corresponding example wavefront, presenting different spot patterns. Different Fourier - type WFSs work by splitting light in the focal plane using different optical elements. The apex angle of the PWFS affects the spacing of intensity patterns in the conjugate pupil plane, and full - frame measurements are more accurate but computationally intensive."
    },
    "356": {
        "figure1": "2505.03480v1_fig1",
        "label1": "fig1",
        "caption1": "Our methodology for users preferences modeling. Music genre trajectories are extracted from user listening histories and processed using the pathlet learning algorithm. This highlights recurring patterns in user behavior, which are then used to generate explicit trajectory embeddings that predict future genre allocation.",
        "text": "This paper is organized as follows: In Section~\\ref{section2}, we present the background and motivation for our study. Section~\\ref{section3} introduces our approach to pathlet learning for user trajectories (see Figure~\\ref{fig1}), while we outline the experimental protocol in Section~\\ref{section4}. Finally, we present our results in Section~\\ref{section5}.\nIn dictionary learning, the $\\alpha$ matrix embeds input data ($P$ in our case). But in this context, each coordinate of the $\\alpha$ matrix corresponds to a candidate pathlet, which may not be part of the final dictionary. To refine this, we can remove rows linked to unselected pathlets, forming a reduced matrix $\\Tilde{\\alpha}$, where columns represent trajectory embeddings. However, for a dictionary containing pathlets like $(rock, rap, jazz)$ and $(rock, jazz)$, the $\\Tilde{\\alpha}$ embedding for trajectory $(pop, rock, rap, jazz, pop)$ could activate both pathlets. This redundancy encodes the same information twice, reducing sparsity in the embeddings. To overcome this, we separate the learning of dictionary from the embedding computation.  Our trajectory embedding algorithm iteratively identifies the longest matching pathlet and updates the corresponding coordinate. By recursively segmenting the trajectory, we reduce redundancy, and prioritizing longer pathlets improves sparsity and expressiveness. This approach captures the sequence of the trajectory in explicit embeddings, useful for downstream tasks, as Figure~\\ref{fig1} illustrates.",
        "summarize_figure": "2505.03480v1_fig1",
        "summarization": "The picture shows a methodology for user preference modeling. Music genre trajectories are extracted from users' listening histories over different time periods (such as music genres like rap, soul, pop). These trajectories are input into a genre graph and processed by a pathlet learning algorithm to identify recurring patterns in user behavior. Then these patterns are encoded as trajectory embeddings to form a pathlet basis, ultimately used to predict users' future music genre choices. This method aims to analyze historical listening records to clarify trajectory characteristics, providing useful information for downstream tasks."
    },
    "357": {
        "figure1": "2505.03480v1_variation_types",
        "label1": "fig:variation_types",
        "caption1": "Users' genre listening variation types between 2 successive periods with respect to intra-variability. While \\textit{appearance} and \\textit{disappearance} correspond to emergence or abandonment of genres, persistence rely on variations of listening rate for a genre listened during 2 successive periods. ",
        "text": "In Figure~\\ref{fig:variation_types}, we show how user genre allocation can evolve through 2 successive time windows, thanks to Average Total Variation (ATV, defined in Equation~\\ref{eq:tv}). The figure indicates that a significant share of variations is due to genre appearance and disappearance in our dataset.  Being able to model the emergence and abandonment of genres in user histories is therefore of particular interest, both from a recommendation perspective and a sociological one, as mentioned in Section~\\ref{intro}. Our objective is thus to study the behavior of users with a focus on two phenomena: understanding how new genres appear in users' listening habits, and how they disappear. Next, we define user trajectories accordingly.",
        "summarize_figure": "2505.03480v1_variation_types",
        "summarization": "The chart shows the relationship between users' music genre listening variation types (appearance in light blue, disappearance in blue - gray, and persistence in dark blue) in two successive periods and user intra - variation percentiles. The horizontal axis is the user intra - variation percentile, and the vertical axis is the Average Total Variation (ATV) between the two periods. As the percentile increases, the ATV value rises. The appearance and disappearance of genres contribute significantly to the variation, indicating that in the dataset, the rise and abandonment of genres are the main factors of change. Studying these phenomena is of great significance for recommendation systems and sociological research."
    },
    "358": {
        "figure1": "2505.03480v1_sampling",
        "label1": "fig:sampling",
        "caption1": " Reconstruction error of $X$ over the number of sampled trajectories. ",
        "text": "Our hyperparameter choices are influenced by the constraints of available memory space and the necessity for interpretability in pathlet representations.  Figure~\\ref{fig:sampling} shows that sampling $1k$ trajectories per candidate from $A^+$ and $A^-$ capture a significant amount of co-listening trends. For each of the two dictionaries, we randomly select $5k$ associated candidate trajectories.  The candidate pathlets consist of the 10,000 most popular sub-paths of size less than 10 among the selected trajectories, allowing for a more manageable representation of user behavior.",
        "summarize_figure": "2505.03480v1_sampling",
        "summarization": "The chart shows the relationship between the number of sampled trajectories and the reconstruction error. The horizontal axis represents the number of sampled trajectories, and the vertical axis represents the reconstruction error. As the number of sampled trajectories increases from 0 to 1000, the reconstruction error drops rapidly, indicating that more sampled trajectories can significantly improve the reconstruction effect. After the number of sampled trajectories reaches a certain level, the reconstruction error stabilizes. This shows that sampling 1000 trajectories can effectively capture the co - listening trend, providing a basis for subsequently selecting appropriate pathlets from candidate pathlets to represent user behavior."
    },
    "359": {
        "figure1": "2505.03482v1_Homothetictube",
        "label1": "Fig:learningtubeMPC",
        "caption1": "Learning-based tube MPC framework, where $x_k$, $u_k$, $w_k$ are state, control input, and disturbance respectively; $\\hat{\\mathbb{W}}_k$ is the learned disturbance set; and $\\hat{w}_{\\max,k}$ is the worst-case estimation. ",
        "text": "To mitigate the conservativeness discussed above, we propose a learning-based MPC algorithm incorporating online uncertainty quantification for discrete-time linear systems subject to bounded additive disturbance, as shown in Fig.~\\ref{Fig:learningtubeMPC}. Since the exact set, $\\mathbb{W}_{\\rm true}$, containing all possible realisations of the disturbance is often unknown, we instead assume an over-approximate polytopic set, $\\mathbb{W}$, of $\\mathbb{W}_{\\rm true}$ is given. Assuming perfect state feedback, we can collect disturbance measurements and use this data to design parameterised sets $\\hat{\\mathbb{W}}_k$ for approximating $\\mathbb{W}_{\\rm true}$ online via the scenario approach \\cite{campi2008exact}. These parameterised sets are referred to as learned disturbance sets.  The main contributions of this paper are summarised as follows. 1) The set $\\hat{\\mathbb{W}}_k$ is parameterised such that it scales $\\mathbb{W}$ in different directions with heterogeneous parameters. This can yield a more accurate estimate of $\\mathbb{W}_{\\rm true}$ than the set parameterisation in \\cite{gao2023learning}, which only allows for uniform scaling. Also, the parameterisation of $\\hat{\\mathbb{W}}_k$ enables the approximation problem to be efficiently solved as a linear program (LP). 2) The learned disturbance sets are updated online and incorporated in the synthesis of a computationally efficient MPC algorithm. In the MPC problem, constraints on predicted trajectories over an infinite horizon are enforced by a finite number of constraints. This number is fixed by leveraging the nestedness property of the time-varying learned disturbance sets. By contrast, in the previous work \\cite{gao2023learning}, the required number of constraints needs to be recomputed at each time step in the learning-based rigid tube MPC problem, increasing computational burdens.",
        "summarize_figure": "2505.03482v1_Homothetictube",
        "summarization": "The picture shows a learning - based tube model predictive control (MPC) framework. The state xₖ, control input uₖ, and disturbance wₖ of the controlled system are input into the disturbance estimation module, and the disturbance wₖ is also input into the learning uncertainty set module to generate the learned disturbance set Ŵₖ. Ŵₖ is used for worst - case estimation to obtain the worst - case estimate ŵₘₐₓ,ₖ. xₖ, uₖ, and ŵₘₐₓ,ₖ are input into the homothetic tube MPC module, which generates the control input uₖ fed back to the controlled system. This framework improves the handling of bounded additive disturbances in discrete - time linear systems through online uncertainty quantification, focusing on more accurately estimating the disturbance set and efficiently integrating it into the MPC algorithm."
    },
    "360": {
        "figure1": "2505.03484v1_ablation",
        "label1": "fig:ablation",
        "caption1": "Ablation study of different components in our model. We evaluate the effectiveness of MoE, PMA, and SSM by removing them, respectively.} \\vspace{-5pt}  \\vspace{-5pt",
        "text": "In this subsection, we conduct comprehensive ablation experiments to evaluate the contribution of each key component in STAR-Rec. Following our model design philosophy of integrating state-space modeling with similarity-enhanced attention, we examine three critical modules: MoE, PMA, and SSM. We create three variants by removing each component individually and report their performance on the Beauty dataset in Figure~\\ref{fig:ablation}.",
        "summarize_figure": "2505.03484v1_ablation",
        "summarization": "The chart shows the results of the ablation study on different components of the model, with the evaluation metrics being NDCG@10 and MRR@10. The four - colored bars represent Default (yellow), w/o MoE (red), w/o PMA (blue), and w/o SSM (purple). Under the NDCG@10 metric, Default performs best, and the metrics decrease after removing each component. Under the MRR@10 metric, Default is also optimal, and the metric drops significantly after removing the PMA component. This indicates that the MoE, PMA, and SSM components in the model all play important roles in improving the model's performance."
    },
    "361": {
        "figure1": "2505.03484v1_Parameter_d",
        "label1": "fig:parameter_d",
        "caption1": "Impact of hidden size $d$ on model performance.}  \\vspace{-5mm",
        "text": "Through Figure \\ref{fig:parameter_d}, we examine the impact of hidden dimensions on model effectiveness. The experimental results show that Star-Rec consistently outperforms the baselines across different hidden sizes, particularly achieving substantial gains at $d=64$ compared to Mamba4Rec's state-space modeling and SASRec's attention mechanism. Notably, both Mamba4Rec and SMLP4Rec show sensitivity to the hidden dimension, with their performance slightly declining as the dimension increases. This suggests these models are more sensitive to the choice of hidden dimension, potentially due to their structural characteristics in processing sequential information.",
        "summarize_figure": "2505.03484v1_Parameter_d",
        "summarization": "The picture shows the impact of hidden size d on model performance. The top sub - graph shows Recall@10 and the bottom shows NDCG@10 for Mamba4Rec, SASRec, Star - Rec, and SMLP4Rec. Star - Rec outperforms others across different d values, especially at d=64. Mamba4Rec and SMLP4Rec are sensitive to d, with performance decreasing as d increases."
    },
    "362": {
        "figure1": "2505.03484v1_fusion1",
        "label1": "fig:fusion1",
        "caption1": "Overview of the adaptive fusion mechanism integrating SSM and PMA paths for sequence modeling.}  \\vspace{-10pt",
        "text": "As shown in Figure~\\ref{fig:fusion1}, we introduce an adaptive fusion mechanism integrating SSM with preference-aware attention. This attention mechanism complements the SSM by capturing static preference relationships and item associations that temporal compression alone cannot model. By integrating cosine similarity into the attention computation, preference-aware attention assigns higher weights to more representative information in scenarios involving short-term behaviors or clusters of similar items.",
        "summarize_figure": "2505.03484v1_fusion1",
        "summarization": "The picture shows an adaptive fusion mechanism for sequence modeling that integrates the State - Space Model path (SSM Path) and the Preference - Aware Attention path (PMA Path). In the SSM path, the input X is linearly projected, and then through discretization and other operations, information is passed between hidden states ht - 1, ht, etc. at different time steps, finally outputting Yssm. In the PMA path, the input X is processed by an enhanced multi - head attention mechanism, and then through matrix multiplication and other operations, Yattn is output. The outputs of the two paths are added together to obtain the final output Yfinal. This mechanism uses preference - aware attention to complement SSM, capturing static preference relationships and item associations."
    },
    "363": {
        "figure1": "2505.03491v1_20250204_QWP2",
        "label1": "fig:QWPresutls",
        "caption1": " Typical birefringence maps (upper) and their histogram (bottom) with the QWP. Red dashed circles in the birefringence maps correspond to the beam diameter at the PDs ($1.2\\unit{mm}$). Red dashed lines in the histograms are the estimated values $19\\unit{deg}$ and $\\pi/4$, respectively. The histograms indicate that the measured values are consistent with the estimated values. ",
        "text": "Table \\ref{tab:results} summarizes the obtained results, and Fig. \\ref{fig:QWPresutls} shows the typical result of the birefringence distributions and their histograms. In Table \\ref{tab:results}, we describe the values of $\\theta$ and $\\alpha_-$ which give the peak values in their histograms. Both the orientation and the differential phase retardation can be measured within $\\sim3\\%$ variations. Within the beam diameter where the beam is most sensitive to disturbance such as birefringence, the measured orientation and differential phase retardation show reasonable homogeneity ($\\sim3\\%$).",
        "summarize_figure": "2505.03491v1_20250204_QWP2",
        "summarization": "The picture shows the typical birefringence maps (upper row) and their histograms (bottom row) of a quarter - wave plate (QWP). The left map in the upper row shows the Orientation, and the right one shows the Differential Phase retardation. The red dashed circles represent the beam diameter at the photodetectors (1.2mm). The left histogram in the bottom row corresponds to the orientation, and the right one corresponds to the differential phase retardation. The red dashed lines are the estimated values of 19 degrees and π/4, respectively. The histograms indicate that the measured values are consistent with the estimated values. Within the beam diameter where the beam is most sensitive to birefringence and other disturbances, the measured orientation and differential phase retardation have a homogeneity of about 3%."
    },
    "364": {
        "figure1": "2505.03493v1_level0",
        "label1": "fig:iter1",
        "caption1": "\\tiny $N_{d_k}=300$, $N_{v_k}=252$} \\end{subfigure} \\begin{subfigure}{.42\\linewidth} \\includegraphics[width=\\linewidth]{MED25/MEDfigures/level0.jpg} \\caption{\\tiny $\\alpha_k = 111$} \\end{subfigure} \\caption{$k=0$",
        "text": "The algorithm was \\href{https://github.com/OumaymaK/Sequential_data-driven_stability_analysis}{coded} using YALMIP and MPT3 toolboxes \\cite{Lofberg2004, MPT3}, was run on a laptop with 32GB memory and AMD Ryzen 7 7735U, and the solver used was MOSEK. The problem was solved in five iterations. Figures \\ref{fig:iter1} to \\ref{fig:iter5} show the results of each iteration.\nFigure~\\ref{fig:levelsets} shows the progression of the level sets from figures~\\ref{fig:iter1} to~\\ref{fig:iter5} over the iterations (from green to blue). All trajectories starting in an invariant set converge to the subset inside it. As for the last invariant set, the trajectories converge to $\\cA$ and then to the equilibrium point. In grey dotted lines, we can see the level sets obtained from a completely data-driven simulation. It converged to $\\cA$ successfully, albeit in one more iteration compared to the informed simulation above. This proves that it is possible to certify the attraction of the equilibrium point in the region without relying on the model, and that the choice of tessellation strongly influences the speed of convergence.",
        "summarize_figure": "2505.03493v1_level0",
        "summarization": "The figure shows results of an algorithm's iteration process. When k = 0, Nₒₖ is 300 and Nᵥₖ is 252, and αₖ is 111. The algorithm, coded using YALMIP and MPT3 toolboxes, ran on a laptop with 32GB memory and AMD Ryzen 7 7735U, and was solved in 5 iterations. Trajectories starting in an invariant set converge to a subset inside it, and in the last invariant set, they converge to 𝒜 and then to the equilibrium point. Grey dotted lines show level sets from a data - driven simulation, which converged to 𝒜 in one more iteration than the informed simulation. This proves it's possible to certify the equilibrium point's attraction without relying on the model, and the tessellation choice affects convergence speed."
    },
    "365": {
        "figure1": "2505.03493v1_level4",
        "label1": "fig:iter5",
        "caption1": "\\tiny $N_{d_k}=370$, $N_{v_k}=239$} \\end{subfigure} \\begin{subfigure}{.42\\linewidth} \\includegraphics[width=\\linewidth]{MED25/MEDfigures/level4.jpg} \\caption{\\tiny $\\alpha_k = 145$, $\\beta_k=2.32$} \\end{subfigure} \\caption{$k=4$",
        "text": "The algorithm was \\href{https://github.com/OumaymaK/Sequential_data-driven_stability_analysis}{coded} using YALMIP and MPT3 toolboxes \\cite{Lofberg2004, MPT3}, was run on a laptop with 32GB memory and AMD Ryzen 7 7735U, and the solver used was MOSEK. The problem was solved in five iterations. Figures \\ref{fig:iter1} to \\ref{fig:iter5} show the results of each iteration.\nFigure~\\ref{fig:levelsets} shows the progression of the level sets from figures~\\ref{fig:iter1} to~\\ref{fig:iter5} over the iterations (from green to blue). All trajectories starting in an invariant set converge to the subset inside it. As for the last invariant set, the trajectories converge to $\\cA$ and then to the equilibrium point. In grey dotted lines, we can see the level sets obtained from a completely data-driven simulation. It converged to $\\cA$ successfully, albeit in one more iteration compared to the informed simulation above. This proves that it is possible to certify the attraction of the equilibrium point in the region without relying on the model, and that the choice of tessellation strongly influences the speed of convergence.",
        "summarize_figure": "2505.03493v1_level4",
        "summarization": "The figure shows the situation of the algorithm at the 4th iteration (k = 4). At this time, Ndₖ is 370, Nvₖ is 239, αₖ equals 145, and βₖ equals 2.32. From the text description, the algorithm solves the problem in 5 iterations, and the figure represents the result of one of the iterations. These data reflect the key parameters of this iteration, which are significant for studying the algorithm's performance and state at this stage and also provide data support for analyzing the overall convergence characteristics of the algorithm."
    },
    "366": {
        "figure1": "2505.03493v1_levelsets+",
        "label1": "fig:levelsets",
        "caption1": "Graphical description of the relationship between the level sets' boundaries obtained iteratively. The progression in terms of set inclusions illustrates the convergence of the proposed algorithm.",
        "text": "In this part, we present the implementation of the iterative method of the optimisation problem described in Algorithm~\\ref{alg} on the damped pendulum with the mathematical model~\\eqref{eq:pendsys}. The tessellation used is ensured to contain a PWA Lyapunov function. We seek to retrieve the attractivity of the origin %$0$  as the equilibrium in the chosen region using Lyapunov candidates computed from data measurements, ignoring the full model~\\eqref{eq:pendsys}. It should be mentioned that even if the next simulation results shown are informed of the model solely in establishing the tessellation, a simulation that respects the assumption of the unavailability of the model at all times is also successful, albeit it needs more iterations. The results are shown for comparison in Figure~\\ref {fig:levelsets}.\nFigure~\\ref{fig:levelsets} shows the progression of the level sets from figures~\\ref{fig:iter1} to~\\ref{fig:iter5} over the iterations (from green to blue). All trajectories starting in an invariant set converge to the subset inside it. As for the last invariant set, the trajectories converge to $\\cA$ and then to the equilibrium point. In grey dotted lines, we can see the level sets obtained from a completely data-driven simulation. It converged to $\\cA$ successfully, albeit in one more iteration compared to the informed simulation above. This proves that it is possible to certify the attraction of the equilibrium point in the region without relying on the model, and that the choice of tessellation strongly influences the speed of convergence.",
        "summarize_figure": "2505.03493v1_levelsets+",
        "summarization": "The figure is a graphical description of the relationship between the boundaries of level sets obtained iteratively, showing the progression of level sets from green to blue during the iteration. Green represents \"Informed tessellation\" and grey dotted lines represent \"Fully data - driven\". According to the text, all trajectories starting in an invariant set converge to a subset inside it, eventually to A and the equilibrium point. The fully data - driven simulation represented by grey dotted lines also converges to A, but with more iterations. This proves that the attraction of the equilibrium point can be verified without relying on the model, and the choice of tessellation affects the convergence speed."
    },
    "367": {
        "figure1": "2505.03509v1_roc_prc_curves_iter3",
        "label1": "fig:roc_prc_hourglass",
        "caption1": "Receiver operation characteristic (left) and Precision-Recall (right) curves after three active learning cycles for the miniImageNet `hourglass' anomaly class starting from five labelled anomalies, adding ten after each iteration. The model achieves an AUROC of 0.96 and an AUPRC of 0.80, highlighting robust anomaly detection capability under severe class imbalance.",
        "text": "Figure~\\ref{fig:roc_prc_hourglass} shows representative receiver operation characteristic and Precision-Recall curves for the \\textit{hourglass} anomaly class after the final active learning cycle. Our approach achieved an AUROC of $0.96$ and AUPRC of $0.80$, indicating strong separation between nominal and anomalous samples.",
        "summarize_figure": "2505.03509v1_roc_prc_curves_iter3",
        "summarization": "The left side of the figure is the Receiver Operating Characteristic (ROC) curve. The vertical axis is the True Positive Rate and the horizontal axis is the False Positive Rate. The blue curve representing the model has an AUROC value of 0.956, higher than the random level indicated by the dashed line, showing the model can distinguish between positive and negative samples well. The right side is the Precision - Recall (PRC) curve, with the vertical axis being Precision and the horizontal axis being Recall. The blue curve has an AUPRC value of 0.796. According to the text, these are results for the \"hourglass\" anomaly class in miniImageNet after three active learning cycles, starting from five labelled anomalies and adding ten per iteration, demonstrating the model's strong anomaly detection ability under severe class imbalance."
    },
    "368": {
        "figure1": "2505.03514v1_sequence_to_infinity",
        "label1": "pic-longarc",
        "caption1": "The sequence of arcs with the Lorentzian length tending to infinity.",
        "text": "(3) It is sufficient to show that the Lorentzian distance from the point $(0,0)$ to the point $(c,w)$ such that $c = \\pi - \\atan{|w|} + \\varepsilon$ is infinite for any $\\varepsilon > 0$. To do this we construct the family of admissible curves from the point $(0,0)$ to the point $(c,w)$ with arbitrary big Lorentzian lengths. Namely, any such curve is a concatenation of three arcs. The first and the third arcs are light-like, while the second one is vertical with the Euclidian length $\\varepsilon$, see Fig.~\\ref{pic-longarc}. It remains to show that this vertical arc is admissible and can have arbitrary big Lorentzian length, see Lemma~\\ref{lem-symmetric-case-no-solution} below.",
        "summarize_figure": "2505.03514v1_sequence_to_infinity",
        "summarization": "The figure shows a sequence of arcs with the Lorentzian length tending to infinity. According to the text, to prove that the Lorentzian distance from the point (0,0) to the point (c, w) where c = π - arctan|w| + ε (ε > 0) is infinite, a family of admissible curves from the point (0,0) to the point (c, w) needs to be constructed. These curves are composed of three arcs. The first and the third arcs are light - like, and the second one is vertical with an Euclidean length of ε. It also needs to be shown that this vertical arc is admissible and its Lorentzian length can be arbitrarily large."
    },
    "369": {
        "figure1": "2505.03514v1_prolate_case_atset",
        "label1": "pic-prolate-case-atset",
        "caption1": "The attainable set for the prolate case where $\\eta = 0.1$ (the surface of revolution with respect to the $c$-axis).",
        "text": "See Fig.~\\ref{pic-prolate-case-atset} for the attainable set.",
        "summarize_figure": "2505.03514v1_prolate_case_atset",
        "summarization": "The figure shows the attainable set for the prolate case where η = 0.1, which is a surface of revolution with respect to the c - axis. In the chart, the w - axis and c - axis form a coordinate system, and the region enclosed by the curve shows a specific shape, representing the range of states that the system can reach under this condition."
    },
    "370": {
        "figure1": "2505.03517v1_spatial_plots_example",
        "label1": "fig: mVAM-ZimVAC example",
        "caption1": "Comparison of direct estimates of the prevalence of households with poor food consumption patterns for May 2023 and May 2024. These estimates represent district-level prevalence in rural Zimbabwe. The mobile-phone survey (mVAM) is designed for province-level analysis in Zimbabwe, and its direct estimates are shown in the first and third panels, while the face-to-face survey (ZimVAC) direct estimates, representative at the district level, are displayed in the second and fourth panels. The colour scale (blue: $0$\\% to orange: $100$\\%) shifts at $20$\\% to highlight prevalence considered at least moderately high.",
        "text": "Figure \\ref{fig: mVAM-ZimVAC example} illustrates the differences between mobile phone and F2F survey estimates at the second administrative level in Zimbabwe. In Zimbabwe, the first administrative level corresponds to provinces, while the second administrative level consists of districts. Since the mobile-phone survey is designed for analysis at a higher administrative level, it may fail to capture localised variations in food security. Moreover, differences in survey modality and sample composition can further introduce biases, potentially underestimating food security prevalence in certain areas. In contrast, the face-to-face survey, designed to be representative at the district level, offers a more granular perspective and reveals spatial variations in food security that mobile-phone-based direct estimates may overlook.",
        "summarize_figure": "2505.03517v1_spatial_plots_example",
        "summarization": "The figure compares the direct estimates of the prevalence of households with poor food consumption patterns in rural Zimbabwe in May 2023 and May 2024. The left two maps are data from the mobile - phone survey (mVAM) for provincial analysis in these two months, and the right two maps are data from the face - to - face survey (ZimVAC) representative at the district level. The color ranges from blue (0%) to orange (100%), with values above 20% indicating at least moderately high prevalence. It can be seen that there are differences in prevalence among regions under different survey methods and times. mVAM may fail to capture local differences, while ZimVAC provides a more detailed perspective."
    },
    "371": {
        "figure1": "2505.03517v1_corr_plot_validation",
        "label1": "figure: correlation",
        "caption1": "Scatter plots comparing modelled estimates to ZimVAC ground truth estimates for May 2023 and May 2024. This figure evaluates the performance of different models by comparing their poor food consumption prevalence estimates to the ZimVAC ground truth estimates. Panel A presents the comparison for May 2023, while Panel B shows the comparison for May 2024. The dashed line $y=x$ indicates perfect agreement between the modelled estimates and the ground truth, with points closer to the line reflecting better model performance.",
        "text": "Modelled estimates based on mobile-phone-only data, including both MR and MRP (DHS-Census), show incremental improvements in correlation and error metrics, with MRP generally performing better, for example, achieving a Pearson correlation of 0.632 in May 2024 compared to 0.574 for MR. However, both models exhibit consistent negative bias, indicating underestimation of poor food consumption prevalence (e.g., MBE = –0.161 for MR and –0.172 for MRP in May 2024). This is also evident in the scatter plots in Figure~\\ref{figure: correlation}, where mobile-phone-only estimates fall below the diagonal line.\nFor jMRP estimates, poststratification cells and their weights are derived either by raking DHS data to match Census marginals or directly from the ZimVAC 2023 survey (see Table~\\ref{tab: types of estimates}). In addition to achieving narrower credible intervals while maintaining coverage close to nominal levels, jMRP estimates also exhibit lower bias. Both jMRP (DHS-Census) and jMRP (ZimVAC) perform well, with the latter achieving the strongest in-sample performance in 2023 (Pearson = 0.901, rMSE = 0.053, bias $<$ 0.001), and the former excelling out-of-sample in 2024 (Pearson = 0.568, rMSE = 0.118, bias = –0.001). These models demonstrate strong agreement with the ZimVAC direct estimates, as illustrated by high correlation and CCC values (above 0.8 in-sample, around 0.5 out-of-sample), and by the tighter clustering of points along the identity line in Figure~\\ref{figure: correlation}, indicating close alignment with the reference estimates.",
        "summarize_figure": "2505.03517v1_corr_plot_validation",
        "summarization": "The figure shows scatter plots comparing modelled estimates to ZimVAC ground truth estimates for May 2023 and May 2024. Panel A is for May 2023 and Panel B for May 2024. The dashed line y = x indicates perfect agreement between modelled and ground truth estimates, and the closer the points are to this line, the better the model performs. According to the text, models MR and MRP (DHS - Census) based on mobile - phone - only data show improved correlation and error metrics, with MRP generally performing better, but both have negative bias, underestimating the prevalence of poor food consumption. jMRP (DHS - Census) and jMRP (ZimVAC) perform well. The latter has the strongest in - sample performance in 2023, and the former excels out - of - sample in 2024. Points in the scatter plots cluster more closely around the identity line, aligning better with the reference estimates."
    },
    "372": {
        "figure1": "2505.03518v1_figure_spderiv",
        "label1": "fig:spatial_deriv",
        "caption1": " The $\\kk' = \\sqrt{g^2(\\kk)}$ spectral curves Eq. (\\ref{eq:spatial_spectrum2}) for centered Eq. (\\ref{eq:spatial_derivativeC}) and staggered Eq. (\\ref{eq:spatial_derivativeSt}) finite difference operators using 4th (red), 10th (blue), 20th (green) and 30th (purple) order accurate formulas. We also show the attenuation function of the binomial filter with dashed gray lines for reference. Here $\\kk_{\\max}=\\pi/\\Delta x$ (i.e. $\\lambda_{\\max}= 2 \\Delta x$) corresponds to the largest frequency representable on the grid. The laser pulse is typically discretized below $0.2\\kk_{\\max}$ (which equals $\\Delta x = \\lambda_0/10$).  We show the spectral range $ > 0.5\\kk_{\\max}$ on the left ($b$) which could contain various numerical defects depending on the finite-difference formula. Temporally, the exponential  propagation inherits the real part of $\\omega(\\kk)$ curve from the  $\\kk'(\\kk)$ spectral functions of the differences shown here. ",
        "text": "On Fig. \\ref{fig:spatial_deriv} we show the spectral functions using Eq. (\\ref{eq:spatial_spectrum2}) for centered Eq. (\\ref{eq:spatial_derivativeC}) and staggered Eq. (\\ref{eq:spatial_derivativeSt}) finite difference operators with various orders of  accuracy. We can see that the formulas show the expected degree of convergence with the accuracy order.   However, the centered differences go to zero at $\\kk_{max}$ which is a bad behavior. For the solution of Maxwell-equations Eqs. (\\ref{eq:maxwell_Ex})-(\\ref{eq:maxwell_Bz}), this formula renders the highest spatial frequencies invisible in the solver, resulting two kind of defective behavior: first there will be spurious numerical longitudinal  (divergence) components for the ${\\bf E}, {\\bf B}$ fields that are invisible to Poisson's equation; and any numerical noise at the highest frequencies propagate much faster than the speed of light (i.e. near instantaneous boundary-boundary reflections). This erroneous behavior makes it evident that we need to use the staggered representation. However, due to the non-exactness even of the staggered finite differences, the highest frequencies should be filtered out from the sources (${\\bf J}$) to suppress artifacts (numerical Cherenkov radiation).\nAs we have shown in Fig. \\ref{fig:spatial_deriv} using staggered finite differences always result in erroneous behavior near the highest frequency $\\kk_{\\max}$ at finite polynomial orders. Any such a high frequency  wave has nearly zero group velocity, which could lead to numerical Cherenkov like radiation during field propagation. At reasonably high orders this is not an issue below $0.5 \\kk_{\\max}$: for example the laser pulse will not exhibit dispersion errors (using high order expansion of the exponentials). However, this issue is intriguing because it cannot be fixed within spatial representation using arbitrary high-order (local) finite difference approximations.",
        "summarize_figure": "2505.03518v1_figure_spderiv",
        "summarization": "The chart displays the kk′=sqrtg2(kk) spectral curves for centered and staggered finite - difference operators of 4th (red), 10th (blue), 20th (green), and 30th (purple) order accuracy. Gray dashed binomial filter attenuation lines serve as reference. kkmax​=pi/Deltax is the grid's max frequency; laser pulses are typically discretized below 0.2kkmax​. The left (b) spectral range above 0.5kkmax​ may have numerical defects. Centered differences go to zero at kkmax​, a bad behavior. Staggered finite differences are an improvement but still have errors near the highest frequency at finite polynomial orders, so highest frequencies must be filtered to suppress artifacts like numerical Cherenkov radiation."
    },
    "373": {
        "figure1": "2505.03518v1_figure_spfilter3",
        "label1": "fig:spatial_filter",
        "caption1": " Spectral attenuation functions of the current filters from Table \\ref{tab:spatial_filter}: binomial and squared binomial filter (gray and brown), our lowpass filters (red, blue, green), divergence correction filter (orange). The worst case spectral functions of the 2nd and 3rd order PIC particle shapes are also shown (purple dotted lines). For reference, we also show the spectral functions of the 20th order staggering (dashed gray) and difference operator (gray). ",
        "text": "To use the staggered spatial representation, we can also construct an operator that staggers or destaggers the fields (grids) on the high-order Lagrange-interpolation polynomial basis (\\ref{subsubsec:lagrange}), in the form of: We list their matrix coefficients using 10th order approximation in Table \\ref{tab:spatial_deriv}. Due to the high-order nature of the this interpolation, we can do this  with minimal accuracy loss (using the same order as the finite difference). This can be convenient when providing input or output for the field solvers. An important property is that $ {\\rm S}^{(\\pm)} = \\left( {\\rm S}^{(\\mp)}\\right)^T$, using which we can approximately reverse its effect as ${\\rm S}^{(+)} {\\rm S}^{(-)} \\approx 1$. The spectral functions of these staggering operators have the same kind of hole at $\\kk_{\\max}$ as the centered differences (we show the spectral function of the 20th order stagger operator in Fig. \\ref{fig:spatial_filter}). However, for consecutively switching back and forth between staggered and non-staggered representation, one must use the inverse of ${\\rm S}^{(-)}$ to prevent the accumulation of numerical errors. We set the convention here  that the operators ${\\rm D}^{(-)},{\\rm S}^{(-)}$ (which stagger down by $-\\Delta x /2$) act on non-staggered coordinates, while the other ones ${\\rm D}^{(+)},{\\rm S}^{(+)}$ act on staggered coordinates.\nWe show the spectral functions of the filters on Fig. \\ref{fig:spatial_filter}. We can see that the  binomial smoothing filter significantly affects our target spectral range $\\lesssim0.5 \\kk_{\\max}$. Our lowpass filters affect this range less and they do not affect the very low frequency components. We tailored these filters especially to dampen the spectral components where the high-order difference is erroneous. Because of this, these are effective tools to suppress high frequency numerical Cherenkov radiation within PIC implementation. Of course any filter can be combined (matrix multiplied) with any other to increase the effectiveness of the damping. For reference, we also plotted the 2nd or 3rd order PIC particle shapes (see \\ref{subsubsec:shapes}) which have some spectral dampening built in - the discussed current filters will be applied on the top of these.\\footnote{More precisely, the spectrum of the deposited current of the particles depends on their position relative to the grid points, which is the consequence of the aliasing error of the discretized particle shape function.}",
        "summarize_figure": "2505.03518v1_figure_spfilter3",
        "summarization": "The chart shows the spectral attenuation functions of various filters, including binomial and squared binomial filters (gray and brown), low - pass filters (red, blue, green), and divergence correction filter (orange). It also presents the worst - case spectral functions of 2nd and 3rd order PIC particle shapes (purple dotted lines), as well as the spectral functions of 20th order staggering and difference operators (gray dashed and solid lines) for reference. The binomial smoothing filter significantly affects the target spectral range ≤0.5kk_max, while low - pass filters have less impact and do not affect low - frequency components. These filters can suppress high - frequency numerical Cherenkov radiation in PIC implementation and can be combined to enhance attenuation."
    },
    "374": {
        "figure1": "2505.03518v1_figure_depositmap2",
        "label1": "fig:particles_depositmap",
        "caption1": "Sketch of our 2D checkered parallel deposition algorithm. At the beginning of the simulation we calculate table of \\emph{parallel zones} which are $4\\times4$ cell blocks, and we assign each zone to $2\\times2$ density groups (shown here by blue, red, white and green). The particles corresponding to each cell are accessed through the \\emph{particle map} which provides the starting point of a linked list. In the example we show here that the cell $(i,k)$ stores the particle index $\\pp_0$,  and the particle at $\\pp_0$ points to $\\pp_1$, and the latter points to $\\pp_n$, which terminates the list with an invalid particle index. Parallel density deposition can be done independently in each zone by depositing the density into 4 separate density buffers corresponding to the proper group and summing the density afterwards. ",
        "text": "Let us denote the number of particles for species $\\sss$ as $M_\\sss$. We implemented this map as  two arrays that store the particle indices $\\pp \\in [0, M_\\sss-1]$:  First, both arrays are filled with an invalid particle index value of -1. Using both of these structures we build one directional linked lists corresponding to $(i,j,k)$ cell indices - using only the $\\pp$  particle indices.  The purpose of each stored $\\pp$ particle index is to point to the next $\\pp'$ particle that belongs to the same list (same cell). We store the starting point of these in array (1), the rest in array (2). Any invalid index value mark the end of these lists. %We show its schematic in  Fig. \\ref{fig:particles_depositmap} in the contex of parallel density deposition.\nWe can easily generalize this parallel deposition method to one (\"interlaced\") and two dimensions (\"checkered pattern\"). Even more interestingly, it is also possible to use 1D  or 2D version of this parallel deposition in 3D by selecting the dimensions in which we form the deposition zones. We illustrate this method in 2D in Fig. \\ref{fig:particles_depositmap}.  This method is GPU compatible because it could provide independent work for large number of threads depending on the number of deposition zones. In the 3D version there is only memory overhead related to allocating the 8 field density buffers, the only computational overhead happens because we have to sum all of these. We reduce this overhead during CPU computations using the above parallelization technique only along the dimension of the highest resolution $M_z$ if the required thread count less than $M_z/4$.",
        "summarize_figure": "2505.03518v1_figure_depositmap2",
        "summarization": "The chart shows a sketch of the 2D checkered parallel deposition algorithm. At the start of the simulation, 4×4 cell blocks called \"parallel zones\" are calculated and assigned to 2×2 density groups (represented by blue, red, white and green). Particles for each cell are accessed via the \"particle map\", which gives the starting point of a linked list. For example, cell (i,k) stores particle index p0, p0 points to p1, and p1 points to pn, which ends the list with an invalid index. Parallel density deposition can be done independently in each zone by depositing into 4 separate density buffers and then summing. This method is GPU - compatible and reduces computational overhead."
    },
    "375": {
        "figure1": "2505.03523v1_Liu_Levelset_a=0.1",
        "label1": "fig:liu",
        "caption1": "$a=0.05$} \\end{subfigure} \\vspace{0.5cm} % Espacio entre filas % Segunda fila \\begin{subfigure}{\\textwidth} \\centering \\begin{minipage}{0.48\\textwidth} \\centering \\includegraphics[width=\\textwidth]{Graficos/Liu_Density_a=0.1.png} \\end{minipage} \\hfill \\begin{minipage}{0.48\\textwidth} \\centering \\includegraphics[width=\\textwidth]{Graficos/Liu_Levelset_a=0.1.png} \\end{minipage} \\caption{$a=0.1$} \\end{subfigure} \\caption{Density estimation (left panel) and corresponding level sets (right panel) of the distribution of $\\sqrt{a_n} \\left( \\Pi(D_n, f_n) - \\Pi(D, f) \\right)$ for a sample of size $500$, computed using \\textit{Liu depth} for different values of $a$. ",
        "text": "Each simulation of \\( R_n \\) is based on a sample of size \\( n = 500 \\) drawn from a bivariate Beta distribution with independent components, where each component follows a \\(\\text{Beta}(2,2)\\) distribution. The density estimate \\(\\hat{f}_n\\) is computed using kernel density estimation, with the bandwidth \\( h \\) chosen according to Silverman’s rule of thumb, see \\citet{silverman1986}. In all cases we choose the gaussian kernel. Three classic depth measures are considered: Liu's depth, Tukey's depth, and a projection-based depth measure. Figures \\ref{fig:liu}, \\ref{fig:tukey}, and \\ref{fig:proj} display the estimated densities and their corresponding contour curves for each depth measure and each level of \\( a \\).",
        "summarize_figure": "2505.03523v1_Liu_Levelset_a=0.1",
        "summarization": "The chart presents density estimation (left) and corresponding level sets (right) for the distribution of √aₙ (Π(Dₙ, fₙ) - Π(D, f)) with a sample size of 500, calculated via Liu depth when a = 0.1. Simulations draw from a bivariate Beta distribution with independent components (each Beta(2,2)). Kernel density estimation, using a Gaussian kernel and bandwidth set by Silverman's rule of thumb, is applied. It showcases the distribution under Liu depth, one of three classic depth measures considered."
    },
    "376": {
        "figure1": "2505.03523v1_Tukey_Levelset_a=0.2",
        "label1": "fig:tukey",
        "caption1": "$a=0.1$} \\end{subfigure} \\vspace{0.5cm} % Espacio entre filas % Segunda fila \\begin{subfigure}{\\textwidth} \\centering \\begin{minipage}{0.48\\textwidth} \\centering \\includegraphics[width=\\textwidth]{Graficos/Tukey_Density_a=0.2.png} \\end{minipage} \\hfill \\begin{minipage}{0.48\\textwidth} \\centering \\includegraphics[width=\\textwidth]{Graficos/Tukey_Levelset_a=0.2.png} \\end{minipage} \\caption{$a=0.2$ } \\end{subfigure} \\caption{Density estimation (left panel) and corresponding level sets (right panel) of the distribution of $\\sqrt{a_n} \\left( \\Pi(D_n, f_n) - \\Pi(D, f) \\right)$ for a sample of size $500$, computed using \\textit{Tukey depth} for different values of $a$.",
        "text": "Each simulation of \\( R_n \\) is based on a sample of size \\( n = 500 \\) drawn from a bivariate Beta distribution with independent components, where each component follows a \\(\\text{Beta}(2,2)\\) distribution. The density estimate \\(\\hat{f}_n\\) is computed using kernel density estimation, with the bandwidth \\( h \\) chosen according to Silverman’s rule of thumb, see \\citet{silverman1986}. In all cases we choose the gaussian kernel. Three classic depth measures are considered: Liu's depth, Tukey's depth, and a projection-based depth measure. Figures \\ref{fig:liu}, \\ref{fig:tukey}, and \\ref{fig:proj} display the estimated densities and their corresponding contour curves for each depth measure and each level of \\( a \\).",
        "summarize_figure": "2505.03523v1_Tukey_Levelset_a=0.2",
        "summarization": "The chart presents the density estimation (left) and corresponding level sets (right) of the distribution of √aₙ (Π(Dₙ, fₙ) - Π(D, f)) for a sample size of 500, calculated using Tukey depth when a = 0.2. Simulations are based on samples from a bivariate Beta distribution with independent components (each Beta(2,2)). Kernel density estimation, with bandwidth selected by Silverman's rule of thumb and a Gaussian kernel, is applied. It illustrates the distribution traits under Tukey depth, one of three classic depth measures."
    },
    "377": {
        "figure1": "2505.03523v1_Projection_Levelset_a=0.2",
        "label1": "fig:proj",
        "caption1": "$a=0.1$} \\end{subfigure} \\vspace{0.5cm} % Espacio entre filas % Segunda fila \\begin{subfigure}{\\textwidth} \\centering \\begin{minipage}{0.48\\textwidth} \\centering \\includegraphics[width=\\textwidth]{Graficos/Projection_Density_a=0.2.png} \\end{minipage} \\hfill \\begin{minipage}{0.48\\textwidth} \\centering \\includegraphics[width=\\textwidth]{Graficos/Projection_Levelset_a=0.2.png} \\end{minipage} \\caption{$a=0.2$} \\end{subfigure} \\caption{Density estimation (left panel) and corresponding level sets (right panel) of the distribution of $\\sqrt{a_n} \\left( \\Pi(D_n, f_n) - \\Pi(D, f) \\right)$ for a sample of size $500$, computed using \\textit{Projection depth} for different values of $a$. ",
        "text": "Each simulation of \\( R_n \\) is based on a sample of size \\( n = 500 \\) drawn from a bivariate Beta distribution with independent components, where each component follows a \\(\\text{Beta}(2,2)\\) distribution. The density estimate \\(\\hat{f}_n\\) is computed using kernel density estimation, with the bandwidth \\( h \\) chosen according to Silverman’s rule of thumb, see \\citet{silverman1986}. In all cases we choose the gaussian kernel. Three classic depth measures are considered: Liu's depth, Tukey's depth, and a projection-based depth measure. Figures \\ref{fig:liu}, \\ref{fig:tukey}, and \\ref{fig:proj} display the estimated densities and their corresponding contour curves for each depth measure and each level of \\( a \\).",
        "summarize_figure": "2505.03523v1_Projection_Levelset_a=0.2",
        "summarization": "The chart shows the density estimation (left) and corresponding level sets (right) of the distribution of √aₙ (Π(Dₙ, fₙ) - Π(D, f)) for a sample size of 500, calculated using projection - based depth with a = 0.2. Simulations are based on samples from a bivariate Beta distribution with independent components (each Beta(2,2)). Kernel density estimation, with bandwidth chosen by Silverman's rule of thumb and a Gaussian kernel, is applied. It presents the distribution traits under projection - based depth, one of three classic depth measures."
    },
    "378": {
        "figure1": "2505.03529v1_GeneralisationLattice3QIDs",
        "label1": "generalisationlattice",
        "caption1": "An generalisation lattice with 3 QIDs generalised to 2,3 and 4 levels respectively.",
        "text": "The size of this search space can be quantified as the product of the number of levels for each QID's generalisation hierarchy, and grows with the number of QIDs for which these hierarchies are defined. For example in Fig.\\ref{generalisationlattice} there are 3 QIDs that are generalised to 2, 3 and 4 levels respectively, and the total number of nodes in the generalisation lattice is 24. Flash~\\cite{Kohlmayer2012}—an existing globally-optimal k-anonymity algorithm—relies on the ability to hold the entire dataset in \"main-memory\" (referring to the primary-volatile memory type RAM) to build the required memory layouts. Flash~\\cite{Kohlmayer2012} mentions that in the case of limited main-memory or very large datasets, a disk-based implementation might be required, which has not yet been developed. A complementary strategy for high-dimensional datasets, Lightning~\\cite{Prasser2016}, provides a heuristic, best-effort solution. However, dataset dimensionality is not a direct determinant of dataset volume, and algorithms such as Lightning, Flash, and Incognito~\\cite{LeFevre2005}, may still struggle when the dataset size far exceeds the available RAM. These algorithms can perform k-anonymisation only on the portion of the data that fits in the available physical memory. Thus, the solutions obtained are far from optimal. In comparison, we offer a solution that outperforms these algorithms with the same constraints.",
        "summarize_figure": "2505.03529v1_GeneralisationLattice3QIDs",
        "summarization": "The chart shows a generalisation lattice where 3 quasi - identifiers (QIDs) are generalised to 2, 3 and 4 levels respectively. Nodes in the lattice are connected by lines, showing relationships between different generalisation levels. The lattice has a total of 24 nodes, reflecting the scale of the search space, which is determined by the product of the number of levels for each QID and grows with the number of QIDs. Existing algorithms have memory - limitation issues when dealing with large - scale datasets, and relevant solutions still need improvement."
    },
    "379": {
        "figure1": "2505.03529v1_chunk_ratio_bar_plot_w500",
        "label1": "fig:dm_ratio_k_chunks",
        "caption1": "DM* Ratio vs. \\textit{k} for different numbers of chunks",
        "text": "For our experiments, we varied the anonymity parameter $k \\in (10, 50, 250, 1000)$ with a varying number of chunks belonging to the set $\\{1, 5, 25,125\\}$. Fig.~\\ref{fig:dm_ratio_k_chunks} presents the results as a bar chart, and visualises the improved performance of SKALD against ARX with an increase in the number of chunks. In particular, the performance improvement using SKALD for 25 chunks is at least \\emph{four-fold} and for 125 chunks is at least \\emph{nine-fold} across different values of k. We can infer from these results that the performance gap between SKALD and ARX in terms of DM* increases as the number of chunks and thus, the dataset size, increases.",
        "summarize_figure": "2505.03529v1_chunk_ratio_bar_plot_w500",
        "summarization": "The chart is a bar graph showing the DM* ratio as the anonymity parameter K (taking values 10, 50, 250, 1000) varies for different numbers of chunks (1 chunk, 5 chunks, 25 chunks, 125 chunks). As the number of chunks increases, the performance improvement of SKALD over ARX is significant. The performance improvement is at least four - fold for 25 chunks and at least nine - fold for 125 chunks. The results indicate that as the number of chunks and the dataset size increase, the performance gap between SKALD and ARX in terms of DM* ratio becomes larger."
    },
    "380": {
        "figure1": "2505.03530v1_fig1_circuit_motifs",
        "label1": "fig:circuit_motifs",
        "caption1": "Visualization of identified circuit motifs in VAEs for different factors. Each column shows the response patterns of network components to interventions on specific factors (shape, scale, and position) across three VAE architectures.",
        "text": "Our causal intervention framework successfully identifies distinct circuit motifs within the VAE that correspond to specific semantic factors. Figure \\ref{fig:circuit_motifs} shows the activation patterns of these circuits in response to different interventions.",
        "summarize_figure": "2505.03530v1_fig1_circuit_motifs",
        "summarization": "The chart visualizes the identified circuit motifs in different Variational Auto - Encoder (VAE) architectures (FactorVAE, Beta VAE, Standard VAE) in response to interventions on specific factors (Shape, Scale, Position). The activation strength is encoded by color. Different sub - plots show the response patterns of network components in each VAE architecture to interventions on corresponding factors, demonstrating that the causal intervention framework can identify VAE circuit motifs corresponding to specific semantic factors."
    },
    "381": {
        "figure1": "2505.03530v1_fig2_causal_graph",
        "label1": "fig:causal_graph",
        "caption1": "Causal graph visualization showing relationships between factors and latent dimensions. The graph reveals how different latent dimensions control specific semantic factors in the reconstruction.",
        "text": "By applying our multi-level intervention framework, we can map the computational graph of the VAE to a causal graph of semantic factors. Figure \\ref{fig:causal_graph} visualizes this causal graph for the dSprites dataset.",
        "summarize_figure": "2505.03530v1_fig2_causal_graph",
        "summarization": "The chart shows a causal graph depicting the relationships between latent dimensions (z0, z1, z3, z5, z7, z8) and semantic factors (Shape, Scale, Orientation, X-Position, Y-Position). Pink lines indicate how different latent dimensions control specific semantic factors during reconstruction. It reflects the result of mapping the VAE computational graph to a causal graph of semantic factors through a multi - level intervention framework, visualized using the dSprites dataset."
    },
    "382": {
        "figure1": "2505.03530v1_fig3_effect_strength",
        "label1": "fig:effect_strength",
        "caption1": "Bar charts showing causal effect strength and intervention specificity for each latent dimension across different VAE architectures. FactorVAE shows consistently higher effect strengths, while specificity values vary across dimensions.",
        "text": "Figure \\ref{fig:effect_strength} shows the distribution of causal effect strength and intervention specificity across the 10 latent dimensions for each architecture.",
        "summarize_figure": "2505.03530v1_fig3_effect_strength",
        "summarization": "The chart, titled \"Causal Effect Analysis Across VAE Architectures,\" has three sub - plots analyzing different Variational Auto - Encoder (VAE) architectures: FactorVAE, Beta VAE, and Standard VAE. Sub - plot A displays that the three architectures have comparable average causal effect strengths. Sub - plot B shows that across key latent dimensions (z0, z3, z5, z7, z8), FactorVAE typically has higher effect strengths. Sub - plot C illustrates the modularity paradox, where Beta VAE has the highest modularity, yet FactorVAE attains better disentanglement via stronger causal effects."
    },
    "383": {
        "figure1": "2505.03533v1_FL_RL_workflow",
        "label1": "fig:FL_RL_workflow",
        "caption1": "Workflow of the proposed resource allocation strategy: In the centralized training phase, each network is deployed at the BS, interacts with the FL environment, and learns from stored experiences. After training, the BS sends the trained agent network parameters to the corresponding clients. In the decentralized execution phase, each client independently decides its sub-band allocation and power level based on local observations.",
        "text": "Building on the previously introduced Dec-POMDP formulation, we employ the QMIX algorithm to derive the optimal spectrum and power allocation policy. As shown in Fig. \\ref{fig:FL_RL_workflow}, this approach effectively coordinates multiple agents to optimize a collaborative objective with the CTDE paradigm.",
        "summarize_figure": "2505.03533v1_FL_RL_workflow",
        "summarization": "The chart shows the workflow of the proposed resource allocation strategy. In the centralized training phase, a mixing network and multiple agent networks are deployed on the server, which samples experiences from the replay memory for learning. After training, the server broadcasts the agent network parameters to the clients. In the decentralized execution phase, clients independently determine sub - band allocation and power levels based on local observations. Based on the Dec-POMDP formulation, this workflow uses the QMIX algorithm to coordinate multiple agents to optimize a collaborative objective through the CTDE paradigm."
    },
    "384": {
        "figure1": "2505.03533v1_QMIX_network",
        "label1": "fig:qmix_network",
        "caption1": "QMIX network: The blue block on the right represents the agent network, while the red block on the left depicts the mixing network. Within this network, the yellow block illustrates the hypernetwork, which generates the weights and biases for its layers.",
        "text": "QMIX approximates the optimal joint Q-value $Q^*_{\\text{tot}}(\\boldsymbol{z}, \\boldsymbol{a}, s)$ using a set of networks parameterized by $\\boldsymbol{\\theta}=\\{\\boldsymbol{\\theta}_\\text{a},\\theta_\\text{m},\\boldsymbol{\\theta}_\\text{h}\\}$, as shown in Fig. \\ref{fig:qmix_network}. This parameter set includes the individual agent networks $\\boldsymbol{\\theta}_\\text{a}= [ \\theta_{\\text{a},1},\\dots,\\theta_{\\text{a},N} ]$, a mixing network $\\theta_\\text{m}$, and a set of hypernetworks $\\boldsymbol{\\theta}_\\text{h}$. Each agent network $\\theta_{\\text{a},n}$ processes its own observation-action pair to produce $Q_n(z_n, a_n;\\theta_{\\text{a},n})$. The mixing network then combines the outputs of all agent networks to produce the joint Q-value $Q_{\\text{tot}}$. Additionally, the hypernetworks dynamically generate the weights and biases for the mixing network $\\theta_\\text{m}$ based on the global state $s$, ensuring that the joint Q-value function $Q_{\\text{tot}}$ maintains a monotonic relationship with individual Q-values $Q_n$. In summary, QMIX integrates all these networks to compute $Q_{\\text{tot}}(\\boldsymbol{z}, \\boldsymbol{a}, s; \\boldsymbol{\\theta})$ as",
        "summarize_figure": "2505.03533v1_QMIX_network",
        "summarization": "The chart shows the QMIX network structure. The blue module on the right is the agent network. Multiple agent networks process their own observation - action pairs to generate Qn(zn,an). The middle pink module is the mixing network, which combines the outputs of all agent networks to produce the joint Q - value Qtot(z,a,s). The yellow hypernetwork module in the left red module dynamically generates the weights and biases of the mixing network based on the global state s. QMIX approximates the optimal joint Q - value through the parameter set θ={θa,θm,θh}, ensuring a monotonic relationship between the joint Q - value function and individual Q - values."
    },
    "385": {
        "figure1": "2505.03533v1_Cifar10_distribution_dir_alpha50_balanced",
        "label1": "fig:non-IID_partition",
        "caption1": "skip}{0pt} \\setlength{\\belowcaptionskip}{5pt} \\centering \\begin{minipage}[b]{0.3\\textwidth} \\centering \\includegraphics[width=\\linewidth]{figure/Cifar10_distribution_dir_alpha0.5_balanced.pdf} \\centerline{a) $\\alpha=0.5$} \\end{minipage} % \\hspace{0.01\\textwidth} \\begin{minipage}[b]{0.3\\textwidth} \\centering \\includegraphics[width=\\linewidth]{figure/Cifar10_distribution_dir_alpha5_balanced.pdf} \\centerline{b) $\\alpha=5$} \\end{minipage} % \\hspace{0.01\\textwidth} \\begin{minipage}[b]{0.3\\textwidth} \\centering \\includegraphics[width=\\linewidth]{figure/Cifar10_distribution_dir_alpha50_balanced.pdf} \\centerline{c) $\\alpha=50$} \\end{minipage} \\caption{The distribution of local training data across different degrees of statistical heterogeneity. Each column represents the dataset of an individual client, and each color indicats a distinct category of training data. ",
        "text": "To simulate statistical heterogeneity, we adopt a balanced Dirichlet partitioning approach, as established in previous studies \\cite{acar2021federated,zeng2023fedlab}. In this setup, each client is assigned an equal number of data samples, and the class distribution for each client follows a Dirichlet distribution $\\text{Dir}(\\alpha)$. Lower values of the concentration parameter $\\alpha$ result in higher levels of statistical heterogeneity. We use the non-IID partition corresponding to $\\alpha \\in\\{ 0.5, 50, 5\\}$, as shown in Fig. \\ref{fig:non-IID_partition}.",
        "summarize_figure": "2505.03533v1_Cifar10_distribution_dir_alpha50_balanced",
        "summarization": "The chart depicts the distribution of local training data for the Cifar10 dataset among different clients under statistical heterogeneity. The x - axis shows the client index, and the y - axis shows the number of samples. Different colors indicate distinct training data categories. A balanced Dirichlet partitioning approach is used to simulate heterogeneity, with each client having an equal number of samples and the class distribution following a Dir(α) distribution. This specific chart corresponds to α = 50, illustrating how data is spread across clients."
    },
    "386": {
        "figure1": "2505.03536v1_er_example",
        "label1": "fig:er_example",
        "caption1": "(i) Example of an E/R model with one weak entity set and two subclasses (adapted from~\\cite{silberschatz2019database}); (ii) DDL to create entities and relationships; (iii) An example query for illustration purposes} \\vspace{-5pt",
        "text": "Figure \\ref{fig:er_example} shows an example of an E/R model (adapted from the running example from~\\cite{silberschatz2019database}), that contains one {\\em weak entity set}, two  subclasses ({\\em specialization}), and various types of relationships (with annotations to indicate the cardinality and participation constraints). The figure also shows the syntax for defining the entities and relationships in our prototype, including the  ability to directly define composite attributes (which would require the use of a separate type in a typical RDBMS) as well as multi-valued attributes (technically not allowed in the relational model, but supported by most RDBMSs today). In addition, the DDL should support: (a) defining {\\em constraints}, (b) specifying the inheritance properties (total vs partial, disjoint vs overlapping, etc.), and (c) adding descriptive text (that can be automatically used, e.g., for creating API documentations).  There is much work on (a) and (b) that we plan to build upon.\nFor querying, we use a variant of SQL with an example shown in Figure \\ref{fig:er_example}. The two main additions to normal SQL that we support: it can be supported through a chain of \\verb|array_agg| and \\verb|group by|'s, we believe this functionality should be supported natively so that the queries can be optimized  properly. We also omit explicit \\verb|group by| clauses as shown in the example, since those can be inferred from the\nA major advantage of the E/R model is the ability to define class hierarchies like the one shown in Figure \\ref{fig:er_example}. When constructing a relational schema from such an E/R diagram, we need to choose between multiple possibilities depending on whether the specialization is total vs partial, and  whether it is overlapping or disjoint, etc. There are at least three possibilities here:  relations only store the additional attributes (along with the key). explicit attributes to keep track of who belongs to which subclass). The choice among these options needs to be made early on, and gets baked into any queries that are written against this schema. Switching between options would likely  require significant modifications to most queries.  This is a less drastic change for the E/R model, although semantic correctness can still be an issue for some queries. This choice can also have a significant impact on performance depending on the workload, something that is not easy to anticipate a priori.",
        "summarize_figure": "2505.03536v1_er_example",
        "summarization": "The chart shows an Entity - Relationship (E/R) model example with one weak entity set (Section), two subclasses (Instructor and Student, subclasses of Person), and various relationships like advisor, teaches, and takes, with cardinality and participation constraints. The right side shows the Data Definition Language (DDL) for creating entities and relationships, and an example query. The E/R model can define class hierarchies. When constructing a relational schema from an E/R diagram, choices need to be made based on specialization (total/partial, overlapping/disjoint), which significantly impact queries and performance."
    },
    "387": {
        "figure1": "2505.03536v1_alternative_mappings",
        "label1": "fig:mappings",
        "caption1": "Mappings to physical representation as covers of the E/R graph} \\vspace{-5pt",
        "text": "In order to explore the space of possible mappings, we first view the E/R diagram as a graph where each entity, relationship, and attribute is a separate node (Figure \\ref{fig:mappings}). Entity nodes are connected to the relationships in which they participate, to subclasses or superclasses, and to their attributes.  A mapping to physical storage representation can be seen as a {\\bf cover} of this graph using {\\bf connected subgraphs}.  Each connected subgraph corresponds to a physical table or data structure, and together all of these constitute the full physical representation.\nWe show three examples in Figure \\ref{fig:mappings}. The first mapping depicts a fully normalized mapping, where each entity gets its own table, many-to-one relationships are folded into the many side, and many-to-many relationships have their own tables. For instance, the \\verb|advisor| relationship is folded into the \\verb|student| table, whereas but only with the attributes that are unique to them. Finally the multi-valued attribute (\\verb|Ph|) is stored in a separate table (along with the key).",
        "summarize_figure": "2505.03536v1_alternative_mappings",
        "summarization": "The chart shows three examples of mapping an Entity - Relationship (E/R) diagram to physical representation. The E/R diagram is regarded as a graph where each entity, relationship, and attribute is a separate node, and the mapping is seen as a cover of the graph using connected subgraphs, with each subgraph corresponding to a physical table or data structure. Figure (i) is a fully normalized mapping with each entity having its own table, many - to - one relationships folded into the \"many\" side, and many - to - many relationships having separate tables. Figures (ii) and (iii) show other mapping approaches, reflecting different physical storage design ideas."
    },
    "388": {
        "figure1": "2505.03536v1_architecture",
        "label1": "fig:arch",
        "caption1": "High-level ErbiumDB Architecture} \\vspace{-5pt",
        "text": "We are building a proof-of-concept system, called ErbiumDB\\footnote{\\url{https://github.com/umddb/ErbiumDB}}, to explore the research questions raised in this paper. Our prototype is written in Python, and built as a layer on top of PostgreSQL (with the goal to support other backends like Apache DataFusion). Figure \\ref{fig:arch} shows a high-level architecture (not all pieces are built as yet). The DDL layer does the heavy lifting here. It creates the E/R graph from the entity/relationship \\verb|create| statements, and keeps it up to date as the schema is modified. The mapping of the E/R graph to physical tables (specified manually today) is maintained in a table in the database as a JSON object, and is read into memory at initialization time. As noted earlier, we construct a table for each connected subgraph in the mapping that is chosen. The DDL layer also constructs mappings between CRUD statements on the entities/relationships to updates on the physical tables in the database. The prototype supports a limited form of SQL, and the queries against the logical schema are translated to queries against the physical tables. Finally, we are also planning to support a RESTful API by default (and possibly gRPC), to ensure compatibility with standard application  development practices. While CRUD operations would be supported by default, additional API calls can be added as needed.",
        "summarize_figure": "2505.03536v1_architecture",
        "summarization": "The chart shows the high - level architecture of ErbiumDB, a proof - of - concept system written in Python and built on top of PostgreSQL, with plans to support other backends. In the architecture, the DDL layer constructs and updates the E/R graph from entity/relationship create statements. The mapping from the E/R graph to physical tables is stored as a JSON object in a database table and loaded into memory during initialization, and a table is constructed for each connected subgraph in the mapping. The DDL layer also maps CRUD operations on entities/relationships to database table updates. The prototype supports a limited form of SQL, translating logical schema queries to physical table queries, and plans to support RESTful API and gRPC by default."
    },
    "389": {
        "figure1": "2505.03537v1_pipeline-shapeloss",
        "label1": "fig:pipeline",
        "caption1": "Framework of the proposed dense action generator.",
        "text": "Based on the action definition in Section~\\ref{sec:action-representations}, we design a dense action generator for garment manipulation, as depicted in Fig.~\\ref{fig:pipeline}. The model's input is the captured RGB image of the garment or fabric, which is processed by an encoder-decoder to extract features. The output head then produces a 3-channel image that represents the action of the manipulator. Each pixel in the output corresponds to a robot action, with the three channels indicating the action's score, direction, and distance. Using this 3-channel image, we can construct a pixel-wise action field as shown in Fig.~\\ref{fig:pipeline}.\nThe input of the model consists of an RGB image. The features are then extracted using a ResNet-18 \\cite{he2016deep} backbone, which functions as the encoder $\\mathbf{E}$. The feature maps from different stages of ResNet-18 are then up-scaled and progressively concatenated to match the resolution of the input images. This process forms the decoder $\\mathbf{D}$, which provides feature maps enriched with both semantic information and fine-grained details, essential for the output head. The $\\mathbf{E}$ and $\\mathbf{D}$ constitute the model backbone, as shown in Fig.~\\ref{fig:pipeline}.",
        "summarize_figure": "2505.03537v1_pipeline-shapeloss",
        "summarization": "The chart shows the framework of the proposed dense action generator. The input is the RGB image of the garment or fabric, which is processed by an encoder - decoder (using ResNet - 18 as the backbone to extract features, and the decoder up - scaling and concatenating feature maps from different stages). The output head generates a 3 - channel image representing action score, direction, and distance, from which a pixel - wise action field is constructed. The framework also involves score map training, considering background penalty and action enlargement. Pixel transformation is used to calculate the difference between the current and target images, resulting in shape loss for garment alignment."
    },
    "390": {
        "figure1": "2505.03537v1_simulated-coverage-dense-visualization",
        "label1": "fig:simulatedcoveragedensevisualization",
        "caption1": "The visualizations of the score map and dense actions, from top to bottom, are the input, the score map, the angle map, and the generated actions (selected at intervals of 7 pixels) within the garment. ",
        "text": "For the qualitative image results shown in Fig.~\\ref{fig:simulatedcoveragedensevisualization}, we present the output images of the model, including score maps (deeper red indicates higher scores), angle maps (deeper red implies larger angle values), and action field visualizations, corresponding to the input images of garments in various states. The score map accurately locates the key region for unfolding, such as the sleeve edge, hem, or neckline, rather than the center areas, which are prone to wrinkle generation by action in any direction. The angle map consistently shows a center location relative to the garment, with directions pointing toward the edge that is determined by the garment mask. This alignment is consistent with the basic rule of garment unfolding. Furthermore, the densely distributed actions, indicated by white arrows, tend to point from the center toward the edges of the garment, clearly demonstrating their effectiveness in radiating the garment.",
        "summarize_figure": "2505.03537v1_simulated-coverage-dense-visualization",
        "summarization": "The chart shows the visualization results of the output from the garment operation model, including input images, score maps, angle maps, and generated action maps (selected at 7 - pixel intervals) from top to bottom. In the score map, the deep red area indicates a high - score region, accurately locating key areas for garment unfolding (such as sleeve edges, hems, or necklines) rather than the central areas prone to wrinkling. In the angle map, deep red implies larger angle values, with directions pointing to the edges determined by the garment mask. The densely distributed actions indicated by white arrows point from the center to the edges of the garment, conforming to the basic rule of garment unfolding and demonstrating the model's effectiveness in garment unfolding operations."
    },
    "391": {
        "figure1": "2505.03543v1_MMCTR",
        "label1": "fig:network",
        "caption1": "The overall architecture of our model.}  \\vspace{-2mm",
        "text": "Inspired by \\cite{TransAct}, sequential modeling and feature interaction learning are combined to effectively capture user interest preferences. As shown in Figure \\ref{fig:network}, it consists of four main components: Embedding Layer, Sequential Feature Learning Module, Feature Interaction Module, and Prediction Layer.",
        "summarize_figure": "2505.03543v1_MMCTR",
        "summarization": "The chart shows the overall architecture of the model. Inspired by relevant research, it combines sequential modeling and feature interaction learning to capture user interest preferences. The architecture has four main components: the Embedding Layer, which converts user interaction history and target item information into cached multimodal embeddings and learnable ID embeddings; the Sequential Feature Learning Module, stacked with multiple Transformer encoders and using padding masks; the Feature Interaction Module, which uses DCNv2 for feature interaction; and the Prediction Layer, which outputs the predicted click - through rate (CTR) via a Multi - Layer Perceptron (MLP)."
    },
    "392": {
        "figure1": "2505.03543v1_ps",
        "label1": "fig:ps",
        "caption1": "Parameter sensitivity analysis on hyperparameters.",
        "text": "We conduct parameter sensitivity analysis on various hyperparameters, as shown in Figure \\ref{fig:ps}. Assigning specific values to certain hyperparameters may cause the model collapse phenomenon (\\emph{e.g.}, high learning rate), and will not be reported.",
        "summarize_figure": "2505.03543v1_ps",
        "summarization": "The chart shows the results of sensitivity analysis on multiple hyperparameters, including embedding_dim, learning_rate, k in Eq.6, transformer_dropout, and net_dropout. By plotting the curves of AUC and Logloss as each hyperparameter changes, it can be seen that different hyperparameters have various impacts on the model's performance. For example, as the embedding dimension increases, AUC rises and Logloss drops; a too - high learning rate deteriorates the model's performance. Some hyperparameter values that cause model collapse are not shown."
    },
    "393": {
        "figure1": "2505.03550v1_staggeredGrid3d",
        "label1": "staggered",
        "caption1": "The staggered grid arrangement used for the discretization of the governing equations.",
        "text": "The rest of the paper is structured as follows. The governing equations and numerical schemes of the GPE and DualACM are presented in section~\\ref{section_numMethods}. Then, in \\S~\\ref{sec:energetics}, we discuss the derivation and interpretation of the kinetic energy equation, which is the crux of the paper. Using numerical examples presented in \\S~\\ref{test_cases}, we discuss the effects of both mass conservation error and dilatation terms. In section~\\ref{salientPoints}, we note down some salient points. Finally, the conclusions are presented in \\S~\\ref{conclusion_section}.     In this section, the governing equations of both GPE and DualACM solvers are presented, along with the numerical discretisation details.  \tThe GPE derived by Toutant~\\cite{Toutant2017} is presented below in its non-dimensional form         where, $\\mathbf{u} = [\\,u \\quad  v\\, \\quad w\\,]^\\top$ is the velocity vector ($u$, $v$ and $w$ being its components in Cartesian coordinate system) and $p$ denotes pressure. $Re$ represents the Reynolds number. The artificial parameters $Ma$, $Pr$ and $\\gamma$ indicate the Mach number, Prandtl number and heat capacity ratio, respectively. \tThe GPE is solved along with the momentum equations (given below) to simulate incompressible flows \tBoth these equations involve only the physical time derivatives, and hence, no extra set of inner iterations is required.         The artificial compressibility method for simulating unsteady incompressible flows involves solving the following pressure and momentum equations         where, $\\mathbf{u}$, $p$, $Re$ and $Ma$ denotes the same parameters described above. This formulation requires marching in both physical time~($t$) and pseudo-time~($\\tau$), and hence, referred to as the dual time stepping scheme. At each physical time advancement, we perform time marching in $\\tau$ until the pseudo-time derivatives reduce to a preset tolerance value. i.e., $\\partial \\mathbf{u}/\\partial \\tau < [\\epsilon_{mom}^x \\quad  \\epsilon_{mom}^y \\quad \\epsilon_{mom}^z]^\\top$ and $Ma^2\\partial p/\\partial \\tau < \\epsilon_{p}$. Hence, the DualACM is equivalent to solving the incompressible Navier-Stokes equations at all time instants; the effect of artificial compressibility, introduced by $Ma$, does not affect the solution at any time step. %Undoubtedly, the convergence criteria mentioned above control the velocity divergence at each time step and by varying tolerances, we obtain the flow field with a specific mass and momentum conservation error. Thus, the DualACM enables us to obtain flow fields for the specified mass conservation error. This feature of DualACM is of great help in systematically investigating the effect of mass conservation error.         In all the simulations that we discuss later in section~\\ref{test_cases}, we set, $\\epsilon_{mom}^x = \\epsilon_{mom}^y = \\epsilon_{mom}^z = \\epsilon_{p} = \\epsilon$.  %The inner iteration count required for convergence in pseudo-time depends on the problem type, time step and the artificial Mach number, and $\\epsilon$~\\cite{Kiris2002}. However, the choice of $\\epsilon$ does not affect the stability of the computation~\\cite{rogers1991upwind}.         The governing equations are written in their integral form, and a finite volume approach is adopted to discretize the governing equations of both GPE and DualACM. We use the Cartesian coordinate system, and the computational stencil is illustrated in figure~\\ref{staggered}. To avoid pressure-velocity decoupling, we use a staggered grid arrangement, where the pressure, $p$, is stored at the cell centre, and the velocity components $u$, $v$, and $w$ at the cell face centres. The convective terms, pressure gradient terms and diffusion terms in the equations are approximated using the central difference scheme similar to~\\cite{Toutant2018,Dupuy2020}. A representative example of discretisation in a uniform Cartesian grid is provided below. All the other terms follow a similar approach.     \t& \\biggl[ \\left( \\frac{u_{i+1,j,k} + u_{i,j,k}}{2} \\right)^2  -      \t& \\biggl[ \\left( \\frac{u_{i,j+1,k} + u_{i,j,k}}{2} \\right)     \t& \\ \\left( \\frac{u_{i,j,k} + u_{i,j-1,k}}{2} \\right)      \t& \\biggl[ \\left( \\frac{u_{i,j,k+1} + u_{i,j,k}}{2} \\right)     \t& \\ \\left( \\frac{u_{i,j,k} + u_{i,j,k-1}}{2} \\right)",
        "summarize_figure": "2505.03550v1_staggeredGrid3d",
        "summarization": "The chart shows the staggered grid arrangement for the discretization of governing equations. In the Cartesian coordinate system, pressure p is stored at the cell center (such as p(i,j,k)), and velocity components u, v, w are stored at the cell - face centers (such as u(i,j,k), u(i+1,j,k), v(i,j,k), v(i,j+1,k), w(i,j,k), w(i,j,k+1)). This arrangement avoids pressure - velocity decoupling. Governing equations are discretized using the finite volume method, and convective, pressure - gradient, and diffusion terms are approximated with the central difference scheme."
    },
    "394": {
        "figure1": "2505.03550v1_dpsl_pdelU_avg",
        "label1": "dpsl_ke_p_dila",
        "caption1": "Evolution of kinetic energy dilatation and pressure dilatation for the doubly periodic shear layer test case: (a)~$\\mathcal{D}_k$ computed using equation~\\eqref{eqn:ke_dila}, (b)~$\\mathcal{D}_p$ computed using equation~\\eqref{eqn:p_dila}, (c)~cycle-averaged $\\mathcal{D}_k$,  (d)~cycle-averaged $\\mathcal{D}_p$.",
        "text": "{ } { } { } { } The variation of kinetic energy dilatation and pressure dilatation until $t=2$ is presented in figure~\\ref{dpsl_ke_p_dila}(a) and (b), respectively. We can observe that both $\\mathcal{D}_k$ and $\\mathcal{D}_p$ exhibit oscillations, whose magnitude increases and the frequency reduces with increasing $Ma$. These observations are consistent with the fact that they arise due to the presence of weak compressibility; the strength of artificial acoustic waves increases and the frequency reduces with increasing $Ma$. The oscillations are physically characterized by the energy exchange between kinetic energy and potential mode, analogous to compressible turbulence~\\cite{sarkar1991,miura1995,wang2021}.\nFor a better physical interpretation, we compute the cycle-averaged values of these dilatation terms, denoted $\\left<\\mathcal{D}_k\\right>$ and $\\left<\\mathcal{D}_p\\right>$, and present their time evolution in figure~\\ref{dpsl_ke_p_dila}(c) and (d), respectively. These values are significant until $t=1$. %It is during this interval that the initial shear layers roll up to form two counter-rotating vortical structures as illustrated in figure~\\ref{dpsl_vort_ma0.2_all}. %After $t=1$, the vortices get dissipated.  The peak in $\\left<\\mathcal{D}_k\\right>$ and $\\left<\\mathcal{D}_p\\right>$ increases with $Ma$, but the time at which it occurs is not strongly influenced by $Ma$. %A key conclusion that can be made from this discussion, and also confirmed in section~\\ref{sec:dipole}, is that during the formation of new vortices, a large positive peak of the dilatation terms is observed.  It can be interpreted from equation~\\eqref{eqn:ke_eqn} that this leads to an enhanced dissipation of the kinetic energy that is directly evident from figure~\\ref{fig:Dpsl_ke}.",
        "summarize_figure": "2505.03550v1_dpsl_pdelU_avg",
        "summarization": "The chart shows the evolution of the pressure dilatation term (Dp) over time t in the doubly periodic shear layer test case, with three curves corresponding to Mach numbers Ma of 0.02, 0.1, and 0.2. Dp oscillates, with the amplitude increasing and the frequency decreasing as Ma increases, due to weak compressibility, where the strength of artificial acoustic waves increases and the frequency decreases with higher Ma. The oscillations represent the energy exchange between kinetic and potential modes. Before t = 1, the cycle - averaged Dp values are significant, with the peak increasing as Ma increases, and the time of the peak is less affected by Ma."
    },
    "395": {
        "figure1": "2505.03555v1_workflow",
        "label1": "fig:workflow",
        "caption1": "\\textbf{\\small Workflow of \\ToolName{}.",
        "text": "Figure \\ref{fig:workflow} shows the complete workflow of our symbolic execution framework. \\ToolName{} first performs a graph analysis on the program to generate multiple MPCs. Then, it uses multiple MPCs to prioritize a subset of paths to guide symbolic execution at run-time. Moreover, \\ToolName{} leverages a simple program dependence analysis to capture the program dependence between a branch and its dependent basic blocks. When symbolic execution encounters an infeasible path after searching multiple MPCs, \\ToolName{} will find a new state to continue execution using the dependence information.",
        "summarize_figure": "2505.03555v1_workflow",
        "summarization": "The chart shows the workflow of the symbolic execution framework. First, it conducts a graph analysis on the program to generate multiple Maximum Path Covers (MPCs). Then, it uses these MPCs to prioritize a subset of paths for guiding symbolic execution at run - time. Additionally, it performs a simple program dependence analysis to capture the program dependence between a branch and its associated basic blocks. When symbolic execution encounters an infeasible path after searching multiple MPCs, it uses the dependence information to find a new state to continue execution."
    },
    "396": {
        "figure1": "2505.03555v1_bugs_hist",
        "label1": "fig:bugs",
        "caption1": "\\textbf{\\small The total number of UBSan and ASan violations discovered by \\ToolName{} and all baseline strategies via replay on 8 benchmark programs. This experiment is repeated 5 times and all security violations are collected.",
        "text": "Based on the second metric, we replay the test cases generated by all search strategies on programs instrumented with UBSan and Address Sanitizer (ASan) to identify additional security violations. This experiment is conducted on 8 benchmark programs (i.e., the 8 programs compatible with \\textsc{Learch} and \\SearchToolStyle{cgs}, as introduced in Section~\\ref{sec:code_cov}). The total number of security violations discovered by all search strategies on these 8 programs is reported in Figure~\\ref{fig:bugs}. \\ToolName{} detects a total of 70 security violations, outperforming the second-best baseline, \\SearchToolStyle{bfs}, by 24 violations. Furthermore, \\ToolName{} uniquely identifies 15 new security violations that are not detected by any of the baseline search strategies. \\revise{These results demonstrate that \\ToolName{} is more effective at detecting security violations compared to baseline search strategies. This is because \\ToolName{} prioritizes certain paths and maximizes code coverage within a limited time frame.}",
        "summarize_figure": "2505.03555v1_bugs_hist",
        "summarization": "The chart shows the total number of UBSan and ASan violations discovered by the proposed tool (Empc) and all baseline strategies through replaying 8 benchmark programs. The experiment was repeated 5 times to collect all security violations. Empc detected 70 violations, outperforming the second - best baseline strategy bfs (46 violations). It also uniquely identified 15 new security violations not detected by any baseline strategy, demonstrating that Empc is more effective in detecting security violations. This is because it prioritizes certain paths and maximizes code coverage within a limited time frame."
    },
    "397": {
        "figure1": "2505.03559v1_overview",
        "label1": "fig:framework",
        "caption1": "The overview of the proposed framework.",
        "text": "In this paper, we develop an encrypted HVAC system attack detection framework, as shown in Fig. \\ref{fig:framework}. The framework integrates a ETU at the local side and a cloud-based attack type identification unit to efficiently monitor, detect, and classify cyber-physical attacks on HVAC systems. The ETU operates locally to continuously monitor the system's measurements and performs a binary classification to determine whether there is a potential attack. If an anomaly is detected, the ETU encrypts the measurements and transmits them to the cloud-based ADU for detailed type classification.",
        "summarize_figure": "2505.03559v1_overview",
        "summarization": "The chart shows the proposed encrypted HVAC system attack detection framework. At the building side, the HVAC system sends measurement data to the Event - triggering Unit (ETU). The ETU performs binary classification (T = 1 for potential attack, T = 0 for none). If an anomaly is detected, it encrypts the data and sends it to the Encrypted Cloud ADU. In the Encrypted Cloud ADU, the data is first processed by the Graph Attention Network (GAT) and then by the Long Short - Term Memory Cell (LSTM Cell) for attack type classification. Finally, the attack type is fed back to the building side."
    },
    "398": {
        "figure1": "2505.03559v1_RTU",
        "label1": "fig:rtu",
        "caption1": "Schematic diagram of experimental RTU.",
        "text": "The datasets used in this study are derived from the LBNL HVAC FDD repository. These datasets have been adapted to simulate various attack scenarios on HVAC systems. The RTU, a commonly used component in commercial HVAC systems, consists of interconnected elements such as compressors, evaporators, condensers, economizers, and sensors, which collaboratively maintain desired environmental conditions by regulating airflow, temperature, and CO$_2$ concentration. A schematic diagram of the system is shown in Fig. \\ref{fig:rtu}. Note that in each component in this diagram, there are data points that are used in training.\nThe dataset contains 24 features, including supply and return air temperatures, damper positions, compressor states, \\emph{etc}. For graph representation, each RTU component is treated as a node. For example, as RTU components shown in Fig. \\ref{fig:rtu}, the condenser and compressor are regarded as nodes.  The physical or logical connections between components are modeled as edges. The graph structure enables the framework to capture both local interactions and global system dynamics effectively.",
        "summarize_figure": "2505.03559v1_RTU",
        "summarization": "The chart is a schematic diagram of an experimental Rooftop Unit (RTU), showing the structure of RTU in a commercial HVAC system. Outdoor air enters through a damper and is sent to the evaporator by the supply fan. The evaporator forms a refrigeration cycle with the compressor, condenser, expansion valve, and refrigerant. Return air mixes with fresh air through another damper, is detected by the temperature sensor, enters the Variable Air Volume (VAV) Box, and is then sent to the room. The outdoor fan assists the condenser in heat dissipation. The components of this system work together to regulate airflow, temperature, etc., maintaining environmental conditions, and the connections between components are used to construct a graph structure for analysis."
    },
    "399": {
        "figure1": "2505.03559v1_newcommunication",
        "label1": "fig:comm_overhead",
        "caption1": "Comparison of communication overhead between the traditional method and the proposed ETU-based framework.",
        "text": "As illustrated in Fig.~\\ref{fig:comm_overhead}, traditional methods transmit data at every time step, leading to constant communication overhead. In contrast, the proposed ETU-based approach selectively transmits data, resulting in an 85\\% reduction in communication overhead. This selective transmission strategy significantly optimizes resource usage, reducing unnecessary cloud computation and network costs, making it well-suited for real-world deployment in resource-constrained environments.",
        "summarize_figure": "2505.03559v1_newcommunication",
        "summarization": "The chart compares the communication overhead between the traditional method and the Event - Triggering Unit (ETU) - based method. The horizontal axis represents time steps, and the vertical axis shows the data transmission status. The traditional method (red line) transmits data at every time step, resulting in constant communication overhead. In contrast, the ETU - based method (blue line) selectively transmits data, reducing communication overhead by 85%. This selective transmission strategy optimizes resource usage, reducing unnecessary cloud computation and network costs, making it suitable for real - world deployment in resource - constrained environments."
    },
    "400": {
        "figure1": "2505.03565v1_my_map",
        "label1": "fig:map",
        "caption1": "Top--down 2D view of the map used as the simulation environment.",
        "text": "The multi-sensor fusion is simulated in Ignition Gazebo. The map has been generated from a LiDAR pointcloud in the test site Zentrum am Berg \\cite{zab}. The sensors used for the evaluation are models of the u-blox ZED-F9P, Ouster OS2 LiDAR, and the Flir ADK thermal camera. In Figure~\\ref{fig:map}, a 2D cutout of this map, which is used for the round trip is shown.",
        "summarize_figure": "2505.03565v1_my_map",
        "summarization": "The chart presents a top - down 2D view of the map used as the simulation environment. This map is generated from the LiDAR pointcloud of the test site Zentrum am Berg. The sensors used for evaluation are models of the u - blox ZED - F9P, Ouster OS2 LiDAR, and Flir ADK thermal camera. This 2D view shows the part of the map used for the round trip, providing a basic environment for multi - sensor fusion simulation in Ignition Gazebo."
    },
    "401": {
        "figure1": "2505.03565v1_pol",
        "label1": "fig:ICPTHE_phi",
        "caption1": "Estimated angle of the extended Kalman filter, and the LiDAR odometry, as well as the ground truth.",
        "text": "When looking at Figure~\\ref{fig:ICPTHE_phi}, the reason for the large drift becomes clear, as the angle estimation of the kalman filter after around 10 minutes drifts significantly. Additionally, the thermal camera odometry misestimates the velocity as it has no depth perception.",
        "summarize_figure": "2505.03565v1_pol",
        "summarization": "The chart presents the estimated angles of the Extended Kalman Filter (EKF, blue), LiDAR odometry (SICP, orange), and ground truth (GT, green) as the angle varies. Around 10 minutes, the EKF angle estimation shows significant drift. Also, thermal camera odometry misestimates velocity due to lack of depth perception, revealing the cause of large deviation."
    },
    "402": {
        "figure1": "2505.03568v1_replies",
        "label1": "fig:survey_replies",
        "caption1": "Histogram of $\\levelofinterest$, \\ie of replies to the question \\question{}. $\\levelofinterest=1$: fully disagree; $\\levelofinterest = 5$: fully agree.} \\Description[Histogram of the replies to the question \\question{}.]{Histogram of the replies to the question \\question{}, \\ie self-reported interest in unfamiliar music. $\\levelofinterest=1$: fully disagree; $\\levelofinterest = 5$: fully agree.",
        "text": "Variable & Description \\\\                     $\\levelofinterest$ & $u$'s  interest in unfamiliar music \\ie reply to ``\\textit{I'm intrigued by [...] more.}'' on a $1$ (fully disagree) to $5$ (fully agree) Likert scale. \\\\                     $N^u$ & Number of distinct \\clusters{} discovered by $u$ in a one-week window.\\\\                     $\\relativepop$ & Popularity of $m$ relative to the average popularity of its cluster $c$. $\\relativepop=0\\%$ means that $m$ has popularity equal to $c$'s average, $\\relativepop>0\\%$ that $m$ is more popular than $c$'s average.\\\\                     $\\relativedist$ & Distance of $m$ from the centroid of its cluster $c$ and relative to the average distance of tracks in $c$. $\\relativedist = 0\\%$ if $m$'s distance from the centroid is equal to the average in $c$, $\\relativedist > 0\\%$ if $m$ is further away from the centroid and less representative of $c$. \\\\                     $p^u_c(t)$ &  Ratio of tracks known to $u$ that are in $c$ at time $t$.\\\\                     $\\mathbf{p}^u(t)$ & $u$'s preferences at time $t$ expressed as vector of ratios of known tracks $p^u_c(t)$. \\\\                     $\\diversity(t)$ & Diversity of $u$'s music preferences at time $t$ measured as Shannon's entropy over components of $\\mathbf{p}^u(t)$.\\\\                     $\\stabilityentropy$ & Stability of $u$'s music preferences measured as time derivative of $\\diversity(t)$, averaged over time. \\\\                     $\\stabilitynorm$ &  Stability of $u$'s music preferences measured as time derivative of $\\left\\lVert \\mathbf{p}^u(t)\\right\\rVert_2$, averaged over time. \\\\                     $\\trackrank$ & $m$'s rank in $u$'s exploration of % $m$'s                      cluster $c$. $\\trackrank=1$ for the first track listened by $u$ in $c$.\\\\             We perform our analyses on a set of users of the music streaming platform \\deezer{}. The dataset was collected within the project \\project{}, whose aim is to study practices on music streaming platforms. In particular, we consider users that took part in a survey, sent to users via email, regarding their music listening practices, musical tastes and socio-demographic variables. The survey was developed to evaluate users' musical sophistication based on the Gold-MSI questionnaire~\\cite{muellensiefen2014goldmsi},\\footnote{Gold-MSI web page: \\url{https://shiny.gold-msi.org/gmsi_toplevel/}} to which users replied on a $5$-point Likert scale. %The set of questions of the Gold-MSI was extended to include standard questions regarding, \\eg demographic traits or questions related to the context in which music is listened to.              We assign $u$ a value of $\\levelofinterest$ equal to their reply to \\question{}. %Among the questions of the Gold-MSI, we consider the question \\question{} as an indication of self-reported interest in unfamiliar music, since it explicitly mentions the desire to explore music of a low familiarity. We denote this value with $\\levelofinterest$, short for Interest in Unfamiliar Music of user $u$. %We refer to the answer to this question as ``Active Engagement'', \\ie by the name of its category according to the Gold-MSI.             In addition, the dataset provides the listening events of these users from 01/01/2022 till 31/12/2022. We only consider listening events that lasted longer than 30 seconds, a threshold often applied in  music streaming for remuneration purposes, and in music recommendation to identify positive feedback~\\cite{jiang2024spotify_threshold,tran2024actr}; to ensure that user $u$ ``familiarized'' with track $m$, we disregard user--track pairs $(u, m)$ for which $u$ listened to $m$ only once in this period. For each user--track pair $\\levelofinterest$ we keep the timestamp $t$ of first interaction as indication of when $u$ discovered $m$ (and the respective \\cluster{} $c$, if no track from $c$ was listened before). We restrict to users that replied to all questions from the Gold-MSI and that provided demographic information.  We apply 10-core filtering and only restrict to users that listened to at least 10 distinct tracks, and tracks that were listened by at least 10 distinct users. This leads to 4{,}070 users, 72{,}555 tracks, and to 28{,}678{,}935 user--track interactions, corresponding to 3{,}425{,}564 unique user--track pairs  $(u, m)$, as summarized in Table~\\ref{tab:interaction_data}. The users constitute of $2{,}207$ male, $1{,}863$ female; the collected replies $\\levelofinterest$ had an average of $3.2$ and a standard deviation of $1.0$. Figure~\\ref{fig:survey_replies} shows the histogram of $\\levelofinterest$ while Table~\\ref{tab:user_data} reports, for each gender, the number of users who responded with each allowed value of $\\levelofinterest$.                        We vary the number $n_\\text{clusters}$ of \\clusters{}  in $\\{8, 16, 32, 64, 128, 256\\}$. To ensure \\cluster{} homogeneity, we select $n_\\text{clusters}$ as the smallest value above the elbow in the WCSS plot, and for which the Jaccard index computed over cluster labels between any pair of editorial playlists is below $1/3$, indicating that for any pair of editorial playlists, the overlapping cluster labels are less than $1/3$ of the labels appearing in any of the two playlists. This leads to $n_\\text{clusters} = 128$.",
        "summarize_figure": "2505.03568v1_replies",
        "summarization": "The chart is a histogram presenting users' replies regarding their \"interest in unfamiliar music\" on a music streaming platform, using a 5-point Likert scale (1 for fully disagree and 5 for fully agree). The dataset is from a survey of platform users, encompassing music listening events. After applying filters, the data includes 4070 users (2207 male and 1863 female), 72555 tracks, and 28678935 user-track interactions. The average value of the \"interest\" replies is 3.2 with a standard deviation of 1.0. To ensure cluster homogeneity, the number of clusters is set to 128 by choosing the smallest value above the elbow in the WCSS plot and with the Jaccard index between any pair of editorial playlists below 1/3."
    },
    "403": {
        "figure1": "2505.03568v1_number_music_communities",
        "label1": "fig:rq1_number_explored_genres",
        "caption1": "Average number $N^u$ of \\clusters{} explored in $[t_1, t2]$ (one week after initial six months) for different values of $\\levelofinterest$. Central dots and error bars represent mean and standard errors estimated with bootstrapping (see Section~\\ref{sec:methodology_previously_unexplored_music}).} \\Description[Number of \\clusters{} explored in one week for different values of self-reported interest in unfamiliar music $\\levelofinterest$.]{Number of \\clusters{} explored in one week for different values of self-reported interest in unfamiliar music $\\levelofinterest$. Central dots and error bars represent mean and standard errors estimated with bootstrapping (see Section~\\ref{sec:methodology_previously_unexplored_music}).",
        "text": "& $\\levelofinterest=1$  & 2 & 3 & 4 & $5$ \\\\\\hline                     m& 135 ($6\\%$) & 393 ($18\\%$) & 794 ($36\\%$) & 683 ($31\\%$) & 202 ($9\\%$)\\\\                     f& 98 ($5\\%$)& 368 ($20\\%$)& 717 ($39\\%$)& 512 ($27\\%$)& 168 ($9\\%$)\\\\     Before moving to the results on RQ1, 2, and 3, we provide more insight on the validation of \\cluster{} labels $c$ and on $\\relativedist$ in relation to \\cluster{} representativeness.       The column ``\\textit{\\# Editorial Tracks}'' in Table~\\ref{tab:cluster_editorial_playlists} shows the number of distinct tracks that are present in the selected editorial playlists,\\footnote{This table refers to the validation of \\cluster{} labels $c$ and representativeness of centroids. It \\emph{does not} describe the full set of tracks of the streaming data used in our analyses. For a summary of the streaming data, we refer the reader to Table~\\ref{tab:interaction_data}.} while the column ``\\textit{\\# Editorial Tracks in the Streaming Data}'' shows the number of tracks appearing both in the selected editorial playlists and in the streaming data used in our analysis. The last column shows the average of $\\relativedist$ over tracks appearing both in the selected editorial playlists and in the streaming data used in our analysis. We see that on average tracks that appear in editorial playlists are closer to cluster centroids than other tracks, since for editorial tracks $\\relativedist< 0$. This motivates our use of $\\relativedist$ as measure of track representativeness of the genre of a \\cluster{}. %use the term ``genre(s)'' to refer to the clusters and cluster labels, and                           Editorial Playlist&\\# Editorial Tracks &\\# Editorial Tracks in the Streaming Data & Average $\\relativedist$                           Classical Essentials &23 &  23 & -31\\% \\\\                          Electronic Essentials & 27 & 18& -7\\%\\\\                          Pop Essentials & 81 & 55& -15\\%\\\\                          Rock Essentials & 70 & 65& -21\\%\\\\             Figure~\\ref{fig:rq1_number_explored_genres} shows the number $N^u$ of explored music \\clusters{} in $[t_1, t_2]$, \\ie within a one-week period after the initial six months $[t_0, t_1]$. The central dots and error bars represent the mean and standard error  of $N^u$ for different values of $\\levelofinterest$, computed as described in Section~\\ref{sec:methodology_previously_unexplored_music}.             Overall the plot shows that users of higher $\\levelofinterest$ tend to explore, within a same time-window, more music \\clusters{} than users of lower $\\levelofinterest$. The most remarkable difference is between users of $\\levelofinterest=1$ and $\\levelofinterest=4$, the latter exploring on average $1.5$ more music \\clusters{} than the former, in one week ($N^u\\simeq 8.5$ vs. $N^u\\simeq 7$). Overall, this trend is confirmed by the positive, albeit low, correlation coefficient ($\\rho=0.06$, $p=0.00161$) computed over all users between $N^u$ and $\\levelofinterest$, indicating a weak correlation.\nThe diversity $\\diversity(t_1)$ of music consumption computed as entropy over listened \\clusters{} at the end of the initial six-months period, is displayed in Figure~\\ref{fig:rq2_entropy_six_months}. Higher values of $\\diversity(t_1)$ indicate music preferences more equally distributed over different clusters, and lower values indicate music preferences more peaked over few clusters. The figure clearly shows an increasing trend, indicating that users who declare a higher interest in unfamiliar music $\\levelofinterest$ also have higher $\\diversity(t_1)$, and hence more diverse music tastes. This dependency is confirmed by the positive correlation coefficient computed over all users ($\\rho=0.19$, $p<10^{-5}$) between $\\levelofinterest$ and $\\diversity(t_1)$. It is in line with the results from previous works~\\cite{ferwerda2019sophistication_exploration,liang2023promoting_music_exploration}, which grouped users according to their active engagement values of the Gold-MSI, a value to which the self-reported interest in unfamiliar music $\\levelofinterest$ also contributes. Notice that this insight differs from the one described in Section~\\ref{sec:rq1}: While Figure~\\ref{fig:rq1_number_explored_genres} indicates that users with higher interest in unfamiliar music explore \\textit{more} \\clusters{}, Figure~\\ref{fig:rq2_entropy_six_months} indicates that they have \\textit{more evenly spread} interests over \\clusters{}.",
        "summarize_figure": "2505.03568v1_number_music_communities",
        "summarization": "The chart displays the average number Nu of music clusters explored by users in the one - week period [t1​,t2​] following an initial six - month period, for various values of self - reported interest in unfamiliar music (levelofinterest). Central dots and error bars, estimated via bootstrapping, represent the mean and standard error respectively. It shows that users with higher levelofinterest explore more music clusters within the same time window. The most notable difference is between levelofinterest=1 and levelofinterest=4 users, with the latter exploring on average 1.5 more clusters in one week. A weak positive correlation (rho=0.06, p=0.00161) between Nu and levelofinterest across all users confirms this trend."
    },
    "404": {
        "figure1": "2505.03572v1_F1",
        "label1": "F1",
        "caption1": " (a) Scheme for detecting chiral molecules by circularly polarized RABBITT. The laser pulses propagate along the z-axis. The ground state electrons first transition to the intermediate continuum states (main peaks) by absorbing an XUV photon with different energy, then transition to the sidebands by absorbing or emitting an IR photon to form the interference fringes. (b) Photoelectron angular distribution in the laser polarization plane ($\\theta=90^{\\circ}$). The peak intensities of XUV and IR fields are $1\\times10^{12}\\,\\mathrm{W/cm^{2}}$ and $1\\times10^{11}\\,\\mathrm{W/cm^{2}}$, respectively, and the wavelength of IR field is 800 nm (c) $\\varphi$-dependent photoelectron yields of different SBs in the polarization plane. (d) $\\varphi$-dependent photoelectron yields of SB20 at different emission angles. ",
        "text": "Figure~\\ref{F1}(a) illustrates the laser configuration and the coordinate system adopted in this work. The circularly polarized attosecond XUV pulse train comprises odd-order harmonics from HH19 ($29.42\\,\\mathrm{eV}$) to HH27 ($41.80\\,\\mathrm{eV}$). The synchronized 800-nm IR field is tuned to be circularly polarized with the same or opposite helicities with respect to the XUV pulse. The two fields are co-polarized in the $x$-$y$ plane and propagate along the $z$ axis. In the standard RABBITT scheme with linearly polarized light, sideband (SB) oscillations as functions of the XUV-IR time delay are recorded, and the corresponding photoionization time delay is obtained by analyzing these RABBITT traces. In circular RABBITT for the unoriented molecule ensemble, the photoionization time delay can be determined via $\\varphi$-resolved photoelectron angular distribution without time-delay scanning~\\cite{SM}. This property greatly reduces the computational cost, which is important for the theoretical study of chiral molecules.\nFigure~\\ref{F1}(b) shows the calculated $\\varphi$-resolved photoelectron energy spectrum in the polarization plane for counter-rotating XUV and IR fields. The five main peaks correspond to photoionization by HH19 to HH27, and the four SBs record the interference of absorption and emission pathways where one IR photon is absorbed (emitted) from the lower(higher)-energy main peak. The distributions of the main peaks are $\\varphi$-independent due to the perturbative nature of the IR field. The SB signals exhibit $2\\varphi$-oscillations. Figures~\\ref{F1}(c) and~\\ref{F1}(d) show photoelectron yields for different SBs in the polarization plane ($\\theta=90^\\circ$), and for SB20 at varying polar angles $\\theta$, respectively. In both cases, clear phase shifts of the RABBITT traces as functions of azimuthal angle $\\varphi$ exist, which encode the phase information of the EWPs and provide rich information of chiral molecule photoionization, as demonstrated below.\nTo understand the enhanced PECD signal and its irregular oscillation with $\\varphi$, we generalize the theory of single- and two-photon ionization of chiral molecules~\\cite{theory1,theory2,theory3,theory4} to circular RABBITT, incorporating interference of the absorption and emission two-photon pathways~\\cite{SM}. The photoelectron angular distribution of the randomly oriented chiral molecular ensemble at SBs can be expanded in terms of spherical harmonics~\\cite{SM},         I(\\theta,\\varphi)=\\sum_{m=0,\\pm2}\\sum_{l=|m|}^{4}\\beta_{lm}Y_{lm}(\\theta,\\varphi), where the anisotropy parameters $\\beta_{lm}$ are generally complex numbers $\\beta_{lm}=|\\beta_{lm}|e^{i\\delta_{lm}}$ and $\\beta_{l,-m}=\\beta_{lm}^{*}$. $\\beta_{l0}$ corresponds to the individual contribution of the two-photon absorption and emission pathways, and forms the background signal in the PECD of the SBs. The contributions of $\\beta_{l,\\pm2}$ result from the interference of the absorption and emission pathways~\\cite{SM},  and they are responsible for the $2\\varphi$-modulation of SB signals in Figs.~\\ref{F1}(c) and ~\\ref{F1}(d). The resulting SB-PECD can be expressed as~\\cite{SM}         +2\\,|\\beta_{32}|\\text{A}_{32}\\text{P}_{3}^{2}(\\cos\\theta)\\cos(2\\varphi+\\delta_{32})}         {\\sum\\limits_{l=0,2,4}\\beta_{l0}\\text{A}_{l0}\\text{P}_{l}(\\cos\\theta)         +2\\sum\\limits_{l=2,4}|\\beta_{l2}|\\text{A}_{l2}\\text{P}_{l}^{2}(\\cos\\theta)\\cos(2\\varphi+\\delta_{l2})}, where $\\text{A}_{lm}$ accounts for the normalized coefficient of the associated Legendre polynomials. As shown in Eq.~\\eqref{PECD}, anisotropy parameters with odd $l$, i.e., $\\beta_{10}$, $\\beta_{30}$, $\\beta_{3,\\pm2}$, are responsible for the PECD signal. The first two terms ($m=0$) in the numerator correspond to the PECD from the incoherent sum of the two pathways, which are independent of $\\varphi$. The third term ($m=2$) is due to the interference of the two pathways, resulting in the $\\varphi$ modulation of the SB PECDs in Fig.~\\ref{F2}. Interference terms also appear in the denominator, and thus the PECD shows irregular oscillation with $\\varphi$ in Figs.~\\ref{F2}(d) and~\\ref{F2}(h). These results indicate that PECD in chiral molecules can be greatly enhanced through interference.\nNow, we turn to the photoionization time delay, which directly reflects the phase of the photoelectrons. We obtain this information by analyzing the 2$\\varphi$-oscillation of the photoelectron yield in SBs [Figs.~\\ref{F1}(c) and~\\ref{F1}(d) ]. For this purpose, we rewrite the photoelectron angular distribution of SBs [Eq.~\\eqref{E1}] to show the oscillation explicitly,",
        "summarize_figure": "2505.03572v1_F1",
        "summarization": "Figure (a) illustrates the scheme for detecting chiral molecules by circularly polarized RABBITT. Laser pulses propagate along the z - axis. Ground - state electrons first transition to intermediate continuum states (main peaks) by absorbing XUV photons of different energies, and then to sidebands by absorbing or emitting IR photons to form interference fringes, with odd - order harmonics from HH19 to HH27 shown. Figure (b) is the calculated φ - resolved photoelectron energy spectrum in the laser polarization plane (θ = 90°). Five main peaks correspond to photoionization by different harmonics, and four sidebands record the interference of absorption and emission paths, with sideband signals showing 2φ oscillations. Figure (c) shows the φ - dependent photoelectron yields of different sidebands in the polarization plane (θ = 90°), and Figure (d) shows those of SB20 at different emission angles θ. Both show obvious phase shifts of RABBITT traces with azimuthal angle φ, encoding the phase information of electron wave packets and providing rich information on chiral molecule photoionization."
    },
    "405": {
        "figure1": "2505.03572v1_F2",
        "label1": "F2",
        "caption1": " (a) Angle-resolved photoelectron distribution for SB20. (b) Angle-resolved PECD for SB20. (c) The PECD as a function of $\\theta$ at $\\varphi=90^{\\circ}$ for different main peaks and SBs. (d) The PECD as a function of $\\varphi$ when $\\theta=30^{\\circ}$ for different main peaks and SBs. The XUV and IR fields are co-rotating circularly polarized. (e)-(h) The same as (a)-(d) but for the counter-rotating fields. ",
        "text": "Before inspecting the phase information, we first analyze the amplitudes of the EWPs. Figures~\\ref{F2}(a) and~\\ref{F2}(e) show the photoelectron angular distributions for SB20 by co- and counter-rotating laser fields, respectively. The photoelectron yields of the SBs in the co-rotating fields are several times higher than those of the counter-rotating fields. The corresponding distributions of PECD, defined as $\\text{PECD}(\\theta,\\varphi)=[I(\\theta,\\varphi)-I(\\pi-\\theta,\\varphi)]/[I(\\theta,\\varphi)+I(\\pi-\\theta,\\varphi)]$, where $I(\\theta,\\varphi)$ denotes the photoelectron angular distributions, are shown in Figs.~\\ref{F2}(b) and~\\ref{F2}(f). The PECD depends on the azimuthal angle $\\varphi$, and reaches values up to $3\\%$ and $8\\%$ for the co-rotating and counter-rotating fields, respectively. Slices of PECD for different SBs at $\\varphi=90^\\circ$ [Figs.~\\ref{F2}(c) and~\\ref{F2}(g)] and $\\theta=30^{\\circ}$ [Figs.~\\ref{F2}(d) and~\\ref{F2}(h)] highlight its angular dependence. The PECD signals of main peaks are also presented for comparison. For both the main peaks and SBs, the PECD signals increase with the polar angle $\\theta$. At $\\theta=30^{\\circ}$, SB PECD signals show irregular oscillation with $\\varphi$, while the main peak PECDs remain isotropic. The maximum value of PECD signals in the SBs exceeds 8\\%, significantly surpassing those of the main peaks. Moreover, the PECD signals of sidebands in the counter-rotating fields are generally larger than the co-rotating fields.\nTo understand the enhanced PECD signal and its irregular oscillation with $\\varphi$, we generalize the theory of single- and two-photon ionization of chiral molecules~\\cite{theory1,theory2,theory3,theory4} to circular RABBITT, incorporating interference of the absorption and emission two-photon pathways~\\cite{SM}. The photoelectron angular distribution of the randomly oriented chiral molecular ensemble at SBs can be expanded in terms of spherical harmonics~\\cite{SM},         I(\\theta,\\varphi)=\\sum_{m=0,\\pm2}\\sum_{l=|m|}^{4}\\beta_{lm}Y_{lm}(\\theta,\\varphi), where the anisotropy parameters $\\beta_{lm}$ are generally complex numbers $\\beta_{lm}=|\\beta_{lm}|e^{i\\delta_{lm}}$ and $\\beta_{l,-m}=\\beta_{lm}^{*}$. $\\beta_{l0}$ corresponds to the individual contribution of the two-photon absorption and emission pathways, and forms the background signal in the PECD of the SBs. The contributions of $\\beta_{l,\\pm2}$ result from the interference of the absorption and emission pathways~\\cite{SM},  and they are responsible for the $2\\varphi$-modulation of SB signals in Figs.~\\ref{F1}(c) and ~\\ref{F1}(d). The resulting SB-PECD can be expressed as~\\cite{SM}         +2\\,|\\beta_{32}|\\text{A}_{32}\\text{P}_{3}^{2}(\\cos\\theta)\\cos(2\\varphi+\\delta_{32})}         {\\sum\\limits_{l=0,2,4}\\beta_{l0}\\text{A}_{l0}\\text{P}_{l}(\\cos\\theta)         +2\\sum\\limits_{l=2,4}|\\beta_{l2}|\\text{A}_{l2}\\text{P}_{l}^{2}(\\cos\\theta)\\cos(2\\varphi+\\delta_{l2})}, where $\\text{A}_{lm}$ accounts for the normalized coefficient of the associated Legendre polynomials. As shown in Eq.~\\eqref{PECD}, anisotropy parameters with odd $l$, i.e., $\\beta_{10}$, $\\beta_{30}$, $\\beta_{3,\\pm2}$, are responsible for the PECD signal. The first two terms ($m=0$) in the numerator correspond to the PECD from the incoherent sum of the two pathways, which are independent of $\\varphi$. The third term ($m=2$) is due to the interference of the two pathways, resulting in the $\\varphi$ modulation of the SB PECDs in Fig.~\\ref{F2}. Interference terms also appear in the denominator, and thus the PECD shows irregular oscillation with $\\varphi$ in Figs.~\\ref{F2}(d) and~\\ref{F2}(h). These results indicate that PECD in chiral molecules can be greatly enhanced through interference.",
        "summarize_figure": "2505.03572v1_F2",
        "summarization": "Figures (a) and (e) show the photoelectron angular distributions for SB20 under co - and counter - rotating laser fields respectively. The photoelectron yields of sidebands in co - rotating fields are several times higher than those in counter - rotating fields. Figures (b) and (f) are the corresponding Photoelectron Circular Dichroism (PECD) distributions. PECD depends on the azimuthal angle φ, reaching up to 3% and 8% for co - and counter - rotating fields respectively. Figures (c) and (g) show the PECD of different sidebands and main peaks as a function of the polar angle θ at φ = 90°, and Figures (d) and (h) show it as a function of φ at θ = 30°. For both sidebands and main peaks, PECD signals increase with θ. At θ = 30°, sideband PECD shows irregular oscillation with φ, while main peak PECD remains isotropic. The maximum sideband PECD signal exceeds 8%, and the sideband PECD signals in counter - rotating fields are generally larger than those in co - rotating fields."
    },
    "406": {
        "figure1": "2505.03582v1_qGaussian",
        "label1": "fig:qGaussian",
        "caption1": "Two trajectories $(\\eta(k))$ of Algorithm \\ref{alg:fixed.point} for the $q$-Gaussian distribution in Example \\ref{eg:q.Gaussian}. The solid curve shows the graph of $T(\\eta)$ on $\\Xi = (-\\infty, 0)$ defined by \\eqref{eqn:iteration} for the given data-set. The dashed line is the graph of the identity map. The fixed point (indicated by the cross) corresponds to the MLE.",
        "text": "Let $\\mathcal{X} = \\mathbb{R}$ and $\\nu$ be the Lebesgue measure. For $q \\in (0, 3)$, the $q$-Gaussian distribution as a scale family can be expressed as a $\\lambda$-exponential family with $\\lambda = 1 - q \\in (-2, 1)$: p(x; \\theta) = (1 - \\lambda \\theta x^2)^{\\frac{1}{\\lambda}} e^{-\\varphi(\\theta)}, \\quad x \\in \\mathbb{R}, where $\\theta \\in \\Theta = (-\\infty, 0)$, $F(x) = x^2$ and $\\varphi(\\theta) = \\frac{-1}{2} \\log (-\\theta) + C_{\\lambda}$ for some constant $C_{\\lambda}$ (see \\cite[Example III.17]{WZ22} for details). The dual parameter is $\\eta = \\nabla^{(\\lambda)} \\varphi(\\theta) = \\frac{1}{2 + \\lambda} \\frac{1}{\\theta} \\in \\Xi = (-\\infty, 0)$, so $\\theta = \\nabla^{(\\lambda)} \\psi(\\eta) = \\frac{1}{2 + \\lambda} \\frac{1}{\\eta}$. In Figure \\ref{fig:qGaussian} we illustrate Algorithm \\ref{alg:fixed.point} with a simulated data-set with $\\lambda = -1.2$ and $n = 500$. The true value of $\\theta$ is $-1$ and we show two trajectories with different initial values. In both cases, the iterates converge quickly to the MLE.% Here, there is a unique fixed point (the MLE), and the algorithm converges quickly.",
        "summarize_figure": "2505.03582v1_qGaussian",
        "summarization": "The chart shows two trajectories (η(k)) of Algorithm \"fixed.point\" for the q - Gaussian distribution in the \"q - Gaussian\" example. The solid curve is the graph of T(η) on (-∞, 0) from a given formula for the data - set, and the dashed line is the identity map. The cross - marked fixed point is the MLE. For q in (0, 3), the q - Gaussian distribution is a λ - exponential family with λ = 1 - q in (-2, 1). With a simulated data - set (λ = -1.2, n = 500, true θ = - 1), the two trajectories from different initial values quickly converge to the MLE."
    },
    "407": {
        "figure1": "2505.03582v1_Dir",
        "label1": "fig:Dir",
        "caption1": "\\bf Left:} True $p$ (red {\\color {red} $\\blacktriangle$}) and samples ($+$) from the Dirichlet perturbation model. {\\bf Right:} Three trajectories $(p(k))_{k \\geq 0}$ of the algorithm \\eqref{eqn:Dirichlet.p.update} with different initial values. They all converge quickly to the MLE (blue {\\color {blue} $\\blacktriangle$}).",
        "text": "We consider a simulated data-set from the Dirichlet perturbation model with $d = 2$, $p = (0.1, 0.4, 0.5)$, $\\sigma = 0.1$ and $n = 100$ (chosen for visualization purposes; the algorithm works for much larger values of $d$ and $n$). In Figure \\ref{fig:Dir} we plot the data and the output of the algorithm \\eqref{eqn:Dirichlet.p.update} with several initial values. In all cases, the iterates converge quickly to the MLE. In practice, we may initialize $p(0)$ by the sample mean $\\frac{1}{n} \\sum_{i = 1}^n q_i$ as it tends to be close to the MLE.",
        "summarize_figure": "2505.03582v1_Dir",
        "summarization": "The left side of the chart shows the true parameter p (represented by a red triangle) and samples (represented by \"+\") from the Dirichlet perturbation model. In the simulated data - set, d = 2, p = (0.1, 0.4, 0.5), σ = 0.1, n = 100. The right side of the chart presents three trajectories (p(k)) of the algorithm \"Dirichlet.p.update\" starting from different initial values, all of which converge quickly to the maximum likelihood estimate (MLE, represented by a blue triangle). In practice, p(0) is often initialized using the sample mean, as it is close to the MLE."
    },
    "408": {
        "figure1": "2505.03585v1_intro_fig_100",
        "label1": "fig:bas-vs-robas",
        "caption1": "Illustration of (approximated) \\baspe, \\baspp and \\robas (ours) with $\\bP_\\theta = \\cN(\\mu, \\sigma^2)$ over a grid of $(\\mu, \\sigma)$ pairs for a fixed $\\epsilon$. In the well-specified case (left), all ambiguity sets include the DGP while \\robas covers a slightly bigger area than \\baspe and \\baspp. For a contaminated dataset (right) \\robas continues to contain the DGP and maintains a similar area, whereas the \\bas formulations exclude it and cover a much larger area of distributions further away from the DGP.}  \\end{center} \\vskip -0.36in",
        "text": "To overcome this, recently developed Bayesian formulations of DRO use posterior beliefs to inform the optimisation problem \\citep{shapiro2023bayesian} or the ambiguity set itself \\citep{dellaporta2024decision}. However, these methods inherit the sensitivity of Bayesian posteriors to model misspecification \\citep[see e.g.][]{grunwald2012safe, walker2013bayesian}.  A key goal in DRO methodology is to choose the size of the ambiguity set such that the DGP falls within it with high probability, as illustrated in Fig. \\ref{fig:bas-vs-robas} (left) for the two formulations of Bayesian Ambiguity Sets (\\bas) \\citep{dellaporta2024decision} and our proposed Robust \\bas (\\robas).  If the estimate is not accurate - for example, when the model is misspecified - then a much larger size will be required to contain the DGP as illustrated in Fig. \\ref{fig:bas-vs-robas} (right). The price to pay for this large size is the inclusion of many probability distributions that are unlikely to occur, and which could be very pessimistic with respect to the objective function, leading to an overly conservative decision. If the decision maker wrongly assumes the model is well-specified and incorrectly chooses an overly optimistic ambiguity set size, then the DGP may not lie in the set, and the decision could be overly optimistic compared to the out-of-sample outcome, often referred to as the optimiser's curse  \\citep{Kuhn2019}.",
        "summarize_figure": "2505.03585v1_intro_fig_100",
        "summarization": "This figure shows the relationships of (approximated) BASPE, BASPP and RoBAS (our proposed method) with the normal distribution N(μ, σ²) over a grid of (μ, σ) pairs for a fixed epsilon. In the well - specified case (left plot), all ambiguity sets include the data - generating process (DGP), and RoBAS covers a slightly larger area than BASPE and BASPP. For the contaminated dataset (right plot), RoBAS continues to contain the DGP and keeps a similar area. In contrast, the BAS formulations exclude the DGP and cover a much larger area of distributions far from the DGP."
    },
    "409": {
        "figure1": "2505.03585v1_bimodal_zoom.001",
        "label1": "fig:bimodal-robas",
        "caption1": "The out-of-sample mean and variance for the Newsvendor problem with a \\emph{misspecified} Gaussian location model and a bimodal Gaussian DGP. Results are shown for the univariate ($D=1$, top) and the multivariate ($D=5$, bottom) cases, with markers representing $\\epsilon$ values. For illustration purposes, the bottom-left area of the multivariate case is shown in a zoomed-in view.}  \\end{center} \\vskip -0.2in",
        "text": "We further observe (\\fig \\ref{fig:bimodal-robas}) that in the univariate case, empirical MMD achieves a lower (difference of $\\leq 1$) out-of-sample mean for most values of $\\epsilon < 1$. However, \\drorobas consistently shows a lower out-of-sample variance. In the multivariate case, the performances of the two methods are similar, though empirical MMD outperforms \\drorobas in both mean and variance. Notably, this comparison pits a Bayesian method under model misspecification against a completely empirical method unaffected by this misspecification. However, it is promising that \\drorobas remains highly competitive against this baseline. The next example, based on contamination models, illustrates a scenario where robustness is crucial for both model-based and empirical methods.",
        "summarize_figure": "2505.03585v1_bimodal_zoom.001",
        "summarization": "This figure shows the out - of - sample mean and variance for the Newsvendor problem with a misspecified Gaussian location model and a bimodal Gaussian DGP. Results for univariate (D = 1, top) and multivariate (D = 5, bottom) cases are presented, with markers indicating epsilon values. The bottom - left area of the multivariate case is zoomed in for illustration. In the univariate case, for most epsilon < 1 values, Empirical MMD achieves a lower out - of - sample mean (difference ≤1), while DRO - RoBAS consistently has a lower out - of - sample variance. In the multivariate case, the two methods have similar performances, though Empirical MMD outperforms DRO - RoBAS in both mean and variance."
    },
    "410": {
        "figure1": "2505.03585v1_cont",
        "label1": "fig:cont-normal-robas",
        "caption1": "Out-of-sample mean-variance trade-off in the Newsvendor problem for a Gaussian location model (top) and an Exponential model (bottom) with a contaminated training dataset. Results are shown for contamination levels $\\eta = 0.0$ (left), $\\eta = 0.1$ (middle), and $\\eta = 0.2$ (right). Each marker represents a specific $\\epsilon$ value, with some labelled for reference.}  \\end{center} \\vskip -0.2in",
        "text": "In the second simulation, we consider the Huber contamination models \\citep{huber1992robust} where the training set is contaminated whereas the test set is not. \\fig \\ref{fig:cont-normal-robas} demonstrates that both \\drobas formulations outperform \\drorobas and the empirical MMD method in the well-specified case, where there is no contamination in the training set, and the training and test distributions are identical. In misspecified cases, where $\\eta > 0$, \\drorobas shows greater robustness compared to the other methods in terms of the out-of-sample mean-variance trade-off. This suggests that the robust posterior and robust distance measures in \\robas contribute to a better-informed ambiguity set concerning the \\textit{test set generating process}.  This example further illustrates that, while the motivation for a robust ambiguity set stemmed from concerns about \\emph{model misspecification}, even entirely empirical methods, like the empirical MMD, can be sensitive to misspecifications arising from discrepancies between the training and test distributions.",
        "summarize_figure": "2505.03585v1_cont",
        "summarization": "This figure shows the out - of - sample mean - variance trade - off in the Newsvendor problem for a Gaussian location model (top) and an Exponential model (bottom) with a contaminated training dataset. Results for contamination levels eta = 0.0 (left), eta = 0.1 (middle), and eta = 0.2 (right) are presented. Each marker represents a specific epsilon value, with some labelled. In the well - specified case (eta = 0.0), both DRO - BAS formulations outperform DRO - RoBAS and the empirical MMD method in terms of out - of - sample mean - variance trade - off. In misspecified cases (eta > 0), DRO - RoBAS shows greater robustness compared to other methods."
    },
    "411": {
        "figure1": "2505.03587v1_CPU_Consumption",
        "label1": "fig:overview",
        "caption1": "CPU consumption among local/cloud processing}  \\vspace{2mm",
        "text": "Experimental results from \\cite{5GERA2024} demonstrate that offloaded object detection achieves a higher success rate and lower battery consumption compared to local resource execution. In this case study, the robot's task is to detect and recognize various objects, with the key \\textbf{KPI1} being being specifically defined as the number of successfully detected unique objects per task. There are 50 possible unique objects, randomly distributed across rooms. YOLO (DarkNet) is applied on edge machines for cloud-based object detection, enabling high success rates and low resource consumption. Meanwhile, on-board object detection is available using YOLO (MobilenetV2) in case the network connection is unstable. The difference in CPU workload is presented in Fig.\\ref{fig:overview} along with the corresponding trends in battery consumption as of \\textbf{KPI2}. In this experiment, the DarkNet detector configured in the edge/cloud can handle all 50 possible objects, while the MobilenetV2 detector can only handle a subset of these objects. The remaining objects are considered out-of-distribution for the on-board detector.",
        "summarize_figure": "2505.03587v1_CPU_Consumption",
        "summarization": "The figure shows the CPU load of deployed models on a robot, titled \"Robot's CPU load for deployed models\". Local YOLO (DarkNet) has the highest CPU load, nearly 60%. Local YOLO (MobilenetV2) follows, at around 40%. Cloud YOLO (DarkNet) and Cloud Mask R-CNN (ResNet-50) have lower CPU loads, both around 10%. According to the text, experiments show offloaded object detection has higher success rates and lower battery consumption. The robot's task is object detection and recognition, and different models' CPU loads vary due to deployment locations and architectures."
    },
    "412": {
        "figure1": "2505.03590v1_spectra",
        "label1": "fig:spectra",
        "caption1": " %Illustration of key challenges in \\ac{mrs} metabolite quantification. From left to right, the figure shows simulated proton \\ac{mrs} spectra: a noiseless spectrum decomposed into individual metabolite components (colored), highlighting substantial spectral overlap due to limited spectral resolution; the same spectrum with broadened peaks (also noiseless), simulating poor B0 shimming that exacerbates overlap and further obscures metabolite features; and a spectrum with reduced \\ac{snr}, demonstrating how noise can mask low-intensity metabolites and complicate accurate quantification (\\ac{snr} = 7 dB). Illustration of key challenges in \\ac{mrs} metabolite quantification. From left to right: a noiseless spectrum with individual metabolite components, showing substantial spectral overlap; the same spectrum with broadened peaks due to poor B0 shimming, increasing overlap; and a low-\\ac{snr} spectrum (\\ac{snr} = 7 dB), where noise obscures low-intensity metabolites. ",
        "text": "The central challenge in \\ac{mrs} metabolite quantification lies in the ill-posed nature of the inverse problem, arising from factors such as inherent spectral overlap due to limited resolution, signal degradation from noise and artifacts, and the presence of unmodeled baseline distortions. To illustrate these challenges, Fig. \\ref{fig:spectra} presents simulated proton \\ac{mrs} spectra, highlighting key issues. Our goal is to move beyond point estimates of metabolite concentrations, as provided by traditional methods like LCModel \\cite{provencher_estimation_1993}, towards a full Bayesian treatment. %This allows us to estimate not only the most likely concentrations but also the inherent variability within these estimates, which is crucial for reliable interpretation.  The aim is to learn the conditional probability distribution $p(\\theta \\,|\\,x)$, where $\\theta$ represents the vector of metabolite concentrations and other signal parameters, and $x$ is the observed \\ac{mrs} signal.",
        "summarize_figure": "2505.03590v1_spectra",
        "summarization": "This figure shows simulated proton MRS spectra. From left to right: a noiseless spectrum decomposed into individual metabolite components (colored), highlighting significant spectral overlap due to limited resolution; the same spectrum with broadened peaks (also noiseless), simulating poor B0 shimming that worsens overlap and blurs metabolite features; a spectrum with reduced SNR (SNR = 7 dB), demonstrating how noise masks low - intensity metabolites and complicates accurate quantification. These illustrate key challenges in MRS metabolite quantification."
    },
    "413": {
        "figure1": "2505.03590v1_architecture",
        "label1": "fig:model",
        "caption1": "The inference network takes the observed spectrum $x$ as input and outputs the parameters of the base distribution $q_0(\\theta_0 \\,|\\,x)$ for the latent variables $\\theta$. \\Acp{snf} are then applied to the base distribution to transform it into a more complex and flexible posterior distribution $q_K(\\theta_K \\,|\\,x)$. Instead of a decoder network, the physics-based \\ac{mrs} signal model directly maps the latent variables $\\theta_K$, representing metabolite concentrations and other relevant parameters, to the spectral domain.",
        "text": "To approximate the intractable true posterior distribution $p(\\theta \\,|\\,x)$, we use a \\ac{snf} model to learn an approximate posterior $q(\\theta \\,|\\,x)$. The model architecture, outlined in Fig. \\ref{fig:model}, consists of a probabilistic, fully-connected, encoder combined with Householder Sylvester flows \\cite{vandenBerg2018SNFs}, with all components implemented as fully connected networks.",
        "summarize_figure": "2505.03590v1_architecture",
        "summarization": "This figure shows the architecture of an inference network. The observed spectrum x is input into the inference network, which outputs the parameters of the base distribution q0(θ0|x) for the latent variables θ. Then, normalizing flows (SNF) are applied to transform the base distribution into a more complex and flexible posterior distribution qK(θK|x). Instead of a decoder network, the physics - based MRS signal model directly maps the latent variables θK, representing metabolite concentrations and other relevant parameters, to the spectral domain."
    },
    "414": {
        "figure1": "2505.03590v1_fits_n_latent",
        "label1": "fig:fits_n_latent",
        "caption1": " %Visualization of a test example spectrum $\\hat{x}$, alongside its fits and latent representations. Top left: the reconstructed spectrum obtained by sampling 1000 latent variables $\\theta_0 \\sim \\mathcal{N}(\\mu(\\hat{x}), \\sigma(\\hat{x}))$, transforming through the flows, and decoding to the corresponding spectra. Top right: LCModel fit of the same spectrum, shown for comparison. Bottom: a pairplot of the latent posterior distributions inferred by the \\ac{snf}, showing individual marginal distributions along the diagonal and correlations between parameters off-diagonal. Dashed boxes indicate the prior distributions for each parameter. LCModel estimates and \\acp{crlb} are also shown for reference. Example test spectrum $\\hat{x}$ fit using the \\ac{snf} and LCModel. Top left: reconstructed spectrum obtained by sampling from the \\ac{snf} posterior. Top right: LCModel fit for the same spectrum. Bottom: pairplot of \\ac{snf}-inferred posterior distributions for \\ac{cr}, \\ac{gaba}, \\ac{glu}, \\ac{gpc}, \\ac{naa}, \\ac{pch}, and \\ac{pcr}. Marginal distributions are shown along the diagonal; correlations are shown off-diagonal. Prior simulation ranges are indicated by dashed boxes; LCModel estimates and \\ac{crlb} are overlaid for reference. ",
        "text": "To qualitatively assess the behavior of the inferred latent posteriors, we visualize in Fig.~\\ref{fig:fits_n_latent} an example test spectrum and its corresponding fits. The \\ac{snf} model samples 1000 latent variables $\\theta_0 \\sim \\mathcal{N}(\\mu(\\hat{x}), \\sigma(\\hat{x}))$, which are then transformed through the flow layers to produce $\\theta_K$, from which reconstructed spectra $p(x ,|, \\theta)$ are generated. The top left panel shows the reconstructed spectrum from the \\ac{snf}, while the top right panel displays the LCModel fit for the same measurement. Below, we visualize the posterior distributions inferred by the \\ac{snf} for key metabolites, including:  \\ac{cr}, \\ac{gaba}, \\ac{glu}, \\ac{gpc}, \\ac{naa}, \\ac{pch}, and \\ac{pcr}. The diagonal of the pairplot shows the marginal distributions, while the off-diagonal elements show pairwise correlations. Prior distributions are indicated by dashed boxes, and LCModel estimates along with \\acp{crlb} are overlaid for comparison. From the posterior distributions, we observe that metabolites such as \\ac{naa} and \\ac{cr} exhibit tight, well-constrained distributions, indicating confident and unambiguous quantification. In contrast, broader distributions are seen for \\ac{gaba} and \\ac{pcr}, reflecting higher uncertainty. This can be attributed to the inherently low concentration of \\ac{gaba} and significant spectral overlap between \\ac{cr} and \\ac{pcr}. Similarly, the strong correlation between \\ac{gpc} and \\ac{pch} highlights challenges in disentangling these highly overlapping signals, visible in the diagonal shape of their joint posterior. While the \\ac{crlb} provides a theoretical lower bound on the variance of an estimator, it does not fully capture the total uncertainty reflected in the posterior distributions.",
        "summarize_figure": "2505.03590v1_fits_n_latent",
        "summarization": "This figure shows a test spectrum example, its fitting results, and latent representations. The top - left part is the reconstructed spectrum generated by sampling 1000 latent variables θ0 ~ N(μ(̂x), σ(̂x)), transforming through SNF, and decoding. The top - right part is the LCModel fit of the same spectrum for comparison. Below is a pairplot of the latent posterior distributions of key metabolites (Cr, GABA, Glu, GPC, NAA, PCh, PCr) inferred by SNF. Diagonal elements show marginal distributions, off - diagonal elements show correlations. Dashed boxes indicate prior distributions, with LCModel estimates and CRLB overlaid for reference. Metabolites like NAA and Cr have tight distributions for more certain quantification, while GABA and PCr have broader distributions with higher uncertainty due to low concentration and spectral overlap. The strong correlation between GPC and PCh reflects the challenge of separating overlapping signals."
    },
    "415": {
        "figure1": "2505.03591v1_fig1",
        "label1": "fig:CONSERVATIVE_HK",
        "caption1": "Black: opinion density at the initial time. Red and blue: opinion density at time $T=300$ with $n=100$ and $\\Delta t = 2$ (red) and at time $T=600$ with $m=200$ and $\\Delta t=1$ (blue). Other parameters were $\\nu=0.25$ and $\\sigma=0.02$.}  \\end{center",
        "text": "As an example,  we construct a compactly supported differentiable probability density on the interval $[-1,1]$ as follows.  First, let $p(x)$  be the uniquely determined cubic polynomial with $p(0)=1$, $p'(0)=p(1)=p'(1)=0$:  $$ p(x) = 2 x^3 - 3x^2+1. $$ Define $f(x) = p(x)$ for $x \\in [0,1]$,  $f(x) = p(-x)$ for $x \\in [-1,0]$, and $f(x) = 0$ outside of $[-1,1]$, or briefly: f(x) =  \\left\\{ \\begin{array}{cl} 2 x^2|x| - 3 x^2+1 & \\mbox{if $|x| \\leq 1$}, \\\\ 0 & \\mbox{otherwise}; see the black curve in Fig.\\ \\ref{fig:CONSERVATIVE_HK}. Let $$ X_i(0) = -1 + \\left(i - \\frac{1}{2} \\right) \\Delta x, ~~~i =1,\\ldots,n, ~~~\\Delta x = \\frac{2}{n}.  $$ (The value of $n$ will be specified shortly.)  Let the  $w_i$  be proportional to $f(X_i(0))$,  with the constant of proportionality chosen to make the sum of the $w_i$ equal to $1$. We solve the equations using the midpoint method with a time step $\\Delta t$ up to a time $T$ (both $\\Delta t$ and $T$ will be specified shortly), then plot the function $$ f(x,T) =  $$ with $\\sigma=0.05$. Figure  \\ref{fig:CONSERVATIVE_HK} shows results obtained with $m=100$, $\\Delta t = 2$, $T=300$ (red) and  then again with $m=200$, $\\Delta t =1$, $T=600$ (blue). The red and blue results are indistinguishable to the eye --- so  the calculations are likely to yield the ``fully resolved\" final state, in the sense that higher resolution, or larger $T$, would not result in visibly different pictures. In this calculation, $\\nu=0.25$. The value of $\\nu$ of course affects the outcome, for instance for $\\nu=0.75$, global consensus is reached. Also, $\\sigma=0.02$ in Figure \\ref{fig:CONSERVATIVE_HK}. Smaller $\\sigma$ would result in tighter peaks, but this is merely a matter of graphics; the red and blue curves in Figure\nThe possibility of discontinuous dependence of the outcome of opportunistic position optimization on voter loyalty had already been pointed out in \\cite{Borgers_et_al_candidate_dynamics}, but there the opinion distribution in the electorate was taken to be  fixed. In \\cite{Borgers_et_al_candidate_dynamics}, discontinuous behavior was only observed for a fixed bimodal electorate.  Here we always let the initial opinion density be  the function given in (\\ref{eq:initial_density}) and indicated in black in Fig.\\ \\ref{fig:CONSERVATIVE_HK}. Multiple peaks emerge as a result of opinion dynamics, as shown in Fig.\\ \\ref{fig:CONSERVATIVE_HK}.",
        "summarize_figure": "2505.03591v1_fig1",
        "summarization": "This figure shows the opinion density distributions at different time points. The black curve represents the opinion density at the initial time (t = 0). The red dashed line indicates the opinion density at time T = 300 with n = 100 and Δt = 2, while the blue dots represent the opinion density at time T = 600 with m = 200 and Δt = 1. Other parameters are ν = 0.25 and σ = 0.02. As time progresses, the opinion density distribution changes, with peaks emerging at certain x - values, indicating a concentration of opinions at these positions. The red and blue results are visually indistinguishable, suggesting that the calculations may have reached a \"fully resolved\" final state."
    },
    "416": {
        "figure1": "2505.03591v1_fig2",
        "label1": "fig:VOTER_OPENMINDEDNESS",
        "caption1": "\\textbf{Varying voter open-mindedness.} Positions taken by left and right candidates as a function of time, with $\\alpha=1$, $\\beta=0.1$, $\\gamma=1$, and three different values of $\\nu$.}  \\end{center",
        "text": "In Figure\\ \\ref{fig:VOTER_OPENMINDEDNESS}, we fixed  candidate opportunism $\\alpha=1$ and $\\beta=0.1$, voter loyalty $\\gamma=1$, and vary voter open-mindedness $\\nu$.  For sufficiently large $\\gamma$, the two candidates coalesce rapidly. For smaller $\\gamma$, $\\ell$ and $r$ approach each other, but eventually, due to increasing polarization of the electorate, they turn away from each other.\nFigure\\ \\ref{fig:VOTER_OPENMINDEDNESS} is a bit surprising: Greater $\\nu$, that is, higher voter open-mindedness, causes the candidate positions to move apart from each other, while smaller $\\nu$, that is, lower voter open-mindedness, causes them to coalesce. The reason is that for the larger value of $\\nu$, there will be slightly more  centrist voters eventually. To capture the center without losing too many left-wing voters, the left-wing candidate moves back towards the center as the centrist camp grows more pronounced.",
        "summarize_figure": "2505.03591v1_fig2",
        "summarization": "This figure shows the positions of left - and right - wing candidates over time, with fixed candidate opportunism (α = 1, β = 0.1) and voter loyalty (γ = 1), for different values of voter open - mindedness (ν). When ν = 0.25, 0.26, 0.27, the red line represents the right - wing candidate's position and the blue line represents the left - wing candidate's position. Larger ν makes the candidates' positions move apart, while smaller ν makes them coalesce. This is because a larger ν leads to more centrist voters eventually, prompting the left - wing candidate to move towards the center to capture them without losing many left - wing voters."
    },
    "417": {
        "figure1": "2505.03591v1_fig3",
        "label1": "fig:VOTER_LOYALTY",
        "caption1": "\\textbf{Varying voter loyalty.} Positions taken by left and right candidates as a function of time, with $\\nu=0.30$, $\\alpha=1$, $\\beta=0.1$, and three different values of $\\gamma$.}  \\end{center",
        "text": "In Figure~\\ref{fig:VOTER_LOYALTY}, we fixed voter open-mindedness $\\nu = 0.30$, candidate opportunism $\\alpha=1$ and $\\beta=0.1$, and varied voter loyalty $\\gamma$.  For sufficiently large $\\gamma$, the two candidates coalesce rapidly. For smaller $\\gamma$, $\\ell$ and $r$ approach each other, but eventually, due to increasing polarization of the electorate, they turn away from each other.",
        "summarize_figure": "2505.03591v1_fig3",
        "summarization": "This figure shows the positions of left - and right - wing candidates over time, with fixed voter open - mindedness (ν = 0.30) and candidate opportunism (α = 1, β = 0.1), for different values of voter loyalty (γ). When γ = 1.12, 1.1, 1.08, the red line represents the right - wing candidate's position and the blue line represents the left - wing candidate's position. When γ is large enough, the two candidates' positions coalesce rapidly. When γ is smaller, the candidates' positions approach each other first but eventually turn away due to increasing voter polarization."
    },
    "418": {
        "figure1": "2505.03593v1_cycleofbias",
        "label1": "fig:cycleofbias",
        "caption1": "The cyclic nature of data, interaction with data, and, data analysis methods/algorithms feeding into each other and building up on biases} \\Description{The cyclic nature of data, interaction with data, and, data analysis methods/algorithms feeding into each other and building up on biases",
        "text": "Other sources of biases are from the data itself and the algorithms used to collect or process the data. If the algorithms are trained on biased data, the outputs of such algorithms are likely to be inherently biased ~\\cite{mehrabi2021survey}. All these sources of bias feed into each other in a cyclic manner in the various stages of research design, data collection, and data analysis. See Fig~\\ref{fig:cycleofbias} for an illustration.",
        "summarize_figure": "2505.03593v1_cycleofbias",
        "summarization": "This figure shows the cyclic relationship among data, data analysis, and user interaction, revealing the process of bias accumulation. The four elements - Data, User Interaction, Data Analysis (Social Sciences), and ML Algorithms (Computer Science) - form a cycle. The arrows between each element indicate the flow direction, and many places are marked with \"Introduction of personal bias\" or \"Introduction of algorithmic bias\". This indicates that personal and algorithmic biases can be introduced and interact with each other in various stages such as data collection, analysis, and user interaction, forming a cycle of continuous bias accumulation."
    },
    "419": {
        "figure1": "2505.03595v1_Hypercube_Hypersphere_Volume",
        "label1": "fig:volume",
        "caption1": "Variation of the volume of a hypersphere/hypercube in high-dimensional spaces, highlighting the motivation for Anant-Net in addressing high-dimensional PDEs in hypercube domain. Here, $R$ denotes the radius of the hypersphere, and $L$ represents the edge length of the hypercube.",
        "text": "High-dimensional PDEs are integral to various scientific and engineering domains, including quantum mechanics, financial mathematics, and optimal control. Their solutions provide crucial insights into complex, multi-scale phenomena that cannot be accurately captured using lower-dimensional approximations. However, solving these equations efficiently remains a significant challenge due to the \\textit{curse of dimensionality}, the exponential growth in computational complexity and data requirements as the number of dimensions increases. This issue is particularly pronounced in numerical approximations, making high-dimensional PDEs computationally intractable with conventional methods. In this work, we introduce \\textit{Anant-Net} (with \\textit{Anant} meaning \\textit{infinite}), a novel neural architecture designed to mitigate the curse of dimensionality and enable the efficient solution of high-dimensional PDEs critical to real-world applications. Our approach provides an efficient framework for handling large datasets, whose size typically scales exponentially with dimensionality. While purely data-driven models might seem a viable alternative, they often struggle with excessive data requirements, making them impractical for high-dimensional problems. To address this, we leverage physics-informed neural surrogates, incorporating the underlying PDE as an additional constraint. This integration significantly reduces the dependency on large datasets, offering a more scalable and computationally efficient framework for solving high-dimensional PDEs. In high-dimensional spaces, the volume of a hypersphere decreases as dimensionality increases when the radius $R>1, <1, = 1$, eventually approaching zero. Conversely, the volume of a hypercube grows exponentially with dimension if the edge length $L>1$, while it remains constant at $L=1$; see figure \\ref{fig:volume}. Since the volume of a hypersphere shrinks with increasing dimensions, enforcing data constraints exactly within this region makes the problem more easier. In contrast, for a hypercube with $L > 1$, the exponentially growing volume poses significant computational challenges, making it difficult to efficiently solve high-dimensional PDEs.   $[0,1]$ ($L=1$)& 0.4 $\\pm$ 2.17e-03 & 0.7 $\\pm$ 2.67e-03  & 0.5 $\\pm$ 3.44e-03\\\\  $[-1,1]$ ($L>1$) & 1.49 $\\pm$ 1.49e-02 & 7.09 $\\pm$ 3.71e-02 & 6.00 $\\pm$ 3.81e-02\\\\ Table \\ref{tab1:weak_strong_runtime} presents the mean and standard deviation of percentage relative $L_2$ error (testing error) of Anant-Net evaluated on randomly selected test points for the high-dimensional Poisson equation. These testing errors show that Anant-Net can accurately solve high-dimensional PDEs.\nIn the context of high-dimensional PDEs, two primary factors determine the efficacy of any method based on scientific machine learning: (1) the sampling strategy for data or collocation points, and (2) the computation of gradients for residual losses via automatic differentiation, which are used as soft constraints in training physics-informed models. It is evident that a significant number of existing methods represent a trade-off between these factors, which ultimately influences their scalability and suitability for solving real-world problems of practical relevance.  While various methods exist for solving high-dimensional PDEs, this work focuses on neural surrogates that not only leverage available data but also incorporate physical laws, ensuring the consistency of the solutions with underlying governing principles.  In recent years, the challenge of solving high-dimensional PDEs has been addressed by several researchers through the use of neural surrogate methods. Han et al. \\cite{han2018solving} introduced a deep neural network framework that reformulates high-dimensional PDEs as backward stochastic differential equations. However, their approach relied on approximating the gradient of the solution using a neural network, which introduced scalability issues, particularly in scenarios involving long-time integration. This limitation was subsequently addressed by Raissi et al. \\cite{raissi2018forward}, who proposed an alternative method that directly models the solution of the PDE with a neural network, rather than its gradient. The efficacy of this improvement was demonstrated through its application to the Black-Scholes-Barenblatt and Hamilton-Jacobi-Bellman equations in 100 dimensions. Hu et al. \\cite{hu2024tackling} introduced a stochastic dimensions gradient descent (SDGD) framework based on physics-informed neural networks to solve high-dimensional PDEs on hypercubes. This method stochastically samples the dimensions of the PDE and computes the residuals only in the selected sample dimensions, rather than performing the computation across all dimensions. Their approach significantly reduced the computational burden on hardware, leading to notable speed-ups and improved accuracy compared to other contemporary methods for high-dimensional PDEs. However, their method bypassed the need for data or boundary point sampling by directly enforcing boundary conditions within the model, particularly for hypersphere, where the internal volume approaches zero as the number of dimensions increases (see Figure \\ref{fig:volume}). In the similar line of work, Hu et al.\\cite{hu2024tackling2} proposed Monte Carlo fractional PINN (MC-fPINN) and its extension, MC-tfPINN for tempered fractional PINN (an extension to fPINN \\cite{pang2019fpinns}) to solve extremely high-dimensional problems. Further, an alternate approach was proposed by the same authors \\cite{hu2024score} extended fractional PINN to solve high-dimensional Fokker-Planck-Lévy (FPL) equations in high-dimensions. Adaptive training methods have also been proposed as supplementary strategies to enhance the accuracy of physics-informed neural network models for high-dimensional PDEs \\cite{zeng2022adaptive}.",
        "summarize_figure": "2505.03595v1_Hypercube_Hypersphere_Volume",
        "summarization": "The figure has two sub - plots showing the volume variations of hyperspheres and hypercubes in high - dimensional spaces. The left sub - plot (Hypersphere) shows that for different radii (dashed line for R > 1, solid line for R = 1, dotted line for R < 1), the volume of the hypersphere first increases with the dimension, reaches a peak, then decreases and approaches zero. The right sub - plot (Hypercube) shows that for different edge lengths (solid line for L > 1, dashed line for L = 1), when L > 1, the volume grows exponentially with the dimension; when L = 1, the volume remains constant. These volume change characteristics reflect the curse of dimensionality in solving PDEs in high - dimensional spaces, which is one of the motivations for proposing Anant - Net to solve high - dimensional PDEs."
    },
    "420": {
        "figure1": "2505.03595v1_AnantNet",
        "label1": "fig:AnantN",
        "caption1": "Schematic representation of general Anant-Net architecture with batch-size $B$, highlighting its core components and computational flow for solving high-dimensional PDEs.",
        "text": "Figure \\ref{fig:AnantN} shows the general architecture of Anant-Net. The architecture comprises a number of body networks equal to the batch size. Let $u \\in \\mathbb{R}^d$ denote the solution to a $d$-dimensional PDE, representing the unknown quantity to be approximated by Anant-Net. For brevity, we consider a batch size of three, the Anant-Net prediction of the solution field, denoted by $\\hat{u}_{\\theta}$ , is expressed as: where the inputs are partitioned as $X_1 = \\{x_1, \\dots, x_{d/3}\\}$, $X_2 = \\{x_{d/3+1}, \\dots,  x_{2d/3}\\}$ , and $X_3 = \\{x_{2d/3+1}, \\dots, x_d\\}$, such that $X_1 \\cap X_2 \\cap X_3 = \\emptyset$. The indices $p \\in \\mathrm{D}_1 = \\{1,2,\\ldots,d/3\\}, q\\in \\mathrm{D}_2 = \\{d/3+1,2,\\ldots,2d/3\\},  r\\in \\mathrm{D}_3 = \\{2d/3+1,2,\\ldots,d\\}$ , corresponding to the number of batch elements, are stochastically sampled from the set of active dimensions, i.e.,  $\\{x_p, x_q, x_r\\} \\subset X =  X_1 \\cup X_2 \\cup X_3$, which sweeps over all dimensional indices.  The network parameters are given by $\\theta = \\bigcup \\theta_i, \\ \\forall i $ (all body networks), where $f_j^{(\\theta_i)}$ denotes the output vector of the $i^{\\text{th}}$ body network. Although there is no strict ordering required for partitioning the dimension set $\\mathrm{D} = \\mathrm{D}_1 \\cup \\mathrm{D}_2 \\cup \\mathrm{D}_3$, we adopt an ascending-order partitioning strategy for all experiments presented in this paper for the sake of consistency. The number of partitioned subsets is arbitrary and can be increased by proportionally increasing the number of body networks in the Anant-Net architecture. However, for brevity, we fix the total number of partitioned sets; and correspondingly, the number of body networks, to three in all experiments discussed herein.  Algorithm \\ref{Alg1} gives the details of training data and collocation points for the Anant-Net architecture. It is also important to note that modifying the number of body networks by changing the number of partitioned dimension subsets must be reflected in the data loader, as described in Algorithm \\ref{Alg1}.",
        "summarize_figure": "2505.03595v1_AnantNet",
        "summarization": "This figure shows a schematic of the general Anant - Net architecture with batch - size B, highlighting its core components and computational flow for solving high - dimensional PDEs. The high - dimensional cube serves as the computational domain, from which data of different dimensions (such as 2D, 3D, 4D) are sampled into the network. The architecture includes body networks (NN) equal in number to the batch size. Input data are partitioned into subsets and fed into each body network, and the outputs are combined to obtain the predicted solution. In the computational flow, by checking if the batch processing is over and if the model has converged, an optimizer minimizes the total loss (composed of PDE loss and data loss) and updates the network until the stop condition is met."
    },
    "421": {
        "figure1": "2505.03595v1_Preconditioning_XHigh",
        "label1": "fig:precond",
        "caption1": "Effect of Preconditioning on High-Dimensional Poisson Equation Training: Training results using (quasi-)Newton optimization for 6D, 9D, 15D, 18D, and 21D show overlapping performance without preconditioning. For 45D and higher (right), preconditioning offers no improvement, due to the linear architecture. Note that these experiments were performed on a T4 GPU with 15 GB RAM.",
        "text": "{ } { } While preconditioning has demonstrated clear advantages in accelerating convergence within the framework of physics-informed deep learning, prior investigations have been primarily constrained to low-dimensional settings (typically $\\leq$ 3 dimensions). In this work, we extend these findings to higher-dimensional PDEs by leveraging the proposed Anant-Net architecture. In particular, we employ a \\textit{linear} version of Anant-Net to validate theoretical predictions through empirical experiments that align closely with the theoretical analysis presented in this section. The corresponding results are illustrated in Figure \\ref{fig:precond}. For clarity, we define \\textit{high-dimensional} as dimensionalities greater than 3 and up to 21, and \\textit{extremely high-dimensional (X-high)} as dimensionalities exceeding 21, as referenced in the figure. Within the high-dimensional regime, we observe that the theoretical benefits of preconditioning are preserved under the linear Anant-Net architecture. Specifically, preconditioning the gradients via Newton-based optimization yields stable training dynamics and favorable convergence behavior; see Figure \\ref{fig:precond} (left) . In contrast, omitting preconditioning causes the model to converge to suboptimal local minima, with negligible performance gains throughout training. However, in the X-high dimensional regime, the linear Anant-Net architecture fails to show any tangible benefit from preconditioning, as evidenced in Figure \\ref{fig:precond} (right). This breakdown of theoretical alignment in extremely high dimensions can be attributed to the inherent limitations of the linear two-layer network, which appears insufficient for capturing the complexity of the solution landscape in such settings. Furthermore, increasing the network's depth or width does not overcome this limitation in the absence of nonlinearities. Nonetheless, as we elaborate in the Results section, these limitations can be effectively addressed by incorporating nonlinear activation functions, thereby enabling the resolution of high-dimensional PDEs. From a practical standpoint, to avoid the computational burden typically associated with second-order optimizers, we implement gradient preconditioning via (quasi-)Newton methods as a secondary optimization step. This step follows an initial training phase using a standard stochastic gradient-based optimizer.%, thereby balancing efficiency with improved convergence characteristics.",
        "summarize_figure": "2505.03595v1_Preconditioning_XHigh",
        "summarization": "This figure shows the effect of preconditioning on the training of high - dimensional Poisson equations. The x - axis is the number of iterations (in units of 10000), and the y - axis is the relative L2 error. The figure contains curves for different dimensions (45D, 60D, 99D) with and without preconditioning. In high - dimensional cases (greater than 3D and up to 21D), preconditioning based on Newton optimization can stabilize training and lead to good convergence; without preconditioning, the model tends to converge to suboptimal local minima. However, in extremely high - dimensional cases (exceeding 21D), due to the limitations of the linear architecture, preconditioning does not bring significant improvement. The experiments were conducted on a T4 GPU with 15GB of VRAM."
    },
    "422": {
        "figure1": "2505.03598v1_time_ex2",
        "label1": "fig: time ex2",
        "caption1": "CPU Time of IFE solutions of Example 2.",
        "text": "Figure \\ref{fig: time ex2} records the CPU times for assembling the global system, solving the linear equations, and computing the errors; the recorded times exhibit a linear growth with respect to the number of DoFs, a trend identical to that observed for Example 1.",
        "summarize_figure": "2505.03598v1_time_ex2",
        "summarization": "This figure shows the CPU times for IFE solutions of Example 2. The x - axis represents the number of degrees of freedom (DoF), and the y - axis represents the time in seconds. There are four curves: Matrix Time (blue line with cross markers), Solve Time (black line with circle markers), Error Time (pink line with dot markers), and Reference: O(DoF) (green line). All four curves show that the time increases with the number of DoF, and the growth trend is approximately linear, which is consistent with the trend observed in Example 1."
    },
    "423": {
        "figure1": "2505.03598v1_domain3D",
        "label1": "fig: domain",
        "caption1": "A three-dimensional domain with interface.}  \\end{center",
        "text": "Let $\\Omega\\subseteq\\mathbb{R}^3$ be an open bounded domain.  We assume that $\\Omega$ is separated into two subdomains $\\Omega^-$ and $\\Omega^+$ by a closed $C^2$ manifold $\\Gamma\\subseteq\\Omega$ known as the interface.   See Figure \\ref{fig: domain} for an illustration.  These subdomains contain different materials identified by a piecewise constant function  $\\beta(\\mathbf{x})$ which is discontinuous across the interface $\\Gamma$, i.e., where $\\beta^\\pm>0$ and $\\mathbf{x}= (x_1,x_2,x_3)$.  We consider the following interface problem of the elliptic type on $\\Omega$  coupled with non-homogeneous jump conditions:  -\\nabla\\cdot(\\beta\\nabla u)=f, &~~~~  \\text{in} ~ \\Omega^-  \\cup \\Omega^+, \\\\   u=g, &~~~~\\text{on} ~ \\partial\\Omega, where $\\mathbf{n}$ is the unit normal vector to $\\Gamma$. Here $\\jump{v}_\\gamma := (v|_{\\Omega^+})|_\\gamma-(v|_{\\Omega^-})|_\\gamma$ denotes the jump of $v$ on any manifold $\\gamma\\subseteq\\Omega$,  and if there is no danger of causing confusion, we shall drop $\\gamma$ for simplicity.  The functions $f$ and $g$ denote the source and boundary data,  and $q_1$ and $q_2$ denote the solution and flux jump across the interface $\\Gamma$.  For simplicity, we denote $u^{\\pm}=u|_{\\Omega^{\\pm}}$, in the rest of this article.",
        "summarize_figure": "2505.03598v1_domain3D",
        "summarization": "This figure shows a three - dimensional domain Ω, which is divided into two subdomains Ω⁻ (the gray part) and Ω⁺ (the white part) by a closed C² manifold Γ (called the interface). The subdomains contain different materials, identified by a piecewise constant function β(x), which is discontinuous across the interface Γ. According to the text, this figure is used to illustrate the elliptic - type interface problem in this three - dimensional domain, including the governing equation -∇·(β∇u) = f (in Ω⁻ ∪ Ω⁺), the boundary condition u = g (on ∂Ω), and the conditions involving the jumps of the solution and flux across the interface Γ. Here, f and g represent the source term and boundary data respectively, and q₁ and q₂ represent the jumps of the solution and flux across the interface."
    },
    "424": {
        "figure1": "2505.03600v1_xapian-cloudsuite",
        "label1": "fig:websearchvsxapian",
        "caption1": "Comparison of \\texttt{Web Search} (CloudSuite) and \\texttt{xapian} (TailBench).",
        "text": "To further illustrate the differences, a direct comparison is made between the web search domain applications --\\texttt{Web Search} and \\texttt{xapian}-- from CloudSuite and TailBench, respectively. Figure \\ref{fig:websearchvsxapian} shows the %comparison of  latency obtained for both workloads as the client QPS increases\\footnote{See Section \\ref{sec:experimental-testbed} for details on the experimental configuration.}. %The X-axis represents the rate of queries per second, while the Y-axis the response time in milliseconds. As observed, \\texttt{xapian} exhibits a broader tail latency range, starting below 5ms, whereas \\texttt{Web Search} remains above 10ms.  As \\texttt{Web Search} lacks direct query rate control and requires indirect tuning, it results in a shorter load span.  Furthermore, performance degradation occurs earlier in CloudSuite (after $QPS=1000$) compared to TailBench (after $QPS=4000$), indicating better scalability in the version of the web search application of TailBench.",
        "summarize_figure": "2505.03600v1_xapian-cloudsuite",
        "summarization": "This figure compares the latency of Web Search in CloudSuite and xapian in TailBench. The x - axis is the client queries per second (Client QPS), and the y - axis is the latency (in milliseconds). The blue dot line (xapian 90th) and orange triangle line (xapian 99th) represent the 90th and 99th percentile latencies of xapian, while the gray square line (Web Search 90th) and dark gray diamond line (Web Search 99th) represent the corresponding percentiles for Web Search. xapian has an initial latency below 5ms and a wider latency range, while Web Search has a latency always above 10ms. Web Search lacks direct query rate control, resulting in a shorter load span, and its performance degrades after QPS = 1000, while TailBench degrades after QPS = 4000, indicating better scalability of the TailBench version of the Web Search application."
    },
    "425": {
        "figure1": "2505.03600v1_tailbench_harness",
        "label1": "fig:harness",
        "caption1": "Overview of TailBench++ harness. Components where new features have been introduced are highlighted in blue.",
        "text": "Figure \\ref{fig:harness} shows a block diagram with the components of the harness, which are grouped into two main modules: the client and the server. The seven components of the harness that have been modified are highlighted in blue. Below, we discuss the implemented extensions.%key modifications that aim to address the limitations of the original TailBench benchmark suite (identified in Section \\ref{sec:motivation}). %the four main features that TailBench++ introduces to address the limitations of the original TailBench benchmark suite (see Section \\ref{sec:motivation}), specifying the changes made in the harness to implement each one.",
        "summarize_figure": "2505.03600v1_tailbench_harness",
        "summarization": "This figure presents an overview of the TailBench++ harness, divided into two main modules: the client and the server. The client module includes Client Class (highlighting functions like constructor, start_req, finireq), Networked Client, Integrated Client, and other files. The server module includes Server Class, Networked Server (highlighting functions such as constructor, recvReq, sendResp, checkNewClient), Integrated server, and other files. Components with new features are highlighted in blue, reflecting the extensions and improvements to the original TailBench benchmark suite."
    },
    "426": {
        "figure1": "2505.03600v1_sistema-horizontal-v2",
        "label1": "fig:system",
        "caption1": "Data flow of client requests to servers in the experimental testbed.",
        "text": "For multi-server experiments, clients send requests to LVS, an open-source load-balancing solution integrated into the Linux kernel designed to distribute network traffic across multiple servers. LVS distributes requests based on a load-balancing policy (by default, round-robin) to ensure scalability. Figure~\\ref{fig:system} illustrates the data flow under this configuration, where requests from a variable number of clients ($i$) to a variable number of servers ($j$). In single-server setups, client VMs connect directly to the server VM, without the need for LVS.",
        "summarize_figure": "2505.03600v1_sistema-horizontal-v2",
        "summarization": "This figure shows the data flow of client requests to servers in the experimental testbed. The left side (Master) contains multiple client virtual machines (Client 1 VM, Client 2 VM, Client i VM) running in a Proxmox environment, and the right side (Worker) contains multiple server virtual machines (Server 1 VM, Server 2 VM, Server j VM) also in a Proxmox environment. The LVS (an open - source load - balancing solution) in the middle distributes requests from clients to servers based on a load - balancing policy (by default, round - robin) to ensure scalability. In single - server setups, client VMs connect directly to the server VM without the need for LVS."
    },
    "427": {
        "figure1": "2505.03607v1_Asian_N100000",
        "label1": "fig:Asian",
        "caption1": "\\baselineskip10pt MSEs of the LR and TruLR estimators for pricing a portfolio of 3 Asian options with the same expiration $T=0.25$, but different strike prices $K_1=100$, $K_2=45$, and $K_3=80$. Let $r=0.05$, $\\delta=0.01$, and other parameter are reported in Table \\ref{Tab:para-AsianPort}. Each MSE is estimated based on $100000$ independent replications.",
        "text": "Varying sample size $n$ from $500$ to $500000$, the MSEs and MSE ratios of these estimators are shown in Figure \\ref{fig:Asian} and Table \\ref{Tab:Asian}, from which we observe: (1) The classical LR estimator {exhibits highly variable behavior compared with TruLR estimators} (e.g., $n=1000, 5000, 10000, 50000$). (2) The TruLR-M and TruLR-S estimators can greatly improve the performance of the LR estimators, and the TruLR-M estimator consistently outperforms the TruLR-S estimator. For example, taking $n=100000$, the MSEs of the TruLR-M and TruLR-S estimators are $50$ and $114$, respectively, while the MSE of the LR estimator is {close to $7000$}, corresponding to a reduction in MSE over the LR estimators of approximately $140$ and $61$ times, respectively.",
        "summarize_figure": "2505.03607v1_Asian_N100000",
        "summarization": "This figure shows the mean squared errors (MSEs) of the LR, TruLR - M, and TruLR - S estimators when pricing a portfolio of 3 Asian options with the same expiration time T = 0.25 but different strike prices (K₁ = 100, K₂ = 45, K₃ = 80). The x - axis is the sample size n, and the y - axis is the MSE. The black solid line represents the LR estimator, whose MSE fluctuates greatly; the blue dashed line represents the TruLR - M estimator, and the green dotted line represents the TruLR - S estimator. The MSEs of the latter two are relatively stable and significantly lower than that of the LR estimator. For example, when n = 100000, the MSEs of TruLR - M and TruLR - S are 50 and 114 respectively, while that of the LR estimator is close to 7000, indicating that TruLR - M and TruLR - S can significantly improve performance, with TruLR - M performing better."
    },
    "428": {
        "figure1": "2505.03612v1_ex00",
        "label1": "fig:ex00",
        "caption1": "$100$ randomly sampled trajectories (dashed line from orange to green cross) of system \\eqref{eq: ex1 system} with synthesized reach-avoid controller. The grey vector-field indicates the function $\\bm{k}_1(\\bm{y}(\\bm{x}))$, from which we construct $\\bm{k}(\\bm{y}(\\bm{x}))$ via backstepping.",
        "text": "The system has a vector relative degree $\\{2,2\\}$. Given the safe set $\\mathcal{C} = \\{ \\bm{x} \\in \\mathbb{R}^4 | 1- \\bm{y}_1(\\bm{x})^2 - \\bm{y}_2(\\bm{x})^2 < 0 \\}$ and target set $\\mathcal{X}^r = \\{ \\bm{x} \\in \\mathbb{R}^4 | 2 (\\frac{\\bm{y}_2(\\bm{x})-0.1}{2})^2 + 3(\\frac{\\bm{y}_1(\\bm{x})+0.4}{3})^4 + (3\\bm{y}_1(\\bm{x})+0.3)^2 (4\\bm{y}_2(\\bm{x})+0.2)^2 < 0.01 \\}$, the control objective is to design a state feedback controller that ensures the output trajectories remain within the safe set $\\mathcal{C}$ and eventually enter into the target set $\\mathcal{X}^r$.      By solving the optimization problem \\eqref{algo: sos reach-avoid controller}, we obtain the controller $\\bm{k}_1(\\bm{y}(\\bm{x}))$ with $\\delta=-3.23 \\times 10^{-10} \\approx 0$, demonstrating that $\\bm{k}_1(\\bm{y}(\\bm{x}))$ is a reach-avoid controller that guarantees the compliance of state $\\bm{y}$ of the single-integrator system \\eqref{eq: single integrator} with the required reach-avoid specifications.      By applying the backstepping-based methodology outlined in \\eqref{theorem: reach-avoid backstepping for MIMO}, we synthesize the reach-avoid controller $\\bm{k}(\\bm{x})$ using $\\bm{k}_1(\\bm{y}(\\bm{x}))$, and implement it in system \\eqref{eq: ex1 system} to form the corresponding closed-loop system.     We randomly sampled $100$ initial points within the set $\\mathcal{C}_\\Psi$ to validate the performance of our proposed approach. As depicted in Fig. \\ref{fig:ex00}, all resulting output trajectories remain within the safe set $\\mathcal{C}$ during evolution and ultimately reach the target set $\\mathcal{X}^r$.      Furthermore, Fig. \\ref{fig:ex01} demonstrates that the function $\\Psi(\\bm{x})$ increases monotonically along all trajectories, confirming that $\\Psi(\\bm{x})$ serves as an ECGBF for the system \\eqref{eq: ex1 system} with respect to safe set $\\mathcal{C}$ and target set $\\mathcal{X}^r$.",
        "summarize_figure": "2505.03612v1_ex00",
        "summarization": "The picture shows 100 randomly sampled trajectories (dashed lines from orange points to green crosses) of system eq: ex1 system with a synthesized reach - avoid controller. The grey vector - field is the function k1​(y(x)), used to construct k(y(x)) via backstepping. The safe set C and target set Xr are defined. All output trajectories stay in C and reach Xr."
    },
    "429": {
        "figure1": "2505.03612v1_ex01",
        "label1": "fig:ex01",
        "caption1": "Evaluation of $\\Psi(\\bm{x})$ along $100$ randomly sampled trajectories.",
        "text": "The system has a vector relative degree $\\{2,2\\}$. Given the safe set $\\mathcal{C} = \\{ \\bm{x} \\in \\mathbb{R}^4 | 1- \\bm{y}_1(\\bm{x})^2 - \\bm{y}_2(\\bm{x})^2 < 0 \\}$ and target set $\\mathcal{X}^r = \\{ \\bm{x} \\in \\mathbb{R}^4 | 2 (\\frac{\\bm{y}_2(\\bm{x})-0.1}{2})^2 + 3(\\frac{\\bm{y}_1(\\bm{x})+0.4}{3})^4 + (3\\bm{y}_1(\\bm{x})+0.3)^2 (4\\bm{y}_2(\\bm{x})+0.2)^2 < 0.01 \\}$, the control objective is to design a state feedback controller that ensures the output trajectories remain within the safe set $\\mathcal{C}$ and eventually enter into the target set $\\mathcal{X}^r$.      By solving the optimization problem \\eqref{algo: sos reach-avoid controller}, we obtain the controller $\\bm{k}_1(\\bm{y}(\\bm{x}))$ with $\\delta=-3.23 \\times 10^{-10} \\approx 0$, demonstrating that $\\bm{k}_1(\\bm{y}(\\bm{x}))$ is a reach-avoid controller that guarantees the compliance of state $\\bm{y}$ of the single-integrator system \\eqref{eq: single integrator} with the required reach-avoid specifications.      By applying the backstepping-based methodology outlined in \\eqref{theorem: reach-avoid backstepping for MIMO}, we synthesize the reach-avoid controller $\\bm{k}(\\bm{x})$ using $\\bm{k}_1(\\bm{y}(\\bm{x}))$, and implement it in system \\eqref{eq: ex1 system} to form the corresponding closed-loop system.     We randomly sampled $100$ initial points within the set $\\mathcal{C}_\\Psi$ to validate the performance of our proposed approach. As depicted in Fig. \\ref{fig:ex00}, all resulting output trajectories remain within the safe set $\\mathcal{C}$ during evolution and ultimately reach the target set $\\mathcal{X}^r$.      Furthermore, Fig. \\ref{fig:ex01} demonstrates that the function $\\Psi(\\bm{x})$ increases monotonically along all trajectories, confirming that $\\Psi(\\bm{x})$ serves as an ECGBF for the system \\eqref{eq: ex1 system} with respect to safe set $\\mathcal{C}$ and target set $\\mathcal{X}^r$.",
        "summarize_figure": "2505.03612v1_ex01",
        "summarization": "The picture shows the evaluation of the function Ψ(x) along 100 randomly sampled trajectories. The horizontal axis is step, and the vertical axis is Ψγ(y). Many blue lines represent the changes of Ψγ(y) with step under different trajectories. It can be seen that Ψγ(y) generally increases as step increases, indicating that the function Ψ(x) increases monotonically along all trajectories. This confirms that Ψ(x) serves as an extended general barrier function (ECGBF) for the system with respect to the safe set C and the target set Xʳ."
    },
    "430": {
        "figure1": "2505.03612v1_ex10",
        "label1": "fig:dubins",
        "caption1": "Simulated trajectories (dashed line from orange dot to green cross) of Dubins car on a racetrack with synthesized reach-avoid controller. The grey vector field indicates the function $\\bm{k}_1(\\bm{y}(\\bm{x}))$, from which we construct $\\bm{k}(\\bm{y}(\\bm{x}))$ via backstepping.",
        "text": "Consider the following modified Dubins car model with $\\bm{x} = [x_1,x_2,\\theta,v]^\\top \\in \\mathbb{R}^4$, where $(x_1,x_2)$ denotes the planar location, $\\theta$ represents the heading angle, and $v$ is the forward velocity. The vehicle is actuated by the angular velocity $\\omega$ and forward acceleration $a$ as control inputs $\\bm{u} = [\\omega, a]^\\top \\in \\mathbb{R}^2$. The system has a vector relative degree $\\{2,2\\}$ with output $\\bm{y} = [y_1, y_2]^\\top = [x_1, x_2]^\\top$. Given the safe set $\\mathcal{C} = \\{\\bm{x} \\in \\mathbb{R}^2 | 2(3.5 - y_2(\\bm{x}))^2 - 1.5y_1(\\bm{x}) - (y_1(\\bm{x})^4 + y_2(\\bm{x})^4 - 3.5^2)^2 > 0 \\}$ and target set $\\mathcal{X}^r= \\{ \\bm{x} \\in \\mathbb{R}^2 | (y_1(\\bm{x})+0.5)^2+4(y_2(\\bm{x})+1.9)^2 < 1\\}$. For simplicity, we neglect the size of the vehicle. Our goal is to generate a feedback controller that drives the vehicle into the target set $\\mathcal{X}^r$ while preventing trajectory violations of the track boundaries throughout the entire maneuver. As shown in Fig. \\ref{fig:dubins}, the SOS optimized controller $\\bm{k}_1(\\bm{y}(\\bm{x}))$ ensures the vector field of single-integrator system \\eqref{eq: single integrator} converges to the target set $\\mathcal{X}^r$ while strictly respecting the safety constraints. The simulated trajectories from randomly sampled initial states demonstrate that the synthesized controller $\\bm{k}(\\bm{x})$ drives the dubins vehicle \\eqref{eq.dubins_car} into the target set without violating the prescribed track boundaries before reaching.",
        "summarize_figure": "2505.03612v1_ex10",
        "summarization": "The picture shows the simulated trajectories (dashed lines from orange dots to green crosses) of the Dubins car on a racetrack under a synthesized reach - avoid controller. The grey vector field represents the function k1(y(x)), from which k(y(x)) is constructed via backstepping. The safe set C is {x ∈ R² | 2(3.5 - y2(x))² - 1.5y1(x) - (y1(x)⁴ + y2(x)⁴ - 3.5²)² > 0 }, and the target set Xʳ is {x ∈ R² | (y1(x) + 0.5)² + 4(y2(x) + 1.9)² < 1 }. The simulated trajectories from randomly sampled initial states show that the synthesized controller k(x) can drive the Dubins car into the target set Xʳ without violating the prescribed track boundaries. The SOS - optimized controller k1(y(x)) ensures that the vector field of the single - integrator system converges to the target set Xʳ while strictly adhering to safety constraints."
    },
    "431": {
        "figure1": "2505.03614v1_sun",
        "label1": "fig1m",
        "caption1": "Convergence plots for Example \\ref{EEe1} with dimension d=100",
        "text": "We consider the nonlinear VIP proposed by Sun in \\cite[Example 5]{sun1994projection}, which reads:   where     B_1(x) = (b_1(x), b_2(x), \\dots, b_d(x)),     B_2(x) = Dx + c,     b_i(x) = x_i^2 + x_i + x_{i-1}x_i + x_ix_{i+1}, \\quad i = 1, 2, \\dots, d,   with boundary conditions \\( x_0 = x_{d+1} = 0 \\).   The matrix \\( D \\) is a square matrix of size \\( d \\times d \\), defined by:     d_{ij} =        4, & i = j, \\\\       1, & i - j = 1, \\\\       -2, & i - j = -1, \\\\       0, & \\text{otherwise}.   where \\( c = (-1, -1, \\dots, -1) \\in \\mathbb{R}^d \\) and the  the feasible set \\( C = \\mathbb{R}_+^d \\).   We set  the control parameters  for example 1 as follows:   The initial point \\( x_0 \\) is uniformly generated and   we conducted a numerical experiment to examine the performance of our methods and the other three algorithms. The performance are compared in the Figure~\\ref{fig1m} below.   It is evident from Figure \\ref{fig1m}  that the proposed method is effective and requires fewer number of iterations to converge, followed by Algo2. However, Algo1 and Algo4 converge with a greater number of iterations. The results demonstrate the computational efficiency and robustness of the proposed approach. \\qed",
        "summarize_figure": "2505.03614v1_sun",
        "summarization": "The convergence plot for an example with dimension d = 100 shows four algorithms (Algo1, Algo2, Algo3, Algo4). The horizontal axis is the number of iterations, and the vertical axis is a convergence - measure index. Algo3 (possibly the proposed method) converges fastest, followed by Algo2, while Algo1 and Algo4 take more iterations. This shows the proposed method's computational efficiency and robustness."
    },
    "432": {
        "figure1": "2505.03614v1_Mp",
        "label1": "fig2e",
        "caption1": "Convergence plots for \\ref{EE22} with dimension n=100",
        "text": "Consider ${H} = \\ell^2$, the real Hilbert space whose elements are square-summable infinite sequences of real numbers, and let $C = \\{ x_n \\in H: \\| x_n\\| \\leq 5 \\}$. The operator $\\Phi$ is given by   where $\\| x_n \\| = \\sqrt{\\sum_i |x_i|^2}$. It is easy to see that the variational inequality problem (VIP) for $\\Phi$ on $C$, denoted by $\\text{VIP}(\\Phi, C)$, is non-empty. The operator $A$ is pseudomonotone with a Lipschitz constant $L = 11$. The projection onto $C$ is explicitly given by:       x_n, & \\text{if }\\, \\| x_n \\| \\leq 5, \\\\   We set $n=50$ and the control parameters as follows:   We examined the performance of the proposed with the other three methods, the result is given in the Figure \\ref{fig2e} below.   In this example, we set the tolerance to $10^{-4}$ and present the numerical result in the Figure \\ref{fig2e}. The graphical representation clearly shows that the proposed method outperforms all the three methods considered, followed by Algo4, and then the remaining method. \\qed",
        "summarize_figure": "2505.03614v1_Mp",
        "summarization": "The convergence plot for an example with dimension n = 100 shows four algorithms (Algo1, Algo2, Algo3, Algo4). The horizontal axis is the number of iterations, and the vertical axis is a convergence - measure index. The proposed method (possibly Algo3) outperforms the others, with Algo4 second, demonstrating its superiority in solving the variational inequality problem."
    },
    "433": {
        "figure1": "2505.03614v1_pol",
        "label1": "fig1aa",
        "caption1": "Convergence plots for Example \\ref{EE33}  with $w_{j}=50$",
        "text": "Next, we consider the application of the proposed method in the matrix game problem   Let us consider a matrix game problem between policeman and the Burglars. There are \\( n \\) houses in a city, where the \\( i \\)-th house has wealth \\( w^*_i \\). Every evening, a Burglar chooses a house \\( i \\) to attack, and a Policeman chooses to post himself near a house \\( j \\). After the burglary starts, the Policeman becomes aware of where it happens, and his probability to catch the Burglar is:   where \\( \\text{dist}(i, j) \\) is the distance between houses \\( i \\) and \\( j \\), and \\( \\alpha \\) is a constant. The Burglar seeks to maximize his expected profit     w^*_i(1 - \\exp(-\\alpha \\cdot \\text{dist}(i, j))),   while the interest of the Policeman is the opposite. The matrix game optimization problem is given by    where   solves the matrix game  The control parameters are presented by: In this example, we set the number of wealthy houses $w_{j}$ to be  50 and $10^{-5}$ to be the stopping criterion. As depicted in Figure \\ref{fig1aa}, the the performance of the proposed method is highly encouraging, followed by Algo1, Alglo2 and then Algo3 respectively. \\qed",
        "summarize_figure": "2505.03614v1_pol",
        "summarization": "The convergence plot for an example with wj​=50 shows four algorithms (Algo1, Algo2, Algo3, Algo4). The horizontal axis is the number of iterations, and the vertical axis is a convergence - measure index. The proposed method performs best, followed by Algo1, Algo2, and Algo3, demonstrating its superiority in the matrix game optimization problem."
    },
    "434": {
        "figure1": "2505.03618v1_2025-01-08_-_Labeling_Workflow",
        "label1": "fig:workflow",
        "caption1": "The conceptual cVIL workflow consists of four phases. The process is bounded by a Bootstrap phase and a Residual Labeling phase, both with a traditional instance-centricc focus. In contrast, the core part of the workflow introduces a class-centric focus. In two highly iterative phases, users find means to identify most promising classes to be labeled next, possibly with computational support (Class Selection Guidance), and conduct the Class-Based Labeling.",
        "text": "The cVIL workflow is illustrated in  Figure~\\ref{fig:workflow}. It divides the labeling process into four subsequent phases. In the initial \\textbf{Bootstrap} phase, users address the cold start problem by collecting initial sample instances for each class. With at least one instance labeled per class, supervised machine learning support can be added to the interactive, iterative, and incremental process. The core cVIL workflow includes two cyclically connected  phases: the \\textbf{Class Selection Guidance} phase, where the system identifies the next class to focus on by human-machine collaboration and the \\textbf{Class-Based Labeling} phase, where users label multiple recommended instances per class. The process concludes with the \\textbf{Residual Labeling} phase, where users shift back to an instance-based focus to label remaining (unsolved or out-of-distribution) instances.  In the following, we describe the cVIL workflow in detail, with analysis tasks associated per workflow phase. % (see Table~\\ref{table:tasks}).",
        "summarize_figure": "2505.03618v1_2025-01-08_-_Labeling_Workflow",
        "summarization": "The picture shows the cVIL labeling workflow, which consists of four phases. The initial \"Bootstrap Phase\" is instance - centric and solves the cold start problem by having users collect initial sample instances for each class. The central part involves two cyclically connected class - centric phases: the \"Class Selection Guidance\" phase determines the next class to focus on through human - machine collaboration, and the \"Class - Based Labeling\" phase has users label multiple recommended instances per class. The process ends with the \"Residual Labeling\" phase, which is instance - centric again, where users label remaining (unsolved or out - of - distribution) instances."
    },
    "435": {
        "figure1": "2505.03618v1_cVIL_labeling_status",
        "label1": "fig:cVIL_labeling_status",
        "caption1": " The Class Label View uses a stacked bar chart to assess the distribution of instances per class and label types, color-coded as Unlabeled (blue), Batch Labeled (orange), and Manually Labeled (green). Below, a minimized version of the bar chart shows the distribution for all classes (scalable for many dozens), with the current subset highlighted with gray area. In the example, the Butterfly class stands out with high instance count, while other classes have comparatively smaller sizes. The within-class assessments reveal differing proportions of label types; for example, the Octopus class has very few batch-labeled instances -- something the user may want to address next.",
        "text": "At the center of the interface is the Class Label View consisting of a stacked bar chart, as can be seen in Figure \\ref{fig:cVIL_labeling_status}. Below the main stacked bar chart, there is a range slider to adjust the data range being displayed, leading to a filter for most relevant instances. Users can gain an overview of the \\emph{Labeling Status} due to the color-encoding stacked bars that display the distribution of unlabeled (blue), batch labeled (orange), and manually labeled (green) instances for each class.  This view allows users to assess class-wise imbalances at a glance.  By clicking on a specific bar in the main focus bar chart, users can select a focus class for labeling, enabling a class-centric focus rather than instance-centric exploration.  By using bars to show classes, the view scales for more classes compared to color-coding of classes, outperforming traditional iVIL interfaces.  This approach is designed to enhance efficiency while encouraging users to address underrepresented classes, which is particularly important for the \\emph{Training Data Class Balance} task.  For example, when users observe that a specific class contains twice as many unlabeled instances as another, they are motivated to prioritize labeling for the underrepresented class, promoting an equitable distribution of instances across all classes.  Sorting helps users focus only on a few relevant classes, while others can be hidden so that even dozens to hundreds of classes could be shown in theory.  Users can easily switch between classes by clicking at the individual bars which is a particularly important for \\emph{Class Focus Guidance}.",
        "summarize_figure": "2505.03618v1_cVIL_labeling_status",
        "summarization": "The picture shows the class distribution by label type through a stacked bar chart, color - coded as Unlabeled (blue), Batch Labeled (orange), and Manually Labeled (green). The number of instances varies among classes, with the Butterfly class having a prominent count and others being relatively fewer. The proportions of label types within each class also differ, such as the Octopus class having very few batch - labeled instances. This chart helps users quickly assess class imbalances. By clicking on the bars, users can select a class of focus for labeling, improving efficiency and encouraging attention to under - labeled classes. Sorting can be used to focus on relevant classes."
    },
    "436": {
        "figure1": "2505.03622v1_Figure7b_10",
        "label1": "CMOS+APAM_shematic",
        "caption1": "(a) A schematic of an APAM cell integrated with a CMOS circuit acting as an optional resistor connected in series with the drain of an NMOS device. (b) Top view schematic of the APAM cell. Schematic cross sections showing the process used to selectively open up regions of silicon for APAM processing: (c) The chip after FEOL through implants, dielectrics, vias, silicides, and tungsten plugs. (d) Photoresist to define the dry etch for the APAM window opening and subsequent thin PECVD oxide deposition to protect the silicon prior to APAM. (e) Neon sputtering to remove the thin oxide and recrystallization of the silicon surface. (f) APAM doping with phosphorus and capping with epitaxial silicon. (g) BEOL to etch away the epitaxial silicon outside of the APAM window, and patterning and deposition of metal routing.",
        "text": "Throughout this work, we have defined the thermal budget of Sandia's 0.35~$\\mu$m CMOS process (\\textbf{Section \\ref{CMOS-budget}}), developed a modified APAM process to reduce the processing temperature for compatibility with the CMOS thermal budget (\\textbf{Section \\ref{APAM-prep}}), and established a patterning technique to integrate the modified APAM process with CMOS (\\textbf{Section \\ref{hardmask}}). In this section, we address the open question of whether APAM can be directly integrated with CMOS technology through three key demonstrations: 1) An APAM resistor (\\textbf{Figure~\\ref{CMOS+APAM_shematic}(a)}), a strip of phosphorous-doped silicon formed and patterned using our modified APAM process, shows decreased resistance compared to an APAM cell without delta doping, indicating success of our combined process and patterning; 2) The CMOS component on the same chip remains operational throughout the integration process, confirming APAM and CMOS compatibility; and 3) We can connect the APAM resistor from point (1) in series with the drain of the NMOS in point (2) using on-chip wiring, allowing us to measure the expected electrical behavior.\nFor this demonstration, we fabricated a CMOS die we refer to as the CMOS test platform (\\textbf{Section \\ref{CMOS-fab}}), which includes discrete process monitors and integrated transistors, using Sandia's 0.35~$\\mu m$ process.  These custom-designed chips were specifically tailored for a split fabrication process, where APAM is inserted at the MOL, to ensure compatibility with our modified APAM process and patterning and post-APAM metallization in a separate cleanroom environment, thereby establishing a proxy split fabrication process. As illustrated in \\textbf{Figure~\\ref{CMOS+APAM_shematic}(c)}, wafers were pulled from the SiFab within the Microsystems Engineering, Science and Applications (MESA) facility at Sandia, after the FEOL and contact formation. The wafer was then diced into die of 5 x 9~mm that contained the full CMOS test platform while meeting size restrictions of the tool used for our modified APAM process. Select pieces from the wafer then received a modified APAM process, developed in \\textbf{Section \\ref{hardmask}}, at the MOL to facilitate integration. The MOL processing focused on a special-purpose cell that was  included to facilitate the creation of an APAM resistor connecting two implanted regions, which we term an APAM cell. Within an APAM cell, a window was opened in the oxide + nitride hardmask using RIE to expose the silicon above and between the implants (\\textbf{Figure~\\ref{CMOS+APAM_shematic}(d)}). The APAM sputter processing was performed (\\textbf{Figure~\\ref{CMOS+APAM_shematic}(e)}), then the sample was optionally dosed with phosphine for doping, followed by epitaxy of a silicon cap (\\textbf{Figure~\\ref{CMOS+APAM_shematic}(f)}). The epitaxial silicon was a blanket deposition across a chip and was removed using photolithography and a plasma etch, leaving it only within the vicinity of the APAM cell. Finally, the BEOL metallization was performed in a separate Sandia facility (\\textbf{Figure~\\ref{CMOS+APAM_shematic}(g)}).\nBeyond confirming APAM can be incorporated into a CMOS chip, we demonstrate that the CMOS components remain fully functional. First, in \\textbf{Figure~\\ref{CMOS+APAM_data}(b)}, we present the transfer characteristics for a control NMOS device with and without APAM processing. The APAM process may result in a slight increase in turn-on voltage and decrease in output current. This demonstrates that the CMOS chip can withstand the modified APAM processing and patterning. We then take an NMOS device and integrate the characterized APAM wire using on-chip wiring (\\textbf{Figure~\\ref{CMOS+APAM_shematic}(a,b)}). We note that in \\textbf{Figure~\\ref{CMOS+APAM_data}(c)} and \\textbf{Figure~\\ref{CMOS+APAM_data}(d)}, the APAM resistor was connected in series with the drain for all measurements; however, bond pads provided direct access to the drain (black dot - Metal Routing) or connection through the APAM in series with the drain (red dot - APAM in Series w/ Drain). The transfer characteristics after integration wiring are shown in \\textbf{Figure~\\ref{CMOS+APAM_data}(c)}. From measurements taken using the Metal Routing bond pad (black line), we can extract a threshold voltage, taken to be the \\textit{x}-axis intercept of the linear region, of 1.62~V. Further, we extract a subthreshold swing, taken to be the average inverse of logarithmic rate of device turn on, of ~89.2 mV~dec$^{-1}$. The threshold voltage and subthreshold swing confirm enhancement-mode behavior and effective electrostatic control of the channel at the oxide interface, respectively. The output characteristics, shown as symbols in \\textbf{Figure~\\ref{CMOS+APAM_data}(d)}, have linear behavior at low drain bias before saturating at higher bias. Linear behavior at low drain bias confirms the contacts to the NMOS device remain ohmic.",
        "summarize_figure": "2505.03622v1_Figure7b_10",
        "summarization": "The picture shows the integration of APAM (Atomic Planar Doped Metal) with CMOS. (a) is a schematic of an APAM cell integrated with a CMOS circuit, where the APAM cell acts as an optional resistor in series with the drain of an NMOS device. (b) is a top - view schematic of the APAM cell. (c - g) are schematic diagrams of the process steps for APAM processing, including the chip structure after front - end - of - line (FEOL) processes (with implants, dielectrics, vias, silicides, and tungsten plugs); using photoresist to define the dry etch for the APAM window and depositing a thin PECVD oxide to protect the silicon; neon sputtering to remove the thin oxide and recrystallize the silicon surface; APAM doping with phosphorus and capping with epitaxial silicon; and back - end - of - line (BEOL) processes to etch away the epitaxial silicon outside the APAM window and pattern and deposit metal routing. It shows the process and structure of APAM - CMOS integration."
    },
    "437": {
        "figure1": "2505.03622v1_Fig8d-IdVd",
        "label1": "CMOS+APAM_data",
        "caption1": "(a) Resistance of an APAM cell resistor receiving all process steps without (red line) and with (black line) dopant dosing as a function of measurement temperature. A single point shows the room temperature resistance of a shadowed APAM window (black circle). (b) Transfer characteristics ($I_{D}-V_{GS}$) of a control NMOS without (black) and with (red) APAM processing. (c) Transfer characteristics ($I_{D}-V_{GS}$) and (d) output characteristics ($I_{D}-V_{DS}$) of the same NMOS device measured under two conditions: only metal routing (circles) and with an APAM resistor routed on-chip in series with the drain (lines). (b), (c), and (d) were taken at room temperature with (b) $V_{DS}$ = 3.3~V, (c) $V_{DS}$ = 1~V, and (d) $V_{GS}$ from 1.2~V to 3~V in steps of 0.6~V.",
        "text": "With the test platform established, we proceeded to validate that the modified APAM process and patterning remain effective when integrated with Sandia's 0.35~$\\mu m$ CMOS process. In \\textbf{Figure~\\ref{CMOS+APAM_data}(a)} we present the resistance of APAM cell windows, having at least partial openings, without doping, and with delta doping as a function of measurement temperature. The delta-doped sample exhibits a room temperature resistance of approximately $11.5~k\\Omega$. With the geometry of the implants contacting the APAM doping and the geometry of the APAM cell window, we expect the resistor to be between 3 and 1 squares yielding a sheet resistance of 3.83 k$\\Omega/sq$ to 11.5 k$\\Omega/sq$. We note that this sheet resistance is greater than the $1/ne\\mu$ = 2.2~k$\\Omega/sq$ (where $n$ is the carrier density, $e$ is the electron charge, and $\\mu$ is the mobility) expected from thick hardmask-defined samples measured at cryogenic temperatures in \\textbf{Section \\ref{hardmask}}. Three factors might contribute to this discrepancy. First, the resistivity is expected to rise between cryogenic and room temperature. Flash cleaned samples show a factor of two change in resistivity from cryogenic temperatures to room temperature. \\cite{mazzola2014disentangling} Unfortunately, leakage currents become significant in our Hall bar samples at elevated temperatures, precluding us from measuring them at room temperature. Second, this measurement was a two point measurement and includes contributions from parasitic and series resistance components (e.g. contact resistance), while the previous were four point measurements that factor out series resistance. Finally, it is equally likely that there are further issues with patterning and dosing the small APAM cell window, as discussed in \\textbf{Section \\ref{hardmask}}, leading to other electrical effects. For example, as temperature decreases, the current-voltage curves became non-linear at around 100~K, possibly indicating the contacts are a source of a temperature-dependent parasitic series resistance due to a Schottky junction. In all cases, the resistance can likely be improved through process optimizations, including repeating the doping and capping process to produce multilayers. \\cite{keizer2015multilayer}\nTo confirm the impact of APAM doping, we compare an APAM cell with and without doping and  investigate the resistance of both as a function of temperature (\\textbf{Figure~\\ref{CMOS+APAM_data}(a)}). The room-temperature resistance of the undoped APAM cell is nearly five orders of magnitude greater than the doped cell, indicating the APAM doping significantly reduced resistance. Furthermore, the resistance of the undoped APAM cell rises sharply as temperature decreases, while the doped cell remains relatively flat indicating doping near or above the metal to insulator transition. The latter is consistent with relatively modest temperature dependence shown earlier in four-point measurements.\\cite{mazzola2014disentangling} The undoped APAM cell forms two back-to-back pn junctions,  between the n-type implants, the p-type device layer, and the unintentionally doped p-type APAM epitaxial cap. \\cite{anderson2020al-cap} In this case, there will always be a reverse biased pn junction leading to low current.  Here, we make an important note that design considerations are needed to avoid making a forward biased pn junction between a doped APAM cell and the substrate, and to keep the resistance of the APAM cell relative to other device terminals in mind during circuit design. For example, if an undoped APAM cell is in series with the gate of a transistor device, unpredictable transistor and circuit behavior will result from having a gate which is nearly open circuit. Finally, we show that the APAM wire does not work when part of the processing failed. In \\textbf{Section \\ref{hardmask}} we demonstrated that shadowing should be a consideration when using this process for APAM patterning. We measured an APAM wire that was completely shadowed (i.e. the APAM cell has no exposed Si) at room temperature and observe a similar resistance to the undoped APAM cell, \\textbf{Figure~\\ref{CMOS+APAM_data}(a)}.\nBeyond confirming APAM can be incorporated into a CMOS chip, we demonstrate that the CMOS components remain fully functional. First, in \\textbf{Figure~\\ref{CMOS+APAM_data}(b)}, we present the transfer characteristics for a control NMOS device with and without APAM processing. The APAM process may result in a slight increase in turn-on voltage and decrease in output current. This demonstrates that the CMOS chip can withstand the modified APAM processing and patterning. We then take an NMOS device and integrate the characterized APAM wire using on-chip wiring (\\textbf{Figure~\\ref{CMOS+APAM_shematic}(a,b)}). We note that in \\textbf{Figure~\\ref{CMOS+APAM_data}(c)} and \\textbf{Figure~\\ref{CMOS+APAM_data}(d)}, the APAM resistor was connected in series with the drain for all measurements; however, bond pads provided direct access to the drain (black dot - Metal Routing) or connection through the APAM in series with the drain (red dot - APAM in Series w/ Drain). The transfer characteristics after integration wiring are shown in \\textbf{Figure~\\ref{CMOS+APAM_data}(c)}. From measurements taken using the Metal Routing bond pad (black line), we can extract a threshold voltage, taken to be the \\textit{x}-axis intercept of the linear region, of 1.62~V. Further, we extract a subthreshold swing, taken to be the average inverse of logarithmic rate of device turn on, of ~89.2 mV~dec$^{-1}$. The threshold voltage and subthreshold swing confirm enhancement-mode behavior and effective electrostatic control of the channel at the oxide interface, respectively. The output characteristics, shown as symbols in \\textbf{Figure~\\ref{CMOS+APAM_data}(d)}, have linear behavior at low drain bias before saturating at higher bias. Linear behavior at low drain bias confirms the contacts to the NMOS device remain ohmic.\nFinally, we characterize the NMOS with the APAM resistor connected in series with the drain using the APAM in Series with Drain bond pad (red dot). The turn-on behavior in \\textbf{Figure~\\ref{CMOS+APAM_data}(c), red line,} remains consistent for both with the only change being a flattening of the drain current at a gate bias of $\\sim$1.5~V with the APAM resistor. This maximum drain current indicates when the resistance of the APAM resistor exceeds the channel resistance. We can confirm this by taking the slope of the output characteristics at a gate bias of 3~V (\\textbf{Figure~\\ref{CMOS+APAM_data}(d)}, red line) which gives a resistance of approximately 13~k$\\Omega$. The majority of the 13~k$\\Omega$ can be accounted for by the APAM resistor with the rest coming from the NMOS and associated parasitics. At a lower gate bias of 1.8~V (blue line), the drain current goes from being limited by the APAM resistor to being limited by saturation in the channel above 2~V. Finally, at an even lower gate bias of 1.2~V, the drain current is limited by saturation in the channel for much of the range of drain bias. This successful integration and understanding highlights the compatibility of our modified APAM process with CMOS to exploit CMOS + APAM for future microelectronic applications.",
        "summarize_figure": "2505.03622v1_Fig8d-IdVd",
        "summarization": "This graph shows the relationship between drain current (vertical axis, in mA) and drain voltage (horizontal axis, in V). Different - colored dots and lines represent cases under different gate voltages (Vg = 1.2V, 1.8V, 2.4V, 3.0V). Dots denote metal routing, and lines denote APAM in series with the drain. As drain voltage increases, drain current first rises and saturates at high bias. Linear behavior at low drain bias indicates ohmic contacts of the NMOS device. The growth trends and saturation values of current vary with different gate voltages."
    },
    "438": {
        "figure1": "2505.03622v1_Figure1_17",
        "label1": "ProcessFlows",
        "caption1": "The three basic parts of the APAM workflow include (a) removal of protective dielectrics (b) patterned dopant incorporation and (c) silicon encapsulation. (a) is typically accomplished with a high temperature anneal. (b) is typically accomplished using hydrogen lithography and exposure to a dopant precursor like phosphine. (c) is typically accomplished using low temperature molecular beam epitaxy. The three basic parts of a CMOS workflow include (d) front end of line (FEOL) for high temperature processing steps (e) middle of line (MOL) for intermediate temperature processing steps and (f) back end of line (BEOL) for low temperature processing steps. Sandia's legacy 0.35 $\\mu $m CMOS process incorporates oxides and nitrides (clear), polysilicon gates (green), and ohmic implants (red) in (d), vias and plugs (orange) in (e), and metallization (yellow) in (f). (g) Shows an integrated device with an APAM resistor attached to the drain of an NMOS transistor. The temperatures required for thermal compatibility are noted. ",
        "text": "APAM uses area-selective attachment of dopant precursor molecules to produce atomically abrupt and strongly doped planar shapes, referred to as delta layers. A simplified illustration of the process flow is shown in \\textbf{Figure \\ref{ProcessFlows}}. A scanning tunneling microscope (STM) removes hydrogen from the cleaned and passivated Si(100) surface, where dopant precursors selectively attach. \\cite{lyding1994nanoscale,tucker1998prospects} These areas are then capped with epitaxial silicon to activate the dopants. This process has been demonstrated down to the single atom level \\cite{fuechsle2012single} to produce spin qubits, where individual dopants are serving to confine electrons into a 0D structure. Many aspirational applications of APAM do not require single-atom templates, but only high dopant densities. For example, the abrupt profiles produce confinement that can be useful in exploring quantum tunneling in devices like an areal tunnel field effect transistor, a low power switch which conducts when electrons confined to the buried 2D sheet can tunnel to a gated hole layer at the surface. \\cite{lu2021path,gao2021vtfet} Other researchers' early attempts at producing three dimensional profiles have achieved an active density two to three times higher \\cite{keizer2015multilayer} than the state-of-the-art, \\cite{ye2020ultrahigh,current2017implant} with near unity activation. This could mitigate the growing resistance of contacts in scaled transistors. Both of these examples, one seeking to discover novel devices and the other to enhance conventional transistors, would benefit from direct integration with high-volume CMOS manufacturing.\nTwo limitations prevent the direct integration of APAM and CMOS, both of which we show simplified process flows for in \\textbf{Figure \\ref{ProcessFlows}}. The first arises from their thermal incompatibility. At a high level, CMOS manufacturing generally orders processes such that the highest temperature steps occur first, and later steps limit their maximum temperature and time to prevent alteration of existing structures. APAM typically uses a $\\sim$$1000~^\\circ$C\\cite{lyding1994nanoscale} vacuum anneal to remove the surface dielectric and produce an atomically ordered surface to template surface chemistry. However, the dopant profiles APAM produces begin diffusing above $500~^\\circ $C,\\cite{oberbeck2004segregation} implying that the earliest point APAM can be inserted is the very end of the front-end-of-line (FEOL), in which transistors are formed. Unfortunately, subjecting the mostly formed CMOS transistors to such high temperatures changes their electrical behavior. Here, we identify a lower thermal budget process that produces atomically ordered surfaces and does not change the electrical behavior of the CMOS transistors. The second limitation arises from the incompatibility of hydrogen lithography with CMOS manufacturing. With hydrogen lithography, chemical precursors need to bind with reactive sites exposed by depassivation before they become contaminated, precluding performing photolithography and processing in separate tools. Instead of hydrogen lithography, we will leverage the polymer based resists used by the rest of CMOS manufacturing in the APAM process. An earlier effort circumvented both incompatibilities through heterogeneous integration (HI) of a chip with atomic-scale APAM devices and another with CMOS elements.\\cite{skeren2018integration} However, problems with alignment and interfacial defects preclude the ability to have a device span both wafers, so HI loses the ability to introduce APAM elements directly into CMOS transistors or accelerate the discovery of new devices that combine CMOS and APAM elements. Here, we will focus on direct integration precisely to keep both of these latter goals in focus.\nIn this work, we formulate an APAM process which addresses both these thermal and lithographic incompatibilities, and then perform a proof-of-principle direct integration of APAM and Sandia's legacy 0.35~$\\mu m$ CMOS process. We start by pinpointing where in Sandia's CMOS process flow an APAM module can be inserted, and determine a thermal budget which preserves the electrical characteristics of the transistors and of the APAM material in \\textbf{Section \\ref{CMOS-budget}}. Next, we electrically characterize APAM-doped material produced by removing protective dielectrics from the surface of silicon using ion sputtering and annealing within this thermal budget to produce a crystalline surface in \\textbf{Section \\ref{APAM-prep}}. We then use this sputter and anneal process in combination with a dielectric hardmask to produce photolithography-compatible APAM-doped material with an activated density and resistivity comparable to the original process in \\textbf{Section \\ref{hardmask}}. Individually, each of these steps builds on procedures well-known to the surface science and microfabrication communities they come from, but we combine them in \\textbf{Section \\ref{APAM+CMOS}} into a cohesive APAM module to produce devices using a flow that inserts the APAM module in between FEOL and back-end-of-line (BEOL). We electrically characterize working CMOS transistors, APAM resistors, and a simple circuit where both work together, illustrated in \\textbf{Figure \\ref{ProcessFlows}(g)}. This work opens the door to the development of an integrated tool that performs the three key APAM steps - low-temperature surface cleaning, chemical doping, and epitaxial growth - enabling APAM in a cleanroom environment for more accessible device research and development.",
        "summarize_figure": "2505.03622v1_Figure1_17",
        "summarization": "The picture shows the process flows of APAM and CMOS. The APAM process includes three basic parts: (a) oxide removal, traditionally at 1000°C and in this work at 600°C; (b) dopant patterning, traditionally using hydrogen lithography and in this work using photolithography; (c) silicon capping at 500°C. The CMOS process includes front - end - of - line (FEOL, 1000°C, with ion implants and poly gate), middle - of - line (MOL, 800°C, with via and plug), and back - end - of - line (BEOL, 500°C, with metallization). Figure (g) shows a device with an APAM resistor connected to the drain of an NMOS transistor, emphasizing the temperatures required for thermal compatibility and reflecting the process steps and thermal management key points of APAM - CMOS integration."
    },
    "439": {
        "figure1": "2505.03625v1_al991_3",
        "label1": "fig:1",
        "caption1": "The numerical reconstruction $U_{h,\\gamma}^{0,\\delta}$  for  $T=1$ with different $\\alpha$ and $\\delta$.",
        "text": "Finally, to illustrate the significant difference between the classical diffusion and the subdiffusion, we test several numerical experiments with the nonlinear term $$f(u) = u - u^3$$ and the piecewise constant initial data \\eqref{eqn:nonsm}. First, we fix the terminal time $T = 1$ and examine the influence of the fractional order $\\alpha$ on the reconstruction of the initial data.  In Figure \\ref{fig:1}, we test the reconstruction of the initial data $ U_{h,\\gamma}^{0,\\delta}$ for $\\alpha = 0.9, 0.99,$ and $ \\delta = 10^{-3}, 5\\times10^{-4}, 2\\times10^{-4}$. As expected, recovering the initial data becomes increasingly difficult as $ \\alpha $ approaches 1.\nWe also examine the more interesting case of a relatively large terminal time, e.g. $T = 10$, in our computation. As shown in Figure \\ref{fig:2}, for $\\alpha = 0.9$, we still observe a reasonable reconstruction; however, it is less accurate compared to the reconstruction for a shorter terminal time $T = 1$ (cf. Figure \\ref{fig:1}). Moreover, as $\\alpha$ approaches one, the numerical recovery of the initial condition becomes increasingly challenging; for example, see case $\\alpha = 0.99$ in Figure \\ref{fig:2}.   In particular, for $\\alpha = 1$, even with a very small noise level and a small terminal time $T$, accurately capturing the correct profile of the initial data becomes extremely difficult due to the severe ill-posedness of the parabolic backward problem, as illustrated in Figure \\ref{fig:3}. This highlights the fundamentally different ill-posed nature of the subdiffusion model compared to the classical diffusion model.",
        "summarize_figure": "2505.03625v1_al991_3",
        "summarization": "The graph shows the distribution of the numerical reconstruction U h,γ 0,δ with different α (taking values 0.9, 0.99) and δ (taking values 10⁻³, 5×10⁻⁴, 2×10⁻⁴) when the terminal time T = 1. The numerical values are represented by color filling, changing from blue to red, with values ranging from 0 to 1. It can be seen that as α approaches 1, it becomes increasingly difficult to recover the initial data, reflecting the influence of the fractional - order α on the reconstruction of the initial data."
    },
    "440": {
        "figure1": "2505.03625v1_al9910_3",
        "label1": "fig:2",
        "caption1": "The numerical reconstruction $U_{h,\\gamma}^{0,\\delta}$ for large $T=10$ with different $\\alpha$ and $\\delta$.",
        "text": "We also examine the more interesting case of a relatively large terminal time, e.g. $T = 10$, in our computation. As shown in Figure \\ref{fig:2}, for $\\alpha = 0.9$, we still observe a reasonable reconstruction; however, it is less accurate compared to the reconstruction for a shorter terminal time $T = 1$ (cf. Figure \\ref{fig:1}). Moreover, as $\\alpha$ approaches one, the numerical recovery of the initial condition becomes increasingly challenging; for example, see case $\\alpha = 0.99$ in Figure \\ref{fig:2}.   In particular, for $\\alpha = 1$, even with a very small noise level and a small terminal time $T$, accurately capturing the correct profile of the initial data becomes extremely difficult due to the severe ill-posedness of the parabolic backward problem, as illustrated in Figure \\ref{fig:3}. This highlights the fundamentally different ill-posed nature of the subdiffusion model compared to the classical diffusion model.",
        "summarize_figure": "2505.03625v1_al9910_3",
        "summarization": "The graph shows the distribution of the numerical reconstruction U h,γ 0,δ with different values of α and δ when the terminal time T = 10. The numerical values are represented by color filling, changing from blue to red, corresponding to values from 0 to 1. When α = 0.9, a relatively reasonable reconstruction can still be observed, but it is less accurate compared to the reconstruction when the terminal time T = 1. As α approaches 1, the numerical recovery of the initial condition becomes increasingly difficult."
    },
    "441": {
        "figure1": "2505.03625v1_al01_3",
        "label1": "fig:3",
        "caption1": "The numerical reconstruction $U_{h,\\gamma}^{0,\\delta}$  for  $T=0.1$ with $\\alpha=1$ and different $\\delta$.",
        "text": "We also examine the more interesting case of a relatively large terminal time, e.g. $T = 10$, in our computation. As shown in Figure \\ref{fig:2}, for $\\alpha = 0.9$, we still observe a reasonable reconstruction; however, it is less accurate compared to the reconstruction for a shorter terminal time $T = 1$ (cf. Figure \\ref{fig:1}). Moreover, as $\\alpha$ approaches one, the numerical recovery of the initial condition becomes increasingly challenging; for example, see case $\\alpha = 0.99$ in Figure \\ref{fig:2}.   In particular, for $\\alpha = 1$, even with a very small noise level and a small terminal time $T$, accurately capturing the correct profile of the initial data becomes extremely difficult due to the severe ill-posedness of the parabolic backward problem, as illustrated in Figure \\ref{fig:3}. This highlights the fundamentally different ill-posed nature of the subdiffusion model compared to the classical diffusion model.",
        "summarize_figure": "2505.03625v1_al01_3",
        "summarization": "The graph presents the distribution of the numerical reconstruction U h,γ 0,δ with T = 0.1, α = 1, and different δ values. Color filling is used to represent the numerical values, which range from 0 to 1 with a gradient from blue to red. It can be seen that due to the severe ill - posedness of the parabolic backward problem, accurately capturing the correct profile of the initial data is extremely difficult even with a very small noise level and a short terminal time, highlighting the fundamental difference in the ill - posed nature between the subdiffusion model and the classical diffusion model."
    },
    "442": {
        "figure1": "2505.03629v1_general_scheme_new_v2",
        "label1": "fig:general",
        "caption1": "An overview of our DNA Tail framework. \\textbf{(a)} Schematic of Tail-length encoding, showing the locations were tails can be grown according to the  natural order on the DNA backbone. \\textbf{(b)} Schematic of the general multi-round, nonbinary approach for recording information in DNA tails (i.e., single-stranded DNA fragments enzymatically synthesized on double-stranded substrates). For low-cost, we use native restriction endonucleases for nicking and the TdT polymerase for tail growths. \\textbf{(c,d)} Schematics of \\emph{rank modulation} for tail and cell ``charges.''} \\vspace{-0.25in",
        "text": "The gist of our approach is to encode nonbinary symbols (labels) using different lengths of single-stranded DNA strings grown at specific nicking (cutting) sites of double-stranded DNA. The sites at which tails are grown are termed nicking sites, while the overall storage paradigm is henceforth referred to as DNA Tails. For DNA, the sugar-phosphate backbone locations naturally serves as a linear order for the encoded symbols.  Following this idea, we designed and implemented a DNA Tail scheme as depicted in Figure~\\ref{fig:general} (a), where the tails are single-stranded DNA fragments enzymatically synthesized on double-stranded substrates. The writing process consists of several rounds of enzymatic nicking at preselected locations (indicated by light green crosses on the DNA duplexes and marked by $0$s in the top row of Figure~\\ref{fig:general} (a)) and ``labeling.'' The results are tails whose different random lengths represent different symbols of a large coding alphabet\\footnote{Note that, as for sequence-based encoding, pools of $100$s of DNA strings encode the same information; since in our case, the tail lengths are random variables, we work with the average tail lengths for each designated nicking location. Furthermore, we quantize the tail lengths in order to allow for a range of tail-length values to represent the same symbol}. Label $0$ stands for an undisturbed location (no nick, nor tail), label $1$ stands for a nicked location without a tail, while all larger labels (e.g., $2$-$7$) correspond to average lengths of the DNA tails of different lengths. The smaller the label, the shorter the average length of the tail. To control the relative average tail lengths, we partition the locations of the tails to be synthesized according to the value of the label, in decreasing order. For example, to ``$70216745$'' at consecutive preselected locations, we start with the two locations that are to store $7$. These are the first locations that we nick, as indicated in the second row of Figure~\\ref{fig:general} (b). Upon this first nicking round, DNA tails are grown under controlled conditions, leading to relatively short tail lengths. The locations where the symbol $6$ appears are nicked next, followed by enzymatic synthesis at all ``exposed'' sites -- i.e., those that are nicked and those that contain tails. Since the sites corresponding to label $7$ are subject to two rounds of enzymatic synthesis, their lengths are expected, on average, to be longer than those of label $6$, as illustrated in the fifth row of the figure. Proceeding, we arrive at the construct in the sixth row in which the tails are of different lengths proportional to the symbol value to be stored. %Note that our encoding procedure involves a staggered nicking-tail extension process, as in this case, it is easier for us to control the relative average tail lengths extended at each nicking site, compared to directly extending the tails to desired lengths.\nHowever, as will become evident from the experiments, relying on the exact values of average tail lengths to determine the encoded symbol is often infeasible due to calibration errors (i.e., not knowing very precisely which tail length rages correspond to which symbols). On the other hand, the staggered nicking-tail extension process naturally guarantees that the nicked sites exposed to the most tail extension rounds will have the longest DNA tails with high probability. This motivates us to use relative ordering of the average tail lengths rather than their values, akin to rank modulation~\\cite{jiang2009rank,farnoud2012rank},  illustrated in Figure~\\ref{fig:general} (c,d). There, to avoid absolute errors,  the exact values are replaced by rank-ordered symbols indicating the largest, second largest, third largest, etc., charge or tail length. Even after charge leakage of all cells or equally reduced tail growths, one still expects the relative order to be preserved. To understand how to implement a rank modulation-like encoding with DNA Tails, one can think of replacing electrons and cells with bases and nicking sites, in which case each average tail length has to be sufficiently different from any other. This makes the recording process less susceptible to errors encountered in the general scheme but at the cost of an increased number of nicking-labeling rounds. For the general scheme, the number of rounds equals the value of the largest label, while for rank modulation scheme, the number of rounds equals the number of distinct nonzero labels.\nWe used the Tail encoding technique to encode real information in different contexts. Specifically, we illustrate an example of topological tail encoding of metadata equal to the number $20$ on the backbone of synthetic DNA image of Novak Djokovic playing tennis shown in Figure~\\ref{fig:rank} (a) to indicate the number of Grand Slam single titles he won until 2021, and the metadata $5030$ into the image of a beach in Uruguay shown in Figure~\\ref{fig:rank} (a) to indicate the country's world cup championship years (1930, 1950). We used IDT gBlocks of length $1,000$ bps to record the image content of these two images; metadata is recorded via the general scheme in Figure~\\ref{fig:general} (a). The images were first compressed using JPEG, parsed into blocks of length $35$ bits each, and then mapped to DNA sequences of length $19$ nts. The redundant $1.5$ bits per block ensure balanced $GC$ content ($45\\%-55\\%$) and eliminate homopolymers of length $\\geq 3$ nts. To enable random access to different images, we also included pairs of unique prefix and suffix primer sequences for each of the images. Furthermore, to indicate the order of the sequences within the image, we use address blocks of length $3$ nts. We also added $7$ random bases at predefined locations to lower IDT ``synthesis complexity.''",
        "summarize_figure": "2505.03629v1_general_scheme_new_v2",
        "summarization": "The graph is an overview of the DNA Tail framework. (a) shows the schematic of tail - length encoding, indicating that DNA tails can be grown at specific positions according to the natural order of the DNA backbone to encode data. (b) presents the general multi - round, non - binary scheme for recording information. Through enzymatic nicking (nicking sites marked as 0) and labeling, DNA tails of different random lengths represent different symbols. (c) and (d) are schematics of rank modulation for tails and cell “charges”. Relative ordering is used instead of specific tail - length values to avoid absolute errors, ensuring the relative order is maintained even with charge leakage or reduced tail growth."
    },
    "443": {
        "figure1": "2505.03629v1_third_exp_new_v2",
        "label1": "fig:rank",
        "caption1": "\\textbf{(a)} Schematic of the image encoding procedure, explained in the main text. \\textbf{(b)} Results of recording a signature DNA tail $20$ on the first synthetic DNA image. \\textbf{(c)} Matching results for gBlocks encoding the right image in (a), with the superimposed value $5030$. (d) Rank modulation experiments on the encoding of the poem \\emph{A Dream Within a Dream} by E. A. Poe using three gBlocks. \\textbf{(e)} Rank modulation errors.",
        "text": "We used the Tail encoding technique to encode real information in different contexts. Specifically, we illustrate an example of topological tail encoding of metadata equal to the number $20$ on the backbone of synthetic DNA image of Novak Djokovic playing tennis shown in Figure~\\ref{fig:rank} (a) to indicate the number of Grand Slam single titles he won until 2021, and the metadata $5030$ into the image of a beach in Uruguay shown in Figure~\\ref{fig:rank} (a) to indicate the country's world cup championship years (1930, 1950). We used IDT gBlocks of length $1,000$ bps to record the image content of these two images; metadata is recorded via the general scheme in Figure~\\ref{fig:general} (a). The images were first compressed using JPEG, parsed into blocks of length $35$ bits each, and then mapped to DNA sequences of length $19$ nts. The redundant $1.5$ bits per block ensure balanced $GC$ content ($45\\%-55\\%$) and eliminate homopolymers of length $\\geq 3$ nts. To enable random access to different images, we also included pairs of unique prefix and suffix primer sequences for each of the images. Furthermore, to indicate the order of the sequences within the image, we use address blocks of length $3$ nts. We also added $7$ random bases at predefined locations to lower IDT ``synthesis complexity.''\nOur experimental results are depicted in Figure~\\ref{fig:rank}. In \\textbf{(b)}, we show the results of recording a signature DNA tail $20$ on a synthetic DNA image on the right in (a). The value $20$ is nicked into gBlocks by using a combination of two nicking enzymes, Nb.BtsI and Nb.BssSI. To determine how to decode the tail lengths to label values, we performed extensive calibration experiments. The plot summarizes the relationship between the average tail length and the corresponding label for up to $6$ cycles of tail extensions (with $r^2$ denoting the squared fitting error). For an average tail length of $18.16$ as shown in the left plot, the fitted calibration model indicates a corresponding label of $2.46$, marked in red. Since $2.46$ is closer to $2$ than $3$, the label is decoded as $2$. The label $0$ can be perfectly recovered, as it corresponds to the absence of any modifications. In \\textbf{(b)}, we provide matching results for gBlocks encoding the right image in (a), with the superimposed value $5030$. Encoding is performed using a combination of four nicking enzymes, Nb.BsmI, Nt.Bpu10I, Nb.BsrDI, and Nb.BssSI. In this case, label $5$ is erroneously read as $6$, while label $3$ is erroneously read as $4$. This further motivates the use of \\emph{rank modulation} coding which only requires that the average lengths of the tails be rank-ordered with a sufficiently large difference in values. (d) Rank modulation experiments on the encoding of the poem \\emph{A Dream Within a Dream} by E. A. Poe using three gBlocks, with a topologically encoded book ISBN numbers $4570015$ in Poem-GBlock 1, cipher $5010126054$ (Poem-GBlock 2), and $0040721$ (Poem-GBlock 3). The characters of the poem were first converted to binary sequences in ASCII format, parsed into blocks, and mapped to DNA sequences. We note that all rankings of tail lengths (\\textbf{(e)}) are consistent with the magnitude of the label, except in Poem-GBlock 1. There, the tail length corresponding to label $7$ ($498.2$) is unacceptably short, falling within the range of lengths designated for label $5$ ($459.07\\sim 501.5$). No such inconsistencies are observed in the other two gBlocks. The identified errors suggest that it is possible for some tails to stop growing even in the rank modulation setting, and such errors are studied in the theoretical analysis to follow.",
        "summarize_figure": "2505.03629v1_third_exp_new_v2",
        "summarization": "The graph shows the experimental results of the DNA Tail encoding technique in different contexts. (a) is a schematic of the image encoding procedure, showing the process of compressing an image with JPEG, parsing it into binary sequences, mapping them to DNA sequences, and recording the image content with gBlocks. (b) shows the results of recording a signature DNA tail 20 on a synthetic DNA image, achieved through two types of enzymatic nicking. Calibration experiments determine the relationship between tail length and label value for decoding. (c) presents the matching results for gBlocks encoding an image with the superimposed value 5030, with label misreading. (d) is a rank modulation encoding experiment of a poem using three gBlocks. Except for the abnormally short tail length corresponding to label 7 in Poem - GBlock 1, the rankings of tail lengths match the label magnitudes."
    },
    "444": {
        "figure1": "2505.03639v1_example",
        "label1": "fig:example",
        "caption1": "An example graph and its assortativity coefficient.",
        "text": "Nowadays, network analysis plays a crucial role in understanding a great variety of complex systems such as social networks \\cite{cinelli2021echo}, transportation networks \\cite{ganin2017resilience}, biological networks \\cite{gosak2018network}, and so on. It is well known that network analysis is often performed using various measures including degree distribution, clustering coefficient, diameter, and assortativity coefficient, to name but a few \\cite{bhattacharya2020impact}. Among them, the assortativity coefficient (see subsection 3.2 for more details) \\cite{newman2002assortative} is used to reflect the tendency of nodes to connect to other nodes with similar attributes or characteristics in networks under consideration, and has received increasing attention from various science communities \\cite{noldus2015assortativity}. One of the main reasons for this is that the assortativity coefficient, as a fundamental measure, plays a key role in understanding the structural characteristics of and dynamics upon networks. Figure \\ref{fig:example} shows an example of assortativity (see Eq.\\ref{eq:3-2-1} for specific expression). Hereinafter, two terms network and graph are used interchangeably.",
        "summarize_figure": "2505.03639v1_example",
        "summarization": "The graph shows a graph G = (V, E) with 7 nodes (v1 - v7) and several edges connecting the nodes. The nodes of the graph are linked by lines, forming a specific network structure. The assortativity coefficient of the graph is marked as 0.375 below it. The assortativity coefficient is used to reflect the tendency of nodes in a network to connect with nodes of similar attributes or characteristics, and it is an important indicator for understanding the structural characteristics and dynamic changes of the network in network analysis."
    },
    "445": {
        "figure1": "2505.03649v1_wrdpg_estimation",
        "label1": "fig:wrdpg_estimation_blocks",
        "caption1": "Latent position estimation. Given an adjacency matrix $\\bbA$ we compute its $k$-th entry-wise power $\\bbA^{(k)}$. The ASE of $\\bbA^{(k)}$ yields the estimates $\\hbX[k]$; see also Section \\ref{ssec:estimation}.",
        "text": "Furthermore, framing our model in terms of (symmetric) adjacency matrices allows us to define latent position estimators based on their spectral decomposition; see Section \\ref{ssec:estimation}. Statistical guarantees can be derived using tools to control the matrix spectrum under perturbations. As shown in \\cite{marenco2021tsipn}, for each fixed moment index $k$, the latent position matrix $\\bbX[k]$ can be consistently recovered (up to an orthogonal transformation) via the ASE of $\\bbW^{(k)}$, the matrix of entry-wise $k$-th powers of $\\bbW$; see Figure \\ref{fig:wrdpg_estimation_blocks} for a schematic diagram of the latent position estimation procedure. In Section \\ref{sec:asymptotic_results} we generalize the result in~\\cite{marenco2021tsipn} by proving that consistency holds for a stronger convergence metric that ensures uniform convergence for all $\\hbx_i$'s. We also establish that the estimator is asymptotically normal  as $N\\to\\infty$, a result that is novel in our WRDPG setup. %\\blue{\\textbf{[GM: Tenemos algo para decir en lo que refiere a nuestros resultados versus la contraparte en~\\cite{gallagher2023spectral}? Son comparables, somos mas generales en algun sentido?]}} %Efficient algorithms for calculating spectral embeddings in both fixed and dynamic scenarios have been also developed \\cite{fiori2023gradient}.",
        "summarize_figure": "2505.03649v1_wrdpg_estimation",
        "summarization": "The graph shows the procedure of latent position estimation. Given an adjacency matrix, first, the entry - wise power operation (ENTRY - WISE POWER) is performed to calculate its k-th entry - wise power. Then, through adjacency spectral embedding (ADJACENCY SPECTRAL EMBEDDING), the estimates are obtained. The right side of the graph visualizes the estimation results at different stages (from [1] to [4]), reflecting the process and status of latent position estimation."
    },
    "446": {
        "figure1": "2505.03649v1_wrdpg_generation_2",
        "label1": "fig:wrdpg_generation_blocks",
        "caption1": "Graph generation. Given the latent positions of each vertex $\\{\\bbX[k]\\}_{k \\geq 0}$, we estimate a weight distribution whose sequence of moments is given by the corresponding dot products. Edge weights are then sampled from this estimated distribution; see also Section \\ref{sec:generative}.",
        "text": "In Section \\ref{sec:generative} we show how one can generate graphs adhering to the proposed WRDPG model, which is non-trivial in a nonparameteric setting and was not considered in~\\cite{gallagher2023spectral}. We develop a methodology to sample random weighted graphs whose edge weight distribution is defined by a sequence of moments given by inner products of connected nodes' latent positions. Depending on the characteristics of the weight distribution--whether discrete, continuous, or a mixture--we can solve for the latent position sequence to ensure the inner products match the prescribed moments. For real-valued (continuous) weights, we rely on the maximum entropy principle subject to moment constraints. We introduce a primal-dual approach to find a probability density function that maximizes entropy, offering improved numerical stability relative to previous algorithms~\\cite{saad2019pymaxent}. Results for mixture distributions allow us to generate graphs that simultaneously replicate the sparsity pattern of the network and the edge weights. In Figure \\ref{fig:wrdpg_generation_blocks} we present a schematic diagram for the graph generation pipeline.\nMoreover, given an observed weighted network (instead of the ground-truth latent position sequence as in Figure \\ref{fig:wrdpg_generation_blocks}), we show how we can use this generative procedure to sample graphs that are similar to the original one, in a well-defined statistical sense. This can be useful for several statistical inference tasks involving network data. If, for instance, one would like to assess the significance of some observed graph characteristic, this procedure allows to generate several `comparable' graphs and construct judicious reference distributions~ \\cite[Section 6.2]{kolaczyk_2009}. In a related use case, one could perform hypothesis testing to determine if a given graph adheres to the WRDPG model.",
        "summarize_figure": "2505.03649v1_wrdpg_generation_2",
        "summarization": "The graph shows the pipeline of graph generation. Taking the latent positions of each vertex {X[k]} (k≥0) as input, it first conducts PDF/PMF estimation (PDF/PMF ESTIMATION), determining the moment sequence of the weight distribution based on the corresponding dot products to estimate the weight distribution. Then, through edge sampling (EDGE SAMPLING), edge weights are sampled from the estimated distribution, and finally, a graph is generated. This pipeline demonstrates how to generate random weighted graphs that adhere to the WRDPG model based on latent positions."
    },
    "447": {
        "figure1": "2505.03649v1_ER_Normal_example",
        "label1": "fig:ER_normal_example",
        "caption1": "True (dashed vertical line) and estimated (histograms) latent positions for an Erdös-Rényi model with $\\mathcal{N}(1,0.01)$ weights.",
        "text": "Figure \\ref{fig:ER_normal_example} shows a histogram of the recovered latent positions up to order $k=6$, with $p=0.5$, $\\mu=1$ and $\\sigma=0.1$. The red dashed line indicates the true latent positions in each case. Apparently, for each $k$ the estimated positions follow a normal distribution centered around the ground-truth value $\\bbx[k]$.",
        "summarize_figure": "2505.03649v1_ER_Normal_example",
        "summarization": "The chart shows the true (red dashed line) and estimated (histograms) latent positions for an Erdös-Rényi model with weights having a mean of 1 and a standard deviation of 0.1. Histograms of the recovered latent positions up to order k = 6 are presented, with p = 0.5. Evidently, for each value of k, the estimated latent positions follow a normal distribution centered around the true value."
    },
    "448": {
        "figure1": "2505.03650v1_Setup",
        "label1": "fig:apparatus",
        "caption1": "Schematic overview of the experimental setup.  Molecules are produced in a cryogenic buffer gas cell (CBGC) via laser ablation of a solid target.  A supercontinuum laser is sent through the molecular cloud and then dispersed on the optical spectrometer.",
        "text": "The method is a variation of white-light absorption spectroscopy: white light is passed through a molecular cloud, and comparing the transmitted spectrum with and without the molecules present yields the molecular absorption spectrum.  A supercontinuum laser generates $\\sim$1~W of white light, covering the range of 500--2,000~nm.  The light passes through a cloud of cryogenically-cooled atoms or molecules inside a CBGC, then enters a custom spectrometer manufactured by LightMachinery Inc. which combines a  virtually imaged phased array (VIPA) and diffraction grating to disperse and image fiber-coupled light onto a large-area sCMOS camera.  The important design features, which will be discussed in detail below, are the use of an optical spectrometer with resolution comparable to the Doppler width of typical atoms and molecules of interest at a temperature of $\\sim$5~K, the high atomic and molecular densities offered by the CBGC method, and a choice of laser and analysis routine to realize near-shot-noise-limited sensitivity.  The integrated spectrometer system is shown schematically in Fig. \\ref{fig:apparatus}.",
        "summarize_figure": "2505.03650v1_Setup",
        "summarization": "This is a schematic diagram of the experimental setup. Molecules are generated in a cryogenic buffer gas cell (CBGC) through laser ablation of a solid target. A supercontinuum laser passes through the molecular cloud and is then dispersed in a spectrometer. The setup includes components such as a vacuum chamber, radiation shields, a window, and a mesh. Inside are CBGC, CaF₂ material, and helium (He). There are a spectrometer and a white - light - generating component above and below respectively, with the ablation laser acting from below."
    },
    "449": {
        "figure1": "2505.03650v1_CaFSingleShot",
        "label1": "fig:singleShotCounts",
        "caption1": "The procedure for going from counts to absorption.  All plots on the right are zoomed-in versions of the plots on the left. \\textit{Top}: Counts vs. spectral element for single shots with and without molecules present.  Counts is as reported by the LightMachinery software and is the average photoelectron count over the pixels combined in each spectral bin.  We can see the slowly-varying envelope from the light source and the oscillations on the $\\sim$1~cm$^{-1}$ scale due to etaloning.  \\textit{Middle}: Uncorrected absorption signal, including polynomial fit to the background.  \\textit{Bottom}: Corrected absorption signal after subtracting the polynomial background.",
        "text": "Due to a frequency-dependent drift of the white light source coupled through the apparatus and into the spectrometer, $a'$ has a fairly large but slowly varying (compared to the instrument resolution) offset; improved alignment could reduce this offset, and could also eliminate the need to save a background shot paired with every signal shot.  We fit this ``slow background'' offset for each pulse as a sextic polynomial $p_i(f_j)$.  We then compute the corrected absorption spectrum     a_i(f_j) = a'_i(f_j) - p_i(f_j). For large enough signals, a single corrected absorption spectrum $a$ can be used to extract spectral information, assuming that the unit has been calibrated (see section \\ref{sec:i2cal}).  A set of typical $a',p,$ and $a$ for CaF are shown in Fig.~\\ref{fig:singleShotCounts}.\nWe can compare this noise level to the estimated photon shot noise contribution.  The spectrometer camera well depth is 10,000~$e^-$/pixel, so we operate with $\\sim$8,000~$e^-$/pixel to avoid saturation.  According to the manufacturer, each spectral element (se) at resolution is obtained by averaging around 50 pixels; we sample the instrument at one fifth of the resolution, so there are around 80,000 $e^-$/se.  The shot noise on this number of photoelectrons is $\\sqrt{80,000}\\approx 280$.  Per the camera manufacturer the read noise per pixel is typically 2~$e^-$/pixel, with $<1$~$e^-$/pixel of dark counts for our exposure length, so we expect approximately 300 $e^-$/se of total noise.  We estimate the shot-noise limited, single-shot absorption noise level to therefore be $300/80,000 \\approx 0.0038$, reasonably close to the measured single-shot value of 0.0090.  Two steps which could get the single-shot value closer to this minimum level are flattening the optical spectrum and reducing etaloning (see Fig.~\\ref{fig:singleShotCounts}) to get all spectral elements closer to saturation, reducing drift in the measured spectrum via more rigid optical mounting, and using a camera with a larger well depth.",
        "summarize_figure": "2505.03650v1_CaFSingleShot",
        "summarization": "The chart shows the process from counts to absorption. The top plots show the relationship between counts and spectral elements for single shots with and without molecules present, revealing the slowly - varying envelope of the light source and etaloning - induced oscillations on a ~1 cm⁻¹ scale. The middle plots display the uncorrected absorption signal, including a polynomial fit to the background. The bottom plots present the corrected absorption signal after subtracting the polynomial background. The plots on the right are zoomed - in versions of those on the left."
    },
    "450": {
        "figure1": "2505.03651v1_topology",
        "label1": "topology",
        "caption1": "Configuration of a 4-terminal AC-MTDC system.",
        "text": "As the most widely adopted type of VSC, MMCs play a critical role in large-scale MTDC systems by enabling efficient bidirectional conversion between AC and DC power. Figure~\\ref{topology}  shows a typical configuration for a four-terminal AC-MTDC transmission system connected to offshore wind farms.",
        "summarize_figure": "2505.03651v1_topology",
        "summarization": "The chart shows the configuration of a four - terminal AC - MTDC system. The system is connected to offshore wind farms with a capacity of 0.1 per unit. It includes multiple AC/DC converters, and the impedance values of various lines, such as 0.03 per unit and 0.05 per unit, are marked, reflecting the power transmission topology."
    },
    "451": {
        "figure1": "2505.03651v1_mmc",
        "label1": "mmc",
        "caption1": "Overview of MMC design.",
        "text": "The fundamental structure of an MMC comprises multiple half-bridge submodules (SMs) connected in series to form arms and then further combined in legs, which create a converter, as shown in Figure~\\ref{mmc}. Unlike conventional two-level or three-level VSCs, which rely solely on pulse-width modulation (PWM) for voltage synthesis, an MMC primarily synthesizes its output voltage by inserting or bypassing SMs in each arm, forming a stepped waveform. PWM techniques may still be applied within individual SMs to enhance waveform quality or control performance. The number of active SMs ($N_{active}$) at any time determines the generated voltage: U_{\\text{arm}} = \\sum_{i=1}^{N} S_i U_{\\text{SM},i}, \\label{eq:MMC_voltage}  where $S_i\\in\\{0,1\\}$ is the switching state of the $i$-th SM, and $U_{\\text{SM},i}$ is its capacitor voltage. The modular index M represents the fraction of SMs engaged in voltage synthesis: M = \\frac{N_{\\text{active}}}{N_{\\text{total}}}. \\label{eq:MMC_modular_index}",
        "summarize_figure": "2505.03651v1_mmc",
        "summarization": "The chart shows an overview of the modular multilevel converter (MMC) design. The basic structure of an MMC consists of multiple half - bridge sub - modules (SMs) connected in series to form arms and then combined into legs to create a converter. The output voltage is synthesized by inserting or bypassing sub - modules in each arm, forming a stepped waveform. The switching state and capacitor voltage of sub - modules determine the arm voltage, and the modular index represents the proportion of sub - modules involved in voltage synthesis."
    },
    "452": {
        "figure1": "2505.03651v1_mmc_operation",
        "label1": "operation modes",
        "caption1": "MMC operation diagram.",
        "text": "A typical MMC employs a hierarchical double-loop control structure, as illustrated in Figure~\\ref{operation modes}. The outer loop, often called the secondary control, regulates the DC voltage and active power exchange based on a predefined droop characteristic. This layer enables coordinated system-wide regulation across multiple terminals, while the inner loop comprising current controllers ensures fast tracking of reference values by modulating converter output in real-time. In addition, the energy control loop monitors the energies of the submodule capacitors and adjusts the internal circulating current to maintain the desired energy levels, ensuring convergence and stability.",
        "summarize_figure": "2505.03651v1_mmc_operation",
        "summarization": "The chart is an operation diagram of the modular multilevel converter (MMC). It adopts a hierarchical double - loop control structure. The outer loop (secondary control) regulates DC voltage and active power exchange based on a predefined droop characteristic for coordinated system - wide regulation across multiple terminals. The inner - loop current controllers quickly track reference values by modulating converter output in real - time. Besides, the energy control loop monitors the energies of submodule capacitors and adjusts the internal circulating current to maintain energy levels for convergence and stability."
    },
    "453": {
        "figure1": "2505.03652v1_learning_scheme",
        "label1": "fig:learning_scheme",
        "caption1": "\\textbf{The annealing-sampling scheme.} The parameter $\\beta$ increases to interpolate from a simple distribution that is close to the base distribution (top left) to the ultimate target distribution (top right).  At each value of $\\beta$, samples generated from previously trained NFs are reweighted and used to train a new NF, from which samples are then drawn.  ",
        "text": "To address this issue, we introduce an annealing scheme in which the target distribution is gradually updated from a distribution that overlaps the base distribution well to the desired target distribution \\cite{Simulated_Annealing}.  Let $p_b(\\mathbf{x})$ denote the base distribution and $p_u(\\mathbf{x})$ denote an update distribution. We define the intermediate target distribution as $\\hat{p}_{\\beta}(\\mathbf{x}) = p_b(\\mathbf{x})  p_u(\\mathbf{x})^\\beta$, where $\\beta\\in[0,1]$ is the annealing parameter. Here, $\\hat{p}_{\\beta}$ is nonnormalized. The corresponding normalized distribution $p_{\\beta} (\\mathbf{x})  \\propto \\hat{p}_{\\beta}(\\mathbf{x})  $ smoothly transitions from the base distribution ($\\beta=0$) to the full target distribution ($\\beta=1$) as $\\beta$ increases (Fig.\\ \\ref{fig:learning_scheme}). In the numerical example that we consider below, the desired full target distribution is the posterior distribution for parameter estimation within a Bayesian framework. We identify the base distribution with the prior distribution and the update distribution with the likelihood function, so that the posterior is proportional to their product.",
        "summarize_figure": "2505.03652v1_learning_scheme",
        "summarization": "The chart shows the annealing - sampling scheme. As the parameter β increases, it interpolates from a simple distribution close to the base distribution (top left) to the ultimate target distribution (top right). At each β value, samples from previously trained normalizing flow (NF) models are reweighted to train new NF models, from which samples are then drawn. The process covers model training, sample generation, and sample reweighting."
    },
    "454": {
        "figure1": "2505.03652v1_Repressilator",
        "label1": "fig:repressilator",
        "caption1": "\\textbf{Repressilator model.} (a) Schematic of the system; each circle represents a gene product, and $i\\dashv j$ represents repression of $j$ by $i$. (b) Solution used to generate the data for fitting; the parameter values are $X_i(t_0) = 2$ for all $i$, $\\alpha_1 = 10$, $\\alpha_2 = 15$, $\\alpha_3 = 20$, $m=4$, and $\\eta = 1$.  (c) Time series of the total concentration of gene products (blue line) and the simulated observable (orange dots) produced by adding Gaussian noise with variance 0.25.",
        "text": "The specific set of ODEs that we study represents a model of the repressilator, a biochemical oscillator that comprises a cycle of three gene products that each represses expression of another  (Fig.\\ \\ref{fig:repressilator}) \\cite{repressilator,biocircuits}: where $X_i$ represents the concentrations of gene product $i$, $\\alpha_i$ is its production rate, $m$ is a Hill coefficient, $\\eta$ is the degradation rate, and $p(i)\\equiv (i\\mod 3)+1$ is a periodic indexing function. We assume that $m$ and $\\eta$ are the same for all gene products.\nWe generate data by integrating the ODEs with the explicit fifth-order Runge-Kutta method of Tsitouras \\cite{TSI5} from $t_0 = 0$ to $t_1 =30.0$, saving states with a time interval of 0.6. The  parameter values are $X_i(t_0) = 2$ for all $i$, $\\alpha_1 = 10$, $\\alpha_2 = 15$, $\\alpha_2 = 20$, $m=4$, and $\\eta = 1$. We treat the total concentration of gene products (i.e., $X_1+X_2+X_3$) as the observable and add Gaussian noise with a variance of $\\sigma^2 =0.25$ to mimic experimental uncertainty. The time series of the concentration of each gene product and the observable are shown in Figs. \\ref{fig:repressilator}b and \\ref{fig:repressilator}c.",
        "summarize_figure": "2505.03652v1_Repressilator",
        "summarization": "The chart shows the Repressilator model. Figure (a) is a schematic diagram of the system, with three circles representing three gene products and arrows indicating the repression relationship between them. Figure (b) is the solution for generating fitting data, presenting parameter values and the oscillatory changes of the concentrations of three gene products (X₁, X₂, X₃) over time. Figure (c) shows the time series of the total concentration of gene products (blue line) and the simulated observable values (orange dots) after adding Gaussian noise with a variance of 0.25 to mimic experimental uncertainty."
    },
    "455": {
        "figure1": "2505.03659v1_meta",
        "label1": "fig:Meta",
        "caption1": "Meta learning",
        "text": "meta-learning for online portfolio selection} The meta-learning framework employed in this paper is model-agnostic meta-learning (MAML) proposed by \\citet{MAML}. The core concept involves learning an initial set of parameters from historical tasks that can quickly adapt to new tasks. The background introduction of MAML is provided in Appendix A. The process of learning from historical tasks is known as meta-training and is performed offline, whereas the adaptation to new tasks is known as meta-testing and is performed online (ref. Figure \\ref{fig:Meta}).",
        "summarize_figure": "2505.03659v1_meta",
        "summarization": "The chart shows the meta - learning process. At the top are historical tasks (task 1, task 2 to task N), which are integrated through meta - initialization (Meta Initialization). At the bottom are new unseen tasks, each consisting of a support set and a query set. Meta - learning aims to learn an initial set of parameters from historical tasks to quickly adapt to new tasks, with meta - training performed offline and meta - testing performed online."
    },
    "456": {
        "figure1": "2505.03659v1_network",
        "label1": "fig:Network structure",
        "caption1": "Neural network structure",
        "text": "The network architecture is shown in Figure \\ref{fig:Network structure} and comprises three components: a two-layer LSTM, an attention mechanism, and fully connected layers \\cite{cipiloglu2022portfolio,rather2021lstm,alzaman2024deep}. The two-layer LSTM is employed to extract features from the time series data and capture sequential dependencies. The self-attention mechanism is utilized to discern correlations among the vectors. The fully connected layers are responsible for mapping the extracted features to the final output space, with the Softmax layer providing the probability distribution over the selected candidate policies.",
        "summarize_figure": "2505.03659v1_network",
        "summarization": "The chart shows the neural network structure, which consists of three components. First, data passes through a two - layer long short - term memory network (LSTM) to extract features from time - series data and capture sequential dependencies. Then, it enters a self - attention layer with two heads to identify correlations among vectors through operations like scaled dot - product attention. Finally, fully connected layers map the extracted features to the final output space, and a Softmax layer provides the probability distribution over the selected candidate policies."
    },
    "457": {
        "figure1": "2505.03659v1_meta-TRAIN_V2",
        "label1": "fig:meta—traning framework",
        "caption1": "Meta-training framework for online portfolio selection",
        "text": "Assuming that we have a set of $T^{train}$ training tasks, each consisting of a support set with $K$ samples and a query set with $Q$ samples, which may come from the same market or different markets.  The training tasks are generated from the original historical return rate data of the $M$ candidate policies by using a rolling window approach. That is, we generate a task in a time window with $K+Q+w$ continue trading days and rolling the time window forward to generate several successive training tasks with overlapping time periods. Please refer to Figure \\ref{fig:meta—traning framework} for generation of the training tasks.\nThe optimization of base parameters for each sub-task $$\\theta_k^{'}\\leftarrow\\theta - \\alpha \\nabla \\mathcal{L}_k(f_{\\theta}(\\mathcal{T}_{l_k}^{sup})),$$ where $\\alpha$ is the sub-task learning rate.  The meta-optimization (\\ref{eq:OUR}) updates the meta-parameter $\\theta$ by measuring the overall performance of the base parameters $\\theta_k'$ based on $\\theta$ on the query set $\\mathcal{T}_{l_k}^{que}$.  The meta-parameters is update by the stochastic gradient descent (SGD) on the $H$ randomly selected  tasks: $$\\theta^* \\leftarrow \\theta - \\beta \\nabla \\left[\\sum_{k=1}^{H} \\mathcal{L}_k(f_{{\\theta_k^{'}}}(\\mathcal{T}_{l_k}^{que}))\\right],$$ where $\\beta$ is the meta-learning rate. We illustrate this process in Figure \\ref{fig:meta—traning framework}.",
        "summarize_figure": "2505.03659v1_meta-TRAIN_V2",
        "summarization": "This picture shows the meta - training framework for online portfolio selection. The timeline is divided into different task intervals, with colored lines representing data changes. Each task has a support set and a query set. The support set updates sub - task base parameters θ'k using the formula θ'k ← θ - α∇[∑{u∈T^{sup}{l_k}} L_k(f_θ(u))], where α is the sub - task learning rate. The query set is for meta - optimization. Meta - parameter θ is updated to θ* via SGD on H randomly selected tasks with the formula θ* ← θ - β∇[∑_{k = 1}^{H} L_k(f_{θ'k}(T^{que}{l_k}))], where β is the meta - learning rate."
    },
    "458": {
        "figure1": "2505.03660v1_Bi2_energy_t_xyave",
        "label1": "fig:Bi2_energy",
        "caption1": " Time evolution of (a) magnetic energy densities (scaled by two) for $xy$-averaged large-scale and fluctuating magnetic fields, (b) contributions to total large-scale magnetic energy, highlighting the balance between shear-driven amplification and EMF-induced dissipation, (c) energy budget for $\\bar B_x$, where the EMF term acts as the primary driver, and (d) energy budget for $\\bar B_y$, showing similar trends as total magnetic energy. In all cases, further averaging along $z$ is applied to obtain final values. ",
        "text": "Time evolution of (a) magnetic energy densities (scaled by two) for $xy$-averaged large-scale and fluctuating magnetic fields, (b) contributions to total large-scale magnetic energy, highlighting the balance between shear-driven amplification and EMF-induced dissipation, (c) energy budget for $\\bar B_x$, where the EMF term acts as the primary driver, and (d) energy budget for $\\bar B_y$, showing similar trends as total magnetic energy. In all cases, further averaging along $z$ is applied to obtain final values. \t} Horizontal planar averaging defines the large-scale field in our investigation of large-scale dynamos in MRI-driven turbulence. Figure~\\ref{fig:Bi2_energy}(a) shows the evolution of magnetic energy densities (scaled by two) for the large-scale ($\\bar B_i^2$) and fluctuating ($\\bar M_{ii} = \\overline{b_i^2}$) magnetic fields. Fluctuating fields are comparable to or stronger than large-scale fields already in the exponential growth phase, with the azimuthal component dominating at both large and small scales throughout nonlinear saturation.\nFigure~\\ref{fig:Bi2_energy} illustrates the contributions from individual terms in the evolution of total mean magnetic energy (\\Fig{fig:Bi2_energy}b), as well as the energy associated with $\\bar B_x$ (\\Fig{fig:Bi2_energy}c) and $\\bar B_y$ (\\Fig{fig:Bi2_energy}d). The Poynting flux associated with shear enhances total magnetic energy by amplifying the azimuthal field energy. Meanwhile, the EMF term extracts energy, reducing the total magnetic energy. Notably, for the radial field component, the EMF acts as the primary energy source, highlighting its key role in sustaining the large-scale dynamo.",
        "summarize_figure": "2505.03660v1_Bi2_energy_t_xyave",
        "summarization": "The four sub - plots show the time evolution of magnetic energy - related quantities. (a) shows the evolution of xy-averaged large - scale (Bˉi2​) and fluctuating (Mˉii​) magnetic energy densities, with fluctuating fields comparable or stronger in the exponential growth phase and the azimuthal component dominant in nonlinear saturation. (b) highlights the balance between shear - driven amplification and EMF - induced dissipation in total large - scale magnetic energy. (c) shows the energy budget for Bˉx​ with the EMF term as the main driver. (d) shows the energy budget for Bˉy​, with trends similar to total magnetic energy. All values are further averaged along z."
    },
    "459": {
        "figure1": "2505.03660v1_BxdtBx_z_xyaver_T60",
        "label1": "fig:BxdtBx_z_xyaver",
        "caption1": " Spatial variation of individual terms arising from the vertical gradient of the azimuthal EMF $\\mathcal{\\bar E}_y$ (Eq.~\\ref{eq:meanBx_xy_2}) responsible for generating $\\bar B_x (z)$, shown at three different stages: (a) MRI exponential growth ($t/T_{\\text{orb}} = 3.34$), (b) initial saturation ($t/T_{\\text{orb}} = 3.978$), and (c) nonlinear stage ($t/T_{\\text{orb}} = 60$). Line styles and colors are consistent across all panels; the legend is displayed in panel (c). ",
        "text": "Spatial variation of individual terms arising from the vertical gradient of the azimuthal EMF $\\mathcal{\\bar E}_y$ (Eq.~\\ref{eq:meanBx_xy_2}) responsible for generating $\\bar B_x (z)$, shown at three different stages: (a) MRI exponential growth ($t/T_{\\text{orb}} = 3.34$), (b) initial saturation ($t/T_{\\text{orb}} = 3.978$), and (c) nonlinear stage ($t/T_{\\text{orb}} = 60$). Line styles and colors are consistent across all panels; the legend is displayed in panel (c). \t\t} Figure~\\ref{fig:BxdtBx_z_xyaver} shows how each term in $\\mathcal{\\bar E}_y$ contributes to the generation of $\\bar B_x(z)$, evaluated at three key MRI stages: (a) exponential growth phase at $t/T_{\\text{orb}} = 3.34$ (\\Fig{fig:BxdtBx_z_xyaver}a), (b) initial saturation phase at $t/T_{\\text{orb}}=3.978$ (\\Fig{fig:BxdtBx_z_xyaver}b), and (c) fully nonlinear phase at $t/T_{\\text{orb}} = 60$ (\\Fig{fig:BxdtBx_z_xyaver}c). To facilitate interpretation, we multiply $\\bar B_x$ on both sides of \\Eq{eq:meanBx_xy_2}, ensuring the source term remains positive whether $\\bar B_x$ is growing positively or negatively. During the exponential growth phase, the dominant source term is the magnetic field gradient (linked to $\\beta_{yyz}$ or $\\eta_{yx}$), which amplifies $\\bar B_x(z)$. In contrast, the time-derivative term has a predominantly dissipative effect. Contributions from terms proportional to $\\bar B_i$ and velocity gradients remain negligible throughout. Additionally, the third-order correlation term exhibits localized variations that can either reinforce or counteract the mean-field contributions. In the early saturation phase (\\Fig{fig:BxdtBx_z_xyaver}b), nonlinear third-order correlators play the most significant role, counteracting the growth driven by the $\\eta_{yx}$-related term, thereby explaining the transition to saturation. This behavior persists in the fully developed nonlinear stage  (\\Fig{fig:BxdtBx_z_xyaver}c), maintaining dynamo self-regulation.",
        "summarize_figure": "2505.03660v1_BxdtBx_z_xyaver_T60",
        "summarization": "The picture shows the spatial variation of terms related to the generation of B_x(z) from the vertical gradient of the azimuthal electromotive force E_y in the nonlinear stage (t/T_orb = 60). Curves of different colors and line styles represent various physical terms, such as B_x times the partial derivative of B_x with respect to t, -B_x times the partial derivative with respect to z times E_y, etc. These physical terms exhibit different fluctuations at various z - values, being positive or negative in certain intervals, reflecting their contributions or inhibitions to the generation of B_x(z) in space, and demonstrating the self - regulation mechanism of the dynamo in this stage."
    },
    "460": {
        "figure1": "2505.03660v1_ui_Fij_x-0p25_y0_T3p34",
        "label1": "fig:ui_Fij_x-0p25_y0_T3p34",
        "caption1": " Spatial variation of individual terms in the evolution equations for fluctuating velocity components and mean Faraday stress during the MRI growth phase, evaluated at $t/T_{\\text{orb}} =3.34$. Panels (a) and (b) show the evolution terms for $u_y$ and $u_x$ (Eq.~\\ref{eq:fluctuatingU}), respectively, while (c) and (d) depict the corresponding terms for $\\bar F_{yz}$ and $\\bar F_{xz}$ (Eq.~\\ref{eq:Fij_exact}). To ensure source terms remain positive, $u_i$ and $\\bar F_{ij}$ are multiplied on both sides of their respective evolution equations (not shown in figure legends). ",
        "text": "Spatial variation of individual terms in the evolution equations for fluctuating velocity components and mean Faraday stress during the MRI growth phase, evaluated at $t/T_{\\text{orb}} =3.34$. Panels (a) and (b) show the evolution terms for $u_y$ and $u_x$ (Eq.~\\ref{eq:fluctuatingU}), respectively, while (c) and (d) depict the corresponding terms for $\\bar F_{yz}$ and $\\bar F_{xz}$ (Eq.~\\ref{eq:Fij_exact}). To ensure source terms remain positive, $u_i$ and $\\bar F_{ij}$ are multiplied on both sides of their respective evolution equations (not shown in figure legends). \t} In the presence of both mean ($\\bar J_x = - \\partial_z \\bar B_y$) and fluctuating ($j_x = - \\partial_z b_y$) currents, magnetic fluctuations $b_z$ interact with these field gradients to generate velocity fluctuations $u_y$. This process is driven by the magnetic tension terms $[(\\bm B \\cdot \\nabla) B_y]_\\text{fluc}$ in the fluctuating Lorentz force, particularly u_y \\sim \\tau_y (b_z \\partial_z \\bar B_y + b_z \\partial_z b_y) . The Coriolis force subsequently converts $u_y$ into $u_x$:  u_x \\sim \\tau_x 2\\Omega u_y \\sim 2 \\Omega \\tau_x (b_z \\partial_z \\bar B_y + b_z \\partial_z b_y), where, $\\tau_i$ is the turbulent correlation time. Figure~\\ref{fig:ui_Fij_x-0p25_y0_T3p34}  illustrates the contribution of individual terms in the fluctuating velocity field equations (see Appendix~\\ref{sec:appendix_EMF}). To  track the energy  we multiply $u_i$ on both sides of $\\partial_t u_i$ equations, %regardless of whether $u_i$ is growing positively or negatively. The magnetic tension component of Lorentz force fluctuations generates $u_y$ (\\Fig{fig:ui_Fij_x-0p25_y0_T3p34}a), which is then converted into $u_x$ via the Coriolis force (\\Fig{fig:ui_Fij_x-0p25_y0_T3p34}b). The contributions from pressure fluctuations are negligible for $u_y$, and remain small (mostly acting as a sink) in the $u_x$ evolution.\nThe generated velocity fluctuation $u_x$ correlates with its source $b_z$, producing an EMF $\\mathcal{\\bar E}_y = \\left< u_z b_x \\right> - \\left< u_x b_z \\right>$, that is proportional to and has the same sign as  $\\bar J_x$: &\\sim -2 \\Omega \\tau_x (\\bar{M}_{zz} \\partial_z \\bar B_y + \\left< b_z b_z \\partial_z b_y \\right>)  \\nonumber\\\\ & \\sim 2 \\Omega \\tau_x (\\bar{M}_{zz} \\bar J_x - \\left< b_z b_z \\partial_z b_y \\right>), where the second term on the RHS forms a third-order correlator.  Through its correlation with positive $\\bar{M}_{zz}$,  $\\bar J_x$,  generates  $\\mathcal{\\bar E}_y$, responsible for the growth of $\\bar B_x$. This process underpins the origin of the negative $\\eta_{yx}^{\\text{mag}}$, through which the large-scale dynamo mechanism---rotation-shear-current effect---operates. Meanwhile,  $u_y$ correlates with $b_z$ to produce   $\\mathcal{\\bar E}_x = \\left< u_y b_z \\right> - \\left< u_z b_y \\right>$, which is proportional to but opposite in sign to $\\bar J_x$: & \\sim \\tau_y (-\\bar{M}_{zz} \\bar J_x + \\left< b_z b_z \\partial_z b_y \\right>). Thus, $\\mathcal{\\bar E}_x$ acts as a turbulent dissipation mechanism for the mean field $\\bar B_y$, counteracting its growth. Figures~\\ref{fig:ui_Fij_x-0p25_y0_T3p34}(c) and (d) illustrates the contributions of individual terms appearing in the evolution equations (see Appendix~\\ref{sec:appendix_EMF}) for $\\bar{F}_{yz} = \\left< u_y b_z \\right>$ and $\\bar{F}_{xz} = \\left< u_x b_z \\right>$, evaluated during the MRI exponential growth phase $t/T_{\\text{orb}} =3.34$. The dominant source term for $\\bar{F}_{yz}$ is $\\bar{M}_{zz} \\partial_z \\bar B_y$ (\\Fig{fig:ui_Fij_x-0p25_y0_T3p34}c), with the Coriolis force converting $\\bar{F}_{yz}$ into $\\bar{F}_{xz}$ (\\Fig{fig:ui_Fij_x-0p25_y0_T3p34}d). In both cases, terms proportional to $\\bar B_i$ are negligible. Notably, while azimuthal gas and magnetic pressure gradients have no contribution to $\\bar{F}_{yz}$, a weak radial pressure gradient contributes to the evolution of $\\bar{F}_{xz}$. A detailed analysis of the contributions from gas and magnetic pressure fluctuations and their effect on the growth of the large-scale radial magnetic field is provided in the Supplemental Material (\\ref{sec:pressure_fluctuations}). While magnetic pressure fluctuations individually support dynamo growth, their effects are largely canceled out by gas pressure fluctuations, leading to a negligible net contribution.",
        "summarize_figure": "2505.03660v1_ui_Fij_x-0p25_y0_T3p34",
        "summarization": "The four sub - plots show the spatial variation of individual terms in the evolution equations for fluctuating velocity components (ux​ and uy​) and mean Faraday stress (Fˉyz​ and Fˉxz​) during the MRI growth phase at t/Ttextorb​=3.34. Panels (a) and (b) depict terms for uy​ and ux​ evolution, with the magnetic tension term driving uy​ generation and the Coriolis force converting uy​ to ux​, and pressure fluctuation contributions being negligible or small. Panels (c) and (d) show terms for Fˉyz​ and Fˉxz​ evolution, where Mˉzz​∂z​Bˉy​ dominates Fˉyz​ generation and the Coriolis force converts Fˉyz​ to Fˉxz​, with terms proportional to Bˉi​ negligible and weak radial pressure gradients contributing to Fˉxz​."
    },
    "461": {
        "figure1": "2505.03666v1_SingleEdgeWorthPoisson10",
        "label1": "fig:EdgeworthExpansionPoisson",
        "caption1": "Edgeworth expansion for the Poisson distribution with $\\lambda = 10$. red line is the Edgeworth expansion, blue line is the Poisson distribution.",
        "text": "The UrQMD model~\\cite{BASS1998255} is employed to test our method. We use simulated data from Au+Au collisions at $\\sqrt{s_{NN}} = 3.5$ GeV. Proton cumulants are measured within the rapidity range $-0.5 < y < 0$ and transverse momentum window $0.4 < p_{T} < 2.0$ GeV/$c$. In the UrQMD simulation, the number of participant nucleons ($N_{\\text{part}}$) is directly accessible, while {\\it RefMult3} denotes the charged-particle multiplicity by excluding protons - a design intended to eliminate auto-correlation effects in the analysis.    Our method consists of three key steps:    In statistics, the Gram–Charlier series provides an effective method for approximating probability distributions based on given cumulants. As a related technique, the Edgeworth expansion is not only widely used to enhance smeared signals but also provides an alternative solution to the moment problem. The Edgeworth expansion approximates a probability distribution $p(x)$ as a series around the normal distribution, incorporating higher-order cumulants to capture deviations of the distribution's shape from that of a standard Gaussian distribution.          p(x) = \\sum_{n=0}^{\\infty} {c_n}\\frac{d^nZ}{dx^n}     where the $Z$ represents the normal distribution. The theoretical basis for this is the Central Limit Theorem, which states that the sum of a large number of independent and identically distributed random variables is approximately normally distributed. To find the coefficients $c_n$, one perform an inverse Fourier transform with respect to the set of Chebyshev-Hermite polynomials:         c_n = \\frac{\\sqrt{\\pi}}{2^{n-1}n!} \\int_{-\\infty}^{\\infty} Z(t)p(t) H_{e_n}(t) dt     where Chebyshev-Hermite polynomials $H_{e_n}$ is defined as:         H_{e_n}(x) = (-1)^n exp(x^2/2) \\frac{d^n}{dx^n} exp(-x^2/2)     The first 3 orders of the polynomial is given by:         H_{e_0}(x) = 1, \\qquad H_{e_1}(x) = x, \\qquad H_{e_2}(x) = x^2 - 1, \\qquad H_{e_3}(x) = x^3 - 3x     Therefore, the Edgeworth expansion can be written in the form of:     here $S_n \\equiv C_n/\\sigma^{2n-2}$, where $\\sigma$ is the standard deviation of the distribution $p(x)$. The Edgeworth expansion provides an asymptotic approximation for probability distributions of arbitrary shape. It achieves high accuracy for distributions that are nearly Gaussian with just a few terms in the expansion. However, for distributions with heavy tails or significant deviations from normality, higher-order cumulants are needed to obtain a reliable approximation. In the Figure~\\ref{fig:EdgeworthExpansionPoisson},  We firstly apply the Edgeworth expansion to the Poisson distribution, which closely resembles a Gaussian, to demonstrate its effectiveness. The results show that the Edgeworth expansion provides a good approximation to the Poisson distribution. Building on this, we test the Edgeworth expansion on proton number distributions for each $N_{\\text{part}}$-defined centrality in UrQMD simulations.",
        "summarize_figure": "2505.03666v1_SingleEdgeWorthPoisson10",
        "summarization": "The picture shows the Edgeworth expansion fitting for the Poisson distribution with λ = 10. The horizontal axis is x and the vertical axis is probability density. Blue circles represent the Poisson distribution, and the red dashed line represents the Edgeworth expansion. When x is small, the probability density rises rapidly as x increases, reaches a peak, then drops rapidly as x increases further, and the decline slows down later. The Edgeworth expansion curve fits well with the blue - circle data points of the Poisson distribution, indicating that the Edgeworth expansion can provide a good approximation to the Poisson distribution."
    },
    "462": {
        "figure1": "2505.03666v1_Proton_Selected_Centralities_PoissonErr",
        "label1": "fig:EdgeworthExpansionProton",
        "caption1": "Edgeworth expansion for the proton number distributions in each $N_{\\text{part}}$-defined centrality from UrQMD.",
        "text": "Figure~\\ref{fig:EdgeworthExpansionProton} presents the reconstructed proton number distributions using the Edgeworth expansion. By inputting the mean, variance, skewness, and kurtosis of the proton distributions from each centrality, we are able to accurately reproduce the full distribution shape. This confirms that the Edgeworth expansion is a robust and reliable tool for describing proton number distributions in heavy-ion collisions.",
        "summarize_figure": "2505.03666v1_Proton_Selected_Centralities_PoissonErr",
        "summarization": "The picture shows the Edgeworth expansion of proton number distributions under different collision centralities based on the UrQMD model. The horizontal axis is the proton number Np, and the vertical axis is the probability density. Each sub - graph corresponds to a different centrality interval, labeled with mean μ, standard deviation sigma, skewness skew, and kurtosis kurt. The curves all show a single - peak shape of rising first and then falling. The peak positions and widths of the curves vary with different centralities, indicating that the characteristics of proton number distribution change with centrality, verifying that the Edgeworth expansion is a reliable tool for describing proton number distributions in heavy - ion collisions."
    },
    "463": {
        "figure1": "2505.03666v1_Forth_CBWC_C2_C3_vs_C1_with_fits_Npart",
        "label1": "fig:C2_C3_vs_C1",
        "caption1": "Variations of $C_2$ and $C_3$ with respect to the $C_1$ in Au+Au collisions at 3.5 GeV from UrQMD. Here red solid lines represent the results of a second-order polynomial fit, indicating that cumulants follow a certain pattern.",
        "text": "In Bayesian modeling, the model parameters are characterized through prior and posterior probability distributions. The prior distribution encapsulates our initial assumptions or beliefs about these parameters, formulated before any data is taken into account. Such priors can be informed by contextual common sense relevant to the application, insights from domain experts, or the underlying physical principles governing the model. The posterior distributions of the parameters are the updates to the prior based on the observations. The relation can be given by the Bayes' theorem:         p(\\theta|Y)=\\frac{p(Y|\\theta)p(\\theta)}{p(Y)}     $\\theta$ represents the parameters of the model and $Y$ denotes the observed experimental data. In the equation \\ref{eq:Bayes}, the term $p(Y|\\theta)$ is known as the likelihood, it quantifies how well the observed data are predicted by the model given the parameters. The $p(\\theta)$ to the right of the equation is the prior probability distribution of the parameters, and the denominator $p(Y)$ then acts as a normalized factor to the probability. The Bayes theorem provides the rule for updating the prior to the posterior. Therefore, the updating process can be expressed as:         posterior \\propto likelihood \\times prior     Parameter estimation can be achieved by finding the Maximum A Posteriori (MAP) estimate, which corresponds to the parameter values that maximize the posterior distribution. MAP estimation is similar to Maximum Likelihood Estimation (MLE) but incorporates prior knowledge about the parameters through a prior distribution. The calculation of MAP estimation requires the integral of posterior, however, it is usually difficult or impossible when the parameter space is huge or the relation in the model is complex. As the result, the Markov chain Monte Carlo (MCMC) method is widely applied. Markov chain is defined as a sequence of random variables $\\theta_i$ depends only on the $\\theta_{i-1}$. For a well-behaved chain of sufficient length, its distribution converges to a stationary distribution that is independent of the chain's initial state, this is the property of memorylessness. The MCMC method is a stochastic method to generate a Markov chain that converges to the posterior distribution. In the simulation, new values of parameters are sampled from the approximate posterior distribution, it can be a joint distribution of all parameters, or a conditional      distribution of one parameter given the others. Once the new values are drawn, it will be compared with the old one, it will be accepted if there is a chance of improving the posterior. However, the actual algorithms are different. Particular MCMC algorithm is usually been called as a \"Sampler\", the Gibbs sampler is one of the most common algorithms because of its simplicity. It samples and updates only one parameter in a time from the conditional distribution\\cite{2006Pattern}. Other samplers include Metropolis-Hastings algorithm, Hamiltonian Monte Carlo, Slice sampling, etc.     Due to the large number of centrality bins, if the cumulants of each order in every centrality bin are treated as parameters, there would be too many parameters, causing the model to lose stability. Fortunately, higher-order moments across different centrality bins exhibit polynomial relationships with the $C_1$, which greatly reduces the model's complexity. In figure \\ref{fig:C2_C3_vs_C1} we show the variations of $C_2$ and $C_3$ with respect to the $C_1$ in Au+Au collisions at 3.5 GeV.     We did not consider the $C_4$ vs $C_1$ pattern due to its large fluctuations. In our program, $C_2$ is modeled as a quadratic polynomial function of $C_1$, while $C_3$ and $C_4$ are indirectly controlled through $C_3/C_2$ and $C_4/C_2$ as quadratic polynomial functions of $C_1$. This approach is physically justified since cumulant ratios follow certain trends. We adopted the $C_1$ derived from the {\\it RefMult3} centrality definition. However, due to small differences with the $C_1$ obtained from $N_{\\text part}$-defined centrality, $C_1$ will be parameterized as a fourth-order polynomial for optimization. However, it's understood that unconstrained multi-modal models often yield multiple solutions. Consequently, applying appropriate constraints is crucial. In this context, we employ constraints based on two-point slopes, which are defined as the slopes between the initial and final points. Since the true values of these slopes are often unknown in real applications and only an approximate range can typically be estimated, we therefore assign a permissible range of fluctuation to them. Specifically, we used the slope constraints between $C_4/C_1$ vs $C_1$, $C_3/C_1$ vs $C_1$, etc. Furthermore, we know that the effect of volume fluctuations is minimal for the most central collisions. Consequently, the difference in central collision should not be significant. Therefore, in this work, we constrained the variation of $C_3$ for the most central collision to be within 5\\%. The aim is to ensure that the cumulant trends, subject to these constraints, can still adequately capture the overall distribution. These constraints are integrated into our program's objective function.",
        "summarize_figure": "2505.03666v1_Forth_CBWC_C2_C3_vs_C1_with_fits_Npart",
        "summarization": "The picture shows the variations of C2 and C3 with respect to C1 in Au+Au collisions at 3.5 GeV. The horizontal axis is C1, and the vertical axes are C2 and C3 respectively. Blue circles are data points, and the red solid lines are second - order polynomial fitting results. C2 and C3 increase as C1 increases, showing a certain regularity, indicating a specific pattern among these cumulants. This helps simplify the model complexity and reflects the characteristics of cumulant changes in heavy - ion collision research."
    },
    "464": {
        "figure1": "2505.03670v1_Figure_1_Revised",
        "label1": "firstfigure",
        "caption1": "An illustration of the isometry $\\bp$ between the simplex $\\Delta^2 \\subseteq \\R^2$ and probability measures on a three node weighted graph $\\G$. Left: Two examples of probability measures, $\\sum_{i=1}^3 p_i \\delta_i \\in \\P(\\G)$. Right: The corresponding  two points in the simplex. When the simplex is endowed with the distance coming from the Wasserstein metric on $\\P(\\G)$,  Gangbo, Li, and Mou \\cite{gangbo2019geodesics} showed that there exist geodesics between points in the interior that touch the boundary.",
        "text": "In the HK setting of finite Borel measures,  the role of the cone $\\mathfrak{C}_\\Omega$ is to allow an extra dimension $r \\in [0,+\\infty)$ that represents the amount of mass at each location $x \\in \\Omega$. Seeking an analogy in the vvOT setting, we allow the role of the cone $\\mathfrak{C}_\\Omega$ to be played by the simplex $ \\Delta^{n-1}$, leveraging the fact that there  is a natural bijection between $\\Delta^{n-1}$  and probability measures on the graph $\\P(\\G)$,  See Figure \\ref{firstfigure} for an illustration of the isometry. Thus, we may consider the simplex $\\Delta^{n-1}$ to be endowed with a distance $d_{\\Delta^{n-1}}$ induced by the graph Wasserstein metric on $\\P(\\G)$. The simplex allows an extra dimension $r \\in \\Delta^{n-1}$ that represents how mass at location $x \\in \\Omega$ is distributed between each of the $n$ labels.",
        "summarize_figure": "2505.03670v1_Figure_1_Revised",
        "summarization": "The image illustrates the isometry between the two - dimensional simplex Δ² ⊆ R² and probability measures on a three - node weighted graph G. The left shows two probability measures with node probabilities (p1, p2, p3) and connection relationships (qij). The right shows corresponding points in the simplex. When the simplex has a distance from the Wasserstein metric on P(G), there are geodesics between interior points touching the boundary."
    },
    "465": {
        "figure1": "2505.03670v1_Figure_2_Revised",
        "label1": "secondfigure",
        "caption1": "An illustration of the way in which probability measures on $\\R \\times \\Delta^{2}$ project down to vector valued measures $\\P(\\R \\times \\G)$, in the case of a three node graph. Bottom: a vector valued measure, chosen so that the distribution of yellow mass coincides  in physical space with the distribution of the first bump of red mass. Top left: the canonical lifting, where the color of the mass determines the corner of the simplex it is lifted to;  shading is used to indicate the distribution of the mass, which is peaked at the center of each bump. Top right: an alternative lifting, where the yellow bump and first red bump are lifted to a distribution along a line halfway between the edge corresponding to pure yellow mass and the edge corresponding to pure red mass.",
        "text": "In this way, following the analogy with the HK distance, we seek  a static distance defined by lifting vector valued measures to probability measures on the product space $\\Rd \\times \\Delta^{n-1}$, via the  projection operator  This operator is clearly surjective, since for any $\\bmu \\in \\P(\\Rd \\times \\G)$, it has a  \\emph{canonical lifting} given by  that satisfies $\\PP \\lambda_\\bmu = \\bmu$, where $\\{e_j\\}_{j=1}^n$ denote the vertices of the simplex $\\Delta^{n-1}$, so that $\\bp(e_j) = \\delta_j  \\in \\P(\\G)$ is the probability measure concentrated on the $j$th node of the graph. See Figure \\ref{secondfigure} for an illustration of a vector valued measure $\\bmu \\in \\P(\\Rd \\times \\G)$, its canonical lifting $\\lambda_{\\bmu} \\in \\P(\\Rd \\times \\Delta^{n-1})$, and another lifting $\\lambda \\in \\P(\\Rd \\times \\Delta^{n-1})$ that projects down to $\\bmu$.",
        "summarize_figure": "2505.03670v1_Figure_2_Revised",
        "summarization": "The picture shows how probability measures on R×Δ² project down to vector - valued measures P(R×G) in the case of a three - node graph. The bottom figure presents a vector - valued measure where the distribution of yellow mass coincides with the first bump of red mass in physical space. The top left figure is the canonical lifting, where the color of the mass determines the corner of the simplex it is lifted to, and shading indicates the mass distribution. The top right figure is an alternative lifting, where the yellow bump and the first red bump are lifted to a distribution along a line halfway between the edges corresponding to pure yellow and pure red mass, illustrating different lifting - projection relationships."
    },
    "466": {
        "figure1": "2505.03672v1_fig2_methods",
        "label1": "magma_main",
        "caption1": "Water concentration in melt ($m_{\\ch{H2O}}^{melt}$) as a function of bulk mantle water content ($m_{\\ch{H2O}}^{mantle}$) considering water saturation and fractional melting. The dashed lines are the solubility-limited \\ch{H2O} content of the magma under the maximum and minimum values of primary magma pressure, $P_{prim} = 1$ and 3 GPa. The black solid lines are the abundance-limited \\ch{H2O} content of the magma assuming fractional melting under the maximum and minimum values of melt fraction, $F = 10$ and 20\\%. For a given $m_{\\ch{H2O}}^{mantle}$, $F$, and $P_{prim}$, the true value of $m_{\\ch{H2O}}^{melt}$ is the minimum from water saturation or fractional melting. The gray shaded region is the range of possible values allowed based on this procedure. The green shaded region labeled \"MORB\" shows the range for mid-ocean ridge basalt magma water content \\citep[$0.1 - 1.5$ wt\\%;][Ch. 7]{Sigurdsson.2015} and Earth's mantle water concentration \\citep[$0.049-0.24$ wt\\%;][]{Ohtani.2020}.",
        "text": "The water concentration in the melt is then taken as the minimum concentration calculated from the water abundance limit (Equation \\ref{melt}) or water saturation limit (Equation \\ref{saturation}). This relationship is shown in Figure \\ref{magma_main} and the parameter ranges we explore are shown in Table \\ref{og_params}.",
        "summarize_figure": "2505.03672v1_fig2_methods",
        "summarization": "The picture shows the relationship between water concentration in the melt (mH₂O^melt) and bulk mantle water content (mH₂O^mantle) considering water saturation and fractional melting. The horizontal axis is solid mantle water content, and the vertical axis is magma water content. Dashed lines represent the water solubility limits of magma at primary magma pressures Pprim of 1 GPa and 3 GPa, and solid lines represent the water abundance limits of magma at melt fractions F of 10% and 20%. For a given mantle water content, melt fraction, and primary magma pressure, the melt water content takes the minimum value between the two. The gray shaded area is the range of possible values, and the green \"MORB\" area represents the range of mid - ocean ridge basalt magma water content and Earth's mantle water concentration."
    },
    "467": {
        "figure1": "2505.03673v1_overview",
        "label1": "fig:method",
        "caption1": " \\textbf{Framework of RoboOS.} RoboOS is a Brain-Cerebellum hierarchical architecture for multi-robot collaboration that comprises three core components: a) Cloud-based Embodied Brain model for high-level cognition and multi-agent coordination; b) Distributed Cerebellum Modules for executing robot-specific skills; and c) Real-Time Shared Memory for enhanced environmental awareness. } \\vspace{-0.5cm",
        "text": "As shown in Fig.~\\ref{fig:method}, \\textbf{\\textit{RoboOS}} is a unified embodied system built on a biologically inspired Brain-Cerebellum hierarchical architecture, comprising three core components: the Embodied Brain Model (\\texttt{RoboBrain~\\cite{ji2025robobrain}}), the Cerebellum Skill Library, and Real-Time Shared Memory. Deployed via the FlagScale MLLMs toolkit~\\cite{FlagScale2024}, the edge-cloud RoboOS framework facilitates seamless multi-robot coordination by synchronizing cognition across the multiple agents. The system operates as follows: First, the Embodied Brain Model manages system-wide tasks, including multi-robot task planning, tool invocation, spatiotemporal memory updates, and adaptive error correction through continuous three-level feedback loops. Second, the Cerebellum Skill Library, deployed on individual robot terminals, offers modular, plug-and-play functionality via standardized Robot Profiles. Finally, the Redis-optimized Shared Memory maintains a dynamic knowledge base of spatial relationships, operational states, and historical data to support real-time decision making. This architecture ensures robust large-scale deployment while maintaining the low-latency interactions essential for embodied AI systems.",
        "summarize_figure": "2505.03673v1_overview",
        "summarization": "The picture shows the framework of RoboOS, a brain - cerebellum hierarchical architecture for multi - robot collaboration. It has three core components: the cloud - based Embodied Brain Model (RoboBrain) for high - level cognition and multi - agent coordination, covering functions like perception and planning; the distributed Cerebellum Skill Library on individual robot terminals, offering modular plug - and - play functions; and the Real - Time Shared Memory for enhanced environmental awareness, including spatial and temporal memory. It also enables task scheduling and multi - robot collaboration. Deployed via the FlagScale toolkit, it enables seamless multi - robot coordination."
    },
    "468": {
        "figure1": "2505.03695v1_dg-fcp",
        "label1": "fig:decision-governor",
        "caption1": "\\textbf{Decision tree for boundary classification of each obstacle}. The decision tree evaluates the lower and upper gaps within the drivable space. If both gaps are available, two approaches can be used: selecting the preferred gap based on cost evaluation or treating the obstacle as a risk in PO.",
        "text": "Since this work focuses primarily on the path planning layer, we demonstrate the efficacy of FCP with a simple decision tree \\cite{song2015decision} for our DG, as shown in Fig.~\\ref{fig:decision-governor}.  This simple decision tree performs well even with moderate noise in the perception and localization data. However, with large noise, it may cause inconsistent obstacle classification (i.e., switching between lower and upper bounds), especially when an obstacle is located near the center of the drivable space. Alternatively, the obstacle can be treated as a risk in the PO (see Section~\\ref{sec:dynamic-obstacle-handling}) to enhance path consistency. Owing to the modularity of our approach, more advanced methods from the literature \\cite{planningSurvey} can be incorporated into DG to improve the robustness of the trajectory planning pipeline.",
        "summarize_figure": "2505.03695v1_dg-fcp",
        "summarization": "The picture shows a decision tree for the boundary classification of each obstacle. It evaluates the lower gap (LG) and upper gap (UG) within the drivable space. If LG is infeasible and UG is feasible, the lower bound (LB) is chosen; if LG is feasible and UG is infeasible, the upper bound (UB) is chosen; if both are feasible, a better gap can be selected based on cost evaluation such as distance to the previous gap, gap size, and distance to the reference path, or the obstacle can be imposed as a risk in FCP; if both are infeasible, the previous solution is used. This decision tree aids in path planning by assessing gap feasibility."
    },
    "469": {
        "figure1": "2505.03704v1_proposed_framework",
        "label1": "fig:proposed_methods",
        "caption1": "Detailed structure of the proposed pipeline for a polymer property prediction model. % First, feature extraction is performed from molecular structure data represented as graphs using a pre-trained GNN model (left side of the figure). The extracted features, along with other data such as molecular descriptors and compounding information, are then used as inputs to a machine learning algorithm to obtain predicted values for the desired polymer properties (right side of the figure).  %\\vspace{-1cm} } \\end{center",
        "text": "GCNs is a representative architecture of GNNs based on message passing.  Specifically, they are neural network models that generate an embedding vector $\\bm{z}$ for the node features $\\bm{X}$ as described below.  The input graph $G = (V, E, \\bm{X})$ is defined by the vertex set $V$, the edge set $E$, and the node features $\\bm{X}$.  Then our $L$-layer GCNs transform the node features $\\bm{X}_v$ of a vertex $v$ into an embedding vector $\\bm{z}_v$ and output it as follows:  where $\\mathcal{N}(v) = \\{u \\in V \\mid \\{v, u\\} \\in E\\}$ is the neighborhood vertex set of $v$, $\\bm{W}^{(l)} \\in \\mathbb{R}^{d_l \\times d_{l-1}}$ and $\\bm{b}^{(l)} \\in \\mathbb{R}^{d_l}$ are parameter matrix and bias vector of $l$-th layer, and $\\sigma$ is an activation function such as sigmoid or ReLU.  The specific architecture is shown on the left side of Figure~\\ref{fig:proposed_methods}.\nIt is worth noting the flexibility in selecting which layer's output from the GCNs to transfer as features.  In general, it is known that in neural network models, features obtained from layers closer to the output layer tend to be more task-specific, while those from layers closer to the input layer are more general-purpose. Although the latter may appear more suitable for transfer learning at first glance, models utilizing overly generic features often do not exhibit high performance in practice.  Indeed, it has been reported that the performance can vary depending on the choice of the transferred layer, even for the same task~\\citep{huang2022frustratingly}. For this reason, we consider options regarding which layer of the GCNs the prediction model should be coupled with. In this study, considering the nature of the task of predicting polymer properties, we judged that features extracted from layers closer to the output layer would function effectively. } Specifically, we consider three options formulated as following Eq.~\\eqref{eq:option1} and Eq.~\\eqref{eq:option2}.  Option 1 connects the $L$-th (final) layer of the GCN ($L$-th layer) with the prediction model, whereas option 2 and 3 conect the $L-1$ and $L-2$ layer with the prediction model:      &=      [\\bm{H}^{(L)}, \\bm{X}^{\\mathrm{tab}}], ~~~     &= [\\bm{H}^{(l)}, \\bm{X}^{\\mathrm{tab}}], ~~~ where, for option~1, $\\bm{H}^{(L)}$ are extracted features from GCNs of $L$-th layer, $\\bm{X}^{\\mathrm{tab}}$ are feature vectors such as molecular descriptors by RDKit and other vectors (e.g. molecular weights, compounding information, etc.).  Furthermore, $[\\bm{a}, \\bm{b}]$ is the symbol for the concatenation of vectors $\\bm{a}$ and $\\bm{b}$.  The basic symbol definition is the same in option 2 and 3.  Finally, we use the new feature vector $\\bm{F}^{(l)}$ $l = L, L-1, L-2$ as the input for our prediction model.  The overall structure of the proposed method's pipeline is illustrated in Figure~\\ref{fig:proposed_methods}. \\\\",
        "summarize_figure": "2505.03704v1_proposed_framework",
        "summarization": "The picture shows the detailed structure of a polymer property prediction model pipeline. On the left, a pre - trained graph neural network (GNN) acts as a feature extractor, processing molecular structure data represented as graphs through graph convolutional network (GCN) layers (including embed and pool layers) and GNN layers to extract features step by step, followed by output through fully connected layers. On the right, the extracted features, along with feature vectors like molecular descriptors, molecular weights, and compounding information, are fed as inputs to machine learning algorithms such as random forest and gradient boosted decision tree, ultimately outputting predicted polymer properties. Also, three options for connecting features extracted from different GCN layers (L - th, L - 1 - th, L - 2 - th) to the prediction model are considered."
    },
    "470": {
        "figure1": "2505.03709v1_terminology_safetycase",
        "label1": "fig:terminology",
        "caption1": "[]{Proposed ontology in the context of safety cases.~\\ArtifactLegend~and \\OntologyLegend~indicate artifacts and other ontology elements. GSN goals, strategies, contextual artifacts, and solutions are represented by \\GoalLegend,~\\StrategyLegend,~\\ContextLegend,~and \\SolutionLegend,~respectively. ",
        "text": "The terms ``safety case'' and ``safety assurance argumentation'' are often used interchangeably. A conceptual distinction is illustrated by \\Cref{fig:terminology} to clarify on their relationship, facilitating communication between stakeholders having concerns with respect to the documentation of system safety.\nMultiple standardized definitions (e.g., \\cite[\\nopp 4.2.37]{UL4600_2023}, \\cite[\\nopp 3.15]{BSI1881_2022}, and \\cite[\\nopp 3.136]{ISO26262_2018}) exist that share certain characteristics attributed to a safety case. Correspondingly, there is an objective to prove \\textit{safety} by a structured \\textit{argument} that is supported by \\textit{evidence} and considered in a specific environment (\\textit{context}). In accordance, \\Cref{fig:terminology} visualizes that evidence supports the claim of sufficient safety as the argumentation objective.  Yet, as safety is defined as absence of unreasonable risk in the context of road vehicles~\\cite[Part 1,~3.132]{ISO26262_2018}, the basis of the argumentation relies on residual risk. Implications for the starting point of the argumentation are discussed in \\Cref{subsec:TL_Claim}.",
        "summarize_figure": "2505.03709v1_terminology_safetycase",
        "summarization": "The image presents ontological concepts of Safety Case. It contains Safety Arguments, with components like Claims (GSN goals), GSN Strategy, Evidence (GSN solutions), and GSN Context, aiming to prove sufficient system safety. A Safety Case is an Assurance Case instantiation for specific safety concerns, arguing based on reasonable residual risk. The lower section shows a semi - formal argumentation (GSN model) structure."
    },
    "471": {
        "figure1": "2505.03712v1_Network_Architecture",
        "label1": "fig:network_architecture",
        "caption1": "The proposed neural network architecture for predicting the parameters of the Asymmetric Laplace Distribution $\\mathcal{AL}(\\theta, \\sigma, \\kappa)$.",
        "text": "The structure of the proposed model is illustrated in Figure~\\ref{fig:network_architecture}, where a shared encoder is followed by three independent heads to estimate the parameters $\\theta$, $\\sigma$, and $\\kappa$ of the ALD distribution. For the purpose of the experiments in Section~\\ref{sec:experiments} with structured data, we use fully connected layers with ReLU activation functions. The outputs of the model connected to $\\theta$, $\\sigma$ and $\\kappa$ are further constrained to be non-negative through an exponential (Exp) activation.  In addition, a residual connection is included to enhance gradient flow and improve model stability. See Appendix \\ref{appendix:b3} for more details about the architecture of the model.",
        "summarize_figure": "2505.03712v1_Network_Architecture",
        "summarization": "The picture shows the neural network architecture for predicting the parameters of the Asymmetric Laplace Distribution (AL(θ, σ, κ)). The input x passes through a Shared Decoder, with a Residual Connection to enhance gradient flow and model stability. Then the output enters three independent Linear layers respectively, and after passing through the exponential activation function (Exp), it outputs the parameters θ, σ, κ of the Asymmetric Laplace Distribution. For experiments with structured data, the model uses fully - connected layers with ReLU activation functions, achieving the prediction of distribution parameters through this architecture."
    },
    "472": {
        "figure1": "2505.03712v1_calibration",
        "label1": "fig:lineplot",
        "caption1": "Examples of best and worst calibration curves. Slope and intercept of the linear fit are shown in the legend.",
        "text": "Complementary to the CensDcal calibration metric, the slope and intercept summaries of the calibration curve provide a more intuitive (and graphical) perspective of the calibration results. Figure~\\ref{fig:lineplot} presents the best (first row) and worst (second row) results from our model on real-world data. The left and right columns represent the curves for Cal$[S(t | \\mathbf{x})]$ and for Cal$[f(t | \\mathbf{x})]$, respectively. The gray dashed line represents the idealized result for which the slope is one and the intercept is zero.",
        "summarize_figure": "2505.03712v1_calibration",
        "summarization": "The picture shows the best (first row) and worst (second row) examples of calibration curves, corresponding to Cal[S(t | x)] and Cal[f(t | x)] from left to right. The gray dashed line is the ideal calibration curve (with a slope of 1 and an intercept of 0). Different colored lines represent the calibration curves of various models, and the legend shows the slope and intercept of the linear fit for each curve. These curves allow for an intuitive comparison of the calibration performance of different models on real-world data, and the slope and intercept values help assess the deviation of model predictions from actual situations."
    },
    "473": {
        "figure1": "2505.03719v1_expt1",
        "label1": "expt1",
        "caption1": "Results of Experiment I: Decentralized Resource Allocation ($n = 20, p = 10, d = 40, \\kappa_C = 99, \\kappa_f = 1000, \\kappa_{A} = 8)$\\protect\\footnotemark.}  \\end{center",
        "text": "The experimental results are presented in \\cref{expt1}. From the figures, we observe that iD2A and MiD2A converge significantly faster than Tracking-ADMM and NECPD, w.r.t. the number of calls to $\\nabla f$ and $\\prox_g$, as well as the number of calls to $\\A$, $\\A\\T$, and $\\nabla \\mh^*$/$\\prox_{\\mh^*}$. Although the commmunication complexity of Tracking-ADMM is slightly lower than that of MiD2A, iD2A still demonstrates the lowest commmunication complexity among all algorithms. Additionally, it is evident that the subproblem solver plays a crucial role in the performance of iD2A. If the communication cost or the cost associated with calls to $\\A$, $\\A\\T$, and $\\nabla \\mh^*$/$\\prox_{\\mh^*}$ is higher than that to calls of $\\nabla f$ and $\\prox_g$, then iDAPG is the better choice. Conversely, if the situation is reversed, we should opt for PDPG or LPD.",
        "summarize_figure": "2505.03719v1_expt1",
        "summarization": "The picture shows the results of Experiment I (decentralized resource allocation, n = 20, p = 10, d = 40, κ_C = 99, κ_f = 1000, κ_A = 8), presenting the change of the optimality gap of different algorithms with the number of calls or communications. The results show that compared with Tracking-ADMM and NECPD, iD2A and MiD2A converge significantly faster in terms of calls to ∇f and prox_g, as well as calls to A, A^T, and ∇h*/prox_h*. Although the communication complexity of Tracking-ADMM is slightly lower than that of MiD2A, iD2A has the lowest communication complexity among all algorithms. In addition, the subproblem solver is crucial for the performance of iD2A. When the communication cost or the cost of calls to A, A^T, and ∇h*/prox_h* is high, iDAPG is a better choice; otherwise, PDPG or LPD should be selected."
    },
    "474": {
        "figure1": "2505.03719v1_expt3",
        "label1": "expt2",
        "caption1": "Results of Experiment II: Decentralized Elastic Net Regression ($n = 8, p = 20, d = 9, \\kappa_C = 25, \\kappa_f = 1, \\kappa_{pd} = 98603$).}  \\end{center",
        "text": "The experimental results are presented in \\cref{expt2}. According to the figures, we observe that iD2A and MiD2A with $\\rho>0$ exhibit significantly lower complexities than DCPA and NPGA across all metrics. Benefiting from Chebyshev acceleration, MiD2A has lower computational complexity (as indicated by the first two graphs) but higher communication complexity compared to iD2A, despite both having theoretical communication complexities of the same order. Therefore, in scenarios where the communication cost is much lower than the computation cost, MiD2A is preferable to iD2A. In contrast, iD2A with $\\rho = 0$ demonstrates much higher computational complexity than iD2A with $\\rho > 0$, but it converges very quickly w.r.t. the number of communications. This makes it a preferable choice when the communication cost dominates the total cost.",
        "summarize_figure": "2505.03719v1_expt3",
        "summarization": "The picture shows the results of Experiment II (decentralized elastic net regression, n = 8, p = 20, d = 9, κ_C = 25, κ_f = 1, κ_pd = 98603), presenting the change of the optimality gap of different algorithms with the number of calls or communications. The results show that iD2A and MiD2A with ρ>0 have significantly lower complexities than DCPA and NPGA across all metrics. MiD2A, benefiting from Chebyshev acceleration, has lower computational complexity (as shown by the first two graphs) but higher communication complexity than iD2A. Thus, MiD2A is preferable when the communication cost is much lower than the computation cost. In contrast, iD2A with ρ = 0 has much higher computational complexity than iD2A with ρ>0 but converges very quickly in terms of the number of communications, making it a better choice when the communication cost dominates the total cost."
    },
    "475": {
        "figure1": "2505.03719v1_expt2",
        "label1": "expt3",
        "caption1": "Results of Experiment III: Decentralized Constrained Linear Regression ($n = 8, p = 9, d = 9, \\kappa_C = 25, \\kappa_f = 1, \\kappa_{\\A_{\\rho}} = 8.24 \\times 10^{15}, \\kappa_{\\A'_{\\rho}} = 4.89 \\times 10^{16}$).}  \\end{center",
        "text": "We adopt the same $W$ and $C$ as in Experiment II, and the parameters for DCPA, NPGA, and iD2A are determined using the same methods as in Experiment II. The parameters of PDPG are set according to \\cite[Remark 1]{li2025inexact}. The experimental results are presented in \\cref{expt3}. From the figures, we observe that iD2A and MiD2A, which use iDAPG as the subproblem solver, exhibit significantly lower complexities than DCPA and NPGA across all metrics. Similar to Experiment II, MiD2A has lower computational complexity but higher communication complexity compared to iD2A. However, when PDPG is choosen as the subproblem solver, the performance of iD2A significantly degrades, even falling below that of NPGA-EXTRA. This aligns with the fact that iDAPG has lower theoretical complexity than PDHG, and also highlights the impact of the subproblem solver on the performance of iD2A.",
        "summarize_figure": "2505.03719v1_expt2",
        "summarization": "The picture shows the results of Experiment III (decentralized constrained linear regression, n = 8, p = 9, d = 9, κ_C = 25, κ_f = 1, κ_Aρ = 8.24×10¹⁵, κ_A'ρ = 4.89×10¹⁶), presenting the change of the optimality gap of different algorithms with the number of calls or communications. The results show that iD2A and MiD2A, which use iDAPG as the subproblem solver, have significantly lower complexities than DCPA and NPGA across all metrics. Similar to Experiment II, MiD2A has lower computational complexity but higher communication complexity than iD2A. However, when iD2A chooses PDPG as the subproblem solver, its performance degrades significantly, even falling below that of NPGA-EXTRA. This reflects that iDAPG has a lower theoretical complexity than PDHG and also highlights the impact of the subproblem solver on the performance of iD2A."
    },
    "476": {
        "figure1": "2505.03721v1_TL-DRL",
        "label1": "fig:DRL",
        "caption1": "The considered multi-agent DRL environment.}  \\vspace{-4mm",
        "text": "To enhance animal condition monitoring and prolong system lifespan, the smart farm utilizes multi-agent DRL (depicted in Fig.~\\ref{fig:DRL}). DRL agents operating on a LoRa gateway seek an optimal policy for selecting low-energy sensors to forward data to high-energy counterparts within each time interval. Specifically, these agents strive to optimize both monitoring quality and the residual energy of solar-powered sensors in the smart farm setting. The DRL agents aim to achieve the following: Here, $\\mathbf{a^*}$ represents the optimal action set $a_t$ chosen by an agent to maximize the sum of monitoring quality $\\mathcal{MQ}(a_t)$ and average remaining energy level $\\mathcal{RE}(a_t)$ of sensor nodes at time $t$, with both $\\mathcal{MQ}(a_t)$ and $\\mathcal{RE}(a_t)$ normalized to the range $[0,1]$ and equally weighted by $w_{MQ}=w_{RE}=0.5$. The formula for $\\mathcal{MQ}(a_t)$ is: where $T_{current}(a_t)$ denotes the operation time step under action $a_t$, $X$ represents the total number of sensed data points, and $d$ is the count of attributes per animal as described in the Node Model (see Section ~\\ref{subsec:node_model}). $GT_{i,j}$ refers to the ground truth data for the $j$th attribute of the $i$th data point, while $x_{i,j}$ denotes the observed data for these parameters. The function $\\mathrm{mq}(i, j)$ measures the monitoring quality for the $j$-th attribute against the $i$-th ground truth data, returning 1 for exact matches ($x_{i,j} == GT_{i,j}$) and 0 otherwise. While it is recognized that ground truth data is unavailable during real-world deployment due to practical constraints—since data collection occurs only at the gateways—we assume its availability during the training phase to enable the development of DRL agents in a controlled or simulated environment. This approach of integrating simulated and real-world data, a standard practice in DRL research, allows agents to learn optimal actions under ideal conditions and enhances their ability to generalize to real-world scenarios \\cite{osinski2020simulation, kang2019generalization, cutler2015efficient}. $\\mathcal{RE}(a_t)$ is defined as: The definition of $\\mathcal{RE}(a_t)$ involves energy consumption of data transmission from sensors to sensors ($\\mathcal{E}_{SS}$) and from sensors to gateways ($\\mathcal{E}_{SG}$), sensors' energy drained in active ($\\mathcal{E}_{active}$) and sleep ($\\mathcal{E}_{sleep}$) mode.  We focus exclusively on the energy consumption of sensor communications for data transmission, as other communication tasks (e.g., sending requests) consume significantly less energy in comparison.  Notably, enhancing $\\mathcal{MQ}(a_t)$ and $\\mathcal{RE}(a_t)$ represents conflicting goals; for instance, activating more sensors to increase data coverage ($\\mathcal{MQ}(a_t)$) will, in turn, reduce the overall energy efficiency ($\\mathcal{RE}(a_t)$) due to the greater energy expenditure on data transmission. The smart farm system is designed with key components: effective energy policy learning, monitoring mechanisms (e.g., DRL-based monitoring using either transfer learning or decision theory or other heuristics; see Section~\\ref{sec:exp-setup}), cyber-attack resilient data aggregation and updating, and a mechanism to identify deceptive data. These components are elaborated in Section~\\ref{sec:proposed-scheme}.",
        "summarize_figure": "2505.03721v1_TL-DRL",
        "summarization": "The picture shows a multi-agent Deep Reinforcement Learning (DRL) environment. DRL agents operating on LoRa gateways in a smart farm optimize monitoring quality and the remaining energy of solar-powered sensors by selecting low-energy sensors to forward data to high-energy sensors. Each agent obtains the state and takes actions within its observation area, receiving reward feedback. Agents and connections of different colors represent different interaction paths. The goal of the agents is to maximize the sum of monitoring quality and the average remaining energy of sensors, both normalized to the range [0,1] with equal weights of 0.5. However, improving monitoring quality and remaining energy are conflicting goals. The system also features key components such as energy policy learning and monitoring mechanisms."
    },
    "477": {
        "figure1": "2505.03721v1_dt_updated",
        "label1": "fig:dt-drl",
        "caption1": "Decision-making procedure of a DT-guided DRL agent.}  \\vspace{-4mm",
        "text": "The DT-guided DRL's effectiveness lies in combining the DT agent's action probabilities with the neural network (NN)'s output (see Fig.~\\ref{fig:dt-drl}). The process unfolds in several steps. \\paragraph{\\bf Generation of action distributions} Each DT agent computes the utility of possible actions in $\\mathcal{A}$ (see the utility function in Eq.~\\eqref{eq:dt-utility}). To integrate the DT into a DRL agent's decision-making, we must first convert the outcome of the utility function into a distribution, similar to how DRL agents select actions based on an action distribution. This ensures consistency in decision-making across both systems.  A \\textit{softmax} function then transforms these utility values into a probability distribution $\\mathrm{Prob}_{DT}(a^*)$, where actions with higher utilities are more likely to be chosen. Adjusting the \\textit{softmax} temperature enhances the DT agent's decisiveness. The DRL distribution by a DNN reflects current environmental learning, with the Proximal Policy Optimization (PPO)~\\cite{schu17} selected for its outperformance. \\paragraph{\\bf Integrating DT and PPO agents}  This merges their \\textit{logits} and probability distribution. The DRL agent's $\\mathrm{Logit}_{NN}(a^*)$ is obtained directly from its output, whereas the DT agent's $\\mathrm{Prob}_{DT}(a^*)$ is computed from the previous step. The two distributions are then summed, applying a weight $w$ to $\\mathrm{Prob}_{DT}(a^*)$ to adjust the DT agent's influence, resulting in $\\mathrm{Logit}_{NN}(a^*)+w\\cdot\\mathrm{Prob}_{DT}(a^*)$.  Initially, $w$ is set to 1, and it decreases over time (i.e., decay rate with 0.0003), allowing the DNN's increasingly accurate policy to gradually take precedence in decision-making.",
        "summarize_figure": "2505.03721v1_dt_updated",
        "summarization": "The picture shows the decision-making process of a Decision Theory (DT)-guided Deep Reinforcement Learning (DRL) agent. After inputting the state St, on one hand, the logits of actions Logit_NN are obtained through a neural network (NN). On the other hand, the DT agent calculates the utility U(st,ai) of each possible action, which is converted into action probabilities Prob_DT through a Softmax function (with a temperature parameter of 0.2). Then, Logit_NN and w·Prob_DT are added together (where w is the weight, initially set to 1 and decaying over time). After passing through a Softmax function (with a temperature parameter of 1), the final action selection probability Prob_DT_PPO is obtained, realizing the integration of DT and DRL decision-making. As time goes on, the DNN's policy gradually dominates the decision-making."
    },
    "478": {
        "figure1": "2505.03725v1_sharp_cost_comparison",
        "label1": "fig:ablations_complete",
        "caption1": "Complete ablation study comparing different BBO methods for constraint parameter optimization across all six simulated manipulation tasks. We evaluate CMA-ES, probabilistic hill climbing (HC), and random sampling (RS) on both pushing and drawing tasks.  We report averaged normalized performances across 5 independent seeds ($\\pm 1.96$ standard deviations). The top row shows the main results discussed in Section~\\ref{sec:Experiments}, while the bottom row provides additional task variations that confirm the observed performance trends.",
        "text": "For completeness, we list the full BBO experiment results across all tasks in Fig.\\ \\ref{fig:ablations_complete}. These results complement Fig.\\ \\ref{fig:ablations_main}, which is truncated in the main part owing to space constraints. The full results confirm the results from the main paper.  Across four of the six tasks, CMA-ES performs the best. For the pentagon drawing task, hill climbing performs the best. Random sampling performed best for this iteration of the obstacle avoidance task.",
        "summarize_figure": "2505.03725v1_sharp_cost_comparison",
        "summarization": "The picture shows the ablation study results of different Population-based Black-Box Optimization (BBO) methods for constraint parameter optimization across six simulated manipulation tasks. It compares the performance of Covariance Matrix Adaptation Evolution Strategy (CMA), Probabilistic Hill Climbing (HC), and Random Sampling (RS) in pushing and drawing tasks. The vertical axis represents the lowest cost, and the horizontal axis represents the number of steps. The results show that CMA performs best in four of the six tasks; HC performs best in the pentagon drawing task; and RS performs best in this iteration of the obstacle avoidance task. The figure also presents the average normalized performance across five independent seeds (±1.96 standard deviations)."
    },
    "479": {
        "figure1": "2505.03727v1_habitat",
        "label1": "fig:habitat",
        "caption1": "Pictorial representation of the one-dimensional (1D) spatial scenario used here. In the viable region, called patch or habitat, the growth rate is positive. This region extends from $x=-\\ell$ to $x=\\ell$. In the surrounding region, hostile conditions are present, implying a negative growth rate. The trajectory (time in the upward direction) depicts a random walk (black) with reset to position $x_r$ (red).",
        "text": "In one dimension, we define a good-quality region---referred to as \\textit{patch} or \\textit{habitat}---as the interval $\\Omega_{\\rm in}=(-\\ell,\\ell)$. In the habitat, the population grows at a rate $a_{\\rm in}$. This region is surrounded by a hostile environment (the complement set of $\\Omega_{\\rm in}$, denoted by $ {\\Omega}_{\\rm out}$), where the growth rate is negative,  $-a_{\\rm out}$, as depicted in Fig.~\\ref{fig:habitat}.  Organisms move via normal diffusion plus sporadic random relocation from a pre-defined region $\\Omega$ to a fixed position $x_r$ either inside or outside the habitat. For the region $\\Omega$ from where reset occurs, two cases will be considered. The first will be the entire space $\\Omega=\\Omega_{\\rm in}\\cup\\, \\Omega_{\\rm out}$ and the second just the outside region $\\Omega=\\Omega_{\\rm out}$. Moreover, we assume that the resets take place at random intervals, following a Poisson process with rate $r$, and each reset brings the walker to the fixed location $x_r$.",
        "summarize_figure": "2505.03727v1_habitat",
        "summarization": "The picture shows a schematic diagram of a one-dimensional spatial scenario. In the viable region (called patch or habitat) within the interval Ω_in = (-ℓ, ℓ), the population grows at a rate ain. In the surrounding hostile region Ω_out (the complement of Ω_in), the population growth rate is -aout. The trajectory in the figure (time upward) represents a random walk (black) and will reset to the position xr (red). Organisms move through normal diffusion plus occasional random re - location from a pre-defined region Ω to a fixed position xr (inside or outside the habitat). Two cases are considered for Ω: the entire space (Ω = Ω_in ∪ Ω_out) and only the outside region (Ω = Ω_out). Resets occur at random intervals according to a Poisson process with rate r."
    },
    "480": {
        "figure1": "2505.03727v1_fig-stat11",
        "label1": "fig:evolution",
        "caption1": " Temporal evolution of the total population size $N(t)$ scaled by the initial values $N_0$, when the half width is $\\ell=1.5$, $x_0=0$ and $x_r=0$ for given values of the resetting rate $r$ indicated in the legend. % The symbols refer to the average over $10^5$ stochastic simulations of Eqs.~(\\ref{eq:mouvement_process}) and (\\ref{eq:growth_process}) with $dt=10^{-5}$. Thin solid lines are given by numerical inversion of Eq.~(\\ref{eq:N_in_laplace})~\\cite{hoog}, while dashed lines correspond to the long-time approximation provided by Eq.~(\\ref{eq:N}). In the particular case $r=0$, we also plot (dotted line) the known closed-form expression~\\cite{skellam1991random,kierstead1953size}, and $s_M=s_{0} = 1-(\\pi/3)^2$ (see Appendix~\\ref{app:timesolution}). % For $r>0$ the values of $s_M$ are obtained numerically and read $s_{0.5}\\approx 0.0281$, $s_{2.0}\\approx 0.2993$. ",
        "text": "In Fig.~\\ref{fig:evolution}, we plot the evolution of $N(t)$ for different values of the reset rate $r$.  Symbols correspond to numerical simulations of the stochastic process defined by Eqs.~(\\ref{eq:mouvement_process}),   while the dashed lines are given by the theoretical prediction provided  by Eq.~(\\ref{eq:N}),  in good agreement with the simulations. In the case of Fig.~\\ref{fig:evolution} ($\\ell=1.5$ and $x_r=0$, which, in the absence of resetting, is slightly subcritical), we observe that the population decays, and the same happens for low reset rates. Conversely, for reset rates above a critical threshold ($r^*\\simeq 0.4$), the population exhibits growth.\nFor a sufficiently long time, when the dominant mode has settled (which can occur very quickly, as observed in Fig.~\\ref{fig:evolution}),  we can approximate $u(x,t) \\approx \\phi(x)\\exp(s_M t)$, which substituted into Eq.~(\\ref{eq:u}) gives  where ${\\cal N}\\equiv {\\cal N}(x_0,x_r,r, \\ell)$ was defined in Eq.~(\\ref{eq:pre_factor0}).  The solution of Eq.~(\\ref{eq:phi}) is the Green function associated to the 1D Helmholtz equation, that is,  where $\\xi_r = \\sqrt{1-r-s_M}$ (see Appendix \\ref{app:timesolution}).  The initial assumption implies that the spatial profile tends to a shape that remains unchanged except for the exponential pre-factor.",
        "summarize_figure": "2505.03727v1_fig-stat11",
        "summarization": "The picture shows the temporal evolution of the total population size N(t) relative to the initial value N0 under the conditions of half width ℓ = 1.5, x0 = 0, and xr = 0 for different reset rate r values. Triangles, squares, and circles represent reset rates r of 0.0, 0.5, and 2.0 respectively. The results show that the population size decays when r = 0 and at low reset rates, while it grows when the reset rate is higher than the critical threshold (r* ≈ 0.4). The symbols in the figure are numerical simulation results, and the dashed lines are theoretical predictions, which are in good agreement, reflecting the significant impact of the reset rate on the change of population size over time."
    },
    "481": {
        "figure1": "2505.03736v1_cc_token_levy_0.1",
        "label1": "fig:ring-synthetic-noises",
        "caption1": "Gaussian Noise} \\end{subfigure} \\hfill \\begin{subfigure}[b]{0.32\\textwidth} \\includegraphics[width=\\linewidth]{images/cc_token_student_1.5.pdf} \\caption{Student's $t$ noise} \\end{subfigure} \\hfill \\begin{subfigure}[b]{0.32\\textwidth} \\includegraphics[width=\\linewidth]{images/cc_token_levy_0.1.pdf} \\caption{Lévy $\\alpha$-stable noise} \\end{subfigure} \\caption{Comparison of performance on a ring graph under various types of injected stochastic gradient noise, measured by the average estimation error $(1/n) \\sum_{i=1}^n \\| \\w_i^t - \\w_* \\|$ over step count $t$.",
        "text": "In Figure \\ref{fig:ring-synthetic-noises}, we evaluate the performance of \\gtnsgdm\\ against baseline methods on a ring graph under various types of injected gradient noise. The results show that while \\dsgd\\ and \\gtdsgd\\ converge under Gaussian noise (which has bounded variance), they become unstable when exposed to heavy-tailed noise. Although \\dsgdgclip\\ remains stable across all noise types, it fails to converge to the optimum. In contrast, both \\gtnsgdm\\ and \\sen\\ exhibit robust convergence behavior and achieve near-optimal performance consistently across all noise scenarios. Although \\sen\\ outperforms \\gtnsgdm\\ by a small margin in this low-dimensional synthetic problem, \\gtnsgdm\\ outperforms it significantly in Transformers training over a real-world corpus.",
        "summarize_figure": "2505.03736v1_cc_token_levy_0.1",
        "summarization": "This graph shows the average estimation error over step count for a ring graph with Lévy α - stable noise. The x - axis is step count (0 - 10000), the y - axis is average estimation error. Different colored and marked curves represent various data series. As step count increases, errors show diverse trends: some drop fast and stabilize, others fluctuate greatly."
    },
    "482": {
        "figure1": "2505.03736v1_varying_nodes",
        "label1": "fig:bounds_dependence",
        "caption1": "Dependence on $\\lambda$} \\end{subfigure} \\hfill \\begin{subfigure}[b]{0.32\\textwidth} \\includegraphics[width=\\linewidth]{images/varying_noise.pdf} \\caption{Dependence on $\\sigma$} \\end{subfigure} \\hfill \\begin{subfigure}[b]{0.32\\textwidth} \\includegraphics[width=\\linewidth]{images/varying_nodes.pdf} \\caption{Dependence on $n$} \\end{subfigure} \\caption{Empirical studies on \\gtnsgdm's dependence on problem parameters $\\lambda, \\sigma, n$.",
        "text": "In Figure \\ref{fig:bounds_dependence}, we study \\gtnsgdm’s empirical dependence on problem parameters including connectivity ($\\lambda$), noise level ($\\sigma$), and the number of nodes ($n$). We vary each parameter while keeping the others fixed. In Figure \\ref{fig:bounds_dependence}(a), we inject the aforementioned Lévy $\\alpha$-stable noise and test the performance of \\gtnsgdm\\ on ring, directed exponential, undirected exponential, and complete graphs with $\\lambda = 0.904, 0.714, 0.6, 0$, respectively. It is shown that \\gtnsgdm\\ achieves comparable final errors under weak connectivity (i.e., large $\\lambda$) comparing to complete graphs, demonstrating its favorable dependence on network connectivity under heavy-tailed noise. In Figure \\ref{fig:bounds_dependence}(b), we evaluate \\gtnsgdm’s performance under different noise levels on a directed exponential graph. Under Gaussian noise with scale 1 (unit variance), \\gtnsgdm\\ reaches the best optimality; the final error increases as $\\sigma$ grows, as observed under both Gaussian and Lévy $\\alpha$-stable noise. In Figure \\ref{fig:bounds_dependence}(c), we inject Lévy $\\alpha$-stable noise and use complete graphs ($\\lambda = 0$ for all $n$) with varying number of nodes. we observe that as $n$ increases from 2 to 40, convergence speed improves. The corresponding final errors, i.e., $[0.4, 0.35, 0.29, 0.20, 0.21]$, demonstrate a clear speedup trend over a certain range of $n$, supporting the theoretical speedup  discussions in Remarks \\ref{rm:n_speedup} and \\ref{rm:improved_dependence}.",
        "summarize_figure": "2505.03736v1_varying_nodes",
        "summarization": "The picture shows the variation of the average estimation error with the step count and the number of nodes n in a complete graph. The horizontal axis is the step count ranging from 0 to 10000, and the vertical axis is the average estimation error. Curves with different colors and line - types represent cases where n equals 2, 5, 10, 20, and 40 respectively. As the step count increases, the average estimation error of each curve decreases. Also, as n increases from 2 to 40, the convergence speed accelerates, and the final error generally decreases, showing an acceleration trend within a certain range."
    },
    "483": {
        "figure1": "2505.03736v1_minigpt_complete_val",
        "label1": "fig:gpt-training-val",
        "caption1": "Ring Graph} \\end{subfigure} \\hfill \\begin{subfigure}[b]{0.32\\textwidth} \\includegraphics[width=\\linewidth]{images/minigpt_exp_val.pdf} \\caption{Directed Exponential Graph} \\end{subfigure} \\hfill \\begin{subfigure}[b]{0.32\\textwidth} \\includegraphics[width=\\linewidth]{images/minigpt_complete_val.pdf} \\caption{Complete Graph } \\end{subfigure} \\caption{Comparison of graph-wide average validation losses in decentralized training of Transformer models, over ring, directed exponential, and complete graphs.",
        "text": "In Figure \\ref{fig:gpt-training-val}, we plot the average validation loss and its standard deviation over five independent runs for each algorithm across three different topologies. The results show that, in this high-dimensional real-world task, \\gtnsgdm\\ is the only algorithm that consistently converges to near-zero loss with low variance, achieving average final loss levels of 0.258, 0.261, and 0.282 on ring, directed exponential, and complete graphs, respectively. Other baseline methods, while stable, plateau at loss levels between 5 and 6. This sharp contrast underscores that \\gtnsgdm\\ is not only theoretically sound but also robust in real-world tasks.",
        "summarize_figure": "2505.03736v1_minigpt_complete_val",
        "summarization": "The picture shows the variation of the average validation loss with the round number for different algorithms in the decentralized training of Transformer models in a complete graph. The horizontal axis is the round number, and the vertical axis is the validation loss. Curves with different colors and markers represent different algorithms. Some curves have high validation losses at the beginning and then drop rapidly. Notably, the algorithm represented by the green star - shaped line drops significantly and eventually approaches 0. Other curves level off at a loss level between 5 and 6. In high - dimensional real - world tasks, the algorithm of the green star - shaped line can consistently converge to a near - zero low loss value with low variance, while other baseline methods, though stable, cannot reduce the loss further."
    },
    "484": {
        "figure1": "2504.00785v1_1102p1",
        "label1": "2008sti_gdp",
        "caption1": "2008 China Stimulus Package QTT on GDP Growth Rate",
        "text": "Our first analysis focuses on the impact of the stimulus program on real GDP growth. Figure \\ref{2008sti_gdp} presents the estimated QTTs of the stimulus program across different quantiles, along with 95\\% confidence intervals based on a blockwise bootstrap method with $B = 1000$ replications.",
        "summarize_figure": "2504.00785v1_1102p1",
        "summarization": "This chart shows the Quantile Treatment Effects (QTT) of China's 2008 stimulus package on GDP growth rate. The horizontal axis is the quantile (τ) and the vertical axis is the growth rate (%). The blue curve represents the QTT of the 2008 stimulus package. The grey area is the 95% confidence interval obtained via a block - wise bootstrap method with 1000 replications. The black dashed lines are the upper and lower bounds of the 95% confidence interval, and the red horizontal line represents 0. As the quantile (τ) increases from 0.1 to 0.8, the QTT of the stimulus package shows a downward trend and mostly stays above 0, indicating that the stimulus package had a positive impact on GDP growth rate in most cases."
    },
    "485": {
        "figure1": "2504.01355v1_Huddy_fig2",
        "label1": "fig:huddy.original",
        "caption1": "Adapted \\citet{huddy2015expressive}, Figure 2A}  \\centering \\includegraphics[width= 0.7\\textwidth]{figures/Huddy_fig2.png} \\begin{minipage}{\\linewidth} \\end{minipage",
        "text": "Methods for analyzing conditional relationships have evolved over time. Traditionally, researchers have relied on linear interaction models to probe these relationships. \\citet{BCG2006} (hereafter, BCG 2006) introduced best practices for estimating and interpreting such models, including the widely used ``marginal effect plot'' to visualize these relationships when the moderator $X$ is continuous. Figure~\\ref{fig:huddy.original}, reproduced from \\citet{huddy2015expressive}, provides an example. The x-axis represents the moderator \\( X \\) (partisan identity), while the y-axis represents the effect of the treatment \\( D \\) (experimental partisan threat) on the outcome \\( Y \\) (anger). The gray ribbon indicates the 95\\% (pointwise) confidence intervals.",
        "summarize_figure": "2504.01355v1_Huddy_fig2",
        "summarization": "The chart titled \"A. Blog Study: Anger\" shows the relationship between partisan identity and the marginal effect of threat on anger. The horizontal axis represents partisan identity with values ranging from 0 to 1, and the vertical axis represents the marginal effect of threat on anger, ranging from -2 to 6. The black curve in the chart indicates the relationship between them, and the gray area is the 95% (point - wise) confidence interval. As the value of partisan identity increases, the marginal effect of threat on anger shows an upward trend, indicating that the stronger the partisan identity, the greater the marginal impact of threat on anger."
    },
    "486": {
        "figure1": "2504.01355v1_chp2_huddy_2015a_linear",
        "label1": "fig:huddy",
        "caption1": "Replicating \\cite{huddy2015expressive} Figure 2A}  \\begin{minipage}{\\linewidth} \\begin{center} \\hspace{-2em}\\includegraphics[width=0.8\\textwidth]{figures/chp2_huddy_2015a_linear.png} \\end{center} {\\footnotesize \\emph{Notes:} The black line represents the CME estimates based on the linear interaction model. The shaded area represent 95\\% pointwise confidence intervals. The dashed lines represent 95\\% uniform confidence intervals. The histograms at the bottom of the figure depict the distributions of $X$ across treatment (pink) and control (gray) groups.} \\end{minipage",
        "text": "To test $H_{0a}$, we can perform a $t$-test using $\\hat\\theta(x)$ and $\\hvar(\\hat\\theta(x))$. This is equivalent to visually inspecting whether the pointwise confidence interval in the marginal effect plot intersects the horizontal zero line. For instance, in Figure~\\ref{fig:huddy}, at $x = 0.5$, the pointwise confidence interval does not overlap with zero. This allows us to reject the null hypothesis that $\\theta(0.5) = 0$ at the 5\\% significance level.\nIn Figure~\\ref{fig:huddy}, the dotted lines represent the uniform confidence intervals, while the shaded area depicts the pointwise confidence intervals. The uniform confidence intervals are consistently wider, reflecting a more conservative approach to interval estimation that adjusts for multiple comparisons across the moderator's range. We observe that the lower bounds of the uniform confidence intervals cross the horizontal zero line around $x = 0.25$, suggesting that for values of the moderator $X$ greater than 0.25, the effect of the treatment becomes statistically distinguishable from zero at the 5\\% level, indicating a significant treatment effect. Therefore, a visual test using the uniform confidence intervals provides a practical method for performing hypothesis testing for $H_{0c}$.",
        "summarize_figure": "2504.01355v1_chp2_huddy_2015a_linear",
        "summarization": "This chart shows the relationship between the moderator \"Partisan Identity\" and the \"Conditional Marginal Effect (CME) of Threat on Anger\". The black solid line represents the CME estimates based on the linear interaction model. The gray shaded area is the 95% point - wise confidence interval, and the dashed lines represent the 95% uniform confidence intervals. The histograms at the bottom show the distribution of variable X in the treatment (pink) and control (gray) groups. As the value of partisan identity increases from 0 to 1, the CME of threat on anger shows an upward trend. When the partisan identity is approximately 0.25, the lower bound of the uniform confidence interval crosses the horizontal zero line, indicating that for values greater than this, the treatment effect is statistically different from 0 at the 5% significance level."
    },
    "487": {
        "figure1": "2504.01355v1_chp2_Noble2023a_kernel_trim",
        "label1": "fig:noble",
        "caption1": "Replicating \\cite{noble2024presidential} Figure 2}  \\begin{subfigure}{\\textwidth} \\centering \\begin{subfigure}{0.45\\textwidth} \\centering \\includegraphics[width=\\textwidth]{figures/chp2_Noble2023a_linear.png} \\end{subfigure} \\hspace{1em} \\begin{subfigure}{0.45\\textwidth} \\centering \\includegraphics[width=\\textwidth]{figures/chp2_Noble2023a_kernel.png} \\end{subfigure} \\caption{Full sample} \\end{subfigure} \\vspace{1em} \\begin{subfigure}{\\textwidth} \\centering \\begin{subfigure}{0.45\\textwidth} \\centering \\includegraphics[width=\\textwidth]{figures/chp2_Noble2023a_linear_trim.png} \\end{subfigure} \\hspace{1em} \\begin{subfigure}{0.45\\textwidth} \\centering \\includegraphics[width=\\textwidth]{figures/chp2_Noble2023a_kernel_trim.png} \\end{subfigure} \\caption{Trimmed sample} \\end{subfigure} \\begin{minipage}{1\\linewidth} {\\footnotesize \\emph{Notes:} Treatment $D$ is the out-partisanship of lawmakers, outcome $Y$ In each figure, the black dashed line represents the CME estimates; the red points (and bars) represent the binning estimates (and 90\\% pointwise confidence intervals); the shaded area and dotted lines represent 95\\% pointwise and uniform confidence intervals, respectively.} \\end{minipage",
        "text": "Using a large observational dataset, lack of overlap arises naturally due to partisan sorting in congressional districts: districts that overwhelmingly support the president tend to elect fewer out-party legislators, leading to imbalance across covariates—particularly the moderator. Figure~\\ref{fig:noble}(a) shows CME estimates based on the linear estimator (left panel), overlaid with binning estimates, and the kernel estimator (right panel). At the bottom of each panel, we plot histograms of the moderator by treatment condition. The plots reveal limited common support at the extremes of the moderator (when $X \\notin [-10, 20]$). Despite this, the linear estimator still produces estimates in these regions, which are not credible as they rely heavily on extrapolation. In contrast, the kernel estimator reflects this lack of support through very wide confidence intervals. It is worth noting that the author is clearly aware of this issue and presents CME estimates over the range \\(x \\in [-4, 16]\\), using the linear estimator, although the estimation is based on the full sample.\nTo ensure reasonable overlap, we trimmed the data based on the central 95\\% quantile range of the moderator \\(X\\). After trimming, both estimators exhibit more stable and nearly linear patterns, as shown in Figure~\\ref{fig:noble}(a). The narrower confidence intervals and smoother trends reflect improved covariate balance between treated and control units in the trimmed sample. The resulting estimates support the original finding: as a constituency becomes more supportive of the president, the gap between out-party and in-party lawmakers in referencing the president in congressional speeches narrows. This example shows that data trimming during the design phase improves the credibility of the estimates and may even help justify simpler models.",
        "summarize_figure": "2504.01355v1_chp2_Noble2023a_kernel_trim",
        "summarization": "The chart titled \"Replicating [citation] Figure 2\" shows the relationship between the moderator \"Presidential Vote Margin\" and the \"Conditional Marginal Effect (CME) of Treatment on Outcome\". The black solid line represents CME estimates, the gray shaded area is the 95% point - wise confidence interval, and the dashed lines are the 95% uniform confidence intervals. The histograms at the bottom show the distribution of the moderator in the treatment (pink) and control (gray) groups. Initially, there was limited common support at the extreme values of the moderator (when X is not in the range [-10, 20]). Linear estimates in these regions were not credible, while kernel estimates reflected the lack of support through wide confidence intervals. After data trimming (based on the middle 95% quantile range of the moderator X), both estimators showed more stable and nearly linear patterns, with narrower confidence intervals and smoother trends, indicating improved covariate balance between treated and control groups. The results support the original finding: the more a constituency supports the president, the narrower the gap between out - party and in - party lawmakers in referencing the president in congressional speeches."
    },
    "488": {
        "figure1": "2504.01441v1_BabyModel_y2_s2",
        "label1": "fig:IRFbiVAR",
        "caption1": "Impulse response functions related to the locally identified SVAR discussed in Section \\ref{sec:geo}.}  \\begin{center} \\subfigure[{response of $y_{1t}$ to $\\e_{1t}$}]{ \\includegraphics[scale=0.22]{./TablesFigures/BabyModel_y1_s1.pdf} } \\subfigure[response of $y_{1t}$ to $\\e_{2t}$]{ \\includegraphics[scale=0.22]{./TablesFigures/BabyModel_y1_s2.pdf} } \\subfigure[response of $y_{2t}$ to $\\e_{1t}$]{ \\includegraphics[scale=0.22]{./TablesFigures/BabyModel_y2_s1.pdf} } \\subfigure[response of $y_{2t}$ to $\\e_{2t}$]{ \\includegraphics[scale=0.22]{./TablesFigures/BabyModel_y2_s2.pdf} } \\end{center} \\begin{minipage}{\\textwidth}% {\\scriptsize{\\textit{Notes}: The bivariate SVAR is characterized by a non-zero restriction. In solid lines we report the IRFs obtained through $Q_1=(q_1^{(1)},\\,q_2^{(1)})$ while in dashed lines those obtained through $Q_2=(q_1^{(2)},\\,q_2^{(2)})$.\\par}} \\end{minipage",
        "text": "For a specific numerical illustration, let the bivariate VAR be characterized by constants that are zero and a single lag with reduced-form parameters B_1=\\left( 0.8     &-0.2\\\\ 0.1     & 0.6 0.49 & -0.14\\\\ -0.14 & 0.13 0.7     & 0\\\\ -0.2    & 0.3 and consider imposing restriction $(A_0)_{[1,1]}^{-1}=0.5 \\:\\Longleftrightarrow\\: (e_1^\\prime \\Chol)\\,q_1 = 0.5$. Following Eq. (\\ref{eq:biVARq1}) and Eq. (\\ref{eq:biVARq21}) - Eq. (\\ref{eq:biVARq22}) in Appendix \\ref{app:Geom}, we calculate the two admissible matrices $Q_1$ and $Q_2$ Q_1=\\left( 0.714   & -0.700\\\\ 0.700   & 0.714 Q_2=\\left( 0.714   & 0.700\\\\ -0.700 &        0.714 with associated admissible $A_{0}$ matrices A_0^{(1)}=\\left( 1.687   & 2.333\\\\ -0.320 & 2.381 A_0^{(2)}=\\left( 0.354   & -2.333\\\\ 1.680 & 2.381 Based on these structural parameter values, Figure \\ref{fig:IRFbiVAR} shows the impulse response of $y_t=(y_{1t},y_{2t})'$ to the structural shocks $\\e_{1t}$ and $\\e_{2t}$. Despite the simplicity of this example, it clearly illustrates the extent to which conclusions depend on the choice of observationally equivalent $Q$ matrices. {\\scriptsize{\\textit{Notes}: The left panel shows the identification of the $q_2^{(1)}$ and $q_2^{(2)}$ vectors (in blue), conditional on the identified $q_1^{(1)}$ (in black). Similarly, the right panel shows the identification of the $q_2^{(1)}$ and $q_2^{(2)}$ vectors (in blue), conditional on the identified $q_1^{(2)}$ (in black).}}",
        "summarize_figure": "2504.01441v1_BabyModel_y2_s2",
        "summarization": "This graph depicts the impulse - response function of y2 to structural shock e2. The solid line is for matrix Q1, the dashed line for Q2. Starting high, it drops fast, hits 0 around 4, enters negative values, and nears 0, showing shock impact weakening. Lines show response differences with matrix Q choices, highlighting conclusion dependence on equivalent Q matrices."
    },
    "489": {
        "figure1": "2504.01474v1_OnePlotCalifornian",
        "label1": "fig:summary-plots",
        "caption1": "Plot of the best objective gap $\\cL_* - \\cL_{best}^k$ achieved so far versus time. For each dataset (multiple instances), we aggregate the individual plots per instance by smoothing them over a same time interval and taking their arithmetic mean.}% %",
        "text": "For all methods, we set the first iterate to be the dual variables of balance constraints \\eqref{constr:uc-balance} in the continuous relaxation of the UC problem. This initialization strategy comes at a cheap cost and we have found it to reduce considerably the order of magnitude of the Lagrangian function compared to other strategies such as initialization at a fixed plausible price, say $\\pi_1 = [40, 40, \\dots, 40]$. This initialization is interpretable: these prices are exactly convex hull prices in the case where we only have minimum up/down times and constant start-up/shut-down costs and the classic tight $3$-bin formulation available in that case is used, see \\cite{hua2016convex}. In addition to the best iterate encountered, we also compute the average of the $10 \\%$ last iterates and, in case it gives a lower objective, return that average. This heuristic is inexpensive (only one additional oracle call) and, in our benchmarks, we typically observe a decrease of the relative error in the order of $2 \\times 10^{-6}$. We evaluate all methods on the 24 described instances. We use a stopping criterion of $15$ minutes for all methods. Results are reported in \\cref{fig:summary-plots} whose plots displays for each method the evolution of the objective function accuracy over time, averaged over all instances in each dataset.",
        "summarize_figure": "2504.01474v1_OnePlotCalifornian",
        "summarization": "This graph shows the best objective gap (L* - L_best^k) over time for the Californian dataset. Curves for methods like Bundle Proximal Level, Bundle Level, etc. are plotted. The y - axis is the best objective gap, the x - axis is time (seconds). All curves decline, showing improving objective function accuracy. Dantzig - Wolfe has too high an error to be visible. A dashed line marks a 5×10^-6 relative error for reference."
    },
    "490": {
        "figure1": "2504.01535v1_coverage_n_1000case_1RDD_kappa1.5",
        "label1": "fig:Case3",
        "caption1": "$n = 500, b = 1.2h$} \\end{subfigure} \\vspace{-0.8cm} % Adjust vertical space between grouped subfigures \\begin{subfigure}{0.9\\linewidth} \\centering \\begin{subfigure}{0.49\\linewidth} \\centering \\includegraphics[width=\\linewidth]{simulation/RDD/size_n_500case_1RDD_kappa1.5.pdf} \\end{subfigure} \\hfill \\begin{subfigure}{0.49\\linewidth} \\centering \\includegraphics[width=\\linewidth]{simulation/RDD/coverage_n_500case_1RDD_kappa1.5.pdf} \\end{subfigure} \\vspace{-0.5cm} \\caption{$n = 500, b = 1.5h$} \\end{subfigure} \\vspace{-0.8cm} \\begin{subfigure}{0.9\\linewidth} \\centering \\begin{subfigure}{0.49\\linewidth} \\centering  \\includegraphics[width=\\linewidth]{simulation/RDD/size_n_1000case_1RDD_kappa1.2.pdf} \\end{subfigure} \\hfill \\begin{subfigure}{0.49\\linewidth} \\centering \\includegraphics[width=\\linewidth]{simulation/RDD/coverage_n_1000case_1RDD_kappa1.2.pdf} \\end{subfigure} \\vspace{-0.5cm} \\caption{$n = 1000, b = 1.2h$} \\end{subfigure} \\vspace{-0.8cm} \\begin{subfigure}{0.9\\linewidth} \\centering \\begin{subfigure}{0.49\\linewidth} \\centering \\includegraphics[width=\\linewidth]{simulation/RDD/size_n_1000case_1RDD_kappa1.5.pdf} \\end{subfigure} \\hfill \\begin{subfigure}{0.49\\linewidth} \\centering \\includegraphics[width=\\linewidth]{simulation/RDD/coverage_n_1000case_1RDD_kappa1.5.pdf} \\end{subfigure} \\vspace{-0.5cm} \\caption{$n = 1000, b = 1.5h$} \\end{subfigure} \\caption{Plots of empirical sizes and coverages as functions of bandwidth over 1000 simulation runs for Model 3.",
        "text": "Similar to Section~\\ref{sec:sim.NR}, we evaluate the performance of Orig, TB, DB, TR and DR methods under this sharp RDD setting. For comparison, we also include the asymptotic robust bias correction approach proposed by \\cite{Calonico2014} (denoted as CCT). To  assess the sensitivity of the competing methods to the bandwidth choices,  Figure~\\ref{fig:Case3} displays the  empirical sizes  and   empirical coverages  based on 1000 simulations over $h \\in \\{0.15,0.18, \\dots, 0.27\\}$ for $n =500$ and $h \\in \\{0.12,0.18, \\dots, 0.24\\}$ for $n = 1000$ under the settings $b = 1.2h$ and $b = 1.5h$.  Table~\\ref{CI.case3} presents the corresponding average interval lengths and coverages.  The results align with the findings in Section~\\ref{sec:sim.NR}.  In particular, we observe a similar size distortion effect for the Orig method  as in \\cite{OTSU2015}. Overall, both TR and DR outperform CCT across the range of $h$ values, delivering empirical sizes and coverages closer to the target levels while keeping average interval lengths comparable. For instance, when \\( (n, h, b) = (1000, 0.21, 1.2h) \\), our proposed methods (TR and DR) achieve more than \\( 94.6\\% \\) empirical coverage with interval lengths of 0.185 and 0.187, respectively,  while CCT provides \\( 93.9\\% \\) coverage with an interval length of 0.19 under \\( (n, h, b) = (1000, 0.18, 1.2h) \\). This pattern holds across other \\( n \\) and \\( b \\) settings. %{\\color{red} See the bolded results for further evidence.}",
        "summarize_figure": "2504.01535v1_coverage_n_1000case_1RDD_kappa1.5",
        "summarization": "This picture shows the empirical coverage as a function of bandwidth in 1000 simulation runs, with sample size n = 1000 and b = 1.5h. The horizontal axis is the bandwidth and the vertical axis is the empirical coverage. There are multiple curves marked with different shapes and colors, representing different methods. Most curves show that as the bandwidth increases from 0.12 to 0.24, the empirical coverage tends to decrease. There are two horizontal dashed lines, possibly representing the target coverage range. The curves of some methods approach or are within this range at certain bandwidths, reflecting the differences in how different methods approach the target coverage at different bandwidths."
    },
    "491": {
        "figure1": "2504.01566v1_post_belief",
        "label1": "fig:post_belief",
        "caption1": "Managers could not identify analyst's use of GPT unless disclosed.} \\includegraphics[width=0.7\\linewidth]{JELS/Results/post_belief.png}\\\\ Note: The error bars indicate 95\\% confidence intervals.",
        "text": "We first test whether managers could identify the content created by analysts' use of GPT without disclosure. We present in Figure \\ref{fig:post_belief} the average percentage of managers who believe in GPT's involvement in content generation \\textit{after} evaluating Human-GPT and No-GPT decks under each experimental condition. The results suggest that managers could not tell GPT's participation in content generation when its source is not disclosed ($M_{Human-GPT} = 77.55\\%$, $M_{No-GPT} = 67.35\\%$, $p=0.26$ given a Chi-squared test of independence), providing evidence of information asymmetry about GPT's participation in content contribution without disclosure. Notably, analysts' non-disclosure of GPT use could make managers, in general, suspect their GPT use given the lower bound of the 95\\% confidence intervals being above 50\\% for both the No-GPT and Human-GPT deck.",
        "summarize_figure": "2504.01566v1_post_belief",
        "summarization": "This picture shows managers' average belief in GPT's involvement in content creation after evaluating decks with GPT participation (Human-GPT Deck) and without GPT participation (No-GPT Deck). The horizontal axis is divided into \"No Disclosure\" and \"Disclosure\". In the no - disclosure case, the belief in GPT's participation is slightly higher for the Human-GPT Deck than the No-GPT Deck. In the disclosure case, the belief is much higher for the Human-GPT Deck. Error bars represent 95% confidence intervals. Results suggest managers can't identify GPT's participation when not disclosed, indicating information asymmetry, yet they still suspect GPT use to some extent even without disclosure."
    },
    "492": {
        "figure1": "2504.01566v1_prompt_section_count_agg",
        "label1": "fig:num_prompts",
        "caption1": "Number of prompts used in each content generation section} \\includegraphics[width=0.6\\linewidth]{JELS/Results/prompt_section_count_agg.png",
        "text": "We first show in Figure \\ref{fig:num_prompts} the number of prompts used to update the content across the six content sections (problem statement, solution overview, implementation details, expected outcomes, team, and timeline). %, or other content such as titles.  To investigate whether the update of the Human-GPT deck from the No-GPT deck is due to the GPT output rather than analysts‘ second thoughts on the No-GPT content regardless of GPT output, we identify the contribution of GPT to the Human-GPT deck by the absolute differences in content similarity between the two versions of the brief (No-GPT and Human-GPT decks) and GPT responses. The content similarity between a deck and GPT responses is defined by the cosine similarity based on the text tokens using BERT tokenization ranging from 0 to 1, where a higher number indicates higher content similarity \\citep{devlin2018bert}.  We show in Figure \\ref{fig:gpt_contribution} the degree of GPT contribution to the content in the Human-GPT (versus the No-GPT) deck. The results shown in Figure \\ref{fig:gpt_contribution} suggest that the Human-GPT deck content results from analysts incorporating the GPT responses.\nAfter the deck evaluation task, the managers were asked to indicate their preference to authorize junior analysts to use GPT when preparing a research brief for each of the six content sections (i.e., the problem statement, solution overview, implementation details, expected outcomes, team, and timeline) on a scale of one to five. We allow managers' preference heterogeneity across different content sections due to variations in how analysts use GPT as shown in Figures \\ref{fig:num_prompts} and \\ref{fig:gpt_contribution}. We also inquired about the perceived usefulness (measured by four items on a scale from one to seven presented in Table \\ref{tab:item}, Cronbach alpha = 0.82) and risk (measured by two items on a scale from one to five presented in Table \\ref{tab:item}, Pearson correlation = 0.43) of using GPT to generate a research brief.",
        "summarize_figure": "2504.01566v1_prompt_section_count_agg",
        "summarization": "This picture shows the statistics of the number of prompts used to update content in six sections (Problem statement, Solution overview, Implementation details, Expected outcomes, Team, Timeline). The \"Expected outcomes\" section has the highest number of prompts, around 50. The \"Problem statement\" section follows, about 32. The \"Solution overview\" and \"Implementation details\" sections have about 13 and 10 prompts respectively. The \"Team\" section has the fewest prompts, about 5, and the \"Timeline\" section has about 7 prompts. There are significant differences in the number of prompts used across different sections."
    },
    "493": {
        "figure1": "2504.02077v1_Figure_1",
        "label1": "fig:payoff_revenue",
        "caption1": "Bob's payoff and auctioneer's expected revenue as a function of latency advantage $T$. As Bob's latency advantage increases, his payoff grows while the auctioneer's revenue decreases.",
        "text": "Without loss of generality, we will fix the parameters $\\sigma=1$ (which is equivalent to normalizing the units of time) and $p_0=1$ (which is equivalent to normalizing the units of price). Then, we can calculate the value of Bob's profit and auctioneer's revenue as function of Bob's latency advantage $T$. For simplicity, we focus on the case $L=0$ (no reserve price), see Section \\ref{fig:payoff_revenue}.",
        "summarize_figure": "2504.02077v1_Figure_1",
        "summarization": "This picture shows the changes of Bob's payoff (green curve) and expected revenue (red curve) with the latency advantage T. As T increases from 0 to 1, Bob's payoff shows an upward trend, growing from a relatively low value. The expected revenue shows a downward trend, decreasing from a relatively high value. When T approaches 1, Bob's payoff exceeds the expected revenue. This indicates that as Bob's latency advantage increases, his payoff increases while the auctioneer's revenue decreases."
    },
    "494": {
        "figure1": "2504.02077v1_TimePressure",
        "label1": "fig:theta",
        "caption1": "The last mover's theta (timing pressure) as a function of the limit price, compared with the monopolist's theta as a function of the limit price.",
        "text": "By plotting this timing pressure as a function of $L$ (see Figure \\ref{fig:theta}), we can see that for the last mover, timing pressure appears to be higher for the last mover than for the monopolist, but that they converge as $L$ approaches $p_0=1$.\nwith the monopolist's theta as a function of the limit price.}\\label{fig:theta}",
        "summarize_figure": "2504.02077v1_TimePressure",
        "summarization": "This picture shows the changes of the timing pressure (Theta) of the last mover (blue curve) and the monopolist (red curve) as a function of the limit price (L). When L is between 0 and about 0.8, the timing pressure of the last mover remains high and stable, while that of the monopolist is nearly 0. As L approaches 1, the timing pressure of the monopolist rises rapidly, and that of the last mover drops rapidly. They converge as L approaches 1. This indicates that as the limit price approaches a specific value (p0 = 1), the timing pressures of the two will change significantly and tend to be the same."
    },
    "495": {
        "figure1": "2504.02899v1_comparison",
        "label1": "fig:rq1_emissions",
        "caption1": "\\footnotesize \\justifying \\textbf{Impact of Meat-Free Days (MFD) on greenhouse gas (GHG) emissions and on-campus meal sales.} \\textbf{(a) GHG emissions:} Average monthly GHG emissions on MFDs (blue) compared to matched non-MFDs (orange). We observe a consistent and significant reduction in GHG emissions throughout the study period, with an overall decrease of 5,053 kgCO$_2$e/day ($p<0.001$), corresponding to a relative reduction of 52.9\\%. \\textbf{(b) Sales:} Average monthly on-campus meal sales on MFDs (blue) compared to non-MFDs (orange). Meal sales significantly decreased by an average of 16.8\\% ($p<0.001$) on MFDs, suggesting increased off-campus dining and potential offsetting of on-campus emission savings.}  \\vspace{-1mm",
        "text": "Fig. \\ref{fig:rq1_emissions}(a) illustrates the average GHG emissions for both MFDs and non-MFDs by month of the year. The reduction in emissions remained consistent and statistically significant throughout all months of the study, suggesting that the effect of the intervention is robust to seasonal variations.\nFig. \\ref{fig:rq1_emissions}(b) illustrates the  daily average on-campus sales for both MFDs and non-MFDs by  month of the year. The difference between meal purchases on MFDs vs.\\ non-MFDs is statistically significant throughout the study period.",
        "summarize_figure": "2504.02899v1_comparison",
        "summarization": "The left chart shows average monthly GHG emissions on Meat - Free (blue) and Non - Meat - Free (orange) days. Emissions on Meat - Free days were consistently lower, with a 5053 kgCO₂e/day reduction (52.9%, p < 0.001), robust to seasons. The right chart shows average monthly on - campus meal sales, with a 16.8% (p < 0.001) decrease on Meat - Free days, suggesting more off - campus dining and potential offset of emission savings."
    },
    "496": {
        "figure1": "2504.02899v1_tradeoff",
        "label1": "fig:tradeoff",
        "caption1": "\\footnotesize \\justifying \\textbf{Impact of off-campus dining on GHG emission savings.} Results from Monte Carlo simulations accounting for variability in the proportion of fast-food customers (x-axis). The x-axis represents the expected proportion of fast-food customers, computed as the mean $\\alpha / (\\alpha + \\beta)$ of a Beta distribution  where $\\alpha$ and $\\beta$ represent the assumed number of fast-food and non-fast-food customers, respectively. The y-axis shows the percentage offset in GHG due to off-campus dining. In the best-case scenario, where no off-campus customers visit fast-food restaurants, the offset effect reaches 58.7\\%. The average offset is not statistically different from the break-even point (red line) at 40\\% (95\\% CI [82.2\\%, 106.1\\%]). On average, the break-even point is reached at 50.2\\% fast-food proportion, beyond which the offset exceeds the emissions savings from MFD.}  \\vspace{-1mm",
        "text": "Under the most optimistic scenario—where no opt-out customers choose fast food—off-campus dining offsets 58.6\\% (95\\% CI [53.1\\%, 64.4\\%]) of the GHG reductions achieved by MFD (see Fig.~\\ref{fig:tradeoff}). This scenario is, however, unlikely. The campus community is composed of roughly 70\\% students, a group more likely to be drawn to the discounted student menu at the fast-food restaurant. In Fig.~\\ref{fig:tradeoff} we show that as the %average  proportion of opting-out customers eating at the fast-food increases, the offset effect intensifies. At 40\\% fast-food participation, off-campus emissions counteract 93.6\\% (95\\% CI $[82.2\\%, 106.1\\%]$) of MFD savings, reaching levels statistically indistinguishable from full offset (i.e., 100\\%).",
        "summarize_figure": "2504.02899v1_tradeoff",
        "summarization": "This picture shows the results of Monte Carlo simulations, reflecting the impact of off - campus dining on the offset of greenhouse gas emissions reduction. The horizontal axis is the proportion of fast - food customers (α/(α+β)), and the vertical axis is the percentage offset of greenhouse gas emissions reduction due to off - campus dining. The red line represents the break - even point. As the proportion of fast - food customers increases from 0 to 90%, the offset percentage keeps rising. In the best - case scenario, when no off - campus customers choose fast food, the offset effect reaches 58.7%. On average, the break - even point is reached when the proportion of fast - food customers is 50.2%. Beyond this proportion, the offset from off - campus dining will exceed the emissions savings from Meat - Free Days."
    },
    "497": {
        "figure1": "2504.03766v1_resource-production",
        "label1": "fig:recruit",
        "caption1": "Tipping recruitment} \\begin{center} \\includegraphics[scale=.4]{resource-production} \\end{center",
        "text": "In particular, I define the tipping recruitment function as: \t\tf(x)=\\begin{cases} \twhere $f(x)$ is the natural growth rate of the resource stock for $x\\ge0$. The tipping point is $x_p>0$ and the tipping penalty is $0<\\pi<1$. Below $x_p$, recruitment has low fecundity (the lower portion of Figure~\\ref{fig:recruit}) and above, recruitment has high fecundity (the upper portion of Figure~\\ref{fig:recruit}). The function $\\tilde f$ is strictly increasing,\\footnote{ \t\tWith the assumption that $\\tilde f$ is strictly increasing in the resource stock, the term ``maximum sustainable yield'' (MSY) is meaningless. This is unrealistic since, in the absence of harvesting, the resource stock would increase without bounds.  This can be remedied with the addition of a ``predation'' and/or ``overcrowding'' term with the idea that at high resource stock levels, predation and/or crowding become more significant. In particular, suppose that \t\tf(x)=\\begin{cases} \t\twhere $\\delta>0$ is the predation and/or overcrowding penalty. The $\\delta x$ term is analogous to depreciation in models of economic growth. Since my focus is not on comparisons between MSY and other possible outcomes, for simplicity I assume that $\\delta=0$.} \ttwice differentiable, concave, $\\tilde f(0)=0$, $\\lim_{x\\rightarrow0}\\tilde f'(x)=\\infty$ and $\\lim_{x\\rightarrow\\infty}\\tilde f'(x)=0$.",
        "summarize_figure": "2504.03766v1_resource-production",
        "summarization": "The graph depicts the relationship between resource stock and recruitment rate. A dashed line at xp marks the tipping point. Below xp, recruitment has low fecundity; above, it has high fecundity. The function f~​ is strictly increasing, twice - differentiable, concave, with f~​(0)=0, limx→0​f~​′(x)=∞, and limx→∞​f~​′(x)=0."
    },
    "498": {
        "figure1": "2504.03766v1_continuous-solution",
        "label1": "fig:no-tip",
        "caption1": "Smooth recruitment optimal harvest}\t \\begin{center} \\subfloat{\\includegraphics[scale=.4]{continuous-solution}} \\end{center",
        "text": "An optimal harvest plan solves: \t\t\tV(x_0)=\\max_{h_t\\ge0}\\int_{0}^\\infty e^{-\\rho t}u(h_t)dt \\\\ \t\t\tx_t\\ge0 \\\\ \twhere $x_0$ is the initial resource stock. The analysis of this otherwise standard problem is complicated by the discontinuity in $f$. \tThe current value Hamiltonian for this problem is: \twhere $\\lambda$ is the costate which represents the value of an infinitessimal increase in the resource stock, $x$. I will proceed to the analysis of \\eqref{eq:hamiltonian} in \\Cref{sec:solution}.  \tIn discussing trajectories (optimal and otherwise), it will be useful to refer to the policy function analogue, $h(x)$, % \tthat dictates the harvest rate when the resource stock is $x$. \tBefore proceeding, I briefly review the analysis for the simpler case where there is no tipping point and the recruitment function is continuous. In particular, consider the recruitment function, $A\\tilde f(x)$ where $A>0$. The solution to \\eqref{eq:hamiltonian} satisfies the following necessary conditions: \t\t&h^{-\\sigma}=\\lambda,\t\\label{eq:foc} \\\\ \t\t&\\dot\\lambda=\\lambda[\\rho-A\\tilde f'(x)],\t\\label{eq:lambda-lom} \\\\ \t\t&\\dot x=A\\tilde f(x)-h.\t\\label{eq:x-lom} \tDifferentiating \\eqref{eq:foc} with respect to $t$ and using \\eqref{eq:lambda-lom} yields: \tThis implies that the optimal harvest rate is increasing over time when the margin\\-al recruitment rate exceeds the social discount rate ($A\\tilde f'(x)>\\rho$) and declining when the marginal recruitment rate is less than the social discount rate ($A\\tilde f'(x)<\\rho$). A transversality condition, \timplies that the discounted value of the resource stock is zero in the limit and ensures that harvests are optimal over the entire time horizon. \tTogether \\eqref{eq:x-lom} and \\eqref{eq:h-lom} represent an autonomous system of first-order differential equations that governs the model's dynamics. \tSince $\\mathcal{H}$ is strictly concave in $(x,h)$, together with \\eqref{eq:transversality-standard}, there is a unique solution, $(\\hat x_t^c,\\hat h_t^c)$, that converges to steady-state $(\\hat x^c,\\hat h^c)$ (\\Cref{fig:no-tip}). Let $\\hat h^c(x)$ and $V^c(x)$ denote the corresponding policy and value functions. The superscript $c$ denotes variables and functions associated with the continuous problem. \t\tBoth here and subsequently, $x$ and $h$ variables with time subscripts are trajectories, an $h$ with a functional argument is a policy function and a ``hat'' denotes optimality. A ``hat'' variable with no time subscript or functional argument is an optimal stationary point.",
        "summarize_figure": "2504.03766v1_continuous-solution",
        "summarization": "The graph shows the relationship between resource stock and harvest rate. An orange dashed line marks x^c. Green and blue curves indicate the harvest rate increases with resource stock. Blue - curve arrows show direction of change: different trends on either side of x^c. It relates to an optimal harvest plan, analyzing resource stock dynamics and strategies."
    },
    "499": {
        "figure1": "2504.03766v1_resource-production-hyst",
        "label1": "fig:hyst-recruit",
        "caption1": "Hysteretic tipping recruitment}\t \\begin{center} \\includegraphics[scale=.4]{resource-production-hyst.pdf} \\end{center",
        "text": "In recent years there is evidence that recovery from environmental damage can be subject to hysteresis (\\citealt{field:coral, storlazzi:sedimentation} for coral reefs, \\citealt{lindig-cisneros:wetland} for wetlands, \\citealt{hirota:global} for rain forests). Hysteresis has become an important factor that marine ecologists consider as they seek to understand tipping points \\citep{selkoe:principles}. \tWhen recruitment is subject to hysteresis, there are two tipping points. If fecundity is high then the tipping point is given by $x_p$. If the resource stock falls below this tipping point, the renewable resource switches to low-fecundity. With hysteresis, a higher tipping point must be reached in order for the renewable resource to transition to high-fecundity; the low-fecundity tipping point is given by $x_p^\\mathcal{h}>x_p$. Functionally, the hysteretic recruitment function has a second argument, $s$: \t\tf(x,s)= \t\t\t(1-s)\\pi\\tilde f(x)+s\\tilde f(x) \twhere $s\\in\\{0,1\\}$ is the ecosystem's state with $s=1$ representing high-fecundity and $s=0$ low-fecundity. For $x<x_p$, $s=0$, for $x\\ge x_p^\\mathcal h$, $s=1$ and for $x\\in[x_p,x_p^\\mathcal h)$, $\\dot s=0$ (i.e., $s$ retains its value). Recruitment can change discontinuously at $x_p$ and $x_p^\\mathcal h$ (\\Cref{fig:hyst-recruit}).",
        "summarize_figure": "2504.03766v1_resource-production-hyst",
        "summarization": "The chart shows the relationship between resource stock and recruitment rate. Two vertical dashed lines mark the tipping points xp and xh p. When the resource stock is below xp, the recruitment rate is in a low - fecundity state; when it is above xh p, the rate is in a high - fecundity state; between xp and xh p, the ecosystem state remains unchanged. The green and gray curves show how the recruitment rate changes with resource stock in different states, reflecting the discontinuous changes in the recruitment rate at xp and xh p due to hysteresis."
    }
}