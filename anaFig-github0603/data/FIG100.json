{
    "0": {
        "figure1": "2502.16879v1_2-1scatter5",
        "label1": "fig:scatter",
        "caption1": "Consumption choices by LLMs. The diagonal line represents the budget constraint, and the dashed lines intersect at the theoretical optimal point. Each point represents one trial response.",
        "text": "First, we plot each model's consumption choices (c1, c2) against the budget constraint line and the theoretical optimal point as shown in Figure~\\ref{fig:scatter}. A point above the budget line indicates over-consumption (infeasible solution), while a point below represents under-consumption (inefficient resource utilization). The theoretical optimal point is marked by the intersection of two dashed lines.",
        "summarize_figure": "2502.16879v1_2-1scatter5",
        "summarization": "The chart shows the consumption choices of large language models (LLMs). The horizontal axis is the value of c1 and the vertical axis is the value of c2. The dashed line represents the budget constraint, and the intersection of two dashed lines is the theoretical optimal point. Points of different colors represent the trial responses of different LLMs (such as Deepseek V3, GPT 4o, etc.). Points above the budget line indicate over - consumption (infeasible solutions), while points below indicate under - consumption (inefficient resource utilization). The points of various models are distributed near and above or below the budget line, reflecting the differences between their consumption choices and the theoretical optimal situation."
    },
    "1": {
        "figure1": "2502.16879v1_2-2Distribution",
        "label1": "fig:distribution",
        "caption1": "Distribution of consumption choices across periods.",
        "text": "The distribution of consumption choices across periods provides another perspective for analyzing model behavior. Figure~\\ref{fig:distribution} presents the distribution of c1 and c2 for each model through box plots, where the boxes represent the interquartile range (25th to 75th percentiles), the middle line indicates the median, and the whiskers extend to the full range excluding outliers.",
        "summarize_figure": "2502.16879v1_2-2Distribution",
        "summarization": "The chart shows the distribution of consumption choices of different large language models (Deepseek V3, GPT 4o, Gemini 1.5 Pro, Claude 3.5 Sonnet, Llama 3.1 405B) in different periods through box plots. The left sub - chart is the c1 distribution, and the right one is the c2 distribution. In the box plots, the box represents the inter - quartile range (25th to 75th percentiles), the middle line is the median, and the whiskers extend to the full range excluding outliers. The position, size of the boxes and length of the whiskers of each model for c1 and c2 are different, reflecting the differences in the distribution of their consumption choices. Some models' data are more concentrated, while others have a higher degree of dispersion, and all deviate from the optimal values (represented by the dashed lines)."
    },
    "2": {
        "figure1": "2502.16879v1_3-1scatter5",
        "label1": "fig:scatter_noutility",
        "caption1": "Consumption choices by LLMs without utility function specification. The diagonal line represents the budget constraint.",
        "text": "First, we examine the consumption choices without utility function guidance. Figure~\\ref{fig:scatter_noutility} shows the scatter plots for each model.",
        "summarize_figure": "2502.16879v1_3-1scatter5",
        "summarization": "The chart shows the consumption choices of large language models (LLMs) without utility function specification. The horizontal axis is the c1 value and the vertical axis is the c2 value, with the dashed line representing the budget constraint. Points of different colors represent different LLMs (such as Deepseek V3, GPT - 4o, etc.). The points of various models are distributed near and above or below the budget line. Some points above the budget line indicate over - consumption (infeasible solutions), while those below indicate under - consumption (inefficient resource utilization), reflecting the differences in consumption choices of each model without utility function guidance and their relationship with the budget constraint."
    },
    "3": {
        "figure1": "2502.17044v1_Figure_3",
        "label1": "fig:SystemLossHist",
        "caption1": "\\textbf{Equity loss distribution for the banking system, for 1,000 synthetic COVID-19 type shocks for the three contagion channels.} Green: Direct losses from firm failures caused by the initial shock, blue: With additional losses from supply chain contagion, Purple: With additional losses from interbank contagion. The vertical lines denote the three different risk measures, Expected Loss (dashed), Value at Risk (solid), and Expected Shortfall (dotted).",
        "text": "Figure~\\ref{fig:SystemLossHist} shows the distribution of aggregated bank system equity losses for the 1,000 synthetic COVID-19 shock scenarios after each of the 3 contagion channels (sequential simulation steps). Direct losses, DI, (green) range from 1.6\\% to 5.5\\%; losses with supply chain contagion, DI+SC, (blue) range from 2.4\\% to 9.4\\% and losses with additional interbank contagion, DI+SC+IB (purple) range from 2.6\\% to 10.3\\%. Note that DI, SC, and, IB are vectors of length 1,000, each element corresponds to the system equity loss for one of the 1,000 COVID-19 shock scenarios. A pairwise two-sample Welch test between the three distributions confirms that the mean values are different with a $p$-value less than $10^{-15}$. While DI is slightly left skewed, the losses with supply chain contagion (DI+SC) show a fat tail towards large losses. Including interbank contagion (DI+SC+IB) the tail becomes slightly more pronounced. This is evident by looking at the difference between ES and VaR for each of the distributions. For DI+SC the difference is ES(DI+SC)-VaR(DI+SC)=0.78\\% and increases to 0.85\\% for DI+SC+IB. For a reference, for DI alone, the difference is 0.23\\%. The three financial risk measures are seen in Fig.~\\ref{fig:SystemLossHist} as vertical lines, Expected Loss (dashed), Value at Risk (solid) and Expected Shortfall (dotted). EL is found to be 3.6, 6.0, 6.6\\% for DI, DI+SC, and DI+SC+IB, respectively, while VaR is 4.5, 7.2, 7.9\\%, and ES is 4.8, 8.0, 8.7\\%.  The average direct loss, EL(DI), is amplified by 68\\% when taking into account supply chain contagion, (EL(DI+SC)/EL(DI)) and 83\\% for (EL(DI+SC+IB)/EL(DI)). The tail of direct losses, ES(DI), is amplified by 67\\% when taking into account supply chain contagion (ES(DI+SC)/ES(DI)) and 83\\% for (ES(DI+SC+IB)/ES(DI)).  This means that the direct shock channel contributes more than half of the overall losses in the banking system. While supply chain contagion still contributes substantially, interbank losses are again relatively small, however, not negligible. For more details, see Sec.~\\ref{sec:App_bankwiserisk}.",
        "summarize_figure": "2502.17044v1_Figure_3",
        "summarization": "The chart shows the distribution of aggregated bank system equity losses under 1,000 synthetic COVID - 19 shock scenarios for three contagion channels. Green represents direct losses (DI) from firm failures due to the initial shock (ranging from 1.6% to 5.5%), blue represents additional losses with supply chain contagion (DI+SC, ranging from 2.4% to 9.4%), and purple represents additional losses with interbank contagion (DI+SC+IB, ranging from 2.6% to 10.3%). Vertical dashed, solid, and dotted lines denote Expected Loss (EL), Value at Risk (VaR), and Expected Shortfall (ES). DI is slightly left - skewed, DI+SC shows a fat - tail towards large losses, and DI+SC+IB has a more pronounced tail. EL is 3.6%, 6.0%, 6.6% for DI, DI+SC, DI+SC+IB respectively; VaR is 4.5%, 7.2%, 7.9%; ES is 4.8%, 8.0%, 8.7%. When considering supply chain contagion, the average direct loss EL(DI) is amplified by 68%, and by 83% when considering interbank contagion. ES(DI) is amplified by 67% and 83% respectively in corresponding cases. The direct shock channel contributes more than half of the overall losses in the banking system. Supply chain contagion contributes significantly, and interbank losses are relatively small but not negligible."
    },
    "4": {
        "figure1": "2502.17044v1_Figure_4",
        "label1": "fig:Fig_3",
        "caption1": "\\textbf{Value at Risk (VaR) at the 95\\% level from the loss distribution created by 1,000 artificial COVID-type scenarios for all 19 banks.} The solid and shaded bars correspond to the scenario with and without supply chain contagion, respectively. Colors indicate the effects from the different parts of the model: Green,losses from the initially failing firm, $\\text{DI}$; Blue marks supply chain effects, $\\text{SC}$. Since there is no supply chain in the first scenario there are no shaded blue bars. Losses from the interbank contagion are in purple $\\text{IB}^\\text{W/WO}$. One can see that for most banks, losses from the supply chain, either direct, or indirect are the most relevant. Some banks, 7 and 8 specifically, are exposed to non-trivial amounts of risk from the interbank market. The total amount of risk exposure varies for each bank from 0 to 70\\% of the bank's equity. ",
        "text": "We now assess the importance of the three contagion channels for \\emph{individual} bank equity losses. We focus on VaR; for EL and ES see ~\\ref{sec:App_RiskMeasures}.  Figure~\\ref{fig:Fig_3} shows the Value at Risk (VaR) at the 95\\% level for all banks.  Colors indicate the contagion channels, Green is the direct loss channel, blue represents losses from supply chain contagion, and purple bars indicate losses due to interbank contagion.  To see the full importance of the supply chain contagion channel, we display two versions of VaR. The first is VaR(DI$_k$+SC$_k$+IB$_k$), as before (solid colored bar). For the second version we apply interbank contagion directly after the direct impacts, DI, (hatched colored bar), i.e., without considering supply chain contagion. This is necessary as the presence of supply chain contagion can affect the magnitude of interbank contagion. The difference in bar height is the total increase of banks' VaR from supply chain contagion.",
        "summarize_figure": "2502.17044v1_Figure_4",
        "summarization": "The chart shows the 95% Value at Risk (VaR) of loss distribution for 19 banks under 1,000 artificial COVID - type scenarios. Different colors represent the impacts of various contagion channels: green for direct losses from initially failing firms (DI), blue for supply chain effects (SC), and purple for losses from interbank contagion (IB^W/WO). Solid bars indicate the scenario with supply chain contagion, while shaded bars indicate the scenario without it. For most banks, losses related to the supply chain, either direct or indirect, are the most significant. Banks 7 and 8 face notable risks in the interbank market. The total risk exposure of each bank ranges from 0 to 70% of its equity."
    },
    "5": {
        "figure1": "2502.17906v3_OST-PriceImpact-Schematic",
        "label1": "fig:Schematic-OST-PI",
        "caption1": " Order-splitting behaviour (left) and the corresponding price impact (right). Institutional traders typically split their large metaorders into a long sequences of small child orders to minimize their transaction cost. The long-term predictability (long memory) of market-order flow arises due to this order splitting because the order signs of the child orders are identical. The resulting market impact---the price change between the initial and final times of metaorders---empirically follows a nonlinear scaling $I(Q)\\propto \\sqrt{Q}$, called the square-root law. ",
        "text": "Order-splitting behaviour (left) and the corresponding price impact (right). Institutional traders typically split their large metaorders into a long sequences of small child orders to minimize their transaction cost. The long-term predictability (long memory) of market-order flow arises due to this order splitting because the order signs of the child orders are identical. The resulting market impact---the price change between the initial and final times of metaorders---empirically follows a nonlinear scaling $I(Q)\\propto \\sqrt{Q}$, called the square-root law.  \t\t}\\label{fig:Schematic-OST-PI}\nThere are two empirical enigmas in financial market microstructure~\\cite{BouchaudText,Cont2001,Slanina}. The first enigma is the long memory of the market-order flow. A market order is an immediate decision to buy or sell stocks, which is represented by $\\eps_t=+1$ ($\\eps_t=-1$) for the buy (sell) market order at time $t$ (e.g., $\\{\\eps_t\\}_{t\\geq 1}=\\{+,+,+,-,+\\}$ signifies three buys, one sell, and one buy orders). The long memory implies that such order signs are easily predictable and, thus, the autocorrelation (ACF) of the market-order signs decays very slowly: \t\tE [\\eps_t\\eps_{t+\\tau}] \\propto \\tau^{-\\gamma}, \\>\\>\\> 0<\\gamma<1, \twhere $E[\\dots]$ represents the ensemble average in the steady state and the ACF's sum diverges as $\\sum_{\\tau =1}^\\infty E[\\eps_t\\eps_{t+\\tau}]=\\infty$. This formula implies that the order flow has persistence for a very long time, such as for a few days~\\cite{BouchaudText}. \tThen, why does this predictability arise ubiquitously in financial markets? This order-sign predictability originates from the metaorder splitters and is well formulated by the Lillo-Mike-Farmer (LMF) model~\\cite{LMF_PRE2005,GeneralizedLMF}. The LMF theory argues that institutional investors have hidden large orders (called metaorders $Q$), which are split into a series of child orders. Since the order signs of the child orders should be identical, the order flow has natural predictability (see Fig.~\\ref{fig:Schematic-OST-PI}). Particularly, the LMF theory predicted a formula connecting the market microstructure and the ACF: \twhere the power-law distribution $\\psi_m(Q)\\propto Q^{-\\alpha-1}$ is assumed for the metaorder size $Q$. Recently, the authors validated this prediction quantitatively by scrutinizing the microscopic dataset of the Tokyo Stock Exchange (TSE)~\\cite{SatoPRL2023,SatoPRR2023} and, thereby, have established the LMF theory as the causality theory for the long memory. \tHowever, this phenomenon is very counter-intuitive: given that the market-order flow is predictable, why is the price dynamics unpredictable? This puzzle arises because the price dynamics would exhibit predictable anomalous diffusion if the price impact were linear regarding the metaorder size, as documented in the linear propagator model~\\cite{BouchaudText,BouchaudProp1,BouchaudProp2} (except under the fine-tuned balance condition between the order-flow and price-impact decays). Why is the price dynamics diffusive despite the predictable order flow? This is the first empirical enigma, particularly because most traditional economic theory predicts linear price-impact models~\\cite{Kyle1985}.\nThe second enigma is the nonlinearity of the price impact, called the square-root law (SRL)~\\cite{BouchaudText,Toth2011} (see Fig.~\\ref{fig:Schematic-OST-PI}). In practice, the linear price-impact model is valid only for small metaorder size $Q$; for large $Q$, it has been reported that the price impact is a nonlinear function of the metaorder size $Q$:  \t\tI(Q):=E [\\eps \\Delta m \\mid Q] \\propto Q^{\\delta}, \\>\\>\\> \\delta\\approx \\frac{1}{2}, \twhere $\\Delta m$ is the price impact by the metaorder $Q$. Recently, the authors scrutinized the TSE dataset, provided a very accurate estimation of $\\delta$, and established the strict universality of the SRL, such that $\\delta$ exactly equals to $1/2$ for all liquid stocks on the TSE within statistical errors~\\cite{SatoPRL2025}. Still now, the cause of the strict universality of the SRL is unclear. One of the most promising models might be the nonlinear propagator model (called the latent-order book model~\\cite{BouchaudText,Toth2011,DonierLOB2015}), but a partially negative evidence was observed on the latent-order book model~\\cite{Guillaume2025}. Thus, there is no consensus yet regarding the microscopic origin of the SRL.",
        "summarize_figure": "2502.17906v3_OST-PriceImpact-Schematic",
        "summarization": "The chart shows the order - splitting behavior (left) and its corresponding price impact (right). Institutional traders usually split large metaorders into a series of small child orders to reduce transaction costs. Since the signs of these child orders are the same, the market order flow has long - term predictability (long memory), and its autocorrelation function decays slowly following the power law E [εtεt+τ] ∝ τ^(-γ) (0<γ<1 ). Regarding price impact, in practice, the linear price - impact model is valid for small metaorder sizes, but for large orders, the price impact I(Q) is a nonlinear function of the metaorder size Q, empirically following the square - root law I(Q) ∝ Q^(δ), where δ is approximately 1/2. These two phenomena are unsolved mysteries in the microstructure of financial markets, and there is currently no consensus on their microscopic origins."
    },
    "6": {
        "figure1": "2502.17906v3_Schematic-v5",
        "label1": "fig:model",
        "caption1": " Model schematic. Assuming $M$ order splitters with a metaorder size distribution $\\psi_m(Q) \\propto Q^{-\\alpha-1}$ and a rest time distribution $\\psi_{r}(\\Delta t)= e^{-\\Delta t/\\tau_r}/\\tau_r$, the price-impact contribution $\\Delta m^{(i)}$ for trader $i$ follows the nonlinear scaling $I(Q) \\propto Q^{\\delta}$. The total price movement $\\Delta m$ results from the independent accumulation of all traders' contributions. ",
        "text": "Model schematic. Assuming $M$ order splitters with a metaorder size distribution $\\psi_m(Q) \\propto Q^{-\\alpha-1}$ and a rest time distribution $\\psi_{r}(\\Delta t)= e^{-\\Delta t/\\tau_r}/\\tau_r$, the price-impact contribution $\\Delta m^{(i)}$ for trader $i$ follows the nonlinear scaling $I(Q) \\propto Q^{\\delta}$. The total price movement $\\Delta m$ results from the independent accumulation of all traders' contributions. \t\t}\\label{fig:model} \tHere we propose a minimal model that naturally extends the LMF framework to incorporate nonlinear price-impact dynamics, thereby unifying these two enigmas from a single, coherent perspective. We consider the system composed of $M$ order splitters whose metaorder-size statistics follow the power law $\\psi_m(Q)\\propto Q^{-\\alpha-1}$ with $1<\\alpha<2$. All traders are assumed to execute their metaorders whose price impact obeys the nonlinear scaling $I(Q)\\propto Q^{\\delta}$ with $0<\\delta\\leq 1$. Crucially, this model is exactly solvable. The price-impact contribution by a single trader can be mapped to the L\\'evy-walk theory~\\cite{KlafterB,gLWPRL1987} with nonlinear walking-speed down~\\cite{KlafterPRA1987}. Our exact solution shows that the price dynamics is always diffusive even in the presence of the long memory if $\\delta=1/2$. This result implies that the SRL plays the crucial role in mitigating the price impact due to the huge liquidity consumption by institutional investors. Additionally, we find that this model exhibits consistent behavior with other empirical laws (such as the inverse-cubic law~\\cite{BouchaudText,Slanina,GabaixNature,Gabaix2006,ICLLetter,ICLCompany,ICLIndex} and volatility clustering~\\cite{Slanina,Cont2001}). Thus, our simple model is very simple but minimally consistent with various empirical enigmas in finance with clear exact solutions.\nHere we additionally consider the price dynamics triggered by such metaorder splittings (see Fig.~\\ref{fig:model} for schematic): At $t=0$, trader $i$ waits for the start of metaorder execution according to the exponential resting-time distribution $\\psi_r(\\Delta t)=(1/\\tau_r)e^{-\\Delta t/\\tau_r}$, where $\\tau_r$ is the average resting time $\\tau_r$. Then, trader $i$ starts a metaorder execution at the initial time $t^{(i)}_{\\rm ini}$ and stops at the final time $t^{(i)}_{\\rm fin}$. Here we assume that the price impact exactly obeys the nonlinear price impact $I(Q)\\propto Q^{\\delta}$ with $\\delta \\in (0,1)$. For simplicity, we assume that the metaorder time interval $\\Delta t^{(i)} := t_{\\rm fin}^{(i)}-t_{\\rm ini}^{(i)}$ is proportional to the metaorder size $Q^{(i)}$, such that $Q^{(i)} = \\nu \\Delta t^{(i)}$, where the executed volume rate $\\nu>0$ is an identical constant among traders. The resulting metaorder price impact $\\Delta m^{(i)} := m^{(i)}(t_{\\rm fin}^{(i)})- m^{(i)}(t_{\\rm ini}^{(i)})$ is assumed to obey the nonlinear scaling: $\\Delta m^{(i)} = c\\eps^{(i)}\\left(Q^{(i)}\\right)^\\delta$, where $c>0$ is a constant, $\\eps^{(i)}$ is the order sign of the trader $i$'s metaorder, and $Q^{(i)}$ is the corresponding metaorder size.  \tUpon completing a metaorder, trader $i$ resets both order sign $\\eps^{(i)}=\\pm 1$ and metaorder size $Q^{(i)}$, takes a rest according to the resting-time distribution $\\psi_r(\\Delta t)$, and restart his next metaorder execution. For simplicity, we set $\\nu=c=1$ by appropriate choice of time and price units. \tIn this work, we consider this deterministic price-impact case as the minimal assumption, since incorporating Gaussian fluctuations has only a minor qualitative effect. Additionally, the volume $Q$ is assumed to be a real number instead of integers for analytical simplicity. Also, the impact decay after metaorder splitting is not considered in this work.",
        "summarize_figure": "2502.17906v3_Schematic-v5",
        "summarization": "The chart is a model schematic. It assumes there are M order splitters with a metaorder - size distribution following the power law ψm(Q) ∝ Q^(-α - 1) (1<α<2) and a rest - time distribution ψr(Δt) = e^(-Δt/τr)/τr. The price - impact contribution Δm^(i) for each trader i follows the nonlinear scaling I(Q) ∝ Q^δ (0<δ≤1), and the total price movement Δm is the independent accumulation of all traders' contributions. Traders wait to execute metaorders according to the rest - time distribution, starting at t^(i)ini and ending at t^(i)fin. The metaorder size Q^(i) is proportional to the time interval Δt^(i), and the price impact follows the nonlinear scaling Δm^(i) = cε^(i)(Q^(i))^δ. This model extends the LMF framework to incorporate nonlinear price - impact dynamics, unifying relevant puzzles in the microstructure of financial markets and being exactly solvable. When δ = 1/2, price dynamics is diffusive even with long memory, and is consistent with other empirical laws."
    },
    "7": {
        "figure1": "2502.17906v3_Diffusion-PDF-v4",
        "label1": "fig:diffusion",
        "caption1": " (a)~Phase diagram between superdiffusion and normal diffusion. The phase boundary is given by $2\\delta=\\alpha$, implying that the price dynamics is always diffusive by assuming the SRL $\\delta=1/2$. (b)~Numerical mean-squared displacement for $\\delta\\in \\{0.25,0.5\\}$, $\\alpha\\in \\{1.25,1.5,1.75\\}$, and $N=1$, showing a consistent behavior with our phase diagram. (c)~Numerical mean-squared displacement for $\\delta\\in \\{0.75,1\\}$, showing the crossover between superdiffusion (orange markers) and normal diffusion (blue markers). More detailed numerical results are presented in \\SM for other parameters. ",
        "text": "This model is exactly solvable because the price-impact contribution by a single trader can be mapped to the L\\'evy-walk theory~\\cite{KlafterB,gLWPRL1987} with nonlinear walking speed~\\cite{KlafterPRA1987} (see Methods). Indeed, let us define the price-impact contribution by trader $i$ as  \twith $\\Delta t_{k}^{(i)}:= \\min\\{t_{k; \\rm fin}^{(i)}, t\\}-t_{k; \\rm ini}^{(i)}$. By interpreting $\\Delta t_{k}^{(i)}$ as the ``flight time\" in L\\'evy walks, $\\Delta m^{(i)}(t)$ represents the cumulative displacement of a L\\'evy-walk particle with rests and nonlinear space-time coupling. Due to independent price-impact accumulation, the model is solved for any $M>0$.  \t\t\t(a)~Phase diagram between superdiffusion and normal diffusion. The phase boundary is given by $2\\delta=\\alpha$, implying that the price dynamics is always diffusive by assuming the SRL $\\delta=1/2$. \t\t\t(b)~Numerical mean-squared displacement for $\\delta\\in \\{0.25,0.5\\}$, $\\alpha\\in \\{1.25,1.5,1.75\\}$, and $N=1$, showing a consistent behavior with our phase diagram. \t\t\t(c)~Numerical mean-squared displacement for $\\delta\\in \\{0.75,1\\}$, showing the crossover between superdiffusion (orange markers) and normal diffusion (blue markers). More detailed numerical results are presented in \\SM for other parameters. \t\t}\\label{fig:diffusion} \tLet us present our first main result. The exact solution for the mean-squared displacement is given by  \t\tE[ \\Delta m^2(t) ] \\propto  \t\t\tt^{1+2\\delta-\\alpha} & \\mbox{if $2\\delta > \\alpha$} \\\\ \t\t\tt & \\mbox{if $2\\delta < \\alpha$} \timplying that superdiffusion arises if and only if $2\\delta > \\alpha$. Under the standard LMF assumption $1<\\alpha <2$, the price dynamics always exhibits normal diffusion for $\\delta \\leq 1/2$ (see Fig.~\\ref{fig:diffusion}). See Methods for the details. Note that this result is consistent with the linear propagator model~\\cite{BouchaudProp1,BouchaudProp2} for the specific case with non-zero permanent impact for $\\delta=1$. \tThis result is surprising because it guarantees price diffusion even in the presence of the long memory in the order flow, assuming the SRL $\\delta=1/2$. In other words, thanks to the sufficiently concave nature of the SRL, the market exhibits strong resilience against large liquidity consumption by order splitters. This interpretation is crucially important: it is the SRL that suppresses the large price movement in the presence of the long memory. This scenario highlights the importance of studying the microscopic origin of the SRL for stable regulation of financial markets.",
        "summarize_figure": "2502.17906v3_Diffusion-PDF-v4",
        "summarization": "The chart shows content related to superdiffusion and normal diffusion. Sub - figure (a) is a phase diagram with the phase boundary given by 2δ = α. When following the square - root law δ = 1/2, the price dynamics is always diffusive. The blue area is superdiffusion, and the yellow area is diffusion. Sub - figure (b) is the numerical mean - squared displacement for δ = 0.25, 0.5, α = 1.25, 1.5, 1.75, and N = 1, consistent with the phase diagram. Sub - figure (c) is the numerical mean - squared displacement for δ = 0.75, 1, with orange markers showing superdiffusion and blue markers showing normal diffusion. Research shows that superdiffusion occurs if and only if 2δ > α. Under the standard LMF assumption 1<α<2, the price dynamics shows normal diffusion when δ≤1/2, revealing the key role of the square - root law in suppressing large price movements when order splitters consume a large amount of liquidity."
    },
    "8": {
        "figure1": "2502.18253v1_0122_duration_ratio",
        "label1": "fig:distn_c_over_duration",
        "caption1": "The boxplot of ratio constant $C$ in 268 sampled experiments.",
        "text": "Empirically, the constant $C$ can be learned through the historical records. Figure~\\ref{fig:distn_c_over_duration} visualizes the values of the constant $C$ in 268 sampled experiments on Weixin experimentation platform. Our empirical result suggests that the value $C$ is relatively stable across the experiments, regardless the duration. Thus, choosing the empirical values of $C=2$ should be relatively robust. Note that in our sampled experiments, the outcome of is interest is some measurement of activeness and the covariates used is active level of participants, whose distribution is changing over the time and is highly correlated to the outcome of interest.",
        "summarize_figure": "2502.18253v1_0122_duration_ratio",
        "summarization": "The chart is a boxplot of the ratio constant C in 268 sampled experiments. The horizontal axis is the experiment duration (in days), and the vertical axis is the value of constant C. The boxplot shows the distribution of C values under different experiment durations. The box represents the inter - quartile range, the middle line is the median, the whiskers indicate the data range, and the diamond - shaped scatter points are outliers. Empirical results show that the value of constant C is relatively stable across experiments regardless of the duration, so choosing the empirical value C = 2 is relatively robust. In the experiments, the outcome of interest is a measure of activeness, and the covariate used is the active level of participants, whose distribution changes over time and is highly correlated with the outcome of interest."
    },
    "9": {
        "figure1": "2502.18253v1_auc",
        "label1": "fig:AUC_simul",
        "caption1": "The AUC score of survival models at different experiment stopping times.",
        "text": "We assess the performance of two survival models, the Kaplan-Meier model and the Cox model, using the AUC score. Specifically, we split the entire experiment dataset into 90\\% for training and 10\\% for testing. Note that at time $t$, we can observe $t$-period experimental data which is right censored, since subjects who have not participated in the experiment are still survived, albeit with an unknown survival duration. We fit the survival model on the training data, assuming that the experiment stops at each time $t$ within 30-day periods, and compute the AUC score on the test data. The results are presented in Figure~\\ref{fig:AUC_simul}. The mean AUC score over time for the Kaplan-Meier model is 0.8270 (s.e. 0.009911), while the mean AUC score for the Cox model is 0.8267 (s.e. 0.009880).",
        "summarize_figure": "2502.18253v1_auc",
        "summarization": "The chart shows the AUC scores of survival models at different experiment stopping times, comparing the Kaplan - Meier model (orange) and the Cox model (blue). The experiment divides the dataset into 90% for training and 10% for testing. Assuming the experiment stops at different times t within 30 days, the survival model is fitted on the training data and the AUC score is calculated on the test data. The results show that as time goes on, the AUC scores of both models gradually increase and stabilize. The mean AUC score of the Kaplan - Meier model is 0.8270 (s.e. 0.009911), and that of the Cox model is 0.8267 (s.e. 0.009880), indicating similar performance of the two models."
    },
    "10": {
        "figure1": "2502.18598v2_fig1",
        "label1": "figuretheorem",
        "caption1": "skip}{-0.1cm} % \\setlength{\\belowcaptionskip}{-0.1cm} \\begin{center}  \\includegraphics[width=1\\columnwidth]{fig1.pdf} \\caption{Truthful opportunity cost and corresponding bounds.} \\end{center} % \\vspace{-0.5cm",
        "text": "We first demonstrate some key characteristics of the bid bound, which directly validates the analytical results presented in the previous sections. Figure~\\ref{figuretheorem} compares the truthful maximum opportunity costs from 500 Monte Carlo samples of netload uncertainty realizations with the proposed chance-constrained opportunity cost bound (with confidence level $\\epsilon=5\\%$) and the deterministic opportunity cost bound motivated by the current CAISO default bids (the 4\\textsuperscript{th} highest day-ahead LMP). The proposed opportunity cost bounds effectively cap the true opportunity cost of storage with a guaranteed confidence level. Hence, the bid bounds which consist of fixed physical cost and opportunity cost bound can reliably cap the truthful bids submitted by storage participants. This result verifies the Theorem~\\ref{tm}. While the deterministic opportunity cost bound lies approximately at the median level across different scenarios, it is more likely to underestimate the truthful opportunity costs, potentially leading to reduced storage profits.\nWe also tested deterministic bid bounds, whose performance closely resembles the proposed bid bounds with $\\epsilon=50\\%$, although our proposed bounds are SoC-dependent. Under the low-uncertainty scenario, deterministic bounds yield a slightly lower cost reduction of 0.18\\% compared to the chance-constrained bounds. More significantly, under the high uncertainty scenario, deterministic bounds increase system costs by 0.01\\% due to underestimated storage opportunity costs, as illustrated in Figure~\\ref{figuretheorem}. Consequently, storage profits are compromised by approximately 20\\% across both low and high uncertainty scenarios. These results suggest that system operators should adopt chance-constrained bid bounds with $\\epsilon$ set around 5\\% to 10\\%.",
        "summarize_figure": "2502.18598v2_fig1",
        "summarization": "The chart compares the true opportunity cost, the chance - constrained bound (with a confidence level of ε = 5%) and the deterministic bound. The true maximum opportunity cost from 500 Monte Carlo samples of netload uncertainty realizations is compared with the two bounds. The chance - constrained bound can effectively cap the true opportunity cost of storage with a guaranteed confidence level. The bid bound, composed of fixed physical cost and opportunity cost bound, can reliably cap the true bids of storage participants. The deterministic bound is approximately at the median level across different scenarios but is more likely to underestimate the true opportunity cost, potentially reducing storage profits. In the low - uncertainty scenario, the cost reduction of the deterministic bound is slightly 0.18% lower than that of the chance - constrained bound. In the high - uncertainty scenario, due to underestimated storage opportunity costs, the deterministic bound increases system costs by 0.01%, and storage profits are compromised by about 20% in both low and high uncertainty scenarios. The results suggest that system operators should adopt chance - constrained bid bounds with ε around 5% - 10%."
    },
    "11": {
        "figure1": "2502.18598v2_fig2",
        "label1": "figsoc",
        "caption1": "Storage discharge and charge bid bounds variations with SoC under 20\\% and 35\\% storage capacity scenarios.} \\end{center}% \\vspace{-0.8cm",
        "text": "Figure~\\ref{figsoc} demonstrates that storage bid bounds decrease monotonically with SoC, confirming Proposition~\\ref{p2}. Comparing the trends at 20\\% and 35\\% storage capacities, the 35\\% case exhibits a significantly stronger dependency on SoC. This indicates that higher storage capacity has a more pronounced impact on system operations and marginal costs. In particular, the lower discharge bid bound at the 35\\% case suggests that storage operations further drove down the system LMPs.",
        "summarize_figure": "2502.18598v2_fig2",
        "summarization": "The chart shows the variations of storage charge and discharge bid bounds with the state of charge (SoC) under scenarios of 20% and 35% storage capacities. It can be seen that the storage bid bounds decrease monotonically with SoC, confirming the relevant proposition. Comparing the trends at 20% and 35% storage capacities, the 35% case shows a significantly stronger dependence on SoC, indicating that a higher storage capacity has a more pronounced impact on system operations and marginal costs. Especially the lower discharge bid bound in the 35% capacity case implies that storage operations further drive down the system's locational marginal prices (LMP)."
    },
    "12": {
        "figure1": "2502.18598v2_fig3",
        "label1": "figuncertainty",
        "caption1": "Storage bid bounds: (a) variations with netload uncertainty and (b) comparison to storage economic withholding bids.} \\end{center}% \\vspace{-0.5cm",
        "text": "Figure~\\ref{figuncertainty} (a) demonstrates that the storage bid bounds monotonically increase with uncertainty, which verifies the Proposition~\\ref{p3}. The discharge and charge bid bounds increase by 50\\% and 68\\%, respectively, as uncertainty scales from 0 to 5.\nThe bid bounds can benchmark storage economic withholding behavior. %\\footnote{economic withholding: If a unit is energy limited, its offer price will therefore exceed the true marginal cost~\\cite{harvey2001market}.}.  As illustrated in Figure~\\ref{figuncertainty} (b), the bids submitted by storage participants increase monotonically with the economic withholding scale.  Therefore, significant economic withholding results in excessively high discharge bids or low charge bids, thereby affecting storage clearing and ultimately impacting social welfare. The proposed bid bounds adjust inefficient storage bids by clipping any values that exceed these bounds. It is observed that bids with scale 0 and 3 economic withholding levels are reasonable, whereas bids with scale 5 economic withholding exceed the storage’s truthful marginal cost and are consequently capped by the bid bounds. Moreover, the bounds can help storage understand the system uncertainty level. Under the current netload uncertainty, the assumed price $\\sigma$ is expected to be around 15 \\$/MWh.",
        "summarize_figure": "2502.18598v2_fig3",
        "summarization": "The chart shows the situation related to storage bid bounds. Sub - figure (a) shows that the storage bid bounds increase monotonically with the netload uncertainty scale, verifying the relevant proposition. When the uncertainty scale increases from 0 to 5, the discharge bid bound increases by 50% and the charge bid bound increases by 68%. Sub - figure (b) indicates that the bid bounds can be used to measure the economic withholding behavior of storage. The bids submitted by storage participants increase monotonically with the economic withholding scale. Excessive economic withholding will lead to excessively high discharge bids or low charge bids, affecting storage clearing and social welfare. The proposed bid bounds adjust inefficient storage bids by limiting values that exceed the bounds. Bids with economic withholding levels of 0 and 3 are reasonable, while bids with a level of 5 exceed the true marginal cost and are capped by the bid bounds. At the same time, the bid bounds can help storage understand the system uncertainty level. Under the current netload uncertainty, the assumed price σ is about 15 $/MWh."
    },
    "13": {
        "figure1": "2502.18876v1_perturbation",
        "label1": "fig:proof",
        "caption1": "Perturbation of Non-Rectangular Regions, with Original Marginals (Solid) and Perturbed Marginals (Dashed)",
        "text": "The proof of \\Cref{thm:rectangle} can be found in the Appendix. The argument is involved, so we sketch the intuition here. We first sketch the intuition behind the first part of \\Cref{thm:rectangle}. By part $(ii)$ of \\Cref{thm:extreme-are-up-sets}, we know that any extreme point $q \\in \\overline{\\cQ}$ must be rationalized by a mixture of indicator functions defined on two nested up-sets $A_1 \\subseteq A_2$. Suppose for contradiction that the two nested up-sets differ not by a rectangle but by a ``flipped Z'' as in the dashed region of \\Cref{fig:proof}.  A key observation is that by choosing the correct ratio of (small enough) $\\varepsilon, \\hat{\\varepsilon} >0$ and perturbing the mixture in the dashed region using the way illustrated by \\Cref{fig:proof}, there exist two distinct monotone $q',q'' \\in \\overline{\\cQ}$ such that $\\nicefrac{1}{2} q'+\\nicefrac{1}{2} q''=q$, as illustrated by the marginals in \\Cref{fig:proof}, contradicting to that $q$ is an extreme point of $\\overline{\\cQ}$. The actual proof shows that for two arbitrary nested up-sets, if they differ by \\textit{any} non-rectangular region, then there must exist such a perturbation. It consists of three steps. First, we combine \\citet{gutmann1991existence}'s characterization of $\\mathcal{Q}$ (\\Cref{lem:gutmann}) and Theorem 1 of \\citet{nikzad2023constrained} to conclude that for any extreme point $q$ of $\\overline{\\cQ}$, $q_1$ must be an extreme point of a convex set of nondecreasing functions majorized by $\\hat{q}_2$, which in turn implies that there exist countably many disjoint intervals $\\{(\\underline{z}_k,\\overline{z}_k]\\}_{k=1}^\\infty$ such that $q_1 \\equiv \\hat{q}_2$ on $[0,1] \\backslash \\cup_{k=1}^\\infty (\\underline{z}_k,\\overline{z}_k]$, and equals a step function with at most two steps on each $(\\underline{z}_k,\\overline{z}_k]$. We then switch the role of $q_1$ and $q_2$ and apply the symmetric argument to $q_2$ fixing $\\hat{q}_1$.  Second, we use these properties and the fact that $(q_1, q_2)$ must be rationalizable by a mixture of two nested up-sets according to \\Cref{thm:extreme-are-up-sets} to derive a set of necessary conditions that $(q_1, q_2)$ must satisfy. In particular, we show that on each interval $(\\underline{z}_k,\\overline{z}_k]$, $q_1$ must be a constant and $\\hat{q}_2$ must be a step function with only one jump (symmetrically, $q_2$ and $\\hat{q}_1$ have the same feature). Moreover, we show that the intervals on which $q_1$ and $q_2$ are constants form a countable collection of disjoint ``off-diagonal'' rectangles on $[0,1]^2$. Third, for any such $(q_1, q_2)$, we explicitly construct a monotone $f$ that rationalizes it, and show that the monotone function $f$ allows for a perturbation similar to the one we construct in \\Cref{fig:proof}, unless it coincides with a mixture of two nested up-sets differing by at most a rectangle as depicted in \\Cref{fig2a}.",
        "summarize_figure": "2502.18876v1_perturbation",
        "summarization": "The figure Perturbation of Non-Rectangular Regions shows three graphs. The upper-left and lower graphs respectively show the variations of q2(z) and q1(z) with z, where solid and dashed lines represent original and perturbed marginals. The upper-right graph is a square region, with the red part marked 1, the white part marked 0, and hatched small rectangles marked with λ, λ + ε, λ - ε. It helps prove a theorem. By perturbing non-rectangular regions like the dashed area, a contradiction to the extreme point property is constructed. If two nested up-sets differ by a non-rectangular region, such a perturbation exists. The proof involves three steps: combining relevant characterizations, deriving necessary conditions for (q1, q2), and constructing a monotone function for rationalization."
    },
    "14": {
        "figure1": "2502.18876v1_MPM",
        "label1": "fig:MPM",
        "caption1": "A Markup-Pooling Mechanism",
        "text": "We say that a mechanism $(p, t)$ is a \\textit{\\textbf{markup-pooling}} mechanism if there exists a nondecreasing function $\\phi$, an interval $I=[c_L, c_H]$, and a constant $k \\in [0, 1]$ such that  A markup-pooling mechanism is illustrated by \\Cref{fig:MPM}. In such a mechanism, trade is implemented if and only if the value $v$ is above a monotone transformation of the cost $\\phi(c)$ (the \\textit{\\textbf{markup function}}), with the exception that when $c$ falls into a specific interval $I$ (the \\textit{\\textbf{pooling interval}}), we simply \\textit{resample} the cost from the two ends of the interval and execute the trade if the buyer's value $v$ is above the marked up, resampled cost $\\phi(\\tilde{c})$.",
        "summarize_figure": "2502.18876v1_MPM",
        "summarization": "The picture illustrates a Markup - Pooling Mechanism. The horizontal axis is c and the vertical axis is v. There is a dashed line labeled φ(c), dividing the region into two parts. The upper - left red area is marked as 1, and the lower - right white area is marked as 0. There is a hatched rectangle between cL and cH, marked as k. In this mechanism, trade occurs if and only if the value v is above the monotonic transformation of the cost φ(c), except when the cost c falls into the specific interval [cL, cH]. In the latter case, the cost is resampled from the two ends of the interval, and trade is executed if v is above the marked - up, resampled cost φ(tilde{c})."
    },
    "15": {
        "figure1": "2502.18970v2_Fig_shock",
        "label1": "fig:lp2",
        "caption1": "Military news shock (proportion of GDP) from \\cite{RameyZubairy2018",
        "text": "The simulation results are summarized in Table \\ref{tab:lp1} for $n=300$ and $500$, where we use ``LP'' to denote the standard LP's OLS estimator. Focusing on the IRF, here we report the coverage frequency and median CI length for each $\\beta_1^{(h)}$.  Under the panel ``Estimation'', PEL enjoys much smaller variance and overall MSE than LP, showing the benefit of taking advantage of the sparsity.  In terms of the coverage frequency for each coefficient $\\beta_1^{(h)}$,  here we report those of the even $h$ as the results under the odd $h$ have virtually no difference in terms of the patterns. The empirical coverage of LP  is lower than the nominal counterpart, while PPEL (with long-run variance estimated under the Parzen kernel) performs much better.  The observation that  LP's coverage frequency does not improve as the sample size expands from $n=300$ to $n=500$ stems from actual data on military news shocks, as depicted in Figure \\ref{fig:lp2}. Historically, a significant portion of America's major shocks --- 88\\% of the total 500 shocks by absolute magnitude --- happened within the initial 300 observations (prior to 1960). The standard deviation of these first 300 shocks is 0.077, whereas it notably drops to 0.012 for the following 200 shocks.\nJohn Maynard Keynes introduced the government spending multiplier in his \\emph{General Theory}, which is foundational to his broader theories on fiscal policy and its role in managing economic activities, especially during recessions.  Despite the importance of the concept, the measurement of the multiplier is by no means straightforward, because it is difficult to isolate it from confounding factors.  An influential recent empirical study of the fiscal multiplier by \\cite{RameyZubairy2018} uses quarterly data from 1889 to 2015.  We follow the same specification to replicate their Figure 5, and then use our method to re-evaluate the IRFs. While our simulation in Section \\ref{subsec:lp} utilizes only the real data of news shock shown in Figure \\ref{fig:lp2}, here in the empirical application we  use the real time series of government spending and GDP. The LP specification in \\eqref{eq:lp1} sets  either government spending or GDP as the target variable, and $\\text{shock}_t$ is again the military spending news. The control variable $\\bfw_{t-1}$ includes four lags of the news shock, government spending, and GDP. A state-dependent alternative version of the model \\eqref{eq:lp1} is also considered for the subsamples of high/low-unemployment states, respectively.",
        "summarize_figure": "2502.18970v2_Fig_shock",
        "summarization": "The figure shows military news shock data (as a proportion of GDP) from 1900 - 2020. The vertical axis is \"shock\", and the data fluctuates significantly. Historically, 88% of America's 500 major shocks by absolute magnitude occurred within the first 300 observations (before 1960). The standard deviation of these 300 shocks was 0.077, dropping to 0.012 for the next 200 shocks. This shock distribution explains why the coverage frequency of the LP estimator didn't improve as the sample size increased from 300 to 500."
    },
    "16": {
        "figure1": "2502.18970v2_Fig_pce_coef_3",
        "label1": "fig:pce_coef",
        "caption1": "The AR(1) coefficient matrix of the 16-sector VAR(1) model}  \\end{center} \\begin{flushleft} Notes: List of the code and name of each sector.\\\\ \\texttt{V1}: \\texttt{Motor vehicles and parts}, \\texttt{V2}: \\texttt{Furnishings and durable household equipment}, \\\\ \\texttt{V3}: \\texttt{Recreational goods and vehicles}, \\texttt{V4}: \\texttt{Other durable goods}, \\texttt{V5}: \\texttt{Food and beverages purchased for off-premises consumption}, \\texttt{V6}: \\texttt{Clothing and footwear}, \\texttt{V7}: \\texttt{Gasoline and other energy goods}, \\texttt{V8}: \\texttt{Other nondurable goods}, \\texttt{V9}: \\texttt{Housing and utilities}, \\texttt{V10}: \\texttt{Health care}, \\\\ \\texttt{V11}: \\texttt{Transportation services}, \\texttt{V12}: \\texttt{Recreation services}, \\texttt{V13}: \\texttt{Food services and accommodations}, \\texttt{V14}: \\texttt{Financial services and insurance}, \\texttt{V15}: \\texttt{Other services}, \\texttt{V16}: \\texttt{Final consumption expenditures of nonprofit institutions serving households (NPISHs)}. \\end{flushleft",
        "text": "To understand the dynamics and sectoral spillover effects, we follow the implementation of our simulation in Section \\ref{subsec:sim-var} to  fit a 16-sector VAR(1). The estimate of  coefficient matrix is shown in Figure \\ref{fig:pce_coef}. While some sectors have upstream-downstream relationships, other sectors are less connected.  One salient feature is that most autoregressive coefficients (those on the diagonal) are non-zeros, which is a key driving force of the persistence of inflation. Secondly, the estimated matrix is overall quite sparse. Notice that \\texttt{V7}: \\texttt{Gasoline and other energy goods} is much more volatile than other sectors, due to weather conditions, geopolitical uncertainty, and occasional energy crises, and therefore the fitting mechanism delivers many more active coefficients along its row to reduce the magnitude of the corresponding residual.",
        "summarize_figure": "2502.18970v2_Fig_pce_coef_3",
        "summarization": "The picture shows the AR(1) coefficient matrix of a 16 - sector VAR(1) model. The vertical and horizontal axes are labeled V1 to V16, representing different sectors such as V1 for Motor vehicles and parts and V7 for Gasoline and other energy goods. The differently - colored squares in the matrix represent coefficient values, ranging from -2 to 2 as the color goes from blue to red. According to the text, most autoregressive coefficients (on the diagonal) are non - zero, which is a key driver of inflation persistence. The coefficient matrix is overall quite sparse. The V7 sector is more volatile due to weather, geopolitical factors, and energy crises, so there are more active coefficients in its row to reduce the residual."
    },
    "17": {
        "figure1": "2502.18970v2_Fig_all_new",
        "label1": "fig:lp1",
        "caption1": "Point estimates of IRF and 95\\% confidence intervals computed from LP (gray dotted line) and PPEL (blue solid line). The first column: the full sample; the second column: subsample of high-unemployment state; the third column: subsample of low-unemployment state",
        "text": "The empirical results are reported in Figure \\ref{fig:lp1}. It consists of six subplots, each being the IRF of the fiscal multiplier of the news shock to government spending (the first row) and GDP (the second row). The three columns represent the full sample (the first column), the high unemployment periods (the second column, 181 observations), and the low-unemployment periods (the third column, 320 observations), following the original empirical study. The split of ``bad times'' and ``good times'' is to check the structural stability of the IRFs under distinctive states of the economy.\nThe two methods, \\cite{RameyZubairy2018}'s LP estimated by OLS (which is the same as the original paper) and this paper's PPEL, return very close point estimates.  The main message is that the fiscal multipliers are mostly much smaller than unity in American history through booms and recessions, so that counter-cyclical policies likely have dampened effects. However, there are visually salient gaps in terms of the interval estimates. Recall that our simulation in Section \\ref{subsec:lp} has shown that, despite the narrower length of CI, the coverage of  LP may deviate from the nominal coverage frequency.  It is echoed by Figure \\ref{fig:lp1} where PPEL has slightly wider CIs than  LP  for obtaining nominal CIs. In particular, in the second column of the subplots, the LP with a small $h =0, 1,\\ldots,5$  has extremely narrow confidence intervals; given a sample size of 181 observations, it is surprising to see that the confidence intervals are even much narrower than those from the full sample of 501 as in the first column. The counterintuitive observation is likely the consequence of the low empirical coverage probability that deviates from the designated nominal coverage rate. On the other hand, the length of the confidence intervals from PPEL suitably reflects the statistical randomness that corresponds to the sample sizes.  The evidence suggests that the latter provides more reasonable quantification of the underlying uncertainty.",
        "summarize_figure": "2502.18970v2_Fig_all_new",
        "summarization": "The picture consists of six sub - plots, showing the point estimates of the impulse response functions (IRF) of the fiscal multiplier of news shocks to government spending (first row) and GDP (second row), along with 95% confidence intervals computed from LP (gray dotted line) and PPEL (blue solid line). The three columns correspond to the full sample, high - unemployment subsample, and low - unemployment subsample. The point estimates from the two methods are quite close, but there are significant differences in interval estimates. The confidence intervals of LP may be unusually narrow due to low coverage probability, while the lengths of PPEL's confidence intervals more reasonably reflect the statistical randomness corresponding to the sample sizes, suggesting that PPEL provides a more reasonable quantification of underlying uncertainty."
    },
    "18": {
        "figure1": "2502.19525v3_figure1_log_likelihood_ratio",
        "label1": "fig:ln_evolution",
        "caption1": "(a) Evolution of \\( l_n \\) over steps \\( n \\) for different privacy budgets and non-private scenario \\( \\epsilon \\). (b) Comparison between exact and approximate solutions for \\( l_n \\) when \\( \\epsilon = 1 \\).",
        "text": "To support \\Cref{thm:smoothly-learning-speed}, we performed simulations to analyze the evolution of the log-likelihood ratio \\( l_n \\) without an approximation. \\Cref{fig:ln_evolution}(a) illustrates the trajectory of \\( l_n \\) under three different privacy budgets and the non-private scenario, revealing a clear relationship between privacy constraints and learning efficiency. Specifically, as the privacy budget decreases, the rate at which the log-likelihood ratio increases becomes steeper. This observation suggests that stronger privacy constraints, while introducing noise into individual actions, paradoxically enhance the speed of belief convergence by amplifying the informativeness of correct actions. This aligns with our theoretical findings that the smooth randomized response mechanism asymmetrically affects the likelihood of correct and incorrect actions, thus improving the efficiency of information aggregation. Moreover, we observe a significant difference in learning speed between the private and non-private settings: In the private setting, the log-likelihood ratio grows at a rate of \\( \\Theta_{\\epsilon}(\\log(n)) \\), whereas in the non-private setting, where privacy budgets approach infinity, asymptotic learning occurs at the slower rate of \\( \\Theta(\\sqrt{\\log(n)}) \\). This confirms that our privacy-aware mechanism leads to a provable acceleration of information aggregation compared to classical sequential learning models.\nFurthermore, we validate the robustness of our approximation approach by comparing the exact evolution of \\( l_n \\) with its approximation, as depicted in \\Cref{fig:ln_evolution}(b). The results indicate that, while there are minor deviations, overall trends remain consistent across different privacy settings. This confirms that the approximation method used in our theoretical analysis reliably captures the key dynamics of sequential learning under privacy constraints. These findings further reinforce the counterintuitive insight that privacy-preserving mechanisms, rather than simply introducing noise that degrades learning performance, can strategically reshape the information structure to accelerate the convergence of beliefs in sequential decision-making settings.",
        "summarize_figure": "2502.19525v3_figure1_log_likelihood_ratio",
        "summarization": "The picture contains two sub - plots. Sub - plot (a) shows the trajectory of the log - likelihood ratio ln over steps n under different privacy budgets (ε = 0.1, ε = 0.5, ε = 1) and the non - private scenario. It can be seen that as the privacy budget decreases, the growth rate of the log - likelihood ratio becomes faster, indicating that stronger privacy constraints, while introducing noise to individual actions, can accelerate the convergence of beliefs by amplifying the informativeness of correct actions. There is a significant difference in learning speed between private and non - private settings. Sub - plot (b) compares the exact and approximate solutions of ln when ε = 1. The results show that although there are minor deviations, the overall trends are consistent, indicating that the approximation method used in the theoretical analysis can reliably capture the key dynamics of sequential learning under privacy constraints."
    },
    "19": {
        "figure1": "2502.19525v3_figure3_binary_non_mon",
        "label1": "fig:binary_non_mon",
        "caption1": "Probability of correct cascade vs. privacy budget (\\( \\epsilon \\)) for different cascade thresholds (\\( k \\)). Each colored line represents the probability of a correct cascade for a specific threshold \\( k \\).",
        "text": "In the non-private sequential learning model with binary signals, the information cascade threshold $k$ is only a function of the probability $p$ of receiving correct signals. However, the randomized response strategy affects the information cascade threshold $k$, which can vary with the privacy budget $\\epsilon$ for a fixed $p$. Without using randomized response, as long as an information cascade is not started, an agent receiving a signal of $s_n = +1$ will choose the action $a_n = +1$ with probability one and the action $a_n = -1$ with probability zero. However, following the randomized response strategy, the probability difference between the choice of different actions narrows, reducing the informational value that each randomized action $x_n$ conveys. Consequently, a larger difference in the number of actions is required to trigger an information cascade. Given an information cascade threshold $k$, for $\\epsilon \\in (\\epsilon_{k+1}, \\epsilon_{k}]$, the information cascade threshold $(k)$ does not change with increasing privacy budget ($\\epsilon$). In this range, $\\epsilon \\in (\\epsilon_{k+1}, \\epsilon_{k}]$, as the privacy budget increases, the flip probability $u$ of the randomized response strategy decreases, the agents are more inclined to select the correct actions and their actions contain more information about their private signals. Consequently, the probability of the correct cascade will increase. However, increasing $\\epsilon$ beyond $\\epsilon_k$ will cause the information cascade threshold to decrease by $1$ from $k$ to $k-1$. With the information cascade occurring earlier for $\\epsilon \\in (\\epsilon_{k},\\epsilon_{k-1}]$ there is less opportunity for information aggregation and we see a drop in the probability of correct cascades by increasing $\\epsilon$ above $\\epsilon_k$ at the end of each $(\\epsilon_{k+1}, \\epsilon_{k}]$ interval; see \\Cref{fig:binary_non_mon}. \\textit{It is worth mentioning that the probability of the correct cascade is not monotonic with changes in the differential privacy budget $\\epsilon$. There are breakpoints at the endpoints of the intervals, where, by increasing the differential privacy budget, the probability of the correct cascade decreases because the information cascade threshold decreases by one. \\underline{This offers an opportunity to set privacy budgets on normative grounds}. In each neighborhood, the interval end points, $\\epsilon_k$, are strictly preferred to the values of $\\epsilon$ that are higher than but close to the interval end points, both from the perspectives of privacy protection and learning efficiency}.",
        "summarize_figure": "2502.19525v3_figure3_binary_non_mon",
        "summarization": "The picture shows the relationship between the probability of correct cascade and the privacy budget ε for different cascade thresholds k. Different colored lines represent the probability of correct cascade for specific k values (k = 2, k = 3, k = 4, k = 5). When ε is in the interval (εk + 1, εk], the information cascade threshold k does not change with the increase of ε. In this interval, as ε increases, the flip probability u of the randomized response strategy decreases, and individuals are more inclined to choose correct actions, so the probability of correct cascade increases. However, when ε exceeds εk, the information cascade threshold decreases from k to k - 1, the cascade occurs earlier, the opportunity for information aggregation decreases, and the probability of correct cascade drops. That is, the probability of correct cascade is non - monotonic with the change of the privacy budget ε, and the interval endpoints are breakpoints."
    },
    "20": {
        "figure1": "2502.19525v3_action_and_normal",
        "label1": "fig:action_and_normal",
        "caption1": "Probability of action $+1$ given signal $s_{n}$, in blue, and the the signal distribution given $\\theta=+1$, in orange",
        "text": "As shown in \\Cref{fig:action_and_normal}, in the continuous model, there exists a threshold value $t(l_{n})$ of the private signal $s_{n}$ for different actions as a function of public belief $l_{n}$, where the optimal action of the agent $n$ changes from $-1$ to $+1$.  when the private signal $s_{n}$ exceeds this threshold, agent $n$ chooses the action $+1$; otherwise, they choose the action $-1$. The existence of this threshold, which is determined by the public log-likelihood ratio $l_{n}$, implies that actions are highly sensitive to the value of signals in the vicinity of the threshold. On the other hand, the definition of the privacy budget in the continuous model (\\Cref{def:dp-cont}) considers all adjacent pairs of neighboring signals $s_{n}$ and $s'_{n}$ with $||s_{n} - s'_{n}||_{1} \\leq 1$, including pairs $s_{n}$ and $s'_{n}$ that fall on the two sides of the threshold. Therefore, all agents $n$ must change their actions with the same constant probability regardless of the threshold value. We summarize this important observation in \\Cref{thm:random-response-C}",
        "summarize_figure": "2502.19525v3_action_and_normal",
        "summarization": "The picture shows, in the continuous model, the probability of taking action +1 given signal sn (in blue) and the signal distribution given θ = +1 (in orange). There exists a threshold value for the private signal sn, which is a function of the public belief ln. When the private signal sn exceeds this threshold, agent n chooses action +1; otherwise, they choose action -1. This indicates that actions are highly sensitive to signal values near the threshold. Also, the definition of the privacy budget in the continuous model takes into account all adjacent signal pairs, including those on either side of the threshold, so all agents n must change their actions with the same constant probability regardless of the threshold value."
    },
    "21": {
        "figure1": "2502.19620v1_calibrated_att_by_het",
        "label1": "fig:calibrated_by_het",
        "caption1": "Estimator performance under various treatment effect heterogeneity scenarios} \\centering \\includegraphics[width=0.9\\linewidth]{calibrated_att_by_het.png}  {\\\\ \\raggedright \\textbf{Notes:} This figure presents the results of 1000 Monte Carlo simulations using the data-generating processes described above. The points represent the average value of the estimate across 1000 simulations. 95\\% confidence intervals are shown. ``No difference'' refers to a scenario with no average treatment effect, and no difference between subgroups. \\par ",
        "text": "Table \\ref{tab:sim_study_att_by_het} and Figure \\ref{fig:calibrated_by_het} highlight two key features of the performance of the estimators for the DATT and the CDATT under the various scenarios for treatment effect heterogeneity.",
        "summarize_figure": "2502.19620v1_calibrated_att_by_het",
        "summarization": "The figure shows the performance of DATT (blue triangles) and CDATT (red dots) estimators under treatment effect heterogeneity scenarios from 1000 Monte Carlo simulations. The horizontal axis lists estimation methods like No controls, 3WFE, IPW, RA, DR. The vertical axis shows estimator parameter values, with error bars for 95% confidence intervals. Five sub - plots, from \"No difference\" to γ values of 0, 0.2, 1, 5, display how estimator mean values and confidence intervals change across scenarios."
    },
    "22": {
        "figure1": "2502.19620v1_calibrated_att_by_psor",
        "label1": "fig:calibrated_psor",
        "caption1": "Simulation study results based on 1000 Monte Carlo trials with $N=1000$} \\centering \\includegraphics[width=0.75\\linewidth]{calibrated_att_by_psor.png}  {\\\\ \\raggedright \\textbf{Notes:} This figure presents the results of 1000 Monte Carlo simulations using the data-generating processes described above with $\\gamma=1$. The points represent the average value of the estimate across 1000 simulations. 95\\% confidence intervals are shown. \\par ",
        "text": "Next, Table \\ref{tab:sim_study_att_psor} and Figure \\ref{fig:calibrated_psor} compare the performance of the different estimators of the CDATT. The results highlight both the double-robustness of the doubly-robust estimator and its semiparametric efficiency.",
        "summarize_figure": "2502.19620v1_calibrated_att_by_psor",
        "summarization": "The picture shows the results of 1000 Monte Carlo simulations (N = 1000, γ = 1), comparing the performance of DATT (blue triangles) and CDATT (red dots) under different estimators (No controls, 3WFE, IPW, RA, DR). The four sub - plots correspond to the cases of correct/incorrect propensity scores and correct/incorrect outcome regressions. The points represent the average value of the estimates across 1000 simulations, and the error bars are 95% confidence intervals, highlighting the double - robustness and semiparametric efficiency of the doubly - robust estimator."
    },
    "23": {
        "figure1": "2502.19620v1_gruber_demographics",
        "label1": "fig:gruber_demographics",
        "caption1": "Descriptive statistics by subgroup} \\centering \\includegraphics[width=0.7\\linewidth]{gruber_demographics.png",
        "text": "To motivate the inclusion of covariates and their role in this analysis, I first highlight the divergence between demographic groups in these covariates. Figure \\ref{fig:gruber_demographics} shows, for example, that single women age 20-40 tend to have higher education than the other demographic groups, are more likely to be non-white, and are more likely to work in a white-collar job. This motivates the desire to separate between the DATT and CDATT parameters. If certain jobs, such as white-collar jobs, are more or less sensitive to the mandated benefits, then any difference in ATTs between the demographic groups might be explained by their different job characteristics rather than their gender, age, or marital status.",
        "summarize_figure": "2502.19620v1_gruber_demographics",
        "summarization": "The picture is a bar chart of descriptive statistics by subgroup, showing the situations of married men aged 20 - 40, single women aged 20 - 40, married women aged 20 - 40, and single men and men over 40 in terms of some college education, non - white, union job, and white - collar job. Among them, single women aged 20 - 40 have relatively high proportions in some college education and white - collar jobs, and also a relatively high proportion in non - white. This indicates that there are differences among different demographic subgroups in these covariates, suggesting the necessity of including covariates and distinguishing between DATT and CDATT parameters in the analysis, as job characteristics may be the reason for differences in treatment effects among groups rather than gender, age, or marital status."
    },
    "24": {
        "figure1": "2502.19640v1_week-zone_change_half_essential2_e3_crime_rate_all_spree_two",
        "label1": "zone_effect_spree",
        "caption1": "Change In Weekly Visits by POI type} %REGRESSION RESULTS - GRAPHS FOR TOTAL CHANGE PER CATEGORY ACROSS DISTANCE (HALVES), BY WEEK (INCLUDING SPREES) \\centering  \\includegraphics[width=\\linewidth]{week-zone_change_half_essential2_e3_crime_rate_all_spree_two.png} \\caption*{\\footnotesize \\textit{*Note:} 95\\% confidence intervals are shown. Standard errors are clustered at the POI level. \"Near\"/\"Far\" refers to POIs below/above median distance from the shooting location.",
        "text": "Figure \\ref{zone_effect_spree} reports the changes in visits and corresponding 95\\% confidence intervals for various categories in the week immediately after the event (Week 0) and approximately two months after the shooting (Week 8).\\footnote{Online Appendix \\ref{app:POI-type} includes the results for Week 2, 4, 6.} The estimates indicate that decreases in visits are immediate and quite general across a variety of POI types near the shooting sites, pointing to widespread economic costs of mass shootings. Notably, for most categories, the increase in visits to farther-away POIs closely mirrors the decrease to nearby POIs, supporting the relocation hypothesis. People start favoring POIs farther away from the shooting site.\nThe relative size of the changes in weekly visits across different POI types in Figure \\ref{zone_effect_spree} suggests that ease of substitution might play a key role in the observed patterns. It is relatively easy to change the recreational spaces you frequent and thus we see a significant shift in visits from nearby places to far-away places for recreation. In contrast, it is difficult to find a new day care, which accounts for the zero effect on daycares two months after the shooting. %despite the initial decrease in visits to nearby daycare locations.",
        "summarize_figure": "2502.19640v1_week-zone_change_half_essential2_e3_crime_rate_all_spree_two",
        "summarization": "The picture shows the changes in weekly visits and 95% confidence intervals for different point - of - interest (POI) types in Week 0 and Week 8 after an event. The vertical axis lists POI types such as Recreation, Education, etc., and the horizontal axis is the change in weekly visits. In Week 0, visits to various POI types near the shooting site decreased immediately, indicating the widespread economic costs of mass shootings. For most types, the increase in visits to farther - away POIs closely matched the decrease in nearby POIs, supporting the relocation hypothesis that people prefer POIs farther from the shooting site. The degree of change in weekly visits across different POI types suggests that the ease of substitution plays a key role. Recreational spaces are easy to substitute, leading to significant changes in visits, while daycare services are hard to substitute, resulting in zero effect two months after the shooting."
    },
    "25": {
        "figure1": "2502.19659v1_TVI_pr",
        "label1": "fig:TVI",
        "caption1": "Posterior probabilities of the TVI components in both regimes",
        "text": "Moving on to the monetary policy shock identification, we find remarkably sharp posterior probabilities of the TVI indicators reported in Figure~\\ref{fig:TVI}. In the first regime indicated by the light color in Figure~\\ref{fig:TVI}, we find strong data support that the monetary policy shock is identified by simultaneous movements of interest rates with output, inflation, and the term spread, combined with no restrictions in the forth row on the simultaneous movements of term spreads to all other variables. The posterior probability of the TVI component indicators for the restriction pattern \\textit{with} $TS$ and \\textit{unrestricted} are numerically equal to one. This suggests that the monetary policy shock is best identified with consideration to the whole bond yield curve signaling adjusted expectations on future economic developments in the spirit of \\cite{diebold2006}.\nIn the second regime visualized by the dark color in Figure~\\ref{fig:TVI}, we find evidence that data support a mixed identification of the monetary policy shock. The highest posterior probability is attached to the restriction pattern allowing interest rates also to respond simultaneously to changes in the monetary aggregate. The posterior value of the TVI component indicator for the third exclusion restriction pattern (\\textit{with} $m$) is 0.45. The posterior probability of the remaining components is lower, with first and second both having 0.27 posterior probability and the fourth (\\textit{only} $m$) 0.01.",
        "summarize_figure": "2502.19659v1_TVI_pr",
        "summarization": "The figure shows the posterior probabilities of TVI components in two monetary policy shock identification regimes. In the light - colored first regime, strong data support shows the monetary policy shock is identified by simultaneous movements of interest rates with output, inflation, and term spread, with the posterior probability of TVI components for \"with TS\" and \"unrestricted\" being 1. In the dark - colored second regime, there's evidence of a mixed identification; the highest posterior probability (0.45) is for the pattern allowing interest rates to respond to monetary aggregate changes, with other components having lower probabilities."
    },
    "26": {
        "figure1": "2502.19659v1_MS_prob_rob",
        "label1": "fig:regimeprob_rob",
        "caption1": "Regime probabilities of the Markov process across alternative models ",
        "text": "Notably, we find that the regime allocations are robust to various changes in the model specification. The estimated regime probabilities, shown in Figure~\\ref{fig:regimeprob_rob}, are very similar to a model where we impose the prior belief that regimes are less persistent or more persistent by setting the prior expected regime duration of the Markov process to one month, called \\emph{prior MS 1}, or 2 years doubling the expected prior regime duration, called \\emph{prior MS 24}, respectively. Moreover, models using alternative measurements for our variables, such as the consumer price index and industrial production index, model called \\emph{CPI, IP}, or the interpolated GDP deflator, model called \\emph{GDPDEF}, yield similar regime allocation probabilities.",
        "summarize_figure": "2502.19659v1_MS_prob_rob",
        "summarization": "The picture shows the regime probabilities of the Markov process under different models. The vertical axis is component probability, and the horizontal axis is time (from Q1 1961 to Q1 2021). Different colored lines represent different models: light blue for prior MS 1, dark green for prior MS 24, light red for CPI, IP, and dark red for GDPDEF. The text indicates that the regime allocations are robust to various model specification changes. The regime probabilities from these different models are very similar. For example, setting the prior expected regime duration of the Markov process to one month (prior MS 1) or two years (prior MS 24), and models using alternative variable measurements like the consumer price index and industrial production index (CPI, IP) or the interpolated GDP deflator (GDPDEF) yield similar regime allocation probabilities."
    },
    "27": {
        "figure1": "2502.20418v1_DEN_SNA_UA_F9_WN_fares",
        "label1": "fig:DEN_SNA_UA_F9_WN_fares",
        "caption1": "Real dollar fare. This figure depicts the mean real dollar fare (in 2022Q1 prices) for three carriers, United Airlines, Frontier Airlines and Southwest Airlines, on the route between Denver International Airport in Colorado and John Wayne Airport in California, between 1999 and 2022. The mean fare for each carrier-quarter is calculated across all direct return tickets in the data. Frontier established dual-presence and entered the route simultaneously in 2003Q3, and so it did not create a distinct threat for United. The shaded area represents the period during which Southwest had dual-presence on the route, starting at time $t_{0}$ and ending when Southwest actually entered the route at time $t_{e}$. Prior to $t_{0}$, Southwest was active at John Wayne but had no flights into or out of Denver. There is some evidence that United's and Frontier's fares fell during the period of Southwest's dual-presence. The activity of United and Frontier fell during the Covid-19 pandemic and neither carrier appears on this route in 2020Q2 after data pre-processing.",
        "text": "To fix ideas, it is useful to consider Fig. \\ref{fig:DEN_SNA_UA_F9_WN_fares}, which plots the real mean dollar fare for travel between Denver International Airport in Colorado and John Wayne Airport in California, calculated across all direct return tickets in the data, for each carrier-quarter. The first incumbent, United Airlines, was present on the route from 1999 onward. Its real fares fell substantially over the next four years, although it did not face any threatened or actual competition on the route. In 2003Q3, Frontier Airlines established dual-presence and entered the route in the same quarter, and so it did not present United with a distinct entry threat. There is no obvious change in United's fares over the next couple of years, during which time it competed directly with Frontier for passengers. Southwest started operations at Denver in 2006Q1. Before then it was active at John Wayne and so this marked the establishment of dual-presence on the route, at time $t_{0}$. Southwest then entered the route with actual service in 2008Q4, at time $t_{e}$. The shaded area represents the period of Southwest's dual-presence, which lasted for two-and-a-half years. United's and Frontier's fares fell during that time, and again after actual entry.",
        "summarize_figure": "2502.20418v1_DEN_SNA_UA_F9_WN_fares",
        "summarization": "The figure depicts the real mean dollar fare (in 2022Q1 prices) for United Airlines, Frontier Airlines, and Southwest Airlines on the Denver - California route from 1999 - 2022. United was on the route from 1999, with early fare drops. Frontier entered in 2003Q3. Southwest established dual - presence in 2006Q1 and actual entry in 2008Q4. The shaded area shows Southwest's dual - presence period, during which United's and Frontier's fares fell, and fell again post - Southwest entry. During the Covid - 19 pandemic, United's and Frontier's activity declined, and they were absent from the route in 2020Q2 after data pre - processing."
    },
    "28": {
        "figure1": "2502.20418v1_WN_regression_coefficients_model_1",
        "label1": "fig:WN_regression_coefficients_model_1",
        "caption1": "Percentage change in mean real fare relative to threat and entry events. This figure shows the standardized point estimates $100(e^{\\beta}-1)$ of quarterly dummies relative to the start of Southwest's dual-presence at $t_{0}$ and actual entry at $t_{e}$. The solid line gives estimated percentage changes from my baseline model (\\ref{eq:baseline}) that are computed from the point estimates $\\beta$ reported in the first column of Table \\ref{tab:baseline_model}. For comparison, the dashed line gives estimated percentage changes from the first column of Goolsbee and Syverson's (2008, Table II) baseline model.",
        "text": "Dual-presence and entry can occur at different calendar times on different routes, and so the threat and entry event times $t_{0} = t_{0}(j)$ and $t_{e} = t_{e}(j)$ are functions of the route. For a given carrier-route, the time $t = t(i,j)$ is in the set $\\{t_{0} - 12, \\ldots, t_{0}, \\ldots \\min(t_{e}, t_{0} + 12)\\} \\cup \\{t_{e}, \\min(t_{e} + \\tau, t_{e} + 12)\\}$, where $t_{e} + \\tau$ is the end of the full sample. I include carrier-route fixed effects $\\alpha_{\\, ij}$ and quarter fixed effects $\\delta_{\\, t}$ in my baseline model, which should capture any time trends in real fares. Goolsbee and Syverson (2008)\\nocite{goolsbee_syverson08} include carrier-quarter fixed effects and not quarter fixed effects. I am unable to do this because of multicollinearity. They also cluster standard errors at the carrier-route level. In my baseline model I use standard errors clustered by carrier-route. I report results with robust standard errors in Appendix \\ref{sec:appendix_robustness}. There is growing evidence that robust standard errors can be too small while clustered standard errors can be too large, leading to unnecessarily wide clustering confidence intervals in some common situations, even when there is an apparent pattern in the point estimates (for example, Abadie et al., 2023\\nocite{abadie_etal23}).\\footnote{In one case, Goolsbee and Syverson (2008, p.1621) argue for a pattern in logged passengers around $t_{0}$ even though all of the point estimates are insignificant: ``The estimates are imprecise, but the point estimates suggest that passenger traffic rises on threatened routes in the period before and around when Southwest enters the second endpoint airport.'' (See their Table II, column 2.) My results support this.} In the discussion I make it clear when I am drawing a conclusion based on clustered significance, or on the basis of point estimates alone (when they are still significant by robust standard errors but not by clustered standard errors). In some specifications, I include control variables $X_{\\, ij \\, t}$, for route-length or a temperature-based proxy for leisure versus business routes. In the estimation of equation (\\ref{eq:baseline}), I weight observations by the number of passengers recorded in the DB1B for each carrier-route-quarter.\\\\ I report the results for logged mean real fares and percentiles in Table \\ref{tab:baseline_model_clustered_ses} (clustered) and Table \\ref{tab:baseline_model} (robust). In column 1, there is no evidence for a significant pre-threat trend that anticipates dual-presence, nor in the point estimates themselves. The point estimates suggest a moderate post-threat effect, whereby incumbents cut mean fares in a ``U-shaped'' pattern as long as dual-presence is maintained, by 6--8\\% in the two quarters after $t_{0}$, which for a fare of \\$400 is between \\$26 and \\$32. If dual-presence continues beyond two quarters without Southwest entering the route, mean fares fall by 4\\% relative to the excluded period, suggesting that the price reaction to the threat diminishes the longer it takes for Southwest to enter. The post-threat reaction is not significant under clustering. When actual entry takes place at $t_{e}$, mean fares fall by 16\\%, with a final post-entry reduction of 18\\%, which for a fare of \\$400 is around \\$72. The post-entry coefficients are economically-important and highly significant in all specifications. Incumbents react convincingly when faced with actual competition, as expected, but there is weaker evidence of a preemptive reaction which suggests that firms do not make strong use of price to deter or accommodate Southwest's entry by reinforcing customer loyalty in an effort to reduce post-entry demand for the entrant. Fig. \\ref{fig:WN_regression_coefficients_model_1} compares the estimated percentage changes in average real fares relative to threat and entry events at $t_{0}$ and $t_{e}$ from Table \\ref{tab:baseline_model} to Goolsbee and Syverson's (2008, Table II)\\nocite{goolsbee_syverson08} baseline model. It shows the absence of a pre-threat trend in my baseline model and smaller post-threat and post-entry effects compared to Goolsbee and Syverson\\nocite{goolsbee_syverson08}.",
        "summarize_figure": "2502.20418v1_WN_regression_coefficients_model_1",
        "summarization": "The picture shows the percentage changes in average real fares relative to the start of Southwest's dual - presence (t0) and actual entry (te), that is, the standardized point estimates 100(e^β - 1). The solid line represents the estimated values from the baseline model for quarterly dummies, and the dashed line represents the estimates from Goolsbee and Syverson's (2008) baseline model. There is no significant pre - trend before t0. After t0, the fares of incumbent firms decrease in a \"U - shaped\" pattern. When dual - presence lasts more than two quarters and Southwest does not enter, the fare reduction decreases. When Southwest actually enters (te), fares drop significantly by 16%, with a final reduction of 18%. Compared with Goolsbee and Syverson's model, this baseline model has no pre - trend, and the post - threat and post - entry effects are smaller."
    },
    "29": {
        "figure1": "2502.20917v1_Univariate",
        "label1": "fig: numerical illustration",
        "caption1": "Test statistic and confidence intervals}  \\includegraphics[width = 0.8\\linewidth]{Univariate.pdf} \\begin{minipage}{0.8\\linewidth} \\footnotesize      NOTE: Conditional inference after one-sided testing with critical value $\\bar x = 1.64$. The figure plots the median-unbiased estimator (solid curve) and the bounds $\\theta(0.975)$ and $\\theta(0.025)$ of the 95\\% conditional confidence interval (bold dashed curves). The thin dashed line is the 45-degree line, representing the unconditional estimate. The thin dash-dotted lines give the usual confidence interval. \\end{minipage",
        "text": "Figure~\\ref{fig: numerical illustration} plots the median-unbiased estimator and the conditional confidence interval as a function of the observed $t$-statistic $x^{\\text{obs}}$. The upper dashed curve represents the upper bound $\\theta(0.025)$ of the confidence interval and the lower dashed curve represents the lower bound $\\theta(0.975)$. The solid curve represents the median unbiased estimator $\\hat \\theta_{MU}$. The dashed line is the 45-degree line. Since we have normalized $X = \\hat \\theta$, it gives the unconditional estimate $\\hat{\\theta}$. The two parallel dash-dotted lines represent the conventional confidence interval $x^{\\text{obs}} \\pm 1.96$.\nFigure \\ref{fig: numerical illustration} reveals several noteworthy insights. When the estimated parameter is highly significant, i.e., when $x^{\\text{obs}}$ is large, selective inference yields almost identical results to conventional inference that ignores the selection problem. In this case, the median-unbiased estimator is close to the conventional parameter estimate ($\\hat{\\theta}_{MU} \\approx \\hat{\\theta}$) and the conditional confidence interval approximates the traditional equal-tailed interval ($\\theta(0.975) \\approx x^{\\text{obs}} - 1.96$ and $\\theta(0.025) \\approx x^{\\text{obs}} + 1.96$).\nThe numerical results in Figure~\\ref{fig: numerical illustration} are easily analytically verified. For example, the median-unbiased estimator is smaller than the critical value $\\bar{x}$ if $\\theta(0.5) < \\bar{x}$ and the upper-bound of the confidence interval is smaller than $\\bar{x}$ if $\\theta(0.025) < \\bar{x}$. The solution $\\theta(p)$ depends on the observed $t$-statistic $x^{\\text{obs}}$. To find the range of $x^{\\text{obs}}$ such that $\\theta(p) < \\bar{x}$, solve the inequality      p > F(x^{\\text{obs}}; \\bar x, \\bar x) = \\frac{\\Phi(x^{\\text{obs}}-\\bar x) - 0.5}{0.5}, for $x^{\\text{obs}}$ to obtain     x^{\\text{obs}} < \\Phi^{-1} ( 0.5p + 0.5) + \\bar x. Thus, unless $x^{\\text{obs}}$ exceeds $\\bar x$ by more than $\\Phi^{-1} (0.5p+0.5)$, $\\theta(p)$ is smaller than $\\bar x$.  For example,  if $x^{obs} < \\Phi^{-1} (0.5 \\times 0.025+0.5) + \\bar x \\approx 0.031+  \\bar x$, the upper bound of the 95\\% confidence interval is less than $\\bar x$.",
        "summarize_figure": "2502.20917v1_Univariate",
        "summarization": "This picture shows the relationship between the test statistic (t - stat) and different estimates and confidence intervals. The solid line represents the median - unbiased estimate. The thick dashed lines represent the upper and lower bounds of the 95% conditional confidence interval. The thin dashed line is the 45 - degree line, representing the unconditional estimate. The thin dash - dotted lines represent the conventional confidence interval. When the estimated parameter is highly significant (large t - stat value), selective inference yields results nearly identical to conventional inference. The median - unbiased estimate approaches the conventional parameter estimate, and the conditional confidence interval approximates the traditional equal - tailed interval."
    },
    "30": {
        "figure1": "2502.20917v1_Twosided",
        "label1": "fig: two-sided numerical illustration",
        "caption1": "Test statistic and confidence intervals conditional on two-sided significance}  \\includegraphics[width = 0.8\\linewidth]{Twosided.pdf} \\begin{minipage}{0.8\\linewidth} \\footnotesize      NOTE: Conditional inference after two-sided testing with critical value $\\bar x = 1.64$. The figure plots the median-unbiased estimator (solid curve) and the bounds $\\theta(0.975)$ and $\\theta(0.025)$ of the 95\\% conditional confidence interval (bold dashed curves). The thin dashed line is the 45-degree line, representing the unconditional estimate. The thin dash-dotted lines give the usual confidence interval. \\end{minipage",
        "text": "Figure \\ref{fig: two-sided numerical illustration} displays the conditionally median-unbiased estimator and the upper and lower bounds of a 95\\% conditional selective confidence interval. The range of the horizontal axis are positive values of $x^{\\text{obs}}$ that lead to a significant parameter estimate. The corresponding graphs for $x^{\\text{obs}} < -\\bar{x}$ are obtained by mirroring the figure around the vertical axis.  The figure demonstrates several key features of conditional inference after selection based on two-sided significance.",
        "summarize_figure": "2502.20917v1_Twosided",
        "summarization": "This picture shows the relationship between the test statistic (t - stat) and different estimates and confidence intervals under two - sided significance testing. The solid line represents the median - unbiased estimate. The thick dashed lines represent the upper and lower bounds of the 95% conditional confidence interval. The thin dashed line is the 45 - degree line, representing the unconditional estimate. The thin dash - dotted lines represent the conventional confidence interval. The horizontal axis shows positive values of the observed t - stat for significant parameter estimates. The graphs for t - stat < - 1.64 can be obtained by mirroring the figure around the vertical axis, demonstrating key features of conditional inference based on two - sided significance."
    },
    "31": {
        "figure1": "2502.21037v2_result_plot",
        "label1": "fig:result",
        "caption1": "\\textbf{Social contagion driven by artificial agents}. \\textbf{(A, D)} Attribute importance in the PS and AA experiments for human and artificial subjects. The social signal plays a much more important role in both GPT-based and Gemini-based artificial subjects' choices, whereas the ranking by the importance of the other attributes remains largely unchanged. \\textbf{(B, E)} Average threshold of subjects in the PS experiment and the AA experiment for both human and artificial subjects. The error bars represent the 95\\% confidence intervals (CI) for the averaged thresholds across all energy policies/messaging apps. Artificial subjects tend to exhibit significantly lower thresholds. \\textbf{(C, F)} Adoption rate as a function of the proportion of artificial agents in the network, with a seeding rate of 1\\% for each diffusion, for both the policy support experiment and the app adoption experiment. The error bars represent the 95\\% CI for the average adoption rate across all energy policies/messaging apps. The higher the proportion of artificial agents in a human-LLM network, the wider the diffusion. ",
        "text": "In contrast to previous hypotheses~\\cite{tsvetkova2024new}, we find that artificial agents exhibit a higher susceptibility to social influence than human subjects. In particular, the social signal has high importance (see Supplementary Note~\\ref{secSI:threshold} for the importance measurement details) for the artificial agents' choices even in a context where it was not important for the human subjects' choices (i.e., in the PS experiment).  Specifically in the AA experiment, the social signal is the most important attribute for both human and artificial agents. More surprisingly, in the PS experiment, the social signal is the first and second important attribute for the artificial agents driven by GPT and Gemini, respectively [average attribute importance $51.4\\%$ (SEM $=0.4\\%$) and $32.4\\%$ (SEM $=0.2\\%$) for GPT and Gemini, respectively], although it was only ranked 6th out of 7th for human subjects (average importance $12.8\\%$ (SEM $=0.5\\%$), see Fig.~\\ref{fig:result}A).   With minor discrepancies, the rankings of other attributes' importance are largely consistent across artificial and human subjects (see Figs.~\\ref{fig:result}A, D). From random utility theory (see Supplementary Note~\\ref{secSI:threshold}), the threshold of a decision maker for a given alternative decreases with their susceptibility to social influence~\\cite{goldenberg2010chilling,tanase2024integrating}.  Therefore, we expect the higher importance of social signals for the artificial agents' choices in the PS experiment to translate into a lower threshold, which is confirmed by our results (Fig.~\\ref{fig:result}B, E).\nAcross both contexts, increasing the proportion $q$ of artificial agents widens the reach of the complex contagion. This holds true both in the PS experiment (Fig.~\\ref{fig:result}C) and the AA experiment (Fig.~\\ref{fig:result}F), both when the contagion originates from central nodes and when it originates from randomly-selected nodes. These findings indicate that artificial subjects act as ``contagion amplifiers\": For example, in a social network comprising $20\\%$ GPT agents with 1\\% randomly seeded nodes, on average, energy policies would receive 1.14 times more support than in a human-only network; Similarly, under the same conditions, 2.22 times more nodes would decide to switch to a new messaging app compared to a human-only network.",
        "summarize_figure": "2502.21037v2_result_plot",
        "summarization": "The picture shows the results of experiments on social contagion driven by artificial agents. In the PS and AA experiments, (A, D) show that the social signal is more important for the choices of artificial agents based on GPT and Gemini, with little change in the ranking of other attributes. (B, E) indicate that artificial agents have significantly lower average thresholds. (C, F) demonstrate that the higher the proportion of artificial agents in the network, the wider the spread. This shows that artificial agents are more susceptible to social influence than human subjects and act as \"contagion amplifiers\"."
    },
    "32": {
        "figure1": "2502.21037v2_net_illustration",
        "label1": "fig:net_illustration",
        "caption1": "\\textbf{An illustrative human-LLM social network with $20$ nodes and different proportions of artificial agents.}  A proportion $1 - q$ of the nodes is populated by human agents and a proportion $q$ by artificial agents. For each value of $q$, the positions of artificial and human agents are randomly assigned. The three panels depict one instance for $q = 0.1$ \\textbf{(A)}, $q = 0.5$ \\textbf{(B)}, and $q = 0.9$ \\textbf{(C)}.",
        "text": "The relevance of the threshold distributions for diffusion outcomes~\\cite{tanase2024integrating} motivates us to examine theoretically whether complex contagions would spread wider in social networks composed of artificial agents compared to networks composed of human subjects.  To answer this question, we perform calibrated simulations of complex contagion on empirical networks, where a proportion $1-q$ of the nodes is occupied by human agents, a proportion $q$ by artificial agents (see Fig.~\\ref{fig:net_illustration}); the human and artificial agents make adoption decisions determined by their thresholds estimated in the conjoint experiments with human subjects and artificial agents, respectively (see Supplementary Note~\\ref{secSI:diffusion_simulation}).   For each study and for each LLM, we measure the adoption rate for 36 products/behaviors in 18 empirical networks for 6 values of $q$ under two seeding policies (top degree and random), which leads to 31,104 distinct diffusion simulations (see Supplementary Note~\\ref{secSI:diffusion_simulation}).",
        "summarize_figure": "2502.21037v2_net_illustration",
        "summarization": "The picture shows an illustrative human - LLM social network with 20 nodes and different proportions of artificial agents. Orange dots represent human agents and gray squares represent LLM artificial agents. Panels A, B, and C correspond to artificial agent proportions q of 0.1, 0.5, and 0.9 respectively, with node positions randomly assigned. These are used to study the spread of complex contagions in networks with different agent compositions."
    },
    "33": {
        "figure1": "2502.21141v1_Densities_census_treat_year",
        "label1": "fig:densities_by_treat_year",
        "caption1": "Distribution of variables in 1850 of dependent on period in which the parish was connected to the railway} \\includegraphics[width=1\\linewidth]{Figures/Densities_census_treat_year.png} \\parbox{1\\textwidth}{\\caption*{\\small{\\textit{Notes}: Density estimates of a number of variables from the census data and other covariates, separated by the year when each parish was connected to the railway. Parishes that were already connected in 1850 (8 out of 1589) are excluded from these plots. ",
        "text": "The density plots shown in Figure \\ref{fig:densities_by_treat_year} offer a visual comparison of key variables from the census data, grouped by the period during which each parish was connected to the railway. These groups consist of parishes connected to the railway before 1860, between 1861 and 1880, between 1881 and 1901, and those that were never connected. The distinction between the periods of connection is crucial as it helps us assess whether there were systematic differences between parishes that received railway connections at different points in time. The very first lines, connecting Copenhagen differs slightly, but not enough that we should be concerned that there is a lack of common support (non-overlapping distributions).",
        "summarize_figure": "2502.21141v1_Densities_census_treat_year",
        "summarization": "The figure presents density estimates of various variables (like distances to cities, HISCAM_avg, and ratios related to population, industry, etc.) from census data, grouped by the year parishes were connected to the railway (1860, 1880, 1901, never). It visually compares these distributions to help assess systematic differences among parishes with different connection times, excluding parishes already connected in 1850."
    },
    "34": {
        "figure1": "2502.21141v1_p6_lnMigration",
        "label1": "fig:decompose_census_calendar",
        "caption1": "Effect decomposed by year (census)} \\centering % First row of three figures \\begin{minipage}[b]{0.3\\textwidth} \\centering \\includegraphics[width=\\textwidth]{Figures/decomposition_census_calendar/p1_lnPopulation.png} \\caption*{(A) log(Pop.)} \\end{minipage} \\hfill \\begin{minipage}[b]{0.3\\textwidth} \\centering \\includegraphics[width=\\textwidth]{Figures/decomposition_census_calendar/p2_lnChild_women_ratio.png} \\caption*{(B) log(Child women ratio)} \\end{minipage} \\hfill \\begin{minipage}[b]{0.3\\textwidth} \\centering \\includegraphics[width=\\textwidth]{Figures/decomposition_census_calendar/p3_lnManufacturing.png} \\caption*{(C) log(Manufacturing + 1)} \\end{minipage} \\vspace{1em} % Adds some vertical space between rows % Second row of three figures \\begin{minipage}[b]{0.3\\textwidth} \\centering \\includegraphics[width=\\textwidth]{Figures/decomposition_census_calendar/p4_lnNotAgriculture.png} \\caption*{(D) log(Not Agriculture + 1)} \\end{minipage} \\hfill \\begin{minipage}[b]{0.3\\textwidth} \\centering \\includegraphics[width=\\textwidth]{Figures/decomposition_census_calendar/p5_HISCAM_avg.png} \\caption*{(E) HISCAM avg} \\end{minipage} \\hfill \\begin{minipage}[b]{0.3\\textwidth} \\centering \\includegraphics[width=\\textwidth]{Figures/decomposition_census_calendar/p6_lnMigration.png} \\caption*{(F) log(Migration)} \\end{minipage} % Notes at the bottom \\parbox{1\\textwidth}{% \\caption*{\\small{\\textit{Notes}: These figures shows the effects of the railway on economic outcomes decomposed by year.}} }%",
        "text": "Figure \\ref{fig:decompose_census_calendar} presents the estimated effect of railway access on the six main census-based outcomes by calendar year, rather than by the time each parish was connected. Panel A shows the impact on the \\textit{log of total population}, indicating that while the effect is somewhat negative around 1860, it becomes clearly positive by 1880 and 1900, suggesting that the railway’s boost to local population took hold after an initial lag. Panel B depicts the \\textit{log of the child-women ratio}—our proxy for fertility—revealing only minor or slightly negative effects in the early years, with little discernible change thereafter. In Panel C, we see the effect on \\textit{log(manufacturing + 1)}, where “manufacturing” is the number of individuals employed in HISCO-coded occupations starting with 7, 8, or 9. The estimates become more positive over time, reflecting a growing presence of manufacturing in railway-connected parishes.",
        "summarize_figure": "2502.21141v1_p6_lnMigration",
        "summarization": "The figure presents the estimated effect of railway access on log(Migration) decomposed by calendar year. The vertical axis shows year groups (1860, 1880, 1901), and the horizontal axis shows the effect. The 1860 effect is negative, while those for 1880 and 1901 are positive, indicating that railway access initially had a negative impact on migration, which then turned positive."
    },
    "35": {
        "figure1": "2502.21306v1_pv_per_hh_sorted",
        "label1": "fig:pv_per_hh",
        "caption1": "Investments in rooftop PV per (representative) household",
        "text": "Investments into rooftop PV vary between 3.51~kW per household in the North of Germany and 5.13~kW per household in the South (see Figure~\\ref{fig:pv_per_hh}). The increasing investments from northern to southern regions in Germany are due to higher solar irradiation, which translates into higher full-load hours of solar PV. On average, we observe that zonal pricing leads to substantially higher rooftop PV investments than nodal pricing. This is driven by wholesale price differences. Prices in the zonal scenario (0.35~EUR/kWh) are on average around twice as high than in the nodal scenario (0.17~EUR/kWh), which makes higher levels of prosumage more profitable. Real-time pricing further leads to slightly higher rooftop PV adoption as compared to time-invariant pricing.  This is because real-time pricing offers the opportunity for prosumers to align the electricity self-consumption with the electricity wholesale market and to substitute grid consumption in high-price hours to a larger extent. The highest PV investments are achieved with a combination of zonal and real-time pricing.",
        "summarize_figure": "2502.21306v1_pv_per_hh_sorted",
        "summarization": "The picture shows the rooftop PV investment per household in different regions of Germany. From north to south, the PV investment per household increases from 3.51 kW to 5.13 kW, due to stronger solar irradiation and more full - load hours in the south. Overall, zonal pricing leads to higher rooftop PV investments than nodal pricing. Real - time pricing promotes more PV adoption than time - invariant pricing. The highest PV investments are achieved with the combination of zonal and real - time pricing."
    },
    "36": {
        "figure1": "2502.21306v1_battery_per_hh_sorted",
        "label1": "fig:battery_per_hh",
        "caption1": "Investments in home batteries per (representative) household",
        "text": "We observe similar effects for home battery investments. These tend to be the higher in the scenario with zonal pricing, which makes self-consumption more profitable because of higher average wholesale prices. Additionally, consumers can avoid high-price periods by storage discharging if they are subjected to real-time pricing, thus they tend to invest more into home batteries. Power ratings of home batteries range from 0.4~kW to 0.7~kW, while energy capacity is between 2.6~kWh and 3.8~kWh (see Figure~\\ref{fig:battery_per_hh}). In terms of storage energy capacity, the effect of real-time pricing on installed capacity is smaller than for storage power rating or PV installations. Zonal pricing still has a positive effect on installed energy capacity. In general, we do not observe a large difference in investments based on the different pricing regimes, while the location of the node and thus the availability factor of solar energy plays a much larger role.",
        "summarize_figure": "2502.21306v1_battery_per_hh_sorted",
        "summarization": "The figure shows household investments in home batteries regarding power rating and energy capacity. Power ratings range from 0.4 - 0.7 kW, and energy capacity from 2.6 - 3.8 kWh. Zonal pricing boosts self - consumption profitability, and real - time pricing enables consumers to dodge high - price periods, driving more home battery investment. Generally, node location and solar energy availability factor have a stronger influence on investment than different pricing regimes, with no large investment differences among these regimes."
    },
    "37": {
        "figure1": "2503.00201v1_empirical_path_dependence",
        "label1": "fig:empirical_path_dependence",
        "caption1": "Examples of path dependence in ETH/USDC pools. The chart shows the magnitude of price impact (as a percentage) for various combinations of swap and liquidity events that occurred within the same 60-second time window. Negative values indicate that executing a swap before adding liquidity resulted in a lower final price than the reverse sequence.",
        "text": "Figure \\ref{fig:empirical_path_dependence} illustrates several notable examples of path dependence observed in our dataset. In the most extreme case, we detected a price impact of -0.68\\%, meaning that if the operations had occurred in the reverse order, the final price would have differed by this percentage. For context, in the highly liquid ETH/USDC market, a price difference of this magnitude represents a significant arbitrage opportunity.",
        "summarize_figure": "2503.00201v1_empirical_path_dependence",
        "summarization": "The figure depicts the distribution of price impact from operation orders in real data for ETH/USDC pools. The horizontal axis shows the price impact percentage, and the vertical axis shows frequency. Most impacts cluster around 0, with the highest frequency there. Negative impacts also occur, and in the extreme case, a -0.68% impact was detected, which represents a significant arbitrage opportunity in the highly liquid ETH/USDC market."
    },
    "38": {
        "figure1": "2503.00201v1_impact_vs_alpha",
        "label1": "fig:impact_vs_alpha",
        "caption1": "Relationship between liquidity proportion (alpha) and price impact. The scatter plot demonstrates a strong negative correlation (-0.992), confirming that larger proportional changes in liquidity lead to more significant path dependence effects.",
        "text": "We found that the alpha value (the proportion of liquidity being added or removed) was the strongest predictor of path dependence impact. As shown in Figure \\ref{fig:impact_vs_alpha}, there is a strong negative correlation between alpha and price impact percentage, with larger liquidity proportions leading to more pronounced path dependence effects.",
        "summarize_figure": "2503.00201v1_impact_vs_alpha",
        "summarization": "The picture shows the relationship between the liquidity change factor (α) and the price impact. The horizontal axis is the liquidity change factor, and the vertical axis is the percentage of price impact. The scatter plot shows a strong negative correlation (-0.992), meaning that larger changes in the proportion of liquidity lead to more significant path - dependence effects. Points of different colors and shapes represent different liquidity amounts (liq_amountUSD) and swap amounts (swap_amountUSD)."
    },
    "39": {
        "figure1": "2503.00201v1_path_dependence_heatmap",
        "label1": "fig:path_dependence_heatmap",
        "caption1": "Path dependence heatmap showing the interaction between swap amounts and liquidity proportions (alpha). Darker regions indicate stronger path dependence effects, with the most significant impacts occurring at high alpha values.",
        "text": "The heatmap in Figure \\ref{fig:path_dependence_heatmap} provides a comprehensive view of how different combinations of swap sizes and alpha values affect the magnitude of path dependence. This visualization confirms our theoretical predictions that path dependence effects increase with both the size of liquidity changes and the magnitude of swaps.",
        "summarize_figure": "2503.00201v1_path_dependence_heatmap",
        "summarization": "The picture is a path - dependence heatmap showing the interaction between swap size (Δx) and the proportion of liquidity addition (α) on price impact. The horizontal axis is the swap size (in ETH quantity), and the vertical axis is the proportion of liquidity addition. Darker colors indicate stronger path - dependence effects. The most significant effects occur at high α values and large swap sizes, confirming that both larger liquidity changes and larger swaps enhance path - dependence effects."
    },
    "40": {
        "figure1": "2503.00549v1_sim_withwrong",
        "label1": "fig1",
        "caption1": "Histogram of t-statistics over 1000 simulation replications, and the standard normal density. The t-statistics are  standardized by either the analytical standard error $\\widehat{\\text{SE}}(\\widehat z_{T+1})$ (top left panel) or the bootstrap interquartile range $\\sigma^*$ (top right panel). The bottom panels use $\\eta_i$ and $\\eta_{it}$ to generate bootstrap residuals.",
        "text": "The top two panels of Figure \\ref{fig1} plot the histograms of the $t$ statistics over 1000 simulations and the standard normal density function. The $t$-statistics are  standardized by either $\\widehat{\\text{SE}}(\\widehat z_{T+1|T})$ (left panel) or $\\sigma^*$ (right panel). We see that although there are only 100 replications, the histograms of the t-statistics fit well to the standard normal density. Hence both proposed methods for quantifying the forecast uncertainty seem promising.\nIt is critical to apply the bootstrap guided by theory. In particular, the resampling must reflect the dominant source of uncertainty, namely the factor shocks from the time series. To illustrate this, we show the distribution one would obtain if the standard bootstrap were applied, i.e. the bootstrap is applied incorrectly. The bottom two panels of Figure \\ref{fig1} are the histograms of the bootstrap $t$-statistics (standardized by the bootstrap interquartile range), but the bootstrap residual is generated as  $ e_{i,t} ^*= (y_{i,t}- \\widehat g(x_{i,t-1})) \\eta_i^* $ (the bottom left panel) and $ e_{i,t} ^* = (y_{i,t}- \\widehat g(x_{i,t-1})) \\eta_{i,t}^* $ (the bottom right panel), where $\\eta_i^*, \\eta_{i,t}^*\\sim \\mathcal N(0,1)$. These bootstraps mistreat the forecast uncertainty as  driven by the cross-sectional variation (because $\\eta_i^*$ and $\\eta_{i,t}^*$ both vary across $i$). Figure \\ref{fig1} clearly shows that the misuse of bootstrap vastly understates the uncertainty.",
        "summarize_figure": "2503.00549v1_sim_withwrong",
        "summarization": "The picture shows the histograms of t - statistics from 1000 simulation replications and the standard normal density curves. The top - left and top - right sub - plots standardize the t - statistics using the analytic standard error (analytic SE) and the bootstrap interquartile range (k - step bootstrap) respectively. Their histograms fit well with the standard normal density, indicating promising methods for quantifying forecast uncertainty. The bottom - left and bottom - right sub - plots show the results of misusing the bootstrap method (using ηi and ηit to generate bootstrap residuals respectively). The wrong approach attributes forecast uncertainty to cross - sectional variation, greatly understating the uncertainty."
    },
    "41": {
        "figure1": "2503.00549v1_size_dist_indivdual",
        "label1": "fig:firm_size",
        "caption1": "Firm Size Distribution our Sample}  \\vspace{-2mm} \\caption*{\\setstretch{1.0} \\scriptsize ...} \\includegraphics[width=1.0\\textwidth]{figs/size_dist_indivdual.pdf",
        "text": "We implement both the mean-variance efficient portfolio (problem \\eqref{eq:mv}) and the uncertainty averse portfolio (\\eqref{eq:uamv}) for the 500, 750 and 1,000 largest stocks.\\footnote{In the early parts of the sample, we encounter a few months in which fewer firms are available. In these cases, we use all the available stocks with a 240-month history.} Throughout, we use a 240-month estimation window to estimate the covariance matrix using the POET estimator \\citep{POET}. By conditioning on large firms with a 240 months history, we deliberately create a sample of very large firms to mitigate concerns over small and illiquid stocks. In the context of cross-sectional anomalies, \\cite{patton2020you} show that transaction cost play an important role and in the machine learning setting, \\cite{avramov2023machine} argue that standard implementations often concentrate the predictability on small and illiquid securities. Figure \\ref{fig:firm_size} plots the median effective normalized size for three scenarios, 500, 750 and 1000 largest firms with a 240-month history. The normalized size ranks the market equity of all firms each month $t$, the ranks are then divided by the number of stocks each period so that the largest firm has a normalized size of 1 and the smallest firm has a normalized size of $1/N_t$, where $N_t$ is the number of firms each period.",
        "summarize_figure": "2503.00549v1_size_dist_indivdual",
        "summarization": "The figure plots the median effective normalized size of market equity for the 500, 750, and 1000 largest firms over time. The x - axis is the year, and the y - axis is the normalized market equity. Each colored line corresponds to a different firm size group. The trends show fluctuations, indicating relative changes in market equity across periods, which is useful for assessing firm - size impacts in portfolio analysis."
    },
    "42": {
        "figure1": "2503.00632v1_homo_hetero_sipp_linear_one_hetero_compare_mean_utility_growth_rate_compare_discrete_multi_budget1",
        "label1": "fig:one_hetero_compare",
        "caption1": "Social welfare as the finite-time growth rate average over all individuals, for all policies (solid lines) under one set of heterogeneous bounds.",
        "text": "Based on this set-up, we simulate the following model: w.l.o.g., we set $M=1$, for each individual $i\\in[N]$, $g_i^-\\sim\\mathcal{N}\\left(bg^+,b^2\\sigma^2\\right)$, $g_i^+\\sim\\mathcal{N}\\left(g^+,\\sigma^2\\right)$, $f_i^-\\sim\\mathcal{N}\\left(f^-,(\\frac{f^-}{g^+})^2\\sigma^2\\right)$, $f_i^+\\sim\\mathcal{N}\\left(f^+,(\\frac{f^+}{g^+})^2\\sigma^2\\right)$ where $g^+,f^-,f^+$ are constant parameters, $b\\in(0,1]$, $\\sigma^2>0$. For each pair of $(b,\\sigma^2)$ parameters, we generate $50$ sets of heterogeneous bounds ($\\{g_i^-,g_i^+,f_i^-,f_i^+\\}_{i=1:N}$). Under each set of heterogeneous bounds ($\\{g_i^-,g_i^+,f_i^-,f_i^+\\}_{i=1:N}$), we average the social welfare obtained over all individuals, averaging over $50$ iterations of the generation process of the intervention and decay function bounds. We present the finite-time social welfare of individuals under different policies using one set of randomly generated bounds $\\{f_i^-,f_i^+,g_i^-,g_i^+\\}$ in Figure~\\ref{fig:one_hetero_compare}. Figure~\\ref{fig:finite_horizon_bounded_heterogeneity} illustrates a heatmap of the percentage of times when the min-U Rawlsian policy has better long-term welfare than the max-U utilitarian policy, for each value of $b$ and $\\sigma$ ($1$ meaning that the min-U Rawlsian policy is better in all iterations). From Figure~\\ref{fig:finite_horizon_bounded_heterogeneity}, we observe the min-U Rawlsian policy maintains the tendency to perform worse the max-U policy in a short-term while surpasses the max-U utilitarian policy in the long-term for a range of $\\sigma$ values (in a sense, for bounded heterogeneity). As $b$ decreases (and therefore the decay functions $g_i$ have a stronger effect), a Rawlsian policy starts performing better by preventing stronger loss caused by the decay of low-welfare individuals.",
        "summarize_figure": "2503.00632v1_homo_hetero_sipp_linear_one_hetero_compare_mean_utility_growth_rate_compare_discrete_multi_budget1",
        "summarization": "The picture shows the changes in social welfare over time step t under a set of heterogeneous bounds, where social welfare is measured as the average of the finite - time growth rate of all individuals. Different - colored curves represent different policies, such as Rawlsian and utilitarian policies. The social welfare of some policies fluctuates greatly in the early stage and then stabilizes. There are significant differences in the social welfare of different policies, reflecting the changes in the effectiveness of different policies in promoting social welfare over time."
    },
    "43": {
        "figure1": "2503.00650v1_opt_one_time_alloc_illustration",
        "label1": "fig:opt_one_time_alloc_illustration",
        "caption1": " Illustration of the effect of the budget and inequality on one-time allocation. ",
        "text": "We illustrate the effect of budget and inequality by simulating one-time allocation for $T=6$ and~$N=10000$ in \\cref{fig:opt_one_time_alloc_illustration}. Consistent with the theory, a high inequality corresponding to $G \\le 2$ can make $W^{t+1} \\le W^t$. Even when this is not the case, a high budget can still favor earlier allocation.",
        "summarize_figure": "2503.00650v1_opt_one_time_alloc_illustration",
        "summarization": "The picture shows the effect of the budget (B/N, in percentage) on one - time allocation, with the vertical axis being ΔWt. Different - colored lines correspond to different G values, representing different levels of inequality. When G ≤ 2, that is, when the inequality is high, it may make Wt + 1 ≤ Wt. Even when this is not the case, a high budget still favors earlier allocation, reflecting the role of budget and inequality in one - time allocation."
    },
    "44": {
        "figure1": "2503.00725v1_stability_cost_corr_v2",
        "label1": "fig:labelcost",
        "caption1": "Tradeoff between the number of human-labeled hold-out datapoints ($\\ell$) and the precision of the simple average treatment effect estimator for each theme.",
        "text": "We can further conceptualize the cost--benefit tradeoff of human scoring in terms of a tradeoff between the cost of producing score labels and the benefit of improved precision in our average treatment effect estimate at the theme level. Using the approach outlined in \\autoref{subsec:Describing-Combining}, we can simulate this tradeoff by estimating $\\hat{\\tau}^\\dagger_j$ for various sizes of the set $\\Lb \\subseteq \\HO$ of human-scored instances in the hold-out set. In particular, for a given $\\ell = |\\Lb| \\leq |\\HO| = 100$, we estimate the variance of $\\hat{\\tau}^\\dagger_j$ via bootstrap, and plot how that value changes with $\\ell$ for each of our five identified themes (holding the fraction of ``treated'' fixed, $\\sfrac{\\ell_1}{\\ell} \\approx \\sfrac{h_1}{h} = 29\\%$). The resulting plot (\\autoref{fig:labelcost}) shows that the precision of the estimator for each theme is monotonically decreasing in $\\ell$. Furthermore, the curvature of each line illustrates decreasing marginal benefit to additional human scoring, implying that it may be optimal (from a decision-theoretic point of view) to human-label a certain number of held-out documents, with the stopping point characterized by the point at which the marginal benefit (in terms of estimator precision) outweighs the marginal cost (in terms of scoring cost).",
        "summarize_figure": "2503.00725v1_stability_cost_corr_v2",
        "summarization": "The picture shows the trade - off between the number of human - labeled hold - out data points (# Hand - Labeled) and the precision of the simple average treatment effect estimator (measured by √Var(τ̂)). Different - colored lines represent different themes (CEI, MET, MIC, FIN, NAP). As the number of human - labeled data points increases, the estimation precision of each theme monotonically decreases, and the curvature of the curves indicates decreasing marginal benefits of additional human labeling, reflecting the trade - off between the cost of human scoring and the benefit of improved estimation precision."
    },
    "45": {
        "figure1": "2503.00757v1_win=6_residualts_Edits_per_regression_0.50_1_sets_202112_to_202311",
        "label1": "fig:time_series_popularnew_single_reg_bootstrap",
        "caption1": "Views} \\end{subfigure} \\hfill \\begin{subfigure}[t]{0.4\\textwidth} \\centering \\includegraphics[width=\\linewidth]{plots_revision/Feb_6.1_popular-all_TS_fix_y_axis/win=6_residualts_Edits_per_regression_0.50_1_sets_202112_to_202311.png} \\caption{Edits} \\end{subfigure} \\caption{Mean residuals for each month $t$ in which activity occurs, among all observations in $\\mathcal{O}(6)$ (at most 6 months old), with bootstrapped standard errors with $1000$ samples. Dashed lines are mean residuals for similar (blue) and dissimilar (red) articles over the pre-GPT and post-GPT periods respectively. Red vertical line indicates date of ChatGPT launch.",
        "text": "This empirical strategy relies on two assumptions.  The first is the ``stable unit treatment value assumption'', which states that our control group, dissimilar articles, is not impacted by ChatGPT. This assumption is motivated by the fact that ChatGPT is less capable of replicating the content of these articles. If the assumption is violated due to indirect effects (e.g., ChatGPT makes Wikipedia contributors more productive), our strategy will still reveal the differential additional effect on similar articles relative to dissimilar ones (but not the full impact on dissimilar articles). Our second assumption is the standard ``parallel trends'' assumption of DiD: absent ChatGPT, similar and dissimilar articles would have evolved in similar fashion. Figure~\\ref{fig:time_series_popularnew_single_reg_bootstrap} below, which shows similar pre-ChatGPT evolution of these two groups, bolsters our confidence in this assumption.\nFigure~\\ref{fig:time_series_popularnew_single_reg_bootstrap} shows a comparative time series of mean residual views and edits in each month during the event study period for articles that are at most $T=6$ months old. For both views and edits, we see that similar articles exhibited little changes in activity from the pre-GPT to the post-GPT period (accounting for controls). Dissimilar articles, on the other hand, show an increase in edits after ChatGPT launched in November 2022 (Figure~\\ref{fig:time_series_popularnew_single_reg_bootstrap}(b)). Views for dissimilar articles also show an increasing trend around the same time, with residuals even rising above those of similar articles (Figure~\\ref{fig:time_series_popularnew_single_reg_bootstrap}(a)).",
        "summarize_figure": "2503.00757v1_win=6_residualts_Edits_per_regression_0.50_1_sets_202112_to_202311",
        "summarization": "The picture shows the changes in the number of residual edits for similar and dissimilar articles over time (t, activity month). The red vertical line indicates the launch time of ChatGPT. Before ChatGPT's launch, the trends of residual edits for the two types of articles were similar. After the launch, the number of edits for dissimilar articles increased, while that for similar articles changed little, reflecting the differential impact of ChatGPT on the editing activities of different types of articles."
    },
    "46": {
        "figure1": "2503.01000v1_figure3",
        "label1": "fig:figure-3",
        "caption1": " Distribution of the number of blocked ads (Panel A) and trackers (Panel B) per website and browser instance. The red dashed horizontal line at 0 highlights the threshold below which ad-blocking or anti-tracking effectiveness is negative. Multiplying the 9 browser instances (Adblock Plus MV2, Adblock Plus MV3, AdGuard MV2, AdGuard MV3, Stands MV2, Stands MV3, uBlock MV2, uBlock MV3, and MV3+) with the number of websites (114) yields the number of observations (N = 1,026).  ",
        "text": "Figure~\\ref{fig:figure-3} presents box plots that summarize the number of blocked ads (Panel A) and trackers (Panel B) for MV3 and MV2 ad blockers as well as for the MV3+ ad blocker group.\nThese results are consistent with those in Figure \\ref{fig:figure-3}, showing that the initial MV3 ad blockers were as effective as their later versions. This consistency across different versions of MV3 ad blockers as compared to their MV2 counterparts suggests that the effectiveness of MV3 ad blockers has remained consistent over time. This outcome suggests that ad blocker providers debuted their MV3 instances of ad blockers with confidence in their effectiveness, as their core functionality did not fluctuate significantly across subsequent releases.",
        "summarize_figure": "2503.01000v1_figure3",
        "summarization": "The picture shows the distribution of the number of blocked ads (Panel A) and trackers (Panel B) for different ad blockers (Adblock Plus, AdGuard, Stands, uBlock, distinguishing between MV2 and MV3 versions, and MV3+). The red dashed line indicates the threshold for negative blocking effectiveness. There are differences in the number of blocked ads and trackers among different ad blockers, and the effectiveness of MV3 ad blockers is comparable to that of their subsequent versions, indicating that the effectiveness of MV3 ad blockers has remained consistent over time."
    },
    "47": {
        "figure1": "2503.01072v1_logitreg_PLOT_pairwisedep",
        "label1": "fig: logitreg_heatmap_corr",
        "caption1": "Posterior summaries for {\\em krkp} (top row) and {\\em spam} (bottom row). Panels~(a,c) are heatmaps of the matrix of Spearman correlations for $(\\valpha^\\top, \\tilde{\\vdelta}^\\top)$ computed exactly using (slow) MCMC. Panels~(b,d) are scatterplots of $\\{\\mbox{corr}(\\alpha_i,\\tilde{\\delta}_i); i=1,\\ldots,m\\}$, with exact posterior values on the horizontal axis, and variational approximations on the vertical axis for GC-F5 (blue dots) and A4 (orange dots).",
        "text": "Figure~\\ref{fig: logitreg_heatmap_corr} shows why GVC-I performs well.  Panels~(a,c) plot the posterior Spearman correlation matrices of $(\\valpha^\\top, \\tilde{\\vdelta}^\\top)$ computed exactly using MCMC for {\\em krkp} and {\\em spam}. The main feature is that the pairs $(\\alpha_i,\\tilde{\\delta}_j)$ exhibit high correlation when $i=j$ (but not when $i\\neq j$), which is common when using global-local shrinkage priors. This feature is captured parsimoniously by $\\Lambda$ in GVC-I.  Panels~(c,d) plot the exact posterior pairwise correlations of pairs $(\\alpha_i,\\tilde{\\delta}_i)$ against their variational estimates for GC-F5 and A4. A low rank VA, such as GC-F5, cannot approximate these values well, whereas A4 is much better in doing so. Ignoring them (as in GMF, BLK and BLK-C) also reduces accuracy.",
        "summarize_figure": "2503.01072v1_logitreg_PLOT_pairwisedep",
        "summarization": "The picture shows the posterior summaries for krkp (top row) and spam (bottom row). Sub - plots (a) and (c) are heatmaps of the Spearman correlation coefficient matrix, computing (α, δ̃) exactly via MCMC. Results show that pairs (αi, δ̃j) exhibit high correlation when i = j, but not when i ≠ j, a common feature of global - local shrinkage priors, which is parsimoniously captured by Λ in GVC - I. Sub - plots (b) and (d) are scatter plots showing the relationship between the exact posterior values (horizontal axis) and the variational approximations of GC - F5 (blue dots) and A4 (orange dots) for (αi, δ̃i). Low - rank variational approximations like GC - F5 cannot approximate well, while A4 performs better."
    },
    "48": {
        "figure1": "2503.01072v1_logitreg_PLOT_qsar",
        "label1": "fig: logitreg_trace_qsar",
        "caption1": "Plot of the ELBO values for the dataset {\\em qsar} ($n=8992, m=1024$) for GC-F5 (blue line) and A4 (orange line) against (a) SGD step number, (b) wall clock time.",
        "text": "The proposed vector copula model VAs can also be fast to calibrate. Table~A1 in Part~D of the Online Appendix provides computation times of the SGD algorithm implemented in Python.  The VAs A3/A4 are almost as fast as GMF, while A5/A6 are only slightly slower than BLK/BLK-C which share the same marginals.  The VAs A1/A2 are slow as $d$ increases (becoming infeasible for $d=10,000$ in the simulated datasets). This is because a fast implementation of the $O(pd^2)$ update of the Cholesky factor $A$ in Section~\\ref{sec:GVCFp} was unavailable in Python, and we employed full $O(d^3)$ evaluation instead. Figure~\\ref{fig: logitreg_trace_qsar} plots ELBO values for A4 and GC-F5 against (a)~SGD step, (b)~wall clock time. Not only is A4 much faster per step, it converges in fewer steps to a higher value.",
        "summarize_figure": "2503.01072v1_logitreg_PLOT_qsar",
        "summarization": "The picture shows the evidence lower bound (ELBO) values of GC - F5 (blue line) and A4 (orange line) on the qsar dataset (n = 8992, m = 1024) against the number of stochastic gradient descent (SGD) steps (Panel a) and wall clock time (Panel b). A4 is not only faster per step but also converges to a higher ELBO value with fewer steps, indicating that A4 outperforms GC - F5 in terms of computational efficiency and convergence."
    },
    "49": {
        "figure1": "2503.01072v1_performance_10states",
        "label1": "fig: corr_10states",
        "caption1": "Comparison of two approximations GC-F5 (blue) and A4 (orange) for the regularized correlation matrix example with $r=10$. Panel~(a) plots the ELBO values against SGD step. Panel~(b) plot the exact posterior mean of $\\vbeta$ computed using MCMC (horizontal axis) against the variational posterior means (vertical axis). Greater alignment on the 45 degree line corresponds to higher accuracy.",
        "text": "In addition, we found that SGD was slow to converge for G-F$p$ and CG-F$p$ models, whereas the vector copula models were fast. To illustrate,  Figure~\\ref{fig: corr_10states} show the performance of VAs GC-F5 and A4 on the $r=10$ state dataset. Panel~(a) shows that the A4 was not only the most accurate VA, but that the SGD algorithm obtained its optimum within a few thousands steps, whereas it took 20,000 steps for GC-F5. For the case of $r=49$ states, G-F5 and GC-F5 did not converge in 200,000 steps.   When $r=10$ it is possible to compute the exact posterior using (slow) MCMC. Figure~\\ref{fig: corr_10states}(b) plots the true posterior mean of $\\vbeta$ against the variational posterior means for the two VAs, further demonstrating the increased accuracy of the VCVI method. %Finally, we note that in experiments we find that the vector copula model VAs dominate the factor models also",
        "summarize_figure": "2503.01072v1_performance_10states",
        "summarization": "The picture compares the performance of two approximation methods, GC - F5 (blue) and A4 (orange), in the regularized correlation matrix example with r = 10. Panel (a) shows the change of the evidence lower bound (ELBO) values against the number of stochastic gradient descent (SGD) steps. A4 converges much faster, reaching the optimum within a few thousands steps, while GC - F5 takes 20,000 steps. Panel (b) compares the exact posterior mean of β computed by MCMC (horizontal axis) with the variational posterior mean (vertical axis). Points closer to the 45 - degree line indicate higher accuracy, showing that A4 has higher accuracy."
    },
    "50": {
        "figure1": "2503.01080v1_Corr4DiagonalBlockC",
        "label1": "fig:BlockStructures",
        "caption1": "Examples of block correlation matrices. Upper-left: Unrestricted Correlation matrix. Upper-right: Block Correlation matrix with blocks defined by subindustries. Lower-left: Sparse Block Correlation matrix with zero correlations between sectors. Lower-right: Diagonal Block Correlation matrix.",
        "text": "Examples of the four types of correlation structures are shown in Figure \\ref{fig:BlockStructures}.\nmatrix. Upper-right: Block Correlation matrix with blocks defined by subindustries. Lower-left: Sparse Block Correlation matrix with zero correlations between sectors. Lower-right: Diagonal Block Correlation matrix.\\label{fig:BlockStructures}}",
        "summarize_figure": "2503.01080v1_Corr4DiagonalBlockC",
        "summarization": "The picture shows an example of a diagonal block correlation matrix. Presented in a grid, square colors denote correlation coefficients: red for positive, blue for negative. Diagonal - placed red squares signify high correlations, likely reflecting strong data associations in certain dimensions, demonstrating the matrix's structural features."
    },
    "51": {
        "figure1": "2503.01780v1_figMH",
        "label1": "figMH",
        "caption1": "Loss and behavior of an agent that truthfully reports her disease level.",
        "text": "It is important to note that the agent's risk type $\\theta'$ is irrelevant in determining her loss function. At this stage, the only relevant variables are her actual disease level $b$, the contract chosen from the menu (indexed by $\\theta$), and the item selected from the contract (indexed by $b'$). Figure \\ref{figMH} illustrates the agent's behavior and her resulting loss when she selects the item from the contract corresponding to her true disease level.",
        "summarize_figure": "2503.01780v1_figMH",
        "summarization": "This picture shows a graph about the loss and behavior of an agent who truthfully reports her disease level. The horizontal axis represents b (a variable related to disease level), and the vertical axis represents R(θ,b) (a variable likely related to loss). There are four regions: Home, Hospital only up to x(θ,b), Hospital: cure x(θ,b) at price C(θ,b), rest at p, and Hospital: cure everything at price p. The curve C(θ,b) represents cost - related situations, and the line min{b,p} is a horizontal reference line. Different regions show how the agent's costs and losses change when choosing to be treated at home or in the hospital under different disease levels, reflecting the agent's behavioral decisions and corresponding losses under various conditions."
    },
    "52": {
        "figure1": "2503.01780v1_allocation",
        "label1": "fig:allocation",
        "caption1": "Optimal allocation $x^*$ corresponding to the menu described in Theorem \\ref{thm:optimal-menu}.",
        "text": "Whenever the contract for risk type $\\theta$ contains two options, $\\{(\\alpha, C_l(\\theta)),(1, C_h(\\theta))\\}$, it follows that $x^*(\\theta, \\cdot)$ has two jumps, as represented in Figure \\ref{fig:allocation}: It is equal to $0$ for values of $b$ less than or equal to $C_l(\\theta)$ (these are the disease levels that will stay at home), equal to $\\alpha$ for values of $b$ in the range $\\left(C_l(\\theta), \\frac{C_h(\\theta) - \\alpha C_l(\\theta)}{1-\\alpha}\\right]$ (these are the disease levels that will go to the hospital and use the low OPC with partial treatment), and equal to $1$ for values of $b$ greater than $\\frac{C_h(\\theta) - \\alpha C_l(\\theta)}{1-\\alpha}$ (these are the disease levels that will go to the hospital and use the high OPC with a full unit of hospital services).\n$G'$ weakly increases the insurer's payoff while still satisfying the \\ref{EQ} condition. Thus, we can also express the insurer's problem as:     &\\max_{q} \\quad \\overline{\\Phi}(\\theta_L,q_{\\theta_L})\\hat{f}(\\theta_L) + \\overline{\\Phi}(\\theta_H,q_{\\theta_H})\\hat{f}(\\theta_H) \\\\     &\\text{s.t.} \\quad q_{\\theta_L}\\hat{f}(\\theta_L)+q_{\\theta_H}\\hat{f}(\\theta_H) = RS(p), In the above formulation, for each risk type, the insurer selects the fraction of agents with a disease level in $(0,p]$ who go to the hospital, denoted by $q_{\\theta}$. This formulation already demonstrates why a solution exists in which the contract offered to each risk type includes at most two options. Indeed, any point in $\\overline{\\Phi}(\\theta, \\cdot)$ can be attained by a distribution $G_{\\theta}$ with at most two elements in its support. This, in turn, results in an allocation function $x(\\theta, \\cdot)$ with at most two jumps, as illustrated in Figure \\ref{fig:allocation}.\nTheorem \\ref{thm:optimal-menu} follows directly from Lemma \\ref{lemma:optimal-menu}. Once we know that $\\pi^*$ is a solution to the primal problem \\ref{OT}, it immediately implies that the following function $x^*$ solves the insurer's problem \\ref{P}: x^*(\\theta,b) = \\int_0^1 \\mathbf{1}_{\\left[b > b_{\\theta}(q)\\right]} \\, d\\pi^*(q \\mid \\theta). Since $\\pi^*(\\cdot \\mid \\theta)$ consists of at most two jumps, and the size of these jumps is independent of the risk type, $x^*$ has the form shown in Figure \\ref{fig:allocation}.",
        "summarize_figure": "2503.01780v1_allocation",
        "summarization": "This picture shows the optimal allocation x*(θ,b) corresponding to the contract menu described in the theorem. The horizontal axis is b, and the vertical axis is x*(θ,b). When b is less than or equal to Cl(θ), x*(θ,b) equals 0, meaning people with this disease level choose to stay at home. When b is in the range (Cl(θ), (Ch(θ) - αCl(θ))/(1 - α)], x*(θ,b) equals α, indicating that people with this disease level go to the hospital and receive partial treatment with a low out - of - pocket cost ratio. When b is greater than (Ch(θ) - αCl(θ))/(1 - α), x*(θ,b) equals 1, representing that people with this disease level go to the hospital and receive full treatment with a high out - of - pocket cost ratio."
    },
    "53": {
        "figure1": "2503.01780v1_figLinearEnv",
        "label1": "fig:concaveenvelope",
        "caption1": "The continuous lines illustrate $\\Phi(\\theta_L,q)$ (in blue) and $\\Phi(\\theta_H,q)$ (in red). The dashed lines are the respective concave envelopes. The figure is drawn assuming that $p=0.8$.",
        "text": "Nonetheless, a question remains: When does the insurer include a second option that rations hospital services or places a cap on treatment? To illustrate the economic forces that lead the insurer to include this option, we assume $\\theta_L = 0.25$, $\\theta_H = 0.35$, $f(\\theta_H) = \\frac{1}{5}$, and $H_{\\theta}(b) = 1 - \\theta + \\theta Q(b)$, where $Q(b) = b^2 \\cdot \\mathbf{1}_{\\left[0 \\leq b \\leq 1\\right]}$. In this case, we have $q\\Phi(\\theta,1) > \\Phi(\\theta,q)$ for all $q \\in (0,1)$. This implies that the concave envelope of $\\Phi(\\theta, \\cdot)$ is the line segment connecting $\\Phi(\\theta,0)$ and $\\Phi(\\theta,1) $, i.e., $\\overline{\\Phi}(\\theta,q) = q\\Phi(\\theta,1)$, as shown in Figure \\ref{fig:concaveenvelope}.\nThe economic forces that lead the insurer to favor the riskier type follow from the fact that the profits of providing any units of hospital services are higher for the high-risk type than for the low-risk type, as illustrated in Figure \\ref{fig:concaveenvelope}. The latter occurs because $\\phi(\\theta_H,q) > \\phi(\\theta_L,q)$, i.e., the sum of the period 1 and period 2 virtual values is higher for the high-risk type than for the low-risk type for any $q$. We can extend this intuition to the infinite-dimensional risk-type space case:",
        "summarize_figure": "2503.01780v1_figLinearEnv",
        "summarization": "This picture shows the variations of Φ(θL,q) (blue solid line) and Φ(θH,q) (red solid line), along with their respective concave envelopes (dashed lines). The figure is drawn assuming p = 0.8. As q increases from 0 to 1, both Φ(θL,q) and Φ(θH,q) show an upward trend, with Φ(θH,q) always above Φ(θL,q), indicating that the high - risk type (θH) has higher profits in providing hospital services. The concave envelope connects Φ(θ,0) and Φ(θ,1), reflecting the impact of relevant economic factors on the insurer's decision - making."
    },
    "54": {
        "figure1": "2503.01893v1_boxplot_per_sector",
        "label1": "fig:boxplot_per_sector",
        "caption1": "Monthly Rate per Sector Per Country",
        "text": "Figure~\\ref{fig:boxplot_per_sector} presents a box plot showing the distribution of CPI change rates across various sectors. It is evident that certain sectors, like Apparel and Transportation, exhibit greater volatility than others across all markets. This higher volatility is likely to make predictions for these sectors more challenging, as anticipated.",
        "summarize_figure": "2503.01893v1_boxplot_per_sector",
        "summarization": "This picture is a box plot showing the monthly inflation rate changes of various sectors in Canada, Norway, and the US. The vertical axis represents the inflation rate, and the horizontal axis shows the countries. Different colors represent sectors like Apparel, Food, Health, Housing, Recreation, and Transportation. It can be seen that Apparel and Transportation sectors have greater inflation rate volatility in all three countries, with wider box and whisker ranges, while sectors like Food and Health have relatively smaller volatility. This indicates that predicting for Apparel and Transportation sectors may be more challenging."
    },
    "55": {
        "figure1": "2503.02074v1_tau_degen",
        "label1": "fig:atoms",
        "caption1": "A transmission function $\\tau : \\mathcal{X} \\to \\mathcal{X}$ where $\\mathcal{X} = [s_1,s_3]$ with three fixed points: $s_1$ and $s_3$ are sinks but $s_2$ is a source. Here, $\\mathcal{X}_0$ and $\\mathcal{X}_3$ are empty, and $\\mathcal{X}_1 = [s_1,s_2]$ and $\\mathcal{X}_2 = [s_2,s_3]$. An initial degenerate distribution at a capital level $y$ in the interior of $\\mathcal{X}_1$ converges to a degenerate distribution at $s_1$. Similarly, an initial degenerate distribution at a capital level $y'$ in the interior of $\\mathcal{X}_2$ converges to a degenerate distribution at $s_3$.",
        "text": "Consider some $k \\in \\{0,\\dots,K\\}$ and a fixed point of the transmission function $s \\in \\mathcal{X}_k$. A distribution that is degenerate at $s$ is a steady state of the population process, i.e. $F^*(x) := \\mathbbm{1}[s \\leq x]$, and the  population growth factor in this steady state is given by $\\mathbb{E}^*[n] = n(s)$. Additionally, if the initial distribution $F_0$ is degenerate at some interior point of $\\mathcal{X}_k$ and $s$ is a sink then, regardless of the shape of the fertility function, the population process converges to a distribution that is degenerate at $s$.  The first part of the above remark is evident: since $s$ is a fixed point of $\\tau$, anyone born with capital level $s$ will have descendants with capital level $\\tau(s)=s$, so a distribution that is degenerate at $s$ is a steady state. Moreover, the number of children that each parent has at $s$ is $n(s)$; this is therefore the population growth factor. The second part of the remark above states that if the initial distribution is $F_0(x) = \\mathbbm{1}[y \\leq x]$ for some $y \\in \\interior \\mathcal{X}_k$ and $s \\in \\mathcal{X}_k$ is a sink, then $\\lim_{t \\to \\infty} F_t(x) = \\mathbbm{1}[s \\leq x]$. The reason is, again, straightforward: since $F_{0}$ is degenerate at some $y \\in \\interior  \\mathcal{X}_k$, every parent in generation $0$ has capital level $y$. This implies that each child in generation $1$ has capital level $\\tau(y)$. In other words, $F_{1}(x) = \\mathbbm{1}[\\tau(y) \\leq x] $. Carrying this forward to generation $t$ gives us F_{t}(x) = \\mathbbm{1}[\\tau^{[t]}(y) \\leq x] . The result then follows because, since $s$ is a sink, $\\lim_{t \\to \\infty} \\tau^{[t]}(y) = s$ for each $y\\in \\interior \\mathcal{X}_k$.\\footnote{We provide a simple formal proof of this in the appendix; see Lemma \\ref{lem:kappa1}.} An example is shown in Figure \\ref{fig:atoms}.",
        "summarize_figure": "2503.02074v1_tau_degen",
        "summarization": "This picture shows a transmission function τ(x) with the domain X = [s1, s3], having three fixed points: s1 and s3 are sinks, while s2 is a source. X0 and X3 are empty, X1 = [s1, s2], and X2 = [s2, s3]. An initial degenerate distribution at a capital level y inside X1 converges to a degenerate distribution at s1, and an initial degenerate distribution at a capital level y' inside X2 converges to a degenerate distribution at s3. It reflects the relationship between the fixed points of the transmission function and the distribution convergence."
    },
    "56": {
        "figure1": "2503.02074v1_exponentialC",
        "label1": "fig:exponential",
        "caption1": "Example A} \\begin{subfigure}[t]{0.325\\linewidth} \\centering \\caption*{(a)} \\includegraphics[width=1\\linewidth]{figs/exponentialA.pdf} \\end{subfigure}% \\hspace{0.03cm} \\begin{subfigure}[t]{0.325\\linewidth} \\centering \\caption*{(b)} \\includegraphics[width=1\\linewidth]{figs/exponentialB.pdf} \\end{subfigure} \\begin{subfigure}[t]{0.325\\linewidth} \\centering \\caption*{(c)} \\includegraphics[width=1\\linewidth]{figs/exponentialC.pdf} \\end{subfigure} \\caption{Fertility, transmission, and the evolution of the distribution of capital for Example A.",
        "text": "In cases (a) and (b), we set $a > 1$ so that the transmission function has a unique fixed point at $s_1=0$ and it is a source. This implies that each child's capital is greater than their parent's capital. As expected, when fertility is flat, this leads to an explosive process in which capital is pushed towards ever greater levels: as can be seen in case (a), where we set $m=0$, the density of capital escapes to the right. In contrast, consider case (b) in which we set $m>0$, so that fertility is decreasing with capital. Now there are two opposing forces. On the one hand, capital is pushed towards higher levels due to the transmission function but, on the other hand, people at higher levels of capital reproduce less rapidly. This process converges to an atomless steady state distribution of capital whose density $f^*$ is shown in Figure \\ref{fig:exponential}. This shows that it would be incorrect, in general, to presume that an intergenerational transmission function that would lead to explosive dynamics under flat fertility must also result in explosive dynamics under differential fertility.",
        "summarize_figure": "2503.02074v1_exponentialC",
        "summarization": "This picture consists of three sub - graphs showing the evolution of fertility, transmission, and capital distribution in Example A. The top sub - graph is the fertility function n(x) ∝ e^(-mx) (m > 0), showing a decreasing trend. The middle sub - graph is the transmission function τ(x) = ax (a < 1), a straight line with a slope less than 1. The bottom sub - graph shows the capital distribution functions from f0 to f*, reflecting the evolution of capital distribution. It demonstrates the convergence of capital distribution to an atomless steady - state distribution under the decreasing fertility function and the action of the transmission function."
    },
    "57": {
        "figure1": "2503.02074v1_multiE",
        "label1": "fig:B",
        "caption1": "Example B} \\begin{subfigure}[t]{0.325\\linewidth} \\centering \\caption*{(a)} \\includegraphics[width=1\\linewidth]{figs/multiB.pdf} \\end{subfigure}% \\hspace{0.03cm} \\begin{subfigure}[t]{0.325\\linewidth} \\centering \\caption*{(b)} \\includegraphics[width=1\\linewidth]{figs/multiD.pdf} \\end{subfigure} \\begin{subfigure}[t]{0.325\\linewidth} \\centering \\caption*{(c)} \\includegraphics[width=1\\linewidth]{figs/multiE.pdf} \\end{subfigure} \\caption{Fertility, transmission, and the evolution of the distribution of capital for Example B.",
        "text": "Let's start with case (a). Here, $n(s_2)/\\tau'(s_2) > n(s_k)/\\tau'(s_k) > n(s_k)$ for $k \\in \\{1,3\\}$ and $s_2$ is not a stationary point of $n(\\cdot)/\\tau'(\\cdot)$. The primitives are therefore nice and generic over $\\mathcal{X}_k$ for each $k$, and the unique argmax of \\eqref{eq:generic} is at $k^*=2$. Theorem \\ref{thm} part (i) allows us to conclude that there is an atomless distribution with density $f^*$ supported on $\\mathcal{X}$ to which the system converges. There is no explicit analytic expression for this density but our result guarantees the existence of such a limiting density to which the population process converges from essentially any initial distribution. Observe that Figure \\ref{fig:B} shows that the density after 10 and after 50 generations is essentially unchanged. The dynamics here work as follows: the transmission drives capital away from $s_2=0$ towards $s_1=-5$ and $s_3=5$, but this effect is counteracted by fertility being greater at $s_2=0$ than at the endpoints of $\\mathcal{X}$, and this is what results in a stable atomless long-run distribution of capital.",
        "summarize_figure": "2503.02074v1_multiE",
        "summarization": "This picture consists of three sub - graphs showing the evolution of fertility, transmission, and capital distribution in Example B. The top sub - graph has a fertility function n(x) = 2, a horizontal line. The middle sub - graph has a transmission function τ(x) = cx + tanh(x) (c = 1 - tanh(5)/5 ≈ 0.8), a curve. The bottom sub - graph shows the capital distribution functions from f0 to f50. It can be seen that the capital distribution density is essentially unchanged after 10 and 50 generations. The transmission moves capital from 0 towards - 5 and 5, but the higher fertility at 0 counteracts this effect, resulting in a stable atomless long - term capital distribution."
    },
    "58": {
        "figure1": "2503.02283v1_non-syn",
        "label1": "figure8",
        "caption1": "The Non-synchronous observations of price processes $X$ and $Y$.",
        "text": "In real financial markets, we usually only observe asynchronous high-frequency data. The issue has gained significant attention in the literature of financial econometrics, particularly with the advent of large-scale high-dimensional data in recent years. Here, we provide a way to adjust our estimator to handle this issue. As we can see in Figure \\ref{figure8}, $t_{i}^{X}$, $t_{i}^{Y}$ are the $i$-th and $t_{n}^{X}$, $t_{n^{*}}^{Y}$ are the last observation time in $[0, T]$, of the price processes $X$ and $Y$, respectively.",
        "summarize_figure": "2503.02283v1_non-syn",
        "summarization": "This picture shows the non - synchronous observations of price processes X and Y. There are two timelines in the figure, corresponding to price processes X (blue) and Y (purple) respectively. Different time points are marked on the timelines, such as ti^X and ti^Y, representing the i - th observation times of the respective price processes, and tn^X and tn*^Y are the last observation times of price processes X and Y within the interval [0, T]. Δi,n^X, Δi+1,n^X, as well as Δi,n^Y' and Δi+1,n^Y' represent certain change amounts between different time points, reflecting the non - synchrony of the two price processes in time."
    },
    "59": {
        "figure1": "2503.02283v1_differentincrement",
        "label1": "figure3",
        "caption1": "The histograms of $U_{n}'(u,v)-\\int_0^1\\mbox{e}^{-\\langle (u, v), ((\\sigma_{s}^{X})^2, (\\sigma_{s}^{Y})^2)\\rangle}\\mathrm{d}s$.",
        "text": "Here, we adjust the estimator $U_{n}$ to $U'_{n}$ to accommodate this type of data. Define U'_{n}=\\sum_{i=1}^{N_T^X}\\Delta_{i,n}^{X}\\cos\\left(\\sqrt{2u}\\frac{\\Delta_i^n{X}}{\\sqrt{\\Delta_{i,n}^{X}}}+\\sqrt{2v}\\frac{\\Delta_{i}^{n}{Y'}}{\\sqrt{\\Delta_{i,n}^{Y'}}}\\right), where $T=1$, $N_T^X$ is the Poisson process with $\\mathbb{E}[N_{T}^{X}]=1760$, and we let $\\Delta_{i,n}^{X}=t^X_{i}-t^X_{i-1}$, $\\Delta_i^nX=X_{t^X_{i}}-X_{t^X_{i-1}}$, and $\\Delta_i^nY'=Y_{t^Y_{i+}}-Y_{t^Y_{i-}}$, with $$t_{i-}^{Y}=\\max_j\\{t_{j}^{Y}: t_{j}^{Y}\\leq t_{i-1}^{X}\\},~t_{i+}^{Y}=\\min_j\\{t_{j}^{Y}: t_{j}^{Y}\\geq t_{i}^{X}\\}, \\Delta_{i,n}^{Y'}=t_{i+}^{Y'}-t_{i-}^{Y'}.$$ Mathematically, $\\Delta_{i,n}^{Y'}$ is the smallest cover of $\\Delta_{i,n}^{X}$ in process $Y$ and  $\\Delta_{i}^{n}{Y'}$ is the corresponding increment. The volatility and price processes settings are the same as Example \\ref{ex1}, and the results of two other Example settings are similar. The performance of the adjusted estimator under $u=3.5$, $v=3.75$ are displayed in Table \\ref{table6} and Figure \\ref{figure3}, respectively. \t\t\t & & \\multicolumn{3}{c}{$u=2.5$} & \\multicolumn{3}{c}{$u=3.5$} & \\multicolumn{3}{c}{$u=4.5$} \\\\ \t\t\t & & \\text{Bias} &\\text{SD} &\\text{MSE} &\\text{Bias} & \\text{SD} & \\text{MSE} & \\text{Bias}&\\text{SD} &\\text{MSE} \\\\ \t\t\t& 3.75 & -0.00015 & 0.02383\t& 0.00057 & -0.00081& 0.02429 & 0.00059\t& -0.00085\t& 0.02336 & 0.00055\\\\ \t\t\t& 4.75 & -0.00012 &0.02322\t&0.00055  &\t-0.00030& 0.02365 & 0.00056\t& 0.00010\t& 0.02396 & 0.00057\\\\ }\nTable \\ref{table6} presents the bias of adjusted estimator $U_{n}^{'}$. We see that they are also very close to zero but mostly larger than $U_{n}$, which demonstrates estimator $U_{n}^{'}$ can be used for non-synchronous data in practice, as MSE and standard deviation perform well. In addition, the histogram of $U_{n}'(u,v)-\\int_0^1\\mbox{e}^{-\\langle (u, v), ((\\sigma_{s}^{X})^2, (\\sigma_{s}^{Y})^2)\\rangle}\\mathrm{d}s$ is very close to the normal distribution, as seen from Figure \\ref{figure3}.",
        "summarize_figure": "2503.02283v1_differentincrement",
        "summarization": "This picture is a histogram of Un'(u,v) - ∫₀¹ e^(-⟨(u, v), ((σₛ^X)², (σₛ^Y)²)⟩) ds. The vertical axis represents density, and the values on the horizontal axis range from - 0.05 to 0.05. The histogram shows a bell - shaped distribution with a high center and low sides, indicating that the distribution of this data is approximately normal. This suggests that the adjusted estimator performs well when dealing with non - synchronous data."
    },
    "60": {
        "figure1": "2503.02518v1_EPEX_2023_LTSC_prediction",
        "label1": "fig:ltsc_day_ahead",
        "caption1": "Comparison of the naive LTSC day-ahead forecast ($\\hat{T}_{d,h}$; \\textit{top panel}) and the introduced in this study extrapolated LTSC forecast ($e\\hat{T}_{d,h}$; \\textit{bottom panel}). In both cases the LTSC is calculated using the moving average (MA) method with a 7-day smoothing window ($m=24\\cdot7+1$).",
        "text": "In this paper, we introduce a new approach to predicting the LTSC for the next day by extrapolating the input price vector before calculating the LTSC. The proposed algorithm consists of the following steps: By following this procedure, we obtain the LTSC forecast for the target day along with the seasonal component of the prices in the calibration window. Therefore, this approach preserves the continuity of the LTSC vector, which transitions smoothly from the calibration period to the target day, see Figure \\ref{fig:ltsc_day_ahead}. In our study, the seasonal component models that use the extrapolated LTSC approach are denoted by a prefix \\textbf{e}: \\textbf{eSCARX}-$*$ for ARX-based models and \\textbf{eSCLEAR}-$*$ for LEAR-based models.",
        "summarize_figure": "2503.02518v1_EPEX_2023_LTSC_prediction",
        "summarization": "The picture has two sub - graphs comparing the naive LTSC day - ahead forecast (^T_d,h in the top panel) and the study - introduced extrapolated LTSC forecast (e^T_d,h in the bottom panel). With Price on the vertical axis and time from December 26, 2018, to January 1, 2019, on the horizontal axis, it shows curves like P_d,h, ^P_d,h, T_d,h. The extrapolated method ensures the LTSC vector transitions smoothly from the calibration period to the target day, maintaining continuity."
    },
    "61": {
        "figure1": "2503.02692v1_fig4",
        "label1": "fig4",
        "caption1": "\\textbf{The Results of Ablation Experiment.} Using RAG or not are marked with \\textcolor{orange}{\\textbf{orange}} and \\textcolor{gray}{\\textbf{gray}} respectively, and the dotted line is used to connect the two and display the difference between them. The triangle's dots represent the average performance of all companies.",
        "text": "In our experimental framework, all baseline models were either implemented by us or derived from existing open-source implementations. First, we tested Accuracy and F1-Score of various methods, and the results were shown in \\autoref{table2} \\& \\autoref{table3}. Then we used the news data set to conduct an ablation experiment of our adaptive-RAG method against the non-RAG method, and the results are shown in \\autoref{fig4}.",
        "summarize_figure": "2503.02692v1_fig4",
        "summarization": "This picture shows the results of an ablation experiment, with two sub - graphs for Accuracy and F1 - Score respectively. The horizontal axis lists different companies (such as AMZN, GOOG, etc.) and the average value (AVG), and the vertical axis shows the corresponding index values. Red dots represent the cases with Retrieval - Augmented Generation (RAG), and gray dots represent cases without RAG. Dashed lines connect them to show the differences, and triangular dots represent the average performance of all companies. Overall, using RAG has higher Accuracy and F1 - Score than not using RAG for most companies."
    },
    "62": {
        "figure1": "2503.02741v1_bootstrap_runtime_150",
        "label1": "fig:bootstrap_runtime",
        "caption1": "Processing time for the bootstrap experiment. ",
        "text": "To evaluate the scalability of SPF, we conduct a bootstrap experiment where we draw documents with replacement from the available corpus to obtain corpora of different size. In particular, we increase the number of documents $D$ in increments of $100k$, up to a total of $D = 1\\,000\\,000$ documents. This bootstrap experiment was conducted exclusively with the SPF model, as both KeyATM and SeededLDA were unable to handle such large corpora on the hardware described in Section~\\ref{sec:computational_details}. Figure~\\ref{fig:bootstrap_runtime} visualizes the run-times observed, indicating a roughly linear increase in run-times as the corpus size grows. This increase may be attributed to the increase in the model's local variational parameters $\\phi^\\rte_\\theta$ and $\\phi^\\shp_\\theta$ with the number of documents. Figure~\\ref{fig:bootstrap_runtime} shows that SPF successfully processes the corpus with $1\\,000\\,000$ documents in approximately 2 hours, demonstrating its capacity to handle large-scale datasets efficiently.",
        "summarize_figure": "2503.02741v1_bootstrap_runtime_150",
        "summarization": "This picture shows the processing time for 150 epochs of the SPF topic model in a bootstrap experiment, varying with the number of documents. The horizontal axis represents the number of documents (in thousands), and the vertical axis represents the computation time (in seconds). As the number of documents increases from 100k to 1000k, the computation time increases approximately linearly. This indicates that as the corpus size grows, the model's runtime increases, likely due to the increase in the model's local variational parameters with the number of documents. The SPF model can process 1 million documents in about 2 hours, demonstrating its ability to handle large - scale datasets."
    },
    "63": {
        "figure1": "2503.02946v1_Rplot",
        "label1": "fig:unbiased est",
        "caption1": "Equilibrium number of firms and surpluses as a function of model variance $V$.",
        "text": "We now illustrate \\Cref{p:symmetric_deterministic} with two numerical examples. In the first example, we consider the OLS model from \\Cref{eg: ols}, which has zero bias. Fixing parameter values $c=0.25, \\underline{u}=-5, \\sigma^2=1$, we can plot how the number of entrants vary with the variance of the model available to all firms. The change in variance could arise, for example, from a change in the number of data points available to firms. \\Cref{fig:unbiased est} plots the number of entrants, producer surplus,  consumer surplus and total surplus, as a function of V.",
        "summarize_figure": "2503.02946v1_Rplot",
        "summarization": "This picture consists of four sub - graphs showing the relationships between the equilibrium number of firms and surpluses (producer surplus, consumer surplus, total surplus) and the model variance V. The \"Number of Entrants\" sub - graph shows that as the variance increases, the number of firms entering the market increases. In the \"Producer Surplus\" sub - graph, the producer surplus changes little with variance. The \"Consumer Surplus\" and \"Total Surplus\" sub - graphs indicate that consumer surplus and total surplus decrease as the variance increases. It reflects the impact of model variance on the number of firms and the surpluses of various parties in the market."
    },
    "64": {
        "figure1": "2503.02946v1_ridge_plot",
        "label1": "fig:ridge",
        "caption1": "Model bias and variance and equilibrium number of firms and surpluses as a function of the ridge parameter $\\lambda$.",
        "text": "In \\Cref{fig:ridge} below, the parameter values used are $c=0.25, \\underline{u}=-20, \\sigma^2=1$, $n=4$, and $k=6$. Each coordinate of $x$ is drawn independently from a standard normal distribution. Finally, for simplicity in this exercise, we assume that $\\beta$ is a fixed $k$-dimensional vector of ones, to avoid taking expectations over the prior of $\\beta$.\nRecall that the expected squared bias $B$ takes the expectation over the unknown function $f$ with respect to the common prior $F(\\cdot)$ over $f$ shared by all agents. Here $f(x)=\\beta x$, so we integrate $\\beta$ over its support. However for simplicity in this exercise, assume that $\\beta=1$ is a known constant, and continue using the parameter values from the previous example: $c=0.25, \\underline{u}=-5, \\sigma^2=1$. Figure \\ref{fig:ridge} shows the bias squared and variance, number of firms, producer, consumer and total surpluses in enquilibrium as a function of the ridge parameter $\\lambda$.",
        "summarize_figure": "2503.02946v1_ridge_plot",
        "summarization": "This picture consists of six sub - graphs showing the relationships between the squared bias, variance, number of entrants, producer surplus, consumer surplus, and total surplus and the ridge parameter λ. The squared bias increases with λ, while the variance decreases with λ. The number of entrants varies at different λ values. The producer surplus, consumer surplus, and total surplus all show a downward trend as λ increases. It reflects the impact of the ridge parameter on the model and relevant market indicators."
    },
    "65": {
        "figure1": "2503.02946v1_Rplot_ridge",
        "label1": "fig:ridge",
        "caption1": "Ridge estimator: equilibrium number of firms and surpluses as a function of the ridge parameter",
        "text": "In \\Cref{fig:ridge} below, the parameter values used are $c=0.25, \\underline{u}=-20, \\sigma^2=1$, $n=4$, and $k=6$. Each coordinate of $x$ is drawn independently from a standard normal distribution. Finally, for simplicity in this exercise, we assume that $\\beta$ is a fixed $k$-dimensional vector of ones, to avoid taking expectations over the prior of $\\beta$.\nRecall that the expected squared bias $B$ takes the expectation over the unknown function $f$ with respect to the common prior $F(\\cdot)$ over $f$ shared by all agents. Here $f(x)=\\beta x$, so we integrate $\\beta$ over its support. However for simplicity in this exercise, assume that $\\beta=1$ is a known constant, and continue using the parameter values from the previous example: $c=0.25, \\underline{u}=-5, \\sigma^2=1$. Figure \\ref{fig:ridge} shows the bias squared and variance, number of firms, producer, consumer and total surpluses in enquilibrium as a function of the ridge parameter $\\lambda$.",
        "summarize_figure": "2503.02946v1_Rplot_ridge",
        "summarization": "The six sub - graphs depict the relationships between bias squared, variance, number of entrants, producer surplus, consumer surplus, total surplus, and the ridge parameter λ in ridge estimation. Bias squared rises with λ, variance drops with λ. The number of entrants shifts at low λ and then stabilizes. Producer surplus fluctuates then stabilizes, while consumer and total surpluses fluctuate initially and then tend to stabilize as λ changes, showing the parameter's impact on the model and market indicators."
    },
    "66": {
        "figure1": "2503.03312v1_yes_no",
        "label1": "fig:yes_no",
        "caption1": "Comparing the `yes' and `no' groups ($\\hat{\\beta}_1$)}  \\hspace{0.2cm}\\begin{minipage}{0.7\\textwidth} \\vspace{0.15cm} \\footnotesize \\textit{Notes}. This figure plots the $\\hat{\\beta}_1$ coefficients obtained from estimating \\eqref{eq_reg_main} for all $t \\in \\{0, 1, ..., 167\\}$. The 95\\% confidence intervals are computed using robust standard errors. \\end{minipage",
        "text": "Figure \\ref{fig:yes_no} presents the estimated $\\hat{\\beta_1}$ coefficients obtained from estimating this regression for all $t \\in \\{0, 1, ..., 167\\}$. Note that this estimate is simply the difference in average prices between the `yes' and `no' groups, correcting for any baseline imbalance in the $p_{-1, i}$ variable. Again, one can see that the difference starts at around 10 percentage points (immediately after the intervention) and declines over the course of the 1 week period. However, there is a clear difference between the groups of around 7.5 percentage points even after 1 week. As one can see from the 95\\% confidence intervals (also depicted in the figure), this difference is statistically distinguishable from zero ($p < 0.01$).",
        "summarize_figure": "2503.03312v1_yes_no",
        "summarization": "The picture shows the estimated β̂₁ coefficients from the regression for all t∈{0, 1, ..., 167}. The vertical axis represents the difference in prices and the horizontal axis represents hours. Right after the intervention, the average price difference between the 'yes' and 'no' groups is about 10 percentage points, which declines over the next week. However, there is still a distinct difference of around 7.5 percentage points between the two groups even after one week. The 95% confidence intervals in the figure, calculated using robust standard errors, indicate that this difference is statistically different from zero (p < 0.01)."
    },
    "67": {
        "figure1": "2503.03312v1_follow_up",
        "label1": "fig:yes_no_followup",
        "caption1": "Results for the Sweepcash markets ($\\hat{\\beta}_1$ estimates)}  \\hspace{0cm}\\begin{minipage}{0.85\\textwidth} \\vspace{0.15cm} \\footnotesize \\textit{Notes}. This figure plots the $\\hat{\\beta}_1$ coefficients obtained from estimating Equation \\eqref{eq_reg_main2} for all $t \\in \\{0, 1, ..., 6\\}$. The 95\\% confidence intervals are computed using robust standard errors. \\end{minipage",
        "text": "As Figure \\ref{fig:yes_no_followup} reveals, two results emerge from this exercise. First, we again see clear evidence of manipulability: on every day for which data is recorded, prices in the `yes' group are higher than prices in the `no' group. Second, we again see clear evidence of reversion: as time goes on, the gap in average prices falls substantially from 8 percentage points. It is perhaps worth noting that the estimated speed of reversion appears to slow down over time, in agreement with our theoretical model. However, the size of the confidence intervals makes it difficult to say anything definite about curvature.",
        "summarize_figure": "2503.03312v1_follow_up",
        "summarization": "The picture shows the β̂₁ coefficients estimated from the equation for all t∈{0, 1, ..., 6}. The vertical axis is the difference in prices and the horizontal axis is the number of days. It can be seen from the figure that, first, there is clear evidence of manipulability: on each day with recorded data, the prices in the 'yes' group are higher than those in the 'no' group. Second, there is clear evidence of price reversion: as time progresses, the average price gap drops significantly from 8 percentage points. The speed of reversion seems to slow down over time, but due to the width of the 95% confidence intervals (calculated using robust standard errors), it's hard to draw definite conclusions about the curvature."
    },
    "68": {
        "figure1": "2503.03312v1_mc_ac",
        "label1": "fig:mc_ac",
        "caption1": "Costs under the constant product rule ($n = y = 10$)\\vspace{-1.5em}} \\hspace{-2em}\\includegraphics[width=14cm, keepaspectratio]{mc_ac.pdf}  \\begin{minipage}{12.8cm} \\vspace{0.3cm} \\footnotesize \\textit{Notes}. This figure displays marginal and average costs under the constant product rule with initial reserves $(y, n) = (10, 10)$. \\end{minipage",
        "text": "As Lemma \\ref{lemma1} emphasises, pricing under the constant product rule is non-linear. For a small purchase of yes shares, both the marginal and average costs are close to $n/(n + y)$ and thus determined by the ratio of yes and no shares held by the AMM (see Figure \\ref{fig:mc_ac}). As the size of the purchase rises, yes shares become more scarce and so both average and marginal costs rise. In the limit as $q \\rightarrow \\infty$, average and marginal costs converge to $1$; notice that such a purchase is necessarily unattractive since the expected value of each share cannot exceed $1$.",
        "summarize_figure": "2503.03312v1_mc_ac",
        "summarization": "The picture shows the marginal cost MC(q) and average cost AC(q) under the constant product rule with initial reserves (y, n) = (10, 10). The horizontal axis is q and the vertical axis ranges from 0 to 1, with the curves starting at n/(n + y). When a small amount of yes - shares is purchased, both the marginal and average costs are close to n/(n + y), determined by the ratio of yes and no shares held by the AMM. As the purchase size increases, yes - shares become scarce, causing both average and marginal costs to rise. As q approaches infinity, average and marginal costs converge to 1. Such a large - scale purchase is unattractive since the expected value of each share does not exceed 1."
    },
    "69": {
        "figure1": "2503.03497v1_P",
        "label1": "fig:boundary",
        "caption1": "Boundary of $P$ and Points with Several Rank Funtions ($A = 0.7$)",
        "text": "Figure \\ref{fig:boundary} shows an example of $P$ under uniform distribution. The blue curve represents $\\partial P$. For comparison, the figure also displays equilibrium prices for prominence and random search search algorithms, i.e., $\\alpha = 0,1,\\frac{1}{2}$.\nAs we increase \\( \\alpha \\), seller 1 starts to receive some portion of the bonus and will, therefore, accept a wider range of \\( p_1 \\)-values. Conversely, as \\( 1 - \\alpha \\) decreases with the increasing \\( \\alpha \\), seller 2 will accept a narrower range of \\( p_2 \\)-values, as depicted in Figure \\ref{fig:boundary} (b). When \\( \\alpha \\) reaches 1, the path traced by the intersection of IC1 and IC2 determines the entire boundary of \\( P \\).\nOne can verify that $(p^*,p^*)$ satisfied $\\delta (p_1,p_2)=0$. Now consider the part of $\\delta(p_1,p_2)$ possibly inside $\\bar{P}$. By Figure \\ref{fig:boundary}, when $A=0.7$, $p_2^{\\text{high}}<0.7$. Thus, for all possible $A$, $p_2\\le \\min\\{A,p_2^{\\text{high}}(A)\\} <0.7$ as $P$ is shrinking with respect to the increase of $A$. As a result, we only need to pay attention to $p_2\\in [p^*,0.7]$ within $\\delta=0$ and then $p_1\\in [0.56,p^*]$ where $\\frac{dp_2}{dp_1}>0$. Within $\\delta=0$, we have which means $x\\in [0,x(0.56)]\\subset [0,0.2]$. Then by $x=(1-p^*_1)p^*_1$, we can induce that $p_1^*\\in [0,0.3]$. Thus, $\\max\\pi_1^2=p_1^{*2}(p_2+1-p_1^*)\\le 0.13$ since $\\frac{\\partial \\max\\pi_1^2}{\\partial p_1^*}=2p_1^*(p_2+1)-3p_1^{*2}>p_1^*(2-3p_1^*)>0$.",
        "summarize_figure": "2503.03497v1_P",
        "summarization": "The graph illustrates P under uniform distribution, with the blue curve as ∂P. It shows equilibrium prices for prominence and random search algorithms at α = 0, 1, 1/2. As α rises, seller 1 takes a wider p₁ - value range with bonus share; seller 2's p₂ - value range narrows as 1 - α drops. When α = 1, IC1 and IC2 intersection defines P's boundary. Derivations determine p₁, p₂ ranges and max profit conclusions."
    },
    "70": {
        "figure1": "2503.03910v1_scatter",
        "label1": "fig:scatter",
        "caption1": "Estimates of WTP and cost} \\centering  \\resizebox{0.75\\textwidth}{!}{ \\includegraphics{scatter.png}} \\begin{minipage}{\\textwidth} \\fontsize{9}{2}\\linespread{1}\\selectfont \\footnotesize{ \\textit{Notes}: Each point represents a single policy. WTP and net cost are normalized with respect to program cost and are in USD. Grey points are estimates from the sample, while pink points are empirical Bayes posterior mean estimates. } \\end{minipage",
        "text": "I obtain empirical Bayes estimates for benefit and cost as described above for all policies in the sample. Figure \\ref{fig:scatter} plots estimates of net cost against estimates of WTP for both the sample estimates in gray and the empirical Bayes estimates in pink. We see a general pattern of empirical Bayes shrinkage: empirical Bayes estimates are more concentrated than sample estimates because noisy sample estimates are adjusted.",
        "summarize_figure": "2503.03910v1_scatter",
        "summarization": "The picture shows estimates of willingness - to - pay (WTP) and net cost for different policies. The horizontal axis is WTP and the vertical axis is net cost, both in USD and normalized with respect to program cost. Grey points represent sample estimates, while pink points represent empirical Bayes posterior mean estimates. Each point represents a single policy. A general pattern of empirical Bayes shrinkage is visible: empirical Bayes estimates are more concentrated than sample estimates as noisy sample estimates are adjusted."
    },
    "71": {
        "figure1": "2503.03996v1_ID",
        "label1": "fig:ID",
        "caption1": "Pairs of candidate values  ($\\theta, F(v)$) that satisfy the restriction $F^*(v\\mid p)=C(F(v),p;\\theta)/p$ for entry probabilities $p=0.10$  and $p=0.25$. The true values are $\\theta_0=5.0$ and $F(v)=0.3$. Each line plots $\\theta \\mapsto Q(F^*(v\\mid p),p;\\theta)$, where $Q(\\cdot,p;\\theta)$ is the inverse function of $C(\\cdot,p;\\theta)/p$",
        "text": "We illustrate this identification problem in Figure \\ref{fig:ID}. The figure plots the candidate pairs of $\\theta$ and $F(v)$ that satisfy Equation \\eqref{eq:restrictions_for_theta_and_F} for a given entry probability $p_n$. Let ${Q}(\\cdot,v;\\theta)$ denote the inverse function of   ${C}(\\cdot,v;\\theta)/v$. The lines in the figure plot $\\theta \\mapsto Q(F^*(v\\mid p),p;\\theta)$, and the copula parameter $\\theta$ is identified if the lines have a single crossing. Denote ${C}_{\\theta}(u,v;\\theta)\\coloneqq\\partial{C}(u,v;\\theta)/\\partial\\theta$ and ${C}_{1}(u,v;\\theta)\\coloneqq\\partial{C}(u,v;\\theta)/\\partial u$. Below, we provide sufficient conditions for the global and local identification of the copula parameter. Note that all the expressions can be estimated from the data, and therefore, the conditions are testable.",
        "summarize_figure": "2503.03996v1_ID",
        "summarization": "The picture shows pairs of candidate values (θ, F(v)) that satisfy the restriction F*(v∣p) = C(F(v), p; θ)/p for entry probabilities p = 0.10 and p = 0.25. The true values are θ₀ = 5.0 and F(v) = 0.3. Each line plots θ ↦ Q(F*(v∣p), p; θ), where Q(·, p; θ) is the inverse function of C(·, p; θ)/p. The copula parameter θ can be identified if the lines have a single crossing. The figure also involves definitions of relevant partial derivatives, and provides sufficient conditions for the global and local identification of the copula parameter. These expressions can be estimated from data, and the conditions are testable."
    },
    "72": {
        "figure1": "2503.04300v1_nage2064_win",
        "label1": "fig:nage2064_win",
        "caption1": "Outlier treatment of the explanatory variables (number of people aged 20-64 years in the household) Left: original variable; right: treated variable.",
        "text": "Figure \\ref{fig:nage2064_win} (left) shows that those aged 20-64 years in a household includes outliers both in the lower age category and the upper age category. The households with no individuals between 20-64 years re-categorised as households with 1 or less individuals aged 20-64 years, and those households with more than 4 individuals were re-categorised as households with 4 or more individuals aged 20-64 years.",
        "summarize_figure": "2503.04300v1_nage2064_win",
        "summarization": "These two pictures show the outlier treatment of the number of people aged 20 - 64 years in a household. The left - hand chart, representing the original variable, shows that there are outliers in both the lower and upper age categories for this demographic. The right - hand chart displays the treated variable. The treatment method is to re - categorize households with no individuals aged 20 - 64 years as those with 1 or fewer such individuals, and households with more than 4 individuals aged 20 - 64 years as those with 4 or more."
    },
    "73": {
        "figure1": "2503.04941v2_gate_automation_structure_uncertainty",
        "label1": "fig:gate_automation_structure_uncertainty",
        "caption1": "Belief updating after an observed compute-automation outcome. Solid lines represent admissible paths that remain consistent with the observation (black dot), while dashed lines are invalidated paths ruled out by the data. The left panel shows a larger reduction in uncertainty with more paths eliminated, while the right panel shows fewer paths ruled out, indicating a narrower range of uncertainty.",
        "text": "Intuitively, the uncertainty add-on allows users to set social planner beliefs over a family of automation functions that are observationally equivalent up to a certain point in the unfolding of automation (all functions in the family have the same shape up to a point, and then the ``incorrect\" ones taper off before full automation is achieved). As the automation process unfolds the social planner gradually learns the ``correct\" automation function that allows for full automation. This is illustrated in figure \\ref{fig:gate_automation_structure_uncertainty}.",
        "summarize_figure": "2503.04941v2_gate_automation_structure_uncertainty",
        "summarization": "These two pictures illustrate the belief updating after observing compute - automation outcomes. The blue dots represent current observations, and the black dots represent full automation requirements. Solid lines denote admissible paths consistent with the observations (black dots), while dashed lines are inadmissible paths ruled out by the data. In the left - hand panel, more paths are eliminated, leading to a greater reduction in uncertainty. In the right - hand panel, fewer paths are ruled out, indicating a narrower range of uncertainty. This reflects the process by which the social planner gradually learns the \"correct\" automation function for full automation as the automation process unfolds."
    },
    "74": {
        "figure1": "2503.05310v1_variance_decomposition",
        "label1": "fig:variance_decomp_percent_anna_temp",
        "caption1": "\\textbf{Variance decomposition.} Variance decomposition~\\eqref{eq:var_decomp} of the unemployment and 6-month unfilled vacancy rate outcomes for the Agriculture and Manufacturing growth paths.",
        "text": "Unemployment rate changes vary much more by occupation than by region. Most broad occupation groups have similar unemployment outcomes across all regions (Figures~\\ref{fig:unemp_manuf_heatmap} and~\\ref{fig:unemp_agr_heatmap}). Decomposing the total variance of unemployment outcomes, we find that differences between regions explain little of the heterogeneity in outcomes.\\footnote{ It is worth noting that this result is consistent with the fact that the productivity shocks simulated in each scenario are homogeneous across regions and that the between-region components of the demand shocks are relatively low as well. } Indeed, occupation network effects, as captured by the between-occupation component of the variance, explain about 43\\% and 46\\% of the variance in unemployment rates in the Manufacturing and Agriculture growth paths respectively (Figure~\\ref{fig:variance_decomp_percent_anna_temp}).",
        "summarize_figure": "2503.05310v1_variance_decomposition",
        "summarization": "These two pictures display the variance decomposition of the unemployment rate and 6 - month unfilled vacancy rate outcomes under the manufacturing and agriculture growth paths. Variance is divided into between - region, between - occupation, and residual components. In the manufacturing growth path, for unemployment rate changes, the between - occupation variance accounts for 43.2% and the between - region variance accounts for 4.2%; for unfilled vacancy rate changes, the between - occupation variance accounts for 53.4% and the between - region variance accounts for 1.6%. In the agriculture growth path, for unemployment rate changes, the between - occupation variance accounts for 46.0% and the between - region variance accounts for 2.8%; for unfilled vacancy rate changes, the between - occupation variance accounts for 52.6% and the between - region variance accounts for 2.4%. Clearly, unemployment rate changes are much more affected by occupation than by region."
    },
    "75": {
        "figure1": "2503.05338v2_figure_3",
        "label1": "fig:censorship_resistance_2",
        "caption1": "Relationship between Censorship Resistance, Gamma, and Gas Price",
        "text": "This refined formulation provides a more nuanced perspective on how increasing blockchain throughput (\\(\\Gamma\\)) enhances censorship resistance. As \\(\\Gamma\\) grows, censorship resistance improves, making it more difficult and less profitable for adversaries to censor competing operations.  Figure~\\ref{fig:censorship_resistance_2} illustrates how variations in gas price and gas available to solvers (throughput), highlighting throughput as a crucial design lever for maintaining a robust and equitable auction environment.",
        "summarize_figure": "2503.05338v2_figure_3",
        "summarization": "The picture shows the relationship between censorship resistance, blockchain throughput (Γ), and gas price. The color represents the degree of censorship resistance. As blockchain throughput Γ increases, censorship resistance improves, making it harder and less profitable for adversaries to censor competing operations. The figure also highlights the variations in gas price and the gas available to solvers (throughput), indicating that throughput is a crucial design factor for maintaining a robust and equitable auction environment."
    },
    "76": {
        "figure1": "2503.05946v1_apps_overview",
        "label1": "fig:apps",
        "caption1": "Overview of Promise Zone Applications by Round}  \\raggedright {\\footnotesize Notes: These categories are the total number of applications that reached each stage and are not mutually exclusive. ",
        "text": "The Promise Zone (PZ) program was announced in 2013 during President Obama’s State of the Union address. The program was an interagency effort led by the Department of Housing and Urban Development (HUD), and there were 3 rounds of designations held in 2014, 2015, and 2016. In each round, organizations applied to one of 3 different categories of PZs: Urban, Rural, and Tribal. Due to the smaller sample sizes of Rural and Tribal categories, this paper focuses on the Urban PZs. The majority of the lead applicants were local governments, but occasionally some NGOs submitted the applications with the local government entities as co-applicants. Eligibility requirements differed slightly across the 3 rounds, but the general requirements were to be a contiguous geographic area of 10,000 to 200,000 residents, with an average poverty rate above 20\\%. There were a total of 13 Urban PZs designated, and Figure \\ref{fig:apps} shows an overview of the applications across each round.\nThese plots show that the treatment effect was greater in the later years, with almost the entire main effect being driven the 2016 designees. The likely reason for this is that the designees in 2016 had higher quality plans for neighborhood improvement than the ones in earlier years. Applications were scored primarily on the plans to improve the neighborhood and the capacity of local implementation partners, and, as mentioned earlier, applications above the threshold of 75 points were considered ``finalists\". Therefore, the applicants were essentially being scored on their predicted treatment effect. As seen in Figure \\ref{fig:apps}, there were 6 finalists for 3 spots in 2014, 16 for 6 spots in 2015, and 36 for 5 spots in 2016, showing that the cohort of 2016 designees were drawn from a pool of stronger applicants and were likely to be stronger candidates.",
        "summarize_figure": "2503.05946v1_apps_overview",
        "summarization": "The picture shows the application situation of the Promise Zone (PZ) program in three rounds in 2014, 2015, and 2016, categorized as applicants, finalists, and designees. In 2014, there were 19 applicants, 6 finalists, and 3 designees; in 2015, 97 applicants, 16 finalists, and 6 designees; in 2016, 64 applicants, 36 finalists, and 5 designees. Led by the Department of Housing and Urban Development, applications were mainly scored based on neighborhood improvement plans and the capacity of local implementation partners, with scores above 75 making applicants finalists. The treatment effect was more significant in later years. The 2016 designees came from a stronger pool of applicants with higher - quality neighborhood improvement plans."
    },
    "77": {
        "figure1": "2503.05946v1_comp_inc_dist_diff",
        "label1": "fig:comp_inc_dist",
        "caption1": "Effects on Relative Shares of Income Distribution}  \\raggedright {\\footnotesize Notes: Panels (a) and (b) show income distribution from pre-period($e=-1$) and post-period($4\\leq e \\leq$ 7) that are formed as population-weighted averages of the respective groups. Panel (c) plots estimated ATTs and 95\\% confidence intervals from the baseline specification on shares of the income groups plus a correction for inflation owing to the differing baseline distributions. All incomes above \\$150k are in the final bucket.",
        "text": "Panels (a) and (b) of Figure \\ref{fig:comp_inc_dist} plot the income distributions of the Designee and the Finalist neighborhoods in the baseline period and the post-period. Both groups of neighborhoods saw improvement over time with smaller shares of residents in the two lowest income groups, although this difference appears larger in the Designee neighborhoods. Panel (c) formalizes that observation by plotting the causal effect of Promise Zones on the income distribution.  These estimates are derived from event-study regressions using the baseline specification on the share of households in different income buckets that are shown in Figure \\ref{fig:comp_inc} \\footnote{Due to the data availability only in nominal terms, Panel (c) also includes a correction for inflation owing to the slightly different baseline distributions. This correction was derived by assuming underlying income was uniform across the income buckets and simulating the difference-in-difference estimate produced by inflation at the observed levels of the Urban CPI. The confidence interval reflect the additional uncertainty from this procedure}. The roughly 2 percentage point decrease in households making less than \\$25,000 a year is consistent with the poverty rates estimate in Panel (a) of Figure \\ref{fig:main_plots}. On the other side of the distribution, there was a similar increase of almost 2 percentage points of households making over \\$100,000 a year in the Designee neighborhoods.",
        "summarize_figure": "2503.05946v1_comp_inc_dist_diff",
        "summarization": "The picture shows the effects of Promise Zones on the relative shares of income distribution. The horizontal axis represents household income (in thousands of US dollars), and the vertical axis represents the difference (in percentage points). Analyzing the income distribution of designee and finalist neighborhoods in the baseline and post - periods, it's found that over time, the share of residents in the two lowest income groups decreased in both areas, more significantly in designee neighborhoods. The figure plots the estimated causal effects of Promise Zones on income distribution and 95% confidence intervals. Results show that the share of households with an annual income of less than $25,000 decreased by about 2 percentage points, consistent with poverty rate estimates. In designee neighborhoods, there was a similar increase of nearly 2 percentage points in the share of households with an annual income of over $100,000."
    },
    "78": {
        "figure1": "2503.06454v1_phi_N50",
        "label1": "fig:phi-50",
        "caption1": "Distribution of the posterior mean of $\\phi$ across $100$ replicates with $N = 50, J = 10$. The true value $\\phi^*$ is indicated by the dotted line.} %% TODO: change w to w^*",
        "text": "Next, we visualize the distribution of the posterior mean estimate of $\\phi$ (error precision) across 100 replicates in Figure~\\ref{fig:phi-50}.  This statistic reflects  how well the \\BVS{} model fits the data.  We only show the result for $N = 50$, as the distribution for $N = 20$ is similar.  Observe that regardless of $\\|w^*\\|_1$, the posterior mean of $\\phi$ increases with $M$, which is expected since a larger sample size enables \\BVS{} to detect more signals in the data, resulting in a smaller estimate of the error variance.  When $\\|w^*\\|_1 = 3$,  \\BVS{} appears to always underestimate $\\phi$ even when the sample size is sufficiently large, which may be related to the fact that the simplex constraint is significantly violated in this case.  Notably, when $\\|w^*\\|_1 = 2$, the posterior estimation of $\\phi$ appears highly accurate for $M \\geq 100$, indicating that our model achieves an optimal balance between model complexity and fitting.",
        "summarize_figure": "2503.06454v1_phi_N50",
        "summarization": "This set of pictures shows the distribution of the posterior mean of the error precision ϕ in 100 replicates with N = 50 and J = 10. The dotted line represents the true value of ϕ. Regardless of the L1 - norm (||w*||₁) of w*, as the sample size M increases, the posterior mean of ϕ increases, since a larger sample size allows the Bayesian variable selection (BVS) model to detect more signals in the data, resulting in a smaller error variance estimate. When ||w*||₁ = 3, the BVS model always seems to underestimate ϕ even with a large enough sample size, possibly due to significant violation of the simplex constraint. When ||w*||₁ = 2 and M ≥ 100, the posterior estimation of ϕ is highly accurate, indicating that the model achieves an optimal balance between complexity and fitting."
    },
    "79": {
        "figure1": "2503.06454v1_tau_N50",
        "label1": "fig:tau-50",
        "caption1": "Distribution of the posterior mean of $\\tau$ across $100$ replicates  with $N = 50, J = 10$.",
        "text": "Finally, we examine the posterior mean estimate of $\\tau$ when $N = 50$, for which the result is presented in Figure~\\ref{fig:tau-50}.  As discussed in Section~\\ref{sec:bvs-model}, $\\tau$ can be considered as an indicator of whether the simplex constraint is satisfied by the given data, and we now explain why Figure~\\ref{fig:tau-50} provides compelling empirical evidence supporting this.  When $\\|w^*\\|_1 = 1$, we see that the posterior mean of $\\tau$ decreases as $M$ increases, since a larger sample size offers stronger evidence that the simplex constraint is satisfied, which draws $\\tau$ towards zero.  In contrast, when $\\|w^*\\|_1 = 2$ or $3$, the trend reverses, since a larger sample size enables \\BVS{} to detect deviations from the simplex constraint.  When $M = 200$, we find that the ratio of the posterior mean estimate of $\\tau$ and that of $\\phi$ equals $0.0003$ for $\\|w^*\\|_1 = 1$, $0.024$ for $\\|w^*\\|_1 = 2$, and $0.081$ for $\\|w^*\\|_1 = 3$. This actually aligns well with the true data-generating mechanism: since we use $J = 10$ for $N = 50$,  we may estimate the ``true'' mean squared deviation of $w^*$ from the simplex constraint by $0.1 \\times \\sum_{j=1}^{10} (w^*_j - 0.1)^2$, which equals $0.021$ for $\\|w^*\\|_1 = 2$ and $0.065$ for $\\|w^*\\|_1 = 3$.   The true value $\\phi^*$ is indicated by the dotted line.} %% TODO: change w to w^*",
        "summarize_figure": "2503.06454v1_tau_N50",
        "summarization": "This set of pictures shows the distribution of the posterior mean of τ in 100 replicates with N = 50 and J = 10. τ can be regarded as an indicator of whether the simplex constraint is satisfied by the given data. When ||w*||₁ = 1, as the sample size M increases, the posterior mean of τ decreases, since a larger sample size provides stronger evidence that the simplex constraint is satisfied, pulling τ towards zero. When ||w*||₁ = 2 or 3, the trend is reversed, as a larger sample size enables the BVS model to detect deviations from the simplex constraint. When M = 200, the ratios of the posterior mean estimates of τ to ϕ for different ||w*||₁ values are consistent with the true data - generating mechanism."
    },
    "80": {
        "figure1": "2503.06582v1_equilibriumopt_c_A=4_c_B=2_alpha=0.2_k=2_theta=10",
        "label1": "fig:equilibrium_opt",
        "caption1": "Visualization of $\\MO$'s utility for different inventory-setting strategies. Game parameters are $c_{\\MO}=4$, $c_{\\IS}=2$, $\\alpha=0.2$, $k=2$, and $\\theta=10$.",
        "text": "Reading off from Lemma \\ref{lemma:qstar_A}, we see that we simply have to compare the optimal utilities for      (1) any $p_{\\MO}$ and $q_{\\MO} = 0$; (2) $p_{\\MO} \\in [p_0, \\psole)$ and $q_{\\MO} =\\qthresh(p_{\\MO})$;     (3) $p_{\\MO} \\in [p_0, \\psole)$ and $q_{\\MO} = \\qthresheps(p_{\\MO})$; and (4) $p_{\\MO} \\in [0, p_0)$ and $q_{\\MO} = Q(p_{\\MO})$.     $\\MO$'s utility for (1) is      $u_{\\MO} = (\\alpha \\psole + k) Q(\\psole)$, which is constant in $p_{\\MO}$. The optimal prices for (2),(3), and (4) can be easily identified using a numerical solver.     Figure \\ref{fig:equilibrium_opt} illustrates how we identify $p^*_{\\MO}$. The utility in (1) is plotted in green. The utilities for (2) are plotted in red, with a point plotted where the maximal utility is achieved. The same is true for (3), in yellow, and (4), in blue. $p^*_{\\MO}$ (circled in red) is then identified by comparing the utility from (1) to the utilities achieved by the maximizing points from (2), (3), and (4) and selecting the price that achieves the highest utility (if (1) has the highest utility, $\\MO$ abstains by setting $p^*_{\\MO}=\\infty$).",
        "summarize_figure": "2503.06582v1_equilibriumopt_c_A=4_c_B=2_alpha=0.2_k=2_theta=10",
        "summarization": "These two pictures show the utility of MO (presumably a certain entity) under different inventory - setting strategies. The game parameters are c_MO = 4, c_IS = 2, α = 0.2, k = 2, and θ = 10. Different colored curves in the figure represent the utilities under different strategies: the green curve represents the utility when q_MO = 0 (a constant value); the red curve represents the utility of q_MO = q⁺(p_MO) (INDUCE COMPETE strategy), with the red dot being the point of maximum utility; the yellow curve represents the utility of q_MO = q⁺(p_MO) - ε (INDUCE WAIT strategy), with the yellow dot being the point of maximum utility; the blue curve represents the utility of q_MO = Q(p_MO) strategy, with the blue dot being the point of maximum utility. By comparing the maximum utilities of these strategies, the optimal price p*_MO (marked by a red circle) is determined. If the utility represented by the green curve is the highest, MO chooses not to participate (sets p*_MO = ∞)."
    },
    "81": {
        "figure1": "2503.06582v1_proportional_alpha=0.2_k=2_theta=10_gamma=1_uAuB",
        "label1": "fig:cA_cB_uA_uB",
        "caption1": "Intensity} \\end{subfigure} \\begin{subfigure}{.45\\textwidth} \\centering % Proportional \\includegraphics[width=\\textwidth]{figs/proportional_alpha=0.2_k=2_theta=10_gamma=1_uAuB.pdf} \\vspace{-20pt} \\caption{Proportional} \\end{subfigure} \\vspace{-5pt} \\caption{Utilities of $\\MO$ and $\\IS$ for each ($c_{\\MO}, c_{\\IS}$) combination under different rationing rules when $\\alpha=0.2$ and $k=2$.",
        "text": "Figure \\ref{fig:cA_cB_phase_diagram} characterizes the equilibrium for different combinations of $c_{\\MO}$. We mark $\\theta(1-\\alpha)$ on the $c_{\\IS}$ axis because this is a critical threshold at which $p_0 = \\theta$; above this threshold, $\\IS$ always abstains regardless of what the $\\MO$ does, as described in Section \\ref{sec:preliminaries}. In Figure \\ref{fig:cA_cB_phase_diagram}, we observe that when $c_{\\MO}$ is high and $c_{\\IS}$ is low, $\\MO$ will prefer to let $\\IS$ sell and $\\MO$ will induce $\\IS$ to compete. When $c_{\\IS}$ is relatively large, $\\MO$ will prefer to sell themselves and $\\IS$ will be induced to abstain. There is also a small region where $\\MO$ and $\\IS$ have roughly equal costs in which $\\MO$ finds it optimal to induce $\\IS$ to wait.  Relating back to the concept of de-monopolization, note that red regions in both plots and the yellow region in the intensity rationing (left) plot correspond to regions where $\\IS$ is de-monopolized at equilibrium. \\tdcomment{Could write this as a corollary in the previous section}  Figure \\ref{fig:cA_cB_prices_and_quantities} visualizes the corresponding prices and quantities. We observe that for the ($c_{\\MO}$, $c_{\\IS}$) combinations that fall in the red ``$\\IS$ competes'' region in Figure \\ref{fig:cA_cB_phase_diagram},  $\\MO$ induces compete by buying only a small quantity, which is enough to induce $\\IS$ to set a slightly lower price than they would otherwise. Figure \\ref{fig:cA_cB_uA_uB} shows the corresponding utilities. We see that the marketplace operator can achieve high utility when their own cost $c_{\\MO}$ is low, but they can also achieve high utility whenever the independent seller's cost $c_{\\IS}$ is low.",
        "summarize_figure": "2503.06582v1_proportional_alpha=0.2_k=2_theta=10_gamma=1_uAuB",
        "summarization": "The two plots display utilities of MO and IS for each (c_MO, c_IS) combination under intensity and proportional rationing rules with α = 0.2 and k = 2. Color indicates utility magnitude, and a black curve marks a critical threshold. High c_MO and low c_IS make MO let IS sell and compete; large c_IS leads MO to sell and IS to abstain. With similar costs, MO makes IS wait. Red regions in both and a yellow region in the left plot signify IS de - monopolization at equilibrium. MO attains high utility with low c_MO or c_IS."
    },
    "82": {
        "figure1": "2503.06582v1_alpha=0.2_k=2_theta=10_welfare_vs_Bonly",
        "label1": "fig:welfare_cA_cB",
        "caption1": "Welfare for different $c_{\\MO}$ and $c_{\\IS}$ combinations at the two-player equilibrium (left) and if only $\\IS$ is able to sell (middle). The difference (Both - $\\IS$ only) is on the right. The game parameters are the same as in Figures \\ref{fig:cA_cB_phase_diagram} and \\ref{fig:cA_cB_prices_and_quantities}. The minimum and maximum values of each colorbar correspond to the minimum and maximum values in the data being displayed.",
        "text": "Figure \\ref{fig:welfare_cA_cB} visualizes the welfare that results from the combinations of game parameters considered in Figures \\ref{fig:cA_cB_phase_diagram} and \\ref{fig:cA_cB_prices_and_quantities}. We remark that this plot is representative of plots for all game parameters and we defer the plots of the difference under different game parameters to Appendix TODO. For all of these plots, the following observations hold: (1) The total welfare when both $\\MO$ and $\\IS$ can choose to sell is always at least as high as the welfare when $\\IS$ is the sole seller. (2) The largest gain in total welfare occurs when $c_{\\IS}$ is large and $c_{\\MO}$ is small. This is not surprising because this is the region in which the equilibrium result is that $\\MO$ is the only seller.",
        "summarize_figure": "2503.06582v1_alpha=0.2_k=2_theta=10_welfare_vs_Bonly",
        "summarization": "This set of pictures shows the welfare under different c_MO and c_IS combinations. The left - hand figure is the welfare when both MO and IS are in a two - player equilibrium, the middle figure is the welfare when only IS can sell, and the right - hand figure is the difference between them. The game parameters are the same as in related figures. Observations are: (1) The total welfare when both MO and IS can choose to sell is always at least as high as when only IS sells. (2) The largest increase in total welfare occurs when c_IS is large and c_MO is small, because in this region the equilibrium result is that MO is the only seller."
    },
    "83": {
        "figure1": "2503.08655v1_psi",
        "label1": "fig.psi",
        "caption1": "\\small (a) Plots of $h(x)=x\\{2F(x)-1\\}$ and $h(x)=x^2$; (b) Plots of $\\psi(X)$ for $X\\sim\\cN(0,\\sigma^2),{\\rm Logistic}(0,\\sqrt{3}\\sigma/\\pi)$ and $U(-\\sqrt{3}\\sigma,\\sqrt{3}\\sigma)$ with different values of $\\sigma$; (c) Plots of $\\psi(cX)$ for $X\\sim t_2$, $t_3$, and $t_4$ with different values of $c>0$; (d) Plots of $\\psi(X)$ for $X\\sim S(\\alpha,0,0.9,0),S(\\alpha,0,1,0)$ and $S(\\alpha,0,1.1,0)$ with different values of $\\alpha$. In (b)-(d), the horizontal dotted lines stand for $\\psi(X)=1$.} \\vspace{-1em",
        "text": "We shall assume a different moment condition on the innovation rather than the usual condition $\\eE\\eta_t^2=1$.   made a similar point when studying the non-Gaussian QMLE of GARCH models. First,  we define where $F(x)=1/\\{1+\\exp(-x)\\}$ is the cumulative distribution function of the standard logistic distribution. Clearly, $h(x)$ is nonnegative and even; see Figure~\\ref{fig.psi}(a).  We claim that all parameters are identifiable if $\\psi(\\eta_t)=1,$ under which the LQMLE is strongly consistent with some extra mild assumptions.  Lemma~{\\color{blue}S.4} of the supplementary material justifies the use  of $\\psi(\\eta_t)=1$. Such a condition relaxes the restriction of $\\eE\\eta_t^2<\\infty$ to $\\eE|\\eta_t|<\\infty$ since $2F(x)-1\\in[-1,1]$ for all $ x\\in\\eR.$   Further, it is worth noting that the conditional variance of $y_t$ (if exists) is proportional to $\\sigma^2(\\bY_{t-1},\\btheta)$, without assuming that  $\\eE\\eta_t^2=1$. This means that $\\sigma^2(\\bY_{t-1},\\btheta)$ can still  be interpreted as the volatility of models~\\eqref{eq.model}-\\eqref{eq.model_2}, similar to the interpretation of re-parameterized GARCH models in \\cite{fan2014quasi}.  For an intuitive understanding of $\\psi(\\eta_t)=1$, we give an example as follows. Some examples of distributions satisfying $\\psi(\\eta_t)=1$ include: $\\mathrm{(i)}$ the standard logistic distribution ${\\rm Logistic}(0,1);$ $\\mathrm{(ii)}$ the normal distribution $\\cN(0,\\sigma^2)$ with $\\sigma\\approx1.75;$ $\\mathrm{(iii)}$ the uniform distribution $U(-a,a)$ with $a\\approx2.85;$ $\\mathrm{(iv)}$ the Student's $t_{\\nu}$-distributions with degrees of freedom $\\nu$, e.g., $c\\, t_2$ with $c\\approx0.96$ and $c\\, t_3$ with $c\\approx1.25;$ $\\mathrm{(v)}$ the standard symmetric stable distribution $S(\\alpha,0,1,0)$ with $\\alpha\\approx1.69.$ %Figure~\\ref{fig.psi} gives more details of $\\psi(\\eta_t)$ See Figure~\\ref{fig.psi} for more details.",
        "summarize_figure": "2503.08655v1_psi",
        "summarization": "This set of pictures, with four sub - figures, shows the function and distribution characteristics related to ψ(η_t) = 1. Figure (a) compares h(x) = x{2F(x) - 1} (red solid line, where F(x) is the cumulative distribution function of the standard logistic distribution) and h(x) = x² (green dashed line), and h(x) is non - negative and even. Figure (b) plots ψ(X) for normal, logistic, and uniform distributions with different σ values, showing their trends. Figure (c) plots ψ(cX) for t - distributions with degrees of freedom 2, 3, and 4 for different c values. Figure (d) plots ψ(X) for S(α, 0, 0.9, 0), S(α, 0, 1, 0), and S(α, 0, 1.1, 0) distributions with different α values. These figures help understand how different distributions satisfy ψ(η_t) = 1 under a specific moment condition, which relaxes the restrictions on the innovation term."
    },
    "84": {
        "figure1": "2503.08761v1_comparison2",
        "label1": "trend_wetbulb",
        "caption1": "[Wet bulb and dry bulb temperature in 2003 in Rajasthan]{\\textbf{Wet bulb and dry bulb temperature in 2003 in Rajasthan",
        "text": "Figure \\ref{trend_wetbulb} exemplary presents the 2003 wet bulb and dry bulb temperature trend in Bharatpur in the state Rajastan. It shows that not only the levels are different (as explained above), but that also the trend in the two temperature measures occasionally differs significantly. In particular, while the highest dry bulb temperatures are reached in June, the highest wet bulb temperatures are only reached in August and September, when humidity levels are highest (indicated by almost identical dry bulb and wet bulb temperatures).",
        "summarize_figure": "2503.08761v1_comparison2",
        "summarization": "This picture shows the trends of wet bulb and dry bulb temperatures in Bharatpur, Rajasthan, India in 2003. The blue curve represents the wet bulb temperature, and the red curve represents the dry bulb temperature. There are differences in their numerical levels and trends. The dry bulb temperature reaches its highest value in June, while the wet bulb temperature peaks in August and September when the humidity level is highest, with the dry bulb and wet bulb temperatures being nearly identical."
    },
    "85": {
        "figure1": "2503.08761v1_Main_results",
        "label1": "figure_results",
        "caption1": "[Heat effects on depression and anxiety (wet bulb temperature)]{\\textbf{Heat effects on depression and anxiety (wet bulb temperature)} - The figure displays the effect of the number of days in a given wet bulb temperature range relative to the number of days with a wet bulb temperature in the reference category (21°C-22.5°C) in the last 30 days on self-reported symptoms of depression and anxiety. Outcomes are binary variables (severe/extreme depression (=1); severe/extreme anxiety (=1)). Coefficients are multiplied by 100. ",
        "text": "Figure \\ref{figure_results} presents the results from the wet bulb temperature bin regression separately for depression (blue) and anxiety (red). Each coefficient plot is interpreted as the percentage point change in the likelihood of suffering from severe/extreme anxiety or depression that is observed if one additional day within the last 30 days had a wet bulb temperature in the range plotted on the x-axis instead of one day with a mean wet bulb temperature range between 21°-22.5°C.\nThe results show that the probability of having suffered from severe/extreme depression in the last 30 days significantly increases with higher wet bulb temperatures. This is especially pronounced if wet bulb temperatures surpass 27°C: one more day above 27°C (instead of a day in the reference temperature range) increases the probability of having suffered from severe/extreme depression by 0.5 percentage points. In relative terms, this corresponds to an increase of 6\\% from a mean prevalence of 8.47\\%. Also for one more day in the temperature range between 25.5°C and 27°C, a significant increase of 0.25 percentage points can be observed. These results are similarly present when I use the continuous depression score (from 1-5) as outcome variable (results are displayed together with the coefficients corresponding to Figure \\ref{figure_results} in Table \\ref{wetbulb_R1} in Appendix C): one more day above 27°C increases the depression score by 0.017 points. The effects remain significant and become smaller (larger) when I shift the temperature bins down (up) by 1°, i.e. such that the highest bin becomes 26° (28°). The results are shown in tables \\ref{wetbulb_R2} and \\ref{wetbulb_R3}.",
        "summarize_figure": "2503.08761v1_Main_results",
        "summarization": "This picture shows the results of a study on the effects of wet bulb temperature on depression and anxiety. Blue dots represent depression, and red squares represent anxiety. The horizontal axis shows different wet bulb temperature ranges, and the vertical axis shows the degree of impact on mental health (coefficients multiplied by 100). Results indicate that over the past 30 days, as the wet bulb temperature rises, the probability of severe/extreme depression increases significantly. When the wet bulb temperature exceeds 27°C, an additional day in this temperature range (compared to the reference range of 21°C - 22.5°C) increases the probability of severe/extreme depression by 0.5 percentage points. There is also a significant increase in the 25.5°C - 27°C range. When using a continuous depression score as the outcome variable, the results are similar, with an additional day above 27°C increasing the depression score by 0.017 points."
    },
    "86": {
        "figure1": "2503.09287v1_Figure_3",
        "label1": "fig: n of resp",
        "caption1": "SPF Participation}  \\begin{center} \\includegraphics[trim = 00mm 00mm 0mm 00mm,clip,scale=.45]{graphics/Figure_3.pdf} \\end{center} \\vspace{-2mm} \\begin{adjustwidth}{1cm}{1cm} \\begin{spacing}{1.0}  \\noindent  \\footnotesize Notes: We show the number of participants in the U.S. Survey of Professional Forecasters, 1968Q4-2023Q2. \\end{spacing} \\end{adjustwidth",
        "text": "The U.S. Survey of Professional Forecasters is a quarterly survey covering several U.S. macroeconomic variables. It was started in 1968Q4 and is currently  conducted and maintained by the Federal Reserve Bank of Philadelphia.\\footnote{For a recent introduction see  \\cite{CroushoreStark2019}.} In Figure \\ref{fig: n of resp} we show the evolution of the number of forecast participants, which declined until 1990Q2, when the Federal Reserve Bank of Philadelphia took control of the survey, after which it has had approximately 40 participants.  Participants stayed for 15 quarters on average, with a minimum of 1 quarter and a maximum of 125 quarters.",
        "summarize_figure": "2503.09287v1_Figure_3",
        "summarization": "This picture shows the evolution of the number of participants in the U.S. Survey of Professional Forecasters (SPF) from the fourth quarter of 1968 to the second quarter of 2023. The vertical axis represents the number of participants, and the horizontal axis represents the year. The data shows that the number of participants declined until the second quarter of 1990, when the Federal Reserve Bank of Philadelphia took over the survey. Since then, the number of participants has stabilized at around 40. Participants stayed for an average of 15 quarters, with a minimum of 1 quarter and a maximum of 125 quarters."
    },
    "87": {
        "figure1": "2503.09287v1_Figure_5d",
        "label1": "fig: mean std",
        "caption1": "SPF Forecast Error Means and Standard Deviations}  \\begin{center} \\quad  \\quad  \\textbf{Growth Mean} \\quad \\quad \\quad  \\quad \\quad \\quad  \\quad \\quad \\quad  \\quad \\textbf{Growth S.D.}\\vspace{2mm} \\\\ \\includegraphics[trim = 00mm 2mm 0mm 0mm,clip,width=0.43\\textwidth]{graphics/Figure_5a.pdf} \\includegraphics[trim = 00mm 2mm 0mm 0mm,clip,width=0.43\\textwidth]{graphics/Figure_5b.pdf} \\\\ \\vspace{4mm} \\quad  \\quad  \\textbf{Inflation Mean} \\quad \\quad  \\quad \\quad \\quad  \\quad \\quad \\quad  \\quad \\textbf{Inflation S.D.}\\vspace{2mm} \\\\ \\includegraphics[trim = 00mm 2mm 0mm 0mm,clip,width=0.43\\textwidth]{graphics/Figure_5c.pdf} \\includegraphics[trim = 00mm 2mm 0mm 0mm,clip,width=0.43\\textwidth]{graphics/Figure_5d.pdf} \\\\ \\end{center} \\begin{adjustwidth}{1cm}{1cm} \\begin{spacing}{1.0}  \\noindent  \\footnotesize Notes: We show time series of cross-sectional means and standard deviations of individual 1-step-ahead forecast errors,  1968Q4-2023Q2.  Gray shaded regions denote recessions. \\end{spacing} \\end{adjustwidth",
        "text": "Because  individual forecast errors drive our analysis,  as per equation \\eqref{MSE2}, we begin by examining their   evolving period-by-period cross-sectional distributions.  In Figure \\ref{fig: mean std} we show the time-series of cross-sectional means and standard deviations. Several features are apparent:",
        "summarize_figure": "2503.09287v1_Figure_5d",
        "summarization": "This picture shows the change in the standard deviation of inflation forecast errors in the U.S. Survey of Professional Forecasters (SPF) from the fourth quarter of 1968 to the second quarter of 2023. Gray shaded areas represent recessions. It can be seen that from the 1970s - 1980s, the standard deviation of inflation fluctuated greatly and was at a high level, then gradually decreased and stabilized, with corresponding fluctuations during recessions, reflecting the changes in the dispersion of inflation forecast errors in different periods."
    },
    "88": {
        "figure1": "2503.09765v1_MEVcpmm",
        "label1": "fig:MEV_CPMM",
        "caption1": "The graph plots the profits of elementary MEV sandwich manipulations $\\hat \\Delta x$ for $x_i=400,000$ and $\\Delta x =40,000$.",
        "text": "Figure \\ref{fig:MEV_CPMM} illustrates the proposition for different values of $\\hat \\Delta x$.",
        "summarize_figure": "2503.09765v1_MEVcpmm",
        "summarization": "The graph shows the relationship between the profits of elementary MEV sandwich manipulations (vertical axis: MEV profits CPMM) and the variable Δ^x (horizontal axis). When xi​ is 400,000 and Δx is 40,000, as Δ^x increases, the MEV profits grow linearly, starting from 0 and rising gradually, indicating a positive correlation between them."
    },
    "89": {
        "figure1": "2503.09765v1_ILCPMM",
        "label1": "fig:GMM_IL_CPMM",
        "caption1": "The graph plots the impermanent losses as a function of the price increase $\\frac{r}{r_o}$.",
        "text": "Mathematically, impermanent loss is defined as the relative loss incurred by an LP compared to simply holding the original asset allocation:\\footnote{See  \\cite{lipton2024unified} for a comprehensive discussion of alternative measures of impermanent losses. Ours correspond to what they call relative IL of borrowed Liquidity Provision.} where \\( V_{i,t}\\equiv y_i+r x_i \\) is the value of the liquidity holdings of the AMM at the current price, and \\( V^0_i\\equiv y_i^0+r x_i^0 \\) is the value of the liquidity holdings if the liquidity provider had kept the initial asset allocation. The impermanent losses for the CPMM has a simple expression under the assumption that the initial ratio of reserves of the AMM and the final ratio of reserves are equal to $r_0$ and $r$, respectively, and so are the initial and final marginal prices of the AMM: where \\( r_0 \\) and \\( r \\) are the initial and final price ratios of the token pair, respectively. Figure \\ref{fig:GMM_IL_CPMM} provides a graph of the impermament losses as a function of the ratio $\\frac{r}{r_0}$ and the next example illustrates the concept.",
        "summarize_figure": "2503.09765v1_ILCPMM",
        "summarization": "The graph shows the functional relationship between the impermanent losses (vertical axis: Impermanent Losses (%)) of the Constant Product Market Maker (CPMM) and the price increase ratio r′/r (horizontal axis). The impermanent losses first decrease and then increase with the change of r′/r. When r′/r is 1, the impermanent losses reach the lowest point of 0%. When r′/r is less than 1, the impermanent losses decrease as it increases, and when r′/r is greater than 1, the impermanent losses increase as it increases."
    },
    "90": {
        "figure1": "2503.09765v1_MEVgmm",
        "label1": "fig:MEV_GMM",
        "caption1": "The graph plots the profits of elementary MEV sandwich manipulations $\\hat \\Delta x$ for $x_i=400,000$, $x=800.000$ and $\\Delta x =40,000$.",
        "text": "Figure \\ref{fig:MEV_GMM} illustrates the proposition for different values of $\\hat \\Delta x$ and compare the results with MEV profits obtained in the CPMM case. The GMM reduces substantially the profitability of MEV attacks and even render them unprofitable.",
        "summarize_figure": "2503.09765v1_MEVgmm",
        "summarization": "The graph shows the relationship between the profits of elementary MEV sandwich manipulations (vertical axis: MEV profits) and the variable Δ^x (horizontal axis), comparing the Constant Product Market Maker (CPMM, blue line) and the Generalized Market Maker (GMM, red line). When xi​ is 400,000, x is 800,000 and Δx is 40,000, the MEV profits of CPMM increase linearly with the increase of Δ^x. The MEV profits of GMM first increase and then decrease with the increase of Δ^x, and may even become unprofitable, indicating that GMM significantly reduces the profitability of MEV attacks."
    },
    "91": {
        "figure1": "2503.10132v6_sgraph-en-new",
        "label1": "fig1",
        "caption1": "The graph of the equilibrium probability of paper} \\begin{center} \\includegraphics[height=7cm]{sgraph-en-new.pdf} \\end{center",
        "text": "In light of the above, this paper introduces Shinohara RPS to researchers and studies its subgame perfect equilibria (SPE), where the expected payoff is the probability of winning.   This game has a unique symmetric SPE, in which the probability of choosing paper is the solution to the equation \\((1-p)^{n-1} + p^{n-1}/n = 1/n\\) (see Figure~\\ref{fig1}) when $n$ players remain.  The game also admits pure-strategy asymmetric SPE, including the following two types. In one type, one player chooses paper and the other players choose rock in each round. In another type, two players choose paper and the other players choose rock in each round.  Furthermore, there is a continuum of SPE in which exactly one player adopts a mixed strategy. The considerable variety of SPE in Shinohara RPS suggests that no particular equilibrium is likely to emerge consistently in actual play.\nCombining the above arguments, it follows that $p_h$ is a solution to the equation  (1-p_h)^{n-1}+\\frac{p_h^{n-1}}{n}=\\frac{1}{n}, which can be rewritten as  We can make two observations from this equation.  First, a solution is determined solely by the number of remaining players $n$.  Second, a solution in the open unit interval is unique since the left-hand side of \\eqref{prob2} is decreasing in $p_h$ with the minimum zero and the maximum one.  Let $\\phi_n\\in (0,1)$ denote the unique solution,   which is plotted in Figure \\ref{fig1} and listed in Table \\ref{table1}.",
        "summarize_figure": "2503.10132v6_sgraph-en-new",
        "summarization": "The graph shows the relationship between the number of remaining players n (horizontal axis) and the equilibrium probability ϕn​ (vertical axis) of choosing paper in Shinohara Rock - Paper - Scissors (RPS) game. As n increases from 1 to 50, ϕn​ shows a decreasing trend. When n is small, ϕn​ drops significantly; as n increases, the decline of ϕn​ gradually slows down, approaching a relatively small value. This reflects the rule of how the equilibrium probability of choosing paper changes with the number of players in this game."
    },
    "92": {
        "figure1": "2503.10821v1_p1",
        "label1": "fig:combined_motives",
        "caption1": "Motives for (not) switching to a new heating system\\\\{\\footnotesize This Figure shows average motives and average beliefs about others' motives. Once again, we merge data from proprietors and landlords.",
        "text": "Figure \\ref{fig:combined_motives} in the Appendix shows average motives between fossil and non-fossil users as well as average beliefs about the behaviors of others. Table \\ref{mot1tab} in the Appendix shows full summary statistics. For fossil users, that the current system still works well and that substantial financial expenses would be necessary to switch appear to be the major motives for staying with fossil technology. For non-fossil users, the picture is more mixed, with all motives attaining substantial allocations, even the “Other” motive. However, the most prominent motive appears to be a willingness to become independent of fossil imports, followed by a feeling of being pressured into using net-zero technologies and a desire to contribute to the common good of protecting the climate. Pressure to use non-fossil technology is thought to be the major motive in the decision-making of others. Regarding users of fossil technology, they tend to overestimate financial motives in others, and to underestimate reliability concerns. Both of these differences are highly significant (paired $t$-test, maximum $p < 0.001$).",
        "summarize_figure": "2503.10821v1_p1",
        "summarization": "The graph shows the average proportion of motives for fossil users, beliefs about fossil users, non - fossil users, and beliefs about non - fossil users when choosing (or not choosing) a new heating system. For fossil users, “system works well” and “high expenses for switching” are the main motives. Non - fossil users have more diverse motives, with “becoming independent of fossil imports” being the most prominent, followed by “pressure to use net - zero technologies” and “contributing to climate protection”. People think the pressure to use non - fossil technology is the main motive for others' decision - making. Fossil users tend to overestimate others' financial motives and underestimate concerns about reliability."
    },
    "93": {
        "figure1": "2503.10821v1_p2",
        "label1": "fig:knowfine",
        "caption1": "Relationship between knowledge and assessment of the Law",
        "text": "Table \\ref{regt1} presents cross-sectional regressions of various outcome variables on covariates.\\footnote{We deviate from our preregistration by excluding motives for heating decisions and beliefs about the heating law from these regressions. The reason for excluding motives is that questions differed between respondents who switched and those who did not. Beliefs about the heating law were not elicited shortly before conducting the survey due to space constraints. Additionally, while we do not directly include the Knowledge Score in our regressions, we provide further correlational evidence related to it below.} In all models, we use the treatments as a regressor. Based on recent findings on the robustness of “feelings integers” \\citep{kaiser2022scientific}, we assume that Likert scales can enter our analyses as linear outcome variables. Similarly, in Table \\ref{regt1k} in the Appendix we use the Knowledge Score as an explanatory variable. For all of our outcomes, in all specifications, knowledge about the Law shows no causal effect on behavior or attitudes. The same holds if we conduct a correlational analysis among those in the baseline treatment (Table \\ref{regt1base} in the Appendix), that is, those who never received information. The standard errors attached to treatment and Knowledge Score coefficients are small across multiple specifications. Hence, these null effects are precisely estimated. Knowledge appears not to matter either as a cause nor as a mere correlational predictor of behaviors and attitudes. Figure \\ref{fig:knowfine} in the Appendix further illustrates that there is no relationship between knowledge and the perception that the “Law is sensible.”",
        "summarize_figure": "2503.10821v1_p2",
        "summarization": "The graph shows the relationship between the Knowledge Score (horizontal axis) and the assessment of the Law's sensibility (Law sensible (mean), vertical axis). The data points and their error bars indicate that as the Knowledge Score ranges from 0 to 12, the mean value of the assessment of the Law's sensibility does not show an obvious trend. This suggests that there is no relationship between the knowledge score about the Law and the perception of whether the \"Law is sensible\", meaning knowledge has neither a causal effect nor a correlational predictive relationship on behaviors and attitudes."
    },
    "94": {
        "figure1": "2503.10821v1_p3",
        "label1": "fig:priofine",
        "caption1": "Relationship between Pro-Environmental Priority and assessment of the Law",
        "text": "Table \\ref{regt2a} presents regressions of behavior and attitudes on treatment and “Pro-Environmental Priority.”\\footnote{Table \\ref{regt2} in the Appendix presents an alternative specification that includes all policy goals (with the “Other” policy goal left out because of multicollinearity).} Table \\ref{regt2a} is otherwise identical to Table \\ref{regt1}. The results indicate that having the pro-environmental priority is strongly predictive of behaviors \\emph{and} attitudes. However, there is no significant relation to knowledge about the Law. Column 4 of Table \\ref{regt2a} indicates that donations to the climate charity are also predicted by the pro-environmental priority, indicating a strong consistency between these items. Figure \\ref{fig:priofine} in the Appendix demonstrates a striking difference in the distributions of “Law sensible” that occurs between those with the pro-environmental priority and those without.",
        "summarize_figure": "2503.10821v1_p3",
        "summarization": "The graph shows the percentage distribution of people's assessments of the Law's sensibility (Law sensible) among those with and without Pro - Environmental Priority. Among those without Pro - Environmental Priority, the proportion of people choosing \"Don't agree at all (1)\" is the highest. Among those with Pro - Environmental Priority, the proportions of people choosing \"Agree (3)\", \"Somewhat agree (4)\", and \"Agree completely (5)\" are relatively high. This indicates that Pro - Environmental Priority has a significant impact on people's attitudes towards the Law's sensibility, and those with Pro - Environmental Priority are more likely to consider the Law sensible."
    },
    "95": {
        "figure1": "2503.11270v1_three_rewards_updated",
        "label1": "fig:three_rewards",
        "caption1": "Profit functions for one firm under Standard Bertrand, Bertrand-Edgeworth, and Logit Bertrand models in a homogeneous duopoly. The Logit Bertrand model shows a smooth and convex reward surface, followed by the Standard Bertrand model, while the Bertrand-Edgeworth model shows greater irregularities.",
        "text": "Figure~\\ref{fig:three_rewards} visualizes the profit functions \\(r\\) for all three market models, assuming the demand to behave as described in Equations~\\eqref{eq:standardDemand}--\\eqref{eq:logitDemand}. In the Standard Bertrand model, the profit surface exhibits a symmetric and concave shape, reflecting the homogeneous nature of the products and the fierce characterization of the resulting competition, which leads to an all-or-nothing outcome for the players. In the Bertrand-Edgeworth model, the profit function retains a similar structure but shows the influence of capacity constraints,  particularly when demand exceeds the capacity limit. Lastly, the Logit Bertrand model introduces greater asymmetry and smoother transitions in the profit function, capturing the effects of product differentiation and consumer preference heterogeneity.",
        "summarize_figure": "2503.11270v1_three_rewards_updated",
        "summarization": "The graph shows the three - dimensional images of the profit functions of a firm in a homogeneous duopoly under the Standard Bertrand, Bertrand-Edgeworth, and Logit Bertrand models. In the Standard Bertrand model, the profit surface is symmetric and concave, reflecting the all - or - nothing outcome due to product homogeneity and intense competition. The Bertrand-Edgeworth model has a similar profit function structure but is affected by capacity constraints, especially when demand exceeds the capacity limit. The Logit Bertrand model has a more asymmetric and smoother profit function, capturing product differentiation and consumer preference heterogeneity. The reward surface of the Logit Bertrand model is smooth and convex, followed by the Standard Bertrand model, while the Bertrand-Edgeworth model shows greater irregularities."
    },
    "96": {
        "figure1": "2503.11464v1_IMG_grid",
        "label1": "fig:sg",
        "caption1": " Left Panel: Hierarchical basis functions at refinement levels $\\ell=1$ (solid black), $\\ell=2$ (dashed blue), and $\\ell=3$ (dash-dotted red). Right Panel: A two-dimensional SG with corresponding refinement levels and colors. ",
        "text": "Left Panel: Hierarchical basis functions at refinement levels $\\ell=1$ (solid black), $\\ell=2$ (dashed blue), and $\\ell=3$ (dash-dotted red).  Right Panel: A two-dimensional SG with corresponding refinement levels and colors. } We aim to approximate the policy function, where each policy is represented as $f: \\Omega \\to \\mathbb{R} $, with $ \\Omega = \\mathbb{R}^d $ and $ \\bb{x} \\subset \\Omega $ rescaled to the unit domain $ [0,1]^d$. In our case $d$ the number of continuous state variables in the economic model of interest, a potentially large number.\\footnote{For illustration and notational brevity, we assume zero boundary conditions. For a discretization scheme with nonzero boundary conditions—such as the Clenshaw-Curtis sparse grid setting employed in all our numerical experiments, see~\\cite{stoyanov2015tasmanian} and \\cite{pflueger10spatially}.} In one dimension, the unit domain $[0,1]$ can be discretized with grid spacing $h_l = 2^{-l}$, grid points at $x_{l,i} = i \\cdot h_l$, where $i \\in \\{1, \\dots, 2^l\\}$ and refinement level $l \\in \\mathbb{N}_+$. The univariate basis functions for this discretization are defined as which have support $[x_{l,i} - h_l,\\, x_{l,i} + h_l]$. In the left panel of Figure~\\ref{fig:sg}, we illustrate a one-dimensional hierarchical piecewise linear basis function. We extended the basis function to a $d$-dimensional domain by first introducing the multi-indices for the refinement level $\\bb{l}=({l}_1,\\ldots, {l}_d) \\in \\mathbb{N}_{+}^d $ and grid index $\\bb{i}=({i}_1,\\ldots, {i}_d)$ with the corresponding grid point $\\bb{x}_{\\bb{l},\\bb{i}} = (x_{l_1,i_1}, \\dots, x_{l_d,i_d})$. The $d$-dimensional basis functions are defined as We define the hierarchical index sets $\\bb{I}_{\\bb{l}}$, and the corresponding hierarchical subspace $W_{\\bb{l}}$ as W_{\\bb{l}} = \\operatorname{span}\\left\\{ \\phi_{\\bb{l},\\bb{i}} : \\bb{i} \\in \\bb{I}_{\\bb{l}} \\right\\}. The restriction to odd indices $i_j$ ensures that the supports of the basis functions are disjoint and collectively cover $[0,1]^d$. For the space of piecewise linear functions,     V_\\ell = \\bigoplus_{ \\norm{\\mathbf{l}}{\\infty} \\leqslant \\ell} W_{\\mathbf{l}}, we can construct a corresponding equidistant Cartesian grid, also called a \\emph{full grid}, with $M_{\\ell}=2^{\\ell}$ number of grid points in each dimension, where $\\ell$ denotes the \\emph{maximum refinement level}.\nAlthough the $L_2$ interpolation error is of order $\\mathcal{O}(M_\\ell^{-2})$, the total number of grid points is $\\mathcal{O}(M_\\ell^d)$, effectively rendering this approach impractical for high-dimensional functions. SG mitigates the curse-of-dimensionality by retaining only the most significant hierarchical subspaces.  Formally, the SG space is defined as V^{\\text{SG}}_\\ell = \\bigoplus_{\\|\\bb{l}\\|_1 \\le \\ell + d - 1} W_{\\bb{l}}. The SG interpolation of $f$ at a point $\\bb{x}$ with a maximum refinement level $\\ell$ is defined as where $\\alpha_{\\bb{l},\\bb{i}} \\in \\mathbb{R}$ (hierarchical surpluses). In the right panel of Figure~\\ref{fig:sg}, we illustrate the grid points for a two-dimensional SG. For functions with bounded mixed second derivatives, the SG interpolation error is of the order $\\mathcal{O}(M_\\ell^{-2} (\\log M_\\ell)^{d-1})$, while the number of grid points is $\\mathcal{O}(M_\\ell (\\log M_\\ell)^{d-1})$---substantially less grid points than the full grid in high dimensions. For further details on SGs, including the error analysis and the computation of hierarchical surpluses $\\alpha_{\\bb{l},\\bb{i}}$, we refer the reader to~\\cite{Bungartz.Griebel:2004} and references therein. Finally, quadrature on SGs, denoted as can be efficiently evaluated by integrating the basis functions defined in Equation~\\eqref{eq:3.2}.",
        "summarize_figure": "2503.11464v1_IMG_grid",
        "summarization": "The picture contains two sub - graphs. The left graph shows one - dimensional hierarchical basis functions at refinement levels ℓ=1 (solid black), ℓ=2 (dashed blue), and ℓ=3 (dash - dotted red), which exhibit piece - wise linear changes in different intervals. The right graph presents a two - dimensional sparse grid (SG), where points of different colors correspond to different refinement levels, reflecting the distribution of grid points. Hierarchical basis functions are used to approximate the policy function. They discretize the unit domain by constructing basis functions and corresponding subspaces at different refinement levels. The sparse grid method mitigates the curse of dimensionality for high - dimensional functions by retaining the most significant hierarchical subspaces."
    },
    "97": {
        "figure1": "2503.11464v1_IMG_combin",
        "label1": "fig:expansion",
        "caption1": "The number of DD component functions, both incremental and cumulative, as a function of expansion order for an \\(8\\)-dimensional (left panel) and a \\(16\\)-dimensional function (right panel). The number of component functions increases significantly with both expansion order and dimensionality. ",
        "text": "The number of component functions increases significantly with both expansion order and dimensionality. } Performing the full expansion in expression~\\eqref{eq:13} up to order \\( k = d \\) is computationally infeasible for problems of nontrivial dimensionality, as it simply reconstructs the original \\( d \\)-dimensional function, thereby negating the intended efficiency gains from dimensional reduction. Moreover, the number of component functions increases combinatorially with the expansion order \\( k \\): which poses significant computational challenges even for moderately sized problems (e.g., 8- or 16-dimensional), as illustrated in Figure~\\ref{fig:expansion}. Therefore, truncating the expansion to a maximum order \\(\\ekk \\ll d\\) is imperative; moreover, the optimal selection of \\(\\ekk\\) is inherently problem-dependent. for tree={     grow=south } [$f_{1234}$,rectangle, draw, fill=gray!20,     [$f_{123}$,rectangle, draw, fill=gray!20,         [$f_{12}$,rectangle, draw, fill=gray!20,             [$f_{1}$,rectangle, draw,                         [$f_{\\emptyset}$,rectangle, draw]             ]             [$f_{2}$,rectangle, draw]         ]         [$f_{13}$,rectangle, draw,             [$f_{3}$,rectangle, draw]         ]         [$f_{23}$,rectangle, draw]     ]     [$f_{124}$,rectangle, draw, fill=gray!20,         [$f_{14}$,rectangle, draw,             [$f_{4}$,rectangle, draw]                 ]         [$f_{24}$,rectangle, draw]     ]     [$f_{134}$,rectangle, draw,         [$f_{34}$,rectangle, draw]     ]     [$f_{234}$,rectangle, draw] ]          Visualization of the component functions summation in Equation~\\eqref{eq:13} of a four-dimensional function with active dimension criteria, where the component index $\\bold{u}=\\{1,2\\}$ is deemed insignificant. For clarity, repeated cells are omitted.  All component functions that are supersets of $\\bold{u}=\\{1,2\\}$ are excluded from the summation (shown in gray). }",
        "summarize_figure": "2503.11464v1_IMG_combin",
        "summarization": "The picture contains two sub - graphs, showing the number of incremental (blue line) and cumulative (black line) dimension - decomposition (DD) component functions as a function of the expansion order (k) for an 8 - dimensional (left graph) and a 16 - dimensional (right graph) function. As the expansion order increases, the number of both types of component functions grows significantly, and the higher the dimension, the more pronounced the growth trend. This indicates that in high - dimensional problems, an increase in the expansion order leads to a substantial increase in computational cost. Therefore, it is necessary to truncate the expansion to a maximum order much smaller than the dimension, and the optimal order selection is problem - dependent."
    },
    "98": {
        "figure1": "2503.11464v1_combin",
        "label1": "fig:3",
        "caption1": "Combinatorial increase in the number of component functions for a $10$-dimensional function with respect to expansion order. <REDO PLOT>",
        "text": "Above, we discussed DD without considering the truncation of expression~\\eqref{eq:13}. However, a full expansion up to $r=d$ is computationally impractical because it merely reconstructs the original $d$-dimensional function, thereby negating any efficiency gains. Moreover, the number of component functions increases combinatorially with the expansion order $r$: This combinatorial growth poses a significant challenge even for a moderately sized $10$-dimensional problem, as illustrated in Figure~\\ref{fig:3}. Thus, it is necessary to truncate the expansion to a maximum order $r^*\\ll d$ by leveraging two features of DD: (i) the significance of each component function, and (ii) the incremental effect of moving to a higher expansion order. We address (i) by applying an \\textit{active dimension}\\footnote{We refer to \\textit{active dimensions} as the component function indices $\\bold{u}$ that contribute most significantly to the summation in Equation~\\eqref{eq:11}.} selection criterion \\citep{Ma:2010:AHS:1751671.1751796,yangetal_2012} to prune the combinatorial tree, and (ii) by introducing a convergence criterion that quantifies the benefit of progressing to the next order in the DD expansion.",
        "summarize_figure": "2503.11464v1_combin",
        "summarization": "The graph shows the number of cumulative (blue line) and incremental (green line) component functions for a 10 - dimensional function as the expansion order increases. As the expansion order increases from 1 to 10, the number of cumulative component functions grows rapidly at first, and then the growth slows down and stabilizes after the expansion order reaches a certain value. The number of incremental component functions first increases, peaks around an expansion order of 4, and then decreases. This reflects the combinatorial growth of the number of component functions with the expansion order, and also indicates that a full expansion to the highest order is computationally infeasible in high - dimensional problems, so the expansion order needs to be truncated."
    },
    "99": {
        "figure1": "2503.11561v2_novelty_exposure",
        "label1": "fig.user-level-exposure",
        "caption1": "\\textbf{Average novelty of articles exposed to across groups.} This figure illustrates the comparison of the average novelty level of articles that users are exposed to during the experimental period, across three groups: Group I, with algorithmic curation; Group II, with peer-shared content along with social cues; and Group III (baseline), presenting peer-shared content without social cues. Note that when comparing Group I with Groups II and III, our findings represent the \\textit{lower bound} of the effects.",
        "text": "Our results show that content shared by peers exhibits significantly higher novelty than algorithmically curated content in terms of content exposure\\footnote{All statistical tests in this paper meet the necessary assumptions, including those related to the Central Limit Theorem.}. As illustrated in Figure~\\ref{fig.user-level-exposure}, articles exposed through peer-sharing (Groups II and III) display greater average content novelty in both non-redundancy and diversity compared to algorithmic curation (Group I) \\((p < 0.01)\\). Specifically, the likelihood of encountering non-redundant content is 1.26\\% (95\\% CI: 1.14\\%-1.39\\%) higher in Group II (peer-shared content with social cues) and 1.31\\% (95\\% CI: 1.19\\%-1.44\\%) higher in Group III (peer-shared content without social cues) relative to Group I. Additionally, marginal diversity, measured by Shannon entropy, is on average 0.0021 (95\\% CI: 0.0015-0.0027) higher in Group II and 0.0016 (95\\% CI: 0.0009-0.0022) higher in Group III compared to Group I. As expected, there is no statistically significant difference between Group II and Group III \\((p > 0.1)\\) in non-redundancy or diversity of an article, as both groups receive peer-shared content and should therefore be indifferent in content exposure.",
        "summarize_figure": "2503.11561v2_novelty_exposure",
        "summarization": "The picture contains two sub - graphs showing the comparison of the average novelty level of articles exposed to users in three groups during the experimental period. The left graph (Non - redundancy) and the right graph (Marginal Diversity) represent the differences in average novelty in terms of non - redundancy and marginal diversity, respectively. Data shows that compared with algorithmically curated content (Group I), peer - shared content (Group II and Group III) has higher average novelty in non - redundancy and diversity (p < 0.01). Specifically, the likelihood of encountering non - redundant content in Group II (peer - shared content with social cues) and Group III (peer - shared content without social cues) is 1.26% (95% CI: 1.14% - 1.39%) and 1.31% (95% CI: 1.19% - 1.44%) higher than that in Group I, respectively. In terms of marginal diversity, Group II and Group III are on average 0.0021 (95% CI: 0.0015 - 0.0027) and 0.0016 (95% CI: 0.0009 - 0.0022) higher than Group I. There is no significant difference between Group II and Group III in the non - redundancy and diversity of articles (p > 0.1)."
    }
}