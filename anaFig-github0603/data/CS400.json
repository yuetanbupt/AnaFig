{
    "0": {
        "figure1": "2410.23373v1_pq",
        "label1": "pq",
        "caption1": "Affinity $\\times$ number of steps in the quantum device.",
        "text": "It is also worth mentioning that at each new trial, the algorithm shuffles the order of the training data, training in a new random sequence. Finally, with the same training data set, the network was trained 59 times, restarting with an initial weight vector defined also randomly. The training also has a maximum limit of 50 steps for each execution. The average result for each step result is illustrated in Figure \\ref{pq}. And we can see behavior that indicates that the perceptron is able to learn as expected.\n\\includegraphics[width=0.5\\textwidth]{pq.png}             \\centering             \\caption{Affinity $\\times$ number of steps in the quantum device.}             \\label{pq}  \\end{figure}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the relationship between affinity and the number of steps for a perceptron in a quantum device. The horizontal axis represents the number of steps, ranging from 0 to 50, and the vertical axis represents affinity, ranging from 0 to 1.0. The curve generally shows an upward trend, indicating that affinity increases with the number of steps. In the first 10 steps, the affinity increases rapidly from near 0 to about 0.4. Subsequently, the growth rate slows down, reaching approximately 0.6 at 20 steps. After 30 steps, the growth is even slower, eventually approaching 1.0 at 50 steps. This suggests that the perceptron is able to learn as expected."
    },
    "1": {
        "figure1": "2410.23373v1_custo",
        "label1": "img2",
        "caption1": "Behavior of the cost function for the sigmoid in the simulator. ",
        "text": "A learning rate of $\\nu=0.1$ was adopted and two stopping conditions were defined: The training should end if the cost function is reduced to less than $10^{-3}$ or if it start to increase. After approximately 4000 steps, the cost function has reduced from an initial value of approximately $5.75\\times10^{-2}$ to approximately $1.74 \\%$ of its initial value, i.e. $1.00\\times10^{-3}$. This behavior can be seen in Figure \\ref{img2}.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the cost function versus step. The cost function starts at approximately 0.0575 and decreases as the steps increase, reaching about 0.001 after approximately 4000 steps. The decreasing trend of the cost function is more apparent in the early stages and then gradually slows down."
    },
    "2": {
        "figure1": "2410.23373v1_sq1",
        "label1": "sq",
        "caption1": "Behavior of the cost function for the sigmoid on the device `\\textit{ibmq\\_santiago}'. ",
        "figure2": "2410.23373v1_sq2",
        "label2": "sq2",
        "caption2": "Behavior of the cost function for the sigmoid on device `\\textit{ibmq\\_5\\_yorktown}'. ",
        "text": "That said, it was necessary to remove the stopping condition for training based simply on the increase in the cost function since it presented an oscillation from the beginning. The result can be seen in Figure \\ref{sq}. At the same time, the same code was executed in another device, \\textit{ibmq\\_5\\_yorktown} ($QV=8$), due to smaller queues. The obtained results can be checked out in Figure \\ref{sq2}.",
        "summarize_figure": "figure1",
        "summarization": "The figure shows the behavior of the cost function for the Sigmoid function on the `textit{ibmq_santiago}` device. The horizontal axis represents the Step, and the vertical axis represents the Cost. The cost function value is approximately 0.046 at step 0, peaks at approximately 0.052 at step 6, then starts to decrease, reaches approximately 0.049 at step 9, and finally is approximately 0.051 at step 10. The overall trend shows fluctuation."
    },
    "3": {
        "figure1": "2410.23373v1_sq2",
        "label1": "sq2",
        "caption1": "Behavior of the cost function for the sigmoid on device `\\textit{ibmq\\_5\\_yorktown}'. ",
        "figure2": "2410.23373v1_sq1",
        "label2": "sq",
        "caption2": "Behavior of the cost function for the sigmoid on the device `\\textit{ibmq\\_santiago}'. ",
        "text": "That said, it was necessary to remove the stopping condition for training based simply on the increase in the cost function since it presented an oscillation from the beginning. The result can be seen in Figure \\ref{sq}. At the same time, the same code was executed in another device, \\textit{ibmq\\_5\\_yorktown} ($QV=8$), due to smaller queues. The obtained results can be checked out in Figure \\ref{sq2}.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the behavior of the cost function for the Sigmoid function with respect to steps on the `ibmq_santiago` device. From the figure, the cost function exhibits a clear oscillatory behavior. In the initial steps, the cost rapidly increases from approximately 0.046 to about 0.049, then decreases to around 0.0455. After step 5, the cost increases significantly, reaching a peak of about 0.0515 between steps 6 and 7, then gradually decreases to about 0.049, and finally rises back to approximately 0.051."
    },
    "4": {
        "figure1": "2410.23393v1_performance_4agents",
        "label1": "fig:performance_4agents",
        "caption1": "Results show performance and resource penalties for various methods with homogeneous agents (vision ranges 0.6-1.2) and heterogeneous agents in 4-agent systems. A star marker indicates the random policy baseline's overall performance.",
        "text": "We first applied all methods on the small environment with 4 agents and got the results in Figure \\ref{fig:performance_4agents}. From the results, our VAE-RL approach consistently outperforms other baseline methods across all tasks including heterogeneous cases, ranging from difficult to easy. This not only underscores the superior performance of our method but also underscores its robustness under different scenarios. Because it is hard to compare the homogeneous scenarios and heterogeneous scenarios directly, for the purpose of analyzing trends, our subsequent findings and discussions in this section will primarily focus on homogeneous cases.",
        "summarize_figure": "figure1",
        "summarization": "The picture compares the performance of different policies in a 4-agents system. The figure shows the performance and resource costs of various methods under homogeneous agents with different vision ranges (0.6 to 1.2) and heterogeneous agents. A red star marker indicates the baseline performance of the random policy. The results show that the VAE-RL method outperforms other baseline methods in all tasks, including heterogeneous cases, ranging from difficult to easy. This indicates that the VAE-RL method is not only superior in performance but also more robust under different scenarios."
    },
    "5": {
        "figure1": "2410.23393v1_performance_10agents",
        "label1": "fig:performance_10agents",
        "caption1": "Results show performance and resource penalties for various methods with homogeneous agents (vision ranges 0.6-1.2) and heterogeneous agents in 10-agent systems. A star marker indicates the random policy baseline's overall performance.",
        "text": "In our expanded experiment involving a larger environment with 10 agents, we tested the VAE-RL method and the BDQN baseline. As previously noted, Flat-RL is not capable of handling this more complex task, leading us to focus solely on the performances of VAE-RL and BDQN, as illustrated in Figure \\ref{fig:performance_10agents}. The results clearly show that VAE-RL continues to outperform the BDQN baseline across all environment settings, including those with heterogeneous conditions. This consistent pattern reinforces the trends observed in smaller environments, indicating that VAE-RL's effectiveness scales well with increased complexity. On the other hand, the BDQN baseline struggles even in the simplest case with agents having 1.2 vision ranges, where its performance is comparable to a random strategy. This outcome suggests that while BDQN remains applicable in this context, it fails to develop a meaningful policy for communication network assignment. In contrast, VAE-RL not only demonstrates strong performance relative to the baseline but also retains its potential for effective implementation in even larger systems with more agents.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates a comparison of different policies in a system with 10 agents, where the x-axis represents varying vision ranges (0.6 to 1.2 and heterogeneous vision), and the y-axis represents the scores. Red stars indicates the resource cost of a random baseline. Resource cost and worker scores for VAE-RL are represented by red and blue bars, respectively. Resource cost and worker scores for BDQN are represented by purple and gray bars, respectively. VAE-RL outperforms the BDQN baseline across all environment settings, including those with heterogeneous conditions. The BDQN baseline struggles even in the simplest case with agents having a 1.2 vision range, where its performance is comparable to a random strategy. In contrast, VAE-RL not only demonstrates strong performance relative to the baseline but also retains its potential for effective implementation in even larger systems with more agents."
    },
    "6": {
        "figure1": "2410.23396v1_performance_comparison_10nodes",
        "label1": "fig:graphs_performance_10nodes",
        "caption1": "The performance comparison of HGRL, Flat-RL and random strategy in the system with 10 nodes. The figures show results from 3 scenarios with different probabilities of behavior imitation. Average social welfare (red bar) during the game and average final social welfare (blue bar) at the end of the game are used as metrics.",
        "figure2": "2410.23396v1_performance_comparison_20nodes",
        "label2": "fig:graphs_performance_20nodes",
        "caption2": "The performance comparison of HGRL, Flat-RL and random strategy in the system with 20 nodes. The figures show results from 3 scenarios with different probabilities of behavior imitation. Average social welfare (red bar) during the game and average final social welfare (blue bar) at the end of the game are used as metrics.",
        "text": "\\caption{The performance comparison of HGRL, Flat-RL and random strategy in the system with 10 nodes. The figures show results from 3 scenarios with different probabilities of behavior imitation. Average social welfare (red bar) during the game and average final social welfare (blue bar) at the end of the game are used as metrics.}    \\label{fig:graphs_performance_10nodes}\nIn our study, we compared the performance of the HGRL model with that of Flat-RL and random strategy within the designed environment in both 10-node networks and 20-node networks. The HGRL model and Flat-RL model use similar sizes of parameters in neural networks. We trained the system manager for 10,000 rounds, with each round including 50 time steps for the 10-node networks and 100 time steps for the 20-node networks. The results presented are averaged from 1,000 test rounds. Our experiment was conducted under three different scenarios, each characterized by varying imitation probabilities $p$ of 0, 0.5, and 1. These probabilities represent different levels of imitation tendency among the agents. To assess the effectiveness of the models, we employed two metrics: average social welfare of the game and average final social welfare at the end of the game. The performance is also averaged by the number of agents in the system. Average social welfare offers a comprehensive evaluation of the models' effectiveness, encompassing not only the general performance but also the pace of improvement in social welfare, as a swifter enhancement results in a higher average social welfare. Meanwhile, the social welfare at the end of the game evaluates the models' ability to guide the system to an optimal or desirable state by the end of the game. This dual-metric approach allows for a comprehensive analysis of HGRL performance in managing the system with network structure. The results of the 10-node networks are in Figure \\ref{fig:graphs_performance_10nodes}, and the results of 20-node networks are in Figure \\ref{fig:graphs_performance_20nodes}.",
        "summarize_figure": "figure1",
        "summarization": "The figure compares the performance of HGRL, Flat-RL, and a random strategy in a 10-node system with an imitation probability of 0. The performance is evaluated using average social welfare (red bar) and average final social welfare (blue bar). The results indicate that HGRL outperforms both Flat-RL and the random strategy in terms of both metrics. Flat-RL performs better than the random strategy. The average social welfare and the average final social welfare of all three strategies are negative."
    },
    "7": {
        "figure1": "2410.23396v1_performance_comparison_20nodes",
        "label1": "fig:graphs_performance_20nodes",
        "caption1": "The performance comparison of HGRL, Flat-RL and random strategy in the system with 20 nodes. The figures show results from 3 scenarios with different probabilities of behavior imitation. Average social welfare (red bar) during the game and average final social welfare (blue bar) at the end of the game are used as metrics.",
        "figure2": "2410.23396v1_performance_comparison_10nodes",
        "label2": "fig:graphs_performance_10nodes",
        "caption2": "The performance comparison of HGRL, Flat-RL and random strategy in the system with 10 nodes. The figures show results from 3 scenarios with different probabilities of behavior imitation. Average social welfare (red bar) during the game and average final social welfare (blue bar) at the end of the game are used as metrics.",
        "text": "In our study, we compared the performance of the HGRL model with that of Flat-RL and random strategy within the designed environment in both 10-node networks and 20-node networks. The HGRL model and Flat-RL model use similar sizes of parameters in neural networks. We trained the system manager for 10,000 rounds, with each round including 50 time steps for the 10-node networks and 100 time steps for the 20-node networks. The results presented are averaged from 1,000 test rounds. Our experiment was conducted under three different scenarios, each characterized by varying imitation probabilities $p$ of 0, 0.5, and 1. These probabilities represent different levels of imitation tendency among the agents. To assess the effectiveness of the models, we employed two metrics: average social welfare of the game and average final social welfare at the end of the game. The performance is also averaged by the number of agents in the system. Average social welfare offers a comprehensive evaluation of the models' effectiveness, encompassing not only the general performance but also the pace of improvement in social welfare, as a swifter enhancement results in a higher average social welfare. Meanwhile, the social welfare at the end of the game evaluates the models' ability to guide the system to an optimal or desirable state by the end of the game. This dual-metric approach allows for a comprehensive analysis of HGRL performance in managing the system with network structure. The results of the 10-node networks are in Figure \\ref{fig:graphs_performance_10nodes}, and the results of 20-node networks are in Figure \\ref{fig:graphs_performance_20nodes}.\n\\caption{The performance comparison of HGRL, Flat-RL and random strategy in the system with 20 nodes. The figures show results from 3 scenarios with different probabilities of behavior imitation. Average social welfare (red bar) during the game and average final social welfare (blue bar) at the end of the game are used as metrics.}    \\label{fig:graphs_performance_20nodes}",
        "summarize_figure": "figure1",
        "summarization": "The picture shows the performance comparison of HGRL, Flat-RL, and random strategies in a system with 20 nodes under different imitation probabilities. The picture includes three subgraphs, corresponding to imitation probabilities of 0, 0.5, and 1, respectively. In each subgraph, the red bar represents the average social welfare, and the blue bar represents the final social welfare. It can be seen from the picture that as the imitation probability increases, the average social welfare and final social welfare of the three strategies decrease. Under different imitation probabilities, the average social welfare and final social welfare of HGRL are better than those of Flat-RL and random strategies."
    },
    "8": {
        "figure1": "2410.23396v1_network_metrics",
        "label1": "fig:network_metrics",
        "caption1": "The evolution of different network metrics over time with various imitation probabilities. The figures show 3 network metrics: average node degree, network diameter, and network modularity. The x-axis is the time step.",
        "text": "\\caption{The evolution of different network metrics over time with various imitation probabilities. The figures show 3 network metrics: average node degree, network diameter, and network modularity. The x-axis is the time step.}    \\label{fig:network_metrics}\nWe only analyzed the behavior of the HGRL manager and the evolution of systems in 20-node networks. Three metrics were used to study the evolution of the networks over time under varying imitation probabilities: average node degrees, network diameter, and network modularity. The results are presented in Figure \\ref{fig:network_metrics}. Generally, the curves for different imitation probabilities begin to diverge after 30 time steps. This pattern indicates that the influence of varying levels of social learning becomes significant after 30 time steps. Consequently, the systems initially exhibit similar trends but gradually diverge under the flexible intervention of HGRL's policies.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the evolution of the average node degree over time steps under different imitation probabilities. The plot contains three curves representing imitation probabilities of 0.0, 0.5, and 1.0. As the time step increases, the average node degree decreases for all three imitation probabilities. The imitation probability of 1.0 shows the fastest decrease and the lowest final value, while the imitation probability of 0.0 shows the slowest decrease and the highest final value. The trend and final value for the imitation probability of 0.5 fall between the other two. The three curves are close to each other at smaller time steps, indicating that the impact of imitation probability on the average node degree is small initially. However, as time passes, the curves gradually separate, suggesting that the impact of imitation probability on the average node degree increases."
    },
    "9": {
        "figure1": "2410.23413v1_figure_label_eff",
        "label1": "fig:data_efficiency",
        "caption1": "Comparison of various methods on the chamber segmentation task for label efficiency. EchoFM, compared with EchoNet without pre-training and SAM pre-trained on natural images, demonstrates superior performance, especially with very limited labeled data. ",
        "text": "One of the key advantages of a foundation model in the medical domain is its label efficiency. We compare the label efficiency of the model on chamber segmentation tasks using varying amounts of labeled data for training. We include three models in our comparison: EchoNet without pre-training, SAM pre-trained on natural images, and EchoFM pre-trained on echocardiography data. In Fig. \\ref{fig:data_efficiency}, the results show that EchoFM consistently outperforms other models at all levels of data use. Remarkably, even with extremely limited labeled data, EchoFM achieves an average Dice score of 83.2\\%, which is nearly equivalent to EchoNet's 83.9\\% score obtained using fully labeled data. Furthermore, our observations indicate that supervised methods without pre-training on large-scale datasets are heavily dependent on the availability of annotated data. These results highlight that EchoFM not only significantly outperforms other models, especially those without pre-training, but also demonstrates remarkable label efficiency.\n\\includegraphics[width=0.35\\textwidth]{fig/figure_label_eff.png} \t\\caption{Comparison of various methods on the chamber segmentation task for label efficiency. EchoFM, compared with EchoNet without pre-training and SAM pre-trained on natural images, demonstrates superior performance, especially with very limited labeled data. } \t\\label{fig:data_efficiency}",
        "summarize_figure": "figure1",
        "summarization": "The image compares the label efficiency of different methods in the chamber segmentation task. The horizontal axis represents the label ratio (percentage), and the vertical axis represents the Dice score (percentage). There are three curves in the figure, representing the EchoNet, SAM, and EchoFM models, respectively. EchoFM consistently outperforms the other models at all data levels. Even with very limited labeled data, EchoFM achieves a Dice score similar to that of EchoNet obtained with fully labeled data."
    },
    "10": {
        "figure1": "2410.23426v1_data_pie",
        "label1": "fig:data_stat",
        "caption1": "The distribution of evaluation instances across different subjects (left) and the distribution of the number of words in different kinds of questions (right). SR: Self-Report, OE: Open-Ended.} \\vspace{-15pt",
        "text": "\\centering     \\includegraphics[width=1\\linewidth]{data_pie.pdf}     \\caption{The distribution of evaluation instances across different subjects (left) and the distribution of the number of words in different kinds of questions (right). SR: Self-Report, OE: Open-Ended.}     \\vspace{-15pt}     \\label{fig:data_stat}",
        "summarize_figure": "figure1",
        "summarization": "The image shows the distribution of evaluation instances across different subjects. Economics accounts for the largest proportion at 12 percent, followed by Ethics and Law, both at 11 percent. Linguistics, Political science, Communication, Sociology, and Education each account for 10 percent. Psychology accounts for 9 percent, and Organizational science accounts for 7 percent.The right side of the image shows the distribution of word counts in different types of questions, where SR represents Self-Report and OE represents Open-Ended questions."
    },
    "11": {
        "figure1": "2410.23426v1_satisfication_rate_self_report",
        "label1": "fig:satis_score_self_report",
        "caption1": "The satisfaction rate of different models in ten subjects (Self-report questions).",
        "text": "\\centering     \\includegraphics[width=1\\linewidth]{satisfication_rate_self_report.pdf}     \\caption{The satisfaction rate of different models in ten subjects (Self-report questions).}     \\label{fig:satis_score_self_report}",
        "summarize_figure": "figure1",
        "summarization": "This is a summary of a self-report questionnaire survey on the satisfaction rate of different models in ten subjects. The image shows the satisfaction rate of different models in ten subjects (Communication Studies, Economics, Educational Studies, Ethics and Moral Psychology, Law and Jurisprudence, Linguistics, Organizational Behavior, Political Science, Psychology, and Sociology). Each subject's chart displays the satisfaction scores for different models, ranging from 40 to 100. There are differences in satisfaction rates among different models, and the differences are small in some subjects and large in others."
    },
    "12": {
        "figure1": "2410.23426v1_satisfication_rate_open_ended",
        "label1": "fig:satis_score_open_ended",
        "caption1": "The satisfaction rate of different models in ten subjects (Open-ended questions).",
        "text": "\\centering     \\includegraphics[width=1\\linewidth]{satisfication_rate_open_ended.pdf}     \\caption{The satisfaction rate of different models in ten subjects (Open-ended questions).}     \\label{fig:satis_score_open_ended}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the satisfaction rates of various models across ten different subjects: Communication Studies, Economics, Educational Studies, Ethics and Moral Psychology, Law and Jurisprudence, Linguistics, Organizational Behavior, Political Science, Psychology, and Sociology. Each subject features a set of bar graphs representing the satisfaction rates of different models, such as GPT-4o, GPT-3.5-turbo, Gemini-1.5-pro, and Claude-3.5-sonnet. The satisfaction rates are expressed as percentages, ranging from 0 to 100. The bars for different models are color-coded for easy differentiation. Overall, the satisfaction rates of the models vary across the different subjects, with some models performing better than others in specific fields."
    },
    "13": {
        "figure1": "2410.23426v1_heatmap",
        "label1": "fig:heat",
        "caption1": "Inconsistency rate (\\%) of LLMs between self-report questions and open-ended questions.} \\vspace{-10pt",
        "text": "\\centering     \\includegraphics[width=1\\linewidth]{heatmap.pdf}     \\caption{Inconsistency rate (\\%) of LLMs between self-report questions and open-ended questions.}     \\vspace{-10pt}     \\label{fig:heat}",
        "summarize_figure": "figure1",
        "summarization": "The figure shows a heatmap of the inconsistency rate (in percentage) of LLMs between self-report questions and open-ended questions. The horizontal axis represents different evaluation domains, including Com.(Commonsense), Eco.(Economics), Edu.(Education), Eth.(Ethics), Law(Law), Lin.(Linguistics), Org.(Organization), Pol.(Politics), Psy.(Psychology), Soc.(Sociology) and Avg.(Average). The vertical axis lists different LLMs, including Llama-3-70B, Llama-3.1-70B, Gemini-1.5-flash, GPT-4o, Claude-3.5-sonnet, Gemini-1.5-pro, Qwen-2.5-72B, Mixtral-8x7B, GLM-4, GPT-4o-mini, Claude-3-opus, Mistral-7B, GPT-3.5-turbo, and Llama-3.1-8B. Mixtral-8x7B shows a high inconsistency rate in several areas, especially in the legal field, reaching 43%. Llama-3.1-8B has the lowest inconsistency rate in the field of psychology, at 4.5%. GPT-3.5-turbo has a high inconsistency rate in ethics and law, at 37% and 40%, respectively."
    },
    "14": {
        "figure1": "2410.23426v1_ft_model",
        "label1": "fig:AdaORPO",
        "caption1": "Performance comparison of different models with AdaORPO training. % \\wjd{ID or OOD evaluation? If it's ID, then the result improvement is expected; consider OOD settings.} }  \\vspace{-10pt",
        "text": "\\centering     \\includegraphics[width=1\\linewidth]{ft_model.pdf}     \\caption{Performance comparison of different models with AdaORPO training.      % \\wjd{ID or OOD evaluation? If it's ID, then the result improvement is expected; consider OOD settings.}     }     \\label{fig:AdaORPO}     \\vspace{-10pt}",
        "summarize_figure": "figure1",
        "summarization": "The picture illustrates the performance comparison of different models after AdaORPO training, assessed in three aspects: self-report question satisfaction rate, open-ended question satisfaction rate, and open-ended question rating score. The horizontal axis represents various models, including GLM-4, Llama-3-70B, Llama-3.1-70B, Qwen-2.5-72B, Mixtral-8x7B, Mistral-7B, and Llama-3.1-8B. The legend shows results for two training methods: Vanilla and AdaORPO.For self-report question satisfaction, Llama-3-70B and Llama-3.1-70B perform best, with satisfaction rates near 90 percent. AdaORPO training generally improves satisfaction for all models.Regarding open-ended question satisfaction, Llama-3-70B and Llama-3.1-70B remain outstanding. Similar to self-report questions, AdaORPO training usually enhances model satisfaction.In open-ended question rating, Llama-3-70B and Llama-3.1-70B achieve the highest scores, close to 4.4. AdaORPO training has a relatively minor impact on open-ended question ratings, with slight improvements for some models."
    },
    "15": {
        "figure1": "2410.23438v1_alpha_and_delta",
        "label1": "fig: alpha and delta",
        "caption1": "\\textbf{Signals $\\alpha_A,\\alpha_V$ and the distance to population process $\\bDelta_{\\A},\\bDelta_{\\V}$.} For the SGD, the distance to the population process of the attention matrix $\\A$ keeps growing and dominates the signal term. That explains the failure to learn the correct attention pattern, which leads to saturation of the signal. In comparison, our proximal methods dramatically help reduce the gradient noise and keep close to the population process. Though $\\|\\bDelta_A\\|$ eventually grows up due to the bias of the gradient estimate (the original signal growth is also slowed down), after normalization it can still approximately learn the correct pattern. Both $\\|\\bDelta_{\\V}\\|$ stay small empirically.",
        "text": "\\centering     \\includegraphics[width=0.9\\textwidth]{figs/alpha_and_delta.pdf}     \\vspace{-0.5cm}     \\caption{\\textbf{Signals $\\alpha_A,\\alpha_V$ and the distance to population process $\\bDelta_{\\A},\\bDelta_{\\V}$.} For the SGD, the distance to the population process of the attention matrix $\\A$ keeps growing and dominates the signal term. That explains the failure to learn the correct attention pattern, which leads to saturation of the signal. In comparison, our proximal methods dramatically help reduce the gradient noise and keep close to the population process. Though $\\|\\bDelta_A\\|$ eventually grows up due to the bias of the gradient estimate (the original signal growth is also slowed down), after normalization it can still approximately learn the correct pattern. Both $\\|\\bDelta_{\\V}\\|$ stay small empirically.}     \\label{fig: alpha and delta}",
        "summarize_figure": "figure1",
        "summarization": "The figure's left plot illustrates the evolution of signal projections (alpha A and alpha V) over iterations. For the Stochastic Gradient Descent (SGD) method, the signal projection for the attention matrix (alpha A) saturates or grows slowly after around 400 iterations, while the signal projection for V (alpha V) continues to increase. In contrast, our proposed method shows a consistent and stronger increasing trend in signal projections for both the attention matrix (alpha A) and V (alpha V). The right plot and text explain this, indicating that the significant growth in the distance to the population process (Delta A) for the attention matrix in SGD inhibits its signal gain, leading to saturation. Our method effectively keeps this distance smaller (Delta A grows much slower), facilitating continued signal growth, whereas Delta V remains small for both methods."
    },
    "16": {
        "figure1": "2410.23438v1_similarity_appendix",
        "label1": "fig: similarity appendix",
        "caption1": "\\textbf{Similarity with the ground-truth.} The figure shows after Stage 1, normalization helps further improve the solution of the proximal method. Meanwhile, with or without normalization, our proximal method always outperforms the vanilla SGD, which fails to recover the ground-truth.}  \\vspace{-0.5cm",
        "text": "\\centering     \\vspace{-0.7cm}     \\includegraphics[width=0.9\\textwidth]{figs/similarity_appendix.pdf}     \\vspace{-0.5cm}     \\caption{\\textbf{Similarity with the ground-truth.} The figure shows after Stage 1, normalization helps further improve the solution of the proximal method. Meanwhile, with or without normalization, our proximal method always outperforms the vanilla SGD, which fails to recover the ground-truth.}     \\label{fig: similarity appendix}             \\vspace{-0.5cm} We tried different orders of state number $N$ and show that $\\ell_1$ regularization is necessary and outperforms SGD when batch size is small (gradient noise is large).  Due to computation limitation, we experiment with real batched gradient on $N\\leq 20, T\\leq 5000$, and do SGD simulation by combining our population gradient + Gaussian noise (to mimic the batch gradient noise) for $N\\leq 500, T\\leq 100000$.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the similarity performance of the proximal method after Stage 1. Normalization helps to further improve the solution of the proximal method. Specifically, for the proximal method, the similarity of solution A,Q increases with the number of iterations, whether normalization is applied or not. After 400 iterations, the similarity of unnormalized solution A,Q rapidly increases to nearly 1.0, while the normalized solution A,Q directly stabilizes at 1.0. Meanwhile, with or without normalization, the proximal method consistently outperforms vanilla SGD, which fails to recover the ground-truth."
    },
    "17": {
        "figure1": "2410.23438v1_whole",
        "label1": "fig: approximation error and similarity exp",
        "caption1": " \\textbf{Convergence analysis:} We plot the distance to the ground truth $\\|\\V-\\bP\\|_\\mu, \\|\\A-\\bQ\\|_\\mu$ in different settings. After stage 1 ends at $\\tau=400$ (when $\\alpha_A,\\alpha_V\\approx 0.1$), we use vanilla SGD and our proximal gradient method to train the transformer. Compared with SGD, the $\\ell_1$ regularized proximal gradient descent quickly converges, and the final solution (the star) recovers the ground truth. SGD either suffers from the large gradient variance (when $\\eta_2$ is large) or a slow convergence rate (small $\\eta_2'$).} % \\vspace{-0.4cm",
        "text": "Our experiments (cf.~Fig.~\\ref{fig: approximation error and similarity exp}) show that after switching to proximal gradient descent after Stage 1 (the signal-boosting stage), both $\\norm{\\bP-\\V}_\\mu$ and $\\norm{\\A-\\bQ}_\\mu$ decrease faster than SGD. The final distance to the ground-truth after normalization gets close to 0, and the similarity between the ground truth and parameters quickly converges close to 1. In comparison, SGD struggles to converge with the same small batch size and large learning rate, while the convergence rate is too slow when a smaller learning rate is used. This phenomenon verifies our theory that the variance of the original stochastic gradient will be too large for SGD to converge when $T\\gg Q, N$, while proximal gradient descent with an $\\ell_1$ regularizer can resolve this issue.\n\\centering   \\includegraphics[width=0.9\\textwidth]{figs/whole.pdf}       % \\vspace{-0.3cm}   \\caption{    \\textbf{Convergence analysis:} We plot the distance to the ground truth $\\|\\V-\\bP\\|_\\mu, \\|\\A-\\bQ\\|_\\mu$ in different settings. After stage 1 ends at $\\tau=400$ (when $\\alpha_A,\\alpha_V\\approx 0.1$), we use vanilla SGD and our proximal gradient method to train the transformer. Compared with SGD, the $\\ell_1$ regularized proximal gradient descent quickly converges, and the final solution (the star) recovers the ground truth. SGD either suffers from the large gradient variance (when $\\eta_2$ is large) or a slow convergence rate (small $\\eta_2'$).}    % \\vspace{-0.4cm}   \\label{fig: approximation error and similarity exp}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the distance to the ground truth versus iteration in different settings. The vertical dashed line indicates the end of Stage 1 at iteration 400. The blue line represents the mu distance of P - V when training the transformer using the proximal gradient method. The orange line represents the mu distance of P - V when training the transformer using standard SGD. The green line represents the mu distance of P - V when training the transformer using SGD with a small learning rate. The star marker represents the final error obtained using the proximal gradient method. It can be observed that after Stage 1, the proximal gradient descent converges faster than SGD, and the final solution (star marker) is closer to the ground truth. SGD either suffers from large gradient variance (when eta2 is large) or a slow convergence rate (when eta2' is small)."
    },
    "18": {
        "figure1": "2410.23450v1_Rank_Score",
        "label1": "fig:Rank_Score",
        "caption1": "Rank scores for all baseline algorithms and our methods across the Random, Medium, Medium-R, and Medium-E datasets under BodyMass and JointNoise shift settings in the Walker2D, Hopper, and HalfCheetah environments. Ranks were assigned within each dataset, with the highest-performing algorithm receiving rank 1, followed by rank 2, and so on. In cases of tied scores, algorithms were assigned the same rank, and subsequent ranks were adjusted accordingly. Lower rank scores indicate better overall performance.}  %",
        "text": "\\centering     \\includegraphics[width=\\linewidth]{figure/Rank_Score.png}     \\vspace{-0.4in}     \\caption{Rank scores for all baseline algorithms and our methods across the Random, Medium, Medium-R, and Medium-E datasets under BodyMass and JointNoise shift settings in the Walker2D, Hopper, and HalfCheetah environments. Ranks were assigned within each dataset, with the highest-performing algorithm receiving rank 1, followed by rank 2, and so on. In cases of tied scores, algorithms were assigned the same rank, and subsequent ranks were adjusted accordingly. Lower rank scores indicate better overall performance.}        \\label{fig:Rank_Score}     %\nAdditionally, to facilitate an intuitive comparison between our methods and other baselines, we calculate the rank score for each algorithm. Ranks are assigned within each dataset, with the top-performing algorithm receiving rank 1, the second-best rank 2, and so on. In cases of tied scores, algorithms are assigned the same rank, with subsequent ranks adjusted accordingly. The final rank score for each algorithm is calculated by summing its ranks across all datasets, with lower average rank scores indicating superior performance. \\Cref{fig:Rank_Score} presents the rank scores for all algorithms, clearly demonstrating that the RADT methods we propose outperform others in most cases.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the rank scores for various algorithms. The rank score of BEAR is 185, the rank score of D-BEAR is 188, the rank score of AWR is 120, the rank score of D-AWR is 105, the rank score of BCQ is 78, the rank score of D-BCQ is 63, the rank score of CQL is 80, the rank score of D-CQL is 78, the rank score of MOPO is 187, the rank score of D-MOPO is 173, the rank score of 1T10S-DT is 79, the rank score of RADT-MV is 55, and the rank score of RADT-DARA is 65. Lower rank scores indicate better overall performance."
    },
    "19": {
        "figure1": "2410.23450v1_Abla1_BM",
        "label1": "fig:Abla1_BM",
        "caption1": "\\footnotesize {Performance of RADT methods under varying BodyMass shift settings in the Walker2D Medium and Hopper Medium environments. \\textbf{\"B-x\"} denotes that the body mass in the simulator is set to \\textbf{x}. The target body mass is \\textbf{2.94} in the Walker2D environment and \\textbf{5} in the Hopper environment.}}  \\vspace{-0.1in",
        "figure2": "2410.23450v1_Abla1_JN",
        "label2": "fig:Abla1_JN",
        "caption2": "\\footnotesize {Performance of RADT methods across varying JointNoise shift settings in the Walker2D and Hopper Medium environments. \\textbf{\"N-x\"} denotes the addition of random noise in the range \\textbf{(-x, +x)} to the action. } } \\vspace{-0.1in",
        "figure3": "2410.23450v1_Abla1_JN",
        "label3": "fig:Abla1_JN",
        "caption3": "\\footnotesize {Performance of RADT methods across varying JointNoise shift settings in the Walker2D and Hopper Medium environments. \\textbf{\"N-x\"} denotes the addition of random noise in the range \\textbf{(-x, +x)} to the action. } } \\vspace{-0.1in",
        "figure4": "2410.23450v1_Abla1_shift",
        "label4": "fig:Abla1_shift",
        "caption4": "\\footnotesize {Performance of RADT methods across Kinematic and Morphology Shift settings in the Medium Walker2D and Medium Hopper environments. For more details about these new shift settings, please refer to \\Cref{sec:experiment_setting}.} }  \\vspace{-0.15in",
        "text": "\\centering     \\vspace{-0.15in}         \\includegraphics[width=0.75\\textwidth]{figure/Abla1_BM.png} %         %         \\caption{\\footnotesize {Performance of RADT methods under varying BodyMass shift settings in the Walker2D Medium and Hopper Medium environments. \\textbf{\"B-x\"} denotes that the body mass in the simulator is set to \\textbf{x}. The target body mass is \\textbf{2.94} in the Walker2D environment and \\textbf{5} in the Hopper environment.}}         \\label{fig:Abla1_BM}         \\vspace{-0.1in}     \\centering     \\vspace{-0.1in}         \\includegraphics[width=0.75\\textwidth]{figure/Abla1_JN.png} %         %         \\caption{\\footnotesize {Performance of RADT methods across varying JointNoise shift settings in the Walker2D and Hopper Medium environments. \\textbf{\"N-x\"} denotes the addition of random noise in the range \\textbf{(-x, +x)} to the action. }         \\label{fig:Abla1_JN}}         \\vspace{-0.1in}\nTo investigate the impact of shifting source environments on RADT methods, we evaluated their performance under various BodyMass shift settings, JointNoise settings, Kinematic shifts, and Morphology shifts. The experimental results are presented in \\textcolor{red}{Figures} \\ref{fig:Abla1_BM}, \\ref{fig:Abla1_JN}, and \\ref{fig:Abla1_shift}. From the experiments, we observed that as the body mass shift increases, creating a larger disparity with the target environment, the performance in both the Walker2D and Hopper Medium environments deteriorates. As more noise is added to the actions, performance decreases in both the Walker2D medium and Hopper medium environments, indicating that random noise in the actions increases the likelihood of failure, leading to worse performance as the noise level rises. In the kinematic shift setting, we modify the Walker2D simulator by removing the right thigh and enlarge the head in the Hopper simulator. In the morphology shift setting, we simulate a broken right foot in the Walker2D environment and break a joint in the Hopper simulator. Across all shifting experiments, RADT-MV consistently achieves better performance than RADT-DARA, with the performance gap being particularly pronounced in larger shift settings, such as kinematic and morphology shifts.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the performance of RADT methods under varying BodyMass shift settings in the Walker2D Medium and Hopper Medium environments. In the Walker2D environment, where the target mass is 2.94, the performance of the three algorithms (1T10S, RADT-MV, RADT-DARA) is similar when the mass in the simulator is set to 0.735, 1.47, and 2.205, with a slight upward trend as the mass increases. In the Hopper environment, with a target mass of 5, when the mass in the simulator is set to 1.25, 2.5, and 3.75, the performance difference between the three algorithms widens. RADT-MV and RADT-DARA are significantly better than 1T10S, and the performance improves significantly as the mass increases."
    },
    "20": {
        "figure1": "2410.23450v1_Abla1_JN",
        "label1": "fig:Abla1_JN",
        "caption1": "\\footnotesize {Performance of RADT methods across varying JointNoise shift settings in the Walker2D and Hopper Medium environments. \\textbf{\"N-x\"} denotes the addition of random noise in the range \\textbf{(-x, +x)} to the action. } } \\vspace{-0.1in",
        "figure2": "2410.23450v1_Abla1_BM",
        "label2": "fig:Abla1_BM",
        "caption2": "\\footnotesize {Performance of RADT methods under varying BodyMass shift settings in the Walker2D Medium and Hopper Medium environments. \\textbf{\"B-x\"} denotes that the body mass in the simulator is set to \\textbf{x}. The target body mass is \\textbf{2.94} in the Walker2D environment and \\textbf{5} in the Hopper environment.}}  \\vspace{-0.1in",
        "figure3": "2410.23450v1_Abla1_BM",
        "label3": "fig:Abla1_BM",
        "caption3": "\\footnotesize {Performance of RADT methods under varying BodyMass shift settings in the Walker2D Medium and Hopper Medium environments. \\textbf{\"B-x\"} denotes that the body mass in the simulator is set to \\textbf{x}. The target body mass is \\textbf{2.94} in the Walker2D environment and \\textbf{5} in the Hopper environment.}}  \\vspace{-0.1in",
        "figure4": "2410.23450v1_Abla1_shift",
        "label4": "fig:Abla1_shift",
        "caption4": "\\footnotesize {Performance of RADT methods across Kinematic and Morphology Shift settings in the Medium Walker2D and Medium Hopper environments. For more details about these new shift settings, please refer to \\Cref{sec:experiment_setting}.} }  \\vspace{-0.15in",
        "text": "\\centering     \\vspace{-0.15in}         \\includegraphics[width=0.75\\textwidth]{figure/Abla1_BM.png} %         %         \\caption{\\footnotesize {Performance of RADT methods under varying BodyMass shift settings in the Walker2D Medium and Hopper Medium environments. \\textbf{\"B-x\"} denotes that the body mass in the simulator is set to \\textbf{x}. The target body mass is \\textbf{2.94} in the Walker2D environment and \\textbf{5} in the Hopper environment.}}         \\label{fig:Abla1_BM}         \\vspace{-0.1in}     \\centering     \\vspace{-0.1in}         \\includegraphics[width=0.75\\textwidth]{figure/Abla1_JN.png} %         %         \\caption{\\footnotesize {Performance of RADT methods across varying JointNoise shift settings in the Walker2D and Hopper Medium environments. \\textbf{\"N-x\"} denotes the addition of random noise in the range \\textbf{(-x, +x)} to the action. }         \\label{fig:Abla1_JN}}         \\vspace{-0.1in}\nTo investigate the impact of shifting source environments on RADT methods, we evaluated their performance under various BodyMass shift settings, JointNoise settings, Kinematic shifts, and Morphology shifts. The experimental results are presented in \\textcolor{red}{Figures} \\ref{fig:Abla1_BM}, \\ref{fig:Abla1_JN}, and \\ref{fig:Abla1_shift}. From the experiments, we observed that as the body mass shift increases, creating a larger disparity with the target environment, the performance in both the Walker2D and Hopper Medium environments deteriorates. As more noise is added to the actions, performance decreases in both the Walker2D medium and Hopper medium environments, indicating that random noise in the actions increases the likelihood of failure, leading to worse performance as the noise level rises. In the kinematic shift setting, we modify the Walker2D simulator by removing the right thigh and enlarge the head in the Hopper simulator. In the morphology shift setting, we simulate a broken right foot in the Walker2D environment and break a joint in the Hopper simulator. Across all shifting experiments, RADT-MV consistently achieves better performance than RADT-DARA, with the performance gap being particularly pronounced in larger shift settings, such as kinematic and morphology shifts.",
        "summarize_figure": "figure1",
        "summarization": "The image shows the performance of RADT methods under varying body mass settings in Walker2D and Hopper environments. In the Walker2D environment, performance generally increases for all methods as the simulated body mass increases from B-0.735 to B-2.205. RADT-MV typically performs slightly better than or comparable to 1T10S and RADT-DARA in this environment. In the Hopper environment, performance also improves significantly as the simulated body mass increases from B-1.25 to B-3.75. Notably, at the lowest mass setting B-1.25, RADT-MV achieves significantly better performance than 1T10S and RADT-DARA, while the performance of the three methods becomes similar at higher mass settings."
    },
    "21": {
        "figure1": "2410.23450v1_Abla1_shift",
        "label1": "fig:Abla1_shift",
        "caption1": "\\footnotesize {Performance of RADT methods across Kinematic and Morphology Shift settings in the Medium Walker2D and Medium Hopper environments. For more details about these new shift settings, please refer to \\Cref{sec:experiment_setting}.} }  \\vspace{-0.15in",
        "figure2": "2410.23450v1_Abla1_BM",
        "label2": "fig:Abla1_BM",
        "caption2": "\\footnotesize {Performance of RADT methods under varying BodyMass shift settings in the Walker2D Medium and Hopper Medium environments. \\textbf{\"B-x\"} denotes that the body mass in the simulator is set to \\textbf{x}. The target body mass is \\textbf{2.94} in the Walker2D environment and \\textbf{5} in the Hopper environment.}}  \\vspace{-0.1in",
        "figure3": "2410.23450v1_Abla1_JN",
        "label3": "fig:Abla1_JN",
        "caption3": "\\footnotesize {Performance of RADT methods across varying JointNoise shift settings in the Walker2D and Hopper Medium environments. \\textbf{\"N-x\"} denotes the addition of random noise in the range \\textbf{(-x, +x)} to the action. } } \\vspace{-0.1in",
        "text": "\\centering         \\includegraphics[width=0.75\\textwidth]{figure/Abla1_shift.png} %         %         \\caption{\\footnotesize {Performance of RADT methods across Kinematic and Morphology Shift settings in the Medium Walker2D and Medium Hopper environments. For more details about these new shift settings, please refer to \\Cref{sec:experiment_setting}.}         }         \\label{fig:Abla1_shift}         \\vspace{-0.15in}\nTo investigate the impact of shifting source environments on RADT methods, we evaluated their performance under various BodyMass shift settings, JointNoise settings, Kinematic shifts, and Morphology shifts. The experimental results are presented in \\textcolor{red}{Figures} \\ref{fig:Abla1_BM}, \\ref{fig:Abla1_JN}, and \\ref{fig:Abla1_shift}. From the experiments, we observed that as the body mass shift increases, creating a larger disparity with the target environment, the performance in both the Walker2D and Hopper Medium environments deteriorates. As more noise is added to the actions, performance decreases in both the Walker2D medium and Hopper medium environments, indicating that random noise in the actions increases the likelihood of failure, leading to worse performance as the noise level rises. In the kinematic shift setting, we modify the Walker2D simulator by removing the right thigh and enlarge the head in the Hopper simulator. In the morphology shift setting, we simulate a broken right foot in the Walker2D environment and break a joint in the Hopper simulator. Across all shifting experiments, RADT-MV consistently achieves better performance than RADT-DARA, with the performance gap being particularly pronounced in larger shift settings, such as kinematic and morphology shifts.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the performance of Robust Adversarial Disturbance Transfer (RADT) methods under Kinematic and Morphology shift settings in the Medium Walker2D and Medium Hopper environments. For the Walker2D environment, RADT-MV demonstrates better performance than 1T10S and RADT-DARA under the Kinematic shift, and relatively good performance under the Morphology shift. In the Hopper environment, RADT-MV's performance under the Kinematic shift is significantly higher than the other two methods, but under the Morphology shift, the performance of the three methods is similar. The overall trend indicates that the Kinematic shift has a greater impact on the RADT methods than the Morphology shift."
    },
    "22": {
        "figure1": "2410.23450v1_Abla2",
        "label1": "fig:Abla2",
        "caption1": "\\footnotesize {Performance of RADT-MV under varying target return settings in the Walker2D and Hopper Medium Replay environments with the BodyMass Shift setting. The x-axis represents the target return values, the y-axis represents the normalized return. The dash line represents the line $y = x$. }}  \\vspace{-0.15in",
        "text": "\\centering         \\includegraphics[width=0.75\\textwidth]{figure/Abla2.png} %         %         \\caption{\\footnotesize {Performance of RADT-MV under varying target return settings in the Walker2D and Hopper Medium Replay environments with the BodyMass Shift setting. The x-axis represents the target return values, the y-axis represents the normalized return. The dash line represents the line $y = x$. }}         \\label{fig:Abla2}         \\vspace{-0.15in}\nSetting a large target return is beneficial for maximizing the return in return-based offline reinforcement learning algorithms. We also explore the significance of target return settings in RADT methods within the Medium Replay Walker2D and Hopper environments under BodyMass shift setting. \\Cref{fig:Abla2} illustrates the performance of RADT methods under different target return settings. From the experimental results, we observe that larger target return settings lead to better performance for RADT methods, with RADT-MV outperforming RADT-DARA.",
        "summarize_figure": "figure1",
        "summarization": "This figure illustrates the performance of RADT-MV and RADT-DARA methods under varying target return settings in the Walker2D and Hopper Medium Replay environments with the BodyMass Shift setting. The figure consists of two subplots for the Walker2D and Hopper environments, respectively. The x-axis represents the target return values, and the y-axis represents the normalized return (Performance). The dashed line shows the Oracle performance, where performance equals the target return. As shown in the figure, in both Walker2D and Hopper environments, the performance of both RADT-MV and RADT-DARA generally increases with increasing target return values. In both environments, RADT-MV consistently achieves better performance than RADT-DARA across the tested range of target returns. In the Walker2D environment, for low target returns, the performance of RADT-MV and RADT-DARA is close to or slightly above the Oracle performance, but it falls below the Oracle and plateaus as the target return increases. In the Hopper environment, the performance of both methods is significantly below the Oracle performance for all tested target return values, and the rate at which performance increases with target return is much slower than the Oracle."
    },
    "23": {
        "figure1": "2410.23450v1_Abla3",
        "label1": "fig:Abla3",
        "caption1": "\\footnotesize Comparison of RADT-MV with RADT-MV without the clipping technique across Random, Medium, Medium Replay, and Medium Expert settings in the Walker2D, Hopper environments under BodyMass and JointNoise shifts. For detailed experimental settings, refer to the \\Cref{sec:detailed_setting}. }  %",
        "text": "\\centering     \\includegraphics[width=\\linewidth]{figure/Abla3.png}     %     \\caption{\\footnotesize Comparison of RADT-MV with RADT-MV without the clipping technique across Random, Medium, Medium Replay, and Medium Expert settings in the Walker2D, Hopper environments under BodyMass and JointNoise shifts. For detailed experimental settings, refer to the \\Cref{sec:detailed_setting}. }     \\label{fig:Abla3}     %\nIn RADT-MV, we apply a clipping technique to prevent the appearance of extreme values. We examine the significance of this technique by comparing the performance of RADT-MV with and without clipping in the Medium Walker2D, Hopper, and Halfcheetah settings under BodyMass and JointNoise shifts. \\Cref{fig:Abla3} presents the results. The results suggest that, in most cases, mitigating the occurrence of extreme values enhances the performance of RADT-MV. Meanwhile, there are instances where RADT-MV remains unaffected by extreme values. That is due to the fact that the original ratio value is already good enough and it does not need additional clipping.",
        "summarize_figure": "figure1",
        "summarization": "The figure compares the performance of RADT-MV (purple) with RADT-MV without clipping (green) in Walker2D and Hopper environments under BodyMass (BM) and JointNoise (JN) shifts across Random, Medium, Medium Replay, and Medium Expert settings. The figure shows that RADT-MV generally outperforms RADT-MV without clipping, indicating that mitigating extreme values often enhances performance. However, in some instances, the performance difference between the two methods is minimal, possibly because the original ratio value is already sufficient and does not require additional clipping."
    },
    "24": {
        "figure1": "2410.23450v1_Abla_Walker2D_Consistent",
        "label1": "fig:Abla4",
        "caption1": "\\footnotesize Averaged performance comparison of the \\textbf{1T10S-DT}, \\textbf{RADT-MV}, and \\textbf{RADT-MV (consistent)} algorithms in the Walker2D environment under BodyMass and JointNoise shift settings. Results are averaged across five random seeds.}  \\vspace{-0.2in",
        "text": "\\centering     \\includegraphics[width=\\linewidth]{figure/Abla_Walker2D_Consistent.png}     %     \\caption{\\footnotesize Averaged performance comparison of the \\textbf{1T10S-DT}, \\textbf{RADT-MV}, and \\textbf{RADT-MV (consistent)} algorithms in the Walker2D environment under BodyMass and JointNoise shift settings. Results are averaged across five random seeds.}     \\label{fig:Abla4}     \\vspace{-0.2in}\nThe input sequence in the DT framework maintains consistency, specifically, $R_{t+1} - R_t = r_t$. However, applying RADT-MV to relabel the return in the original sequence may disrupt this consistency. To assess whether this inconsistency affects the performance of RADT-MV, we conduct ablation studies. Specifically, we use an alternative algorithm to relabel the RADT-MV augmented data before slicing the trajectory for input into the DT, as detailed in \\Cref{alg:Relabel}. The results, illustrated in \\Cref{fig:Abla4}, demonstrate that RADT-MV significantly outperforms its consistency-enforced variant, particularly in the medium replay environment setting. This outcome suggests that maintaining consistency does not necessarily enhance performance; rather, it may impair the efficacy of the RADT-MV approach within the framework of off-dynamics offline reinforcement learning. This impairment can be attributed to specific dynamics introduced by the RADT-MV method. Specifically, RADT-MV's relabeling of returns in the original dataset—before using the augmented dataset to train the DT framework—introduces a critical stitching capability. If consistency is enforced, this stitching capability may be compromised. This issue becomes particularly pronounced in the replay dataset, where data distribution is discrete and the stitching ability of the DT is crucial for performance.",
        "summarize_figure": "figure1",
        "summarization": "The picture illustrates the average performance comparison of 1T10S-DT, RADT-MV, and RADT-MV (consistent) algorithms in the Walker2D environment under BodyMass and JointNoise shift settings. In the Random environment, the performance of the three algorithms is similar and relatively low under both BodyMass and JointNoise settings. In the Medium environment, the RADT-MV algorithm performs best under the BodyMass setting, while the performance of the three algorithms is similar under the JointNoise setting. In the Medium Replay environment, the RADT-MV algorithm significantly outperforms the other two algorithms under the BodyMass setting, but its performance is relatively low under the JointNoise setting. In the Medium Expert environment, 1T10S-DT and RADT-MV algorithms perform similarly and better than RADT-MV (consistent) under the BodyMass setting, while the RADT-MV algorithm significantly outperforms the other two algorithms under the JointNoise setting."
    },
    "25": {
        "figure1": "2410.23450v1_Abla_HalfCHeetah_Empirical",
        "label1": "fig:Abla_HalfCHeetah_Empirical",
        "caption1": "\\footnotesize Averaged performance comparison of the \\textbf{1T10S-DT}, \\textbf{RADT-MV}, and \\textbf{RADT-MV (empirical)} algorithms in the HalfCheetah environment under BodyMass and JointNoise shift settings. Results are averaged across five random seeds.}  \\vspace{-0.2in",
        "text": "\\centering     \\includegraphics[width=\\linewidth]{figure/Abla_HalfCHeetah_Empirical.png}     %     \\caption{\\footnotesize Averaged performance comparison of the \\textbf{1T10S-DT}, \\textbf{RADT-MV}, and \\textbf{RADT-MV (empirical)} algorithms in the HalfCheetah environment under BodyMass and JointNoise shift settings. Results are averaged across five random seeds.}     \\label{fig:Abla_HalfCHeetah_Empirical}     \\vspace{-0.2in}\nIn our approach, the estimated mean and variance is obtained by the CQL policy. We sample the multiple actions $ \\{a^S_1, a^S_2, \\ldots, a^S_n\\} $ from the current state $s$ with the CQL policy learned in the source dataset to get an estimation of the return mean and variance and similar to the estimation on the target set. To verify the effect of this design, we compared our current approach with a more straightforward design of the estimation in the mean and variance which is denoted as the RADT-MV (empirical). In RADT-MV (empirical), we directly calculate the mean and variance over the dataset. To be specific, we calculate the return of all the trajectories in the offline dataset and use the mean and variance of these returns to augment the source dataset. The results are shown in the \\Cref{fig:Abla_HalfCHeetah_Empirical}. It is clear that our approach RADT-MV leverages the value function as the estimation outperforms the straightforward design of RADT-MV(empirical) in most of the body mass and joint noise settings in HalfCheetah.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the average performance comparison of the 1T10S-DT, RADT-MV, and RADT-MV(empirical) algorithms in the HalfCheetah environment under BodyMass and JointNoise shift settings. The results are averaged across five random seeds. In the Random setting, the performance of the three algorithms is similar under both BodyMass and JointNoise. In the Medium setting, RADT-MV performs slightly better than the other two algorithms under BodyMass, while under JointNoise, both RADT-MV and RADT-MV(empirical) outperform 1T10S-DT. In the Medium Replay setting, RADT-MV and RADT-MV(empirical) perform better than 1T10S-DT under both shifts. In the Medium Expert setting, RADT-MV significantly outperforms the other two algorithms under BodyMass, while under JointNoise, the performance of the three algorithms is similar."
    },
    "26": {
        "figure1": "2410.23450v1_Abla_shift_halfcheetah",
        "label1": "fig:Abla_shift_halfcheetah",
        "caption1": "\\footnotesize {Performance of RADT methods under varying environment shift settings in the HalfCHeetah Medium environment.  \\textbf{\"B-x\"} denotes that the body mass in the simulator is set to \\textbf{x}. \\textbf{\"N-x\"} denotes the addition of random noise in the range \\textbf{(-x, +x)} to the action.  Refer \\Cref{sec:environment_dataset} for details about Kinematic and Morphology Shift settings in the Medium HalfCheetah environment.}}  \\vspace{-0.2in",
        "figure2": "2410.23450v1_Abla_Clip_halfcheetah",
        "label2": "fig:Abla_Clip_halfcheetah",
        "caption2": "\\footnotesize {Performance comparison of \\textbf{RADT-MV} and \\textbf{RADT-MV without the clipping technique} across BodyMass and JointNoise shift settings in Random(R), Medium(M), Medium-Replay(M-R), and Medium-Expert(M-E) environments in HalfCheetah.}}  \\vspace{-0.2in",
        "figure3": "2410.23450v1_Abla_Clip_halfcheetah",
        "label3": "fig:Abla_Clip_halfcheetah",
        "caption3": "\\footnotesize {Performance comparison of \\textbf{RADT-MV} and \\textbf{RADT-MV without the clipping technique} across BodyMass and JointNoise shift settings in Random(R), Medium(M), Medium-Replay(M-R), and Medium-Expert(M-E) environments in HalfCheetah.}}  \\vspace{-0.2in",
        "text": "We conduct ablation studies in the HalfCheetah environment to verify the effectiveness of the RADT methods. These studies include evaluating the performance of RADT methods under other shift settings in the HalfCheetah environment and assessing the impact of the clipping technique on RADT performance. The experiment results for the two ablation studies are shown in the \\Cref{fig:Abla_shift_halfcheetah} and \\Cref{fig:Abla_Clip_halfcheetah}. In the body mass shift settings, our proposed method RADT-DARA and RADT-MV can both mitigate the off-dynamics problem and the RADT-MV performs better than the RADT-DARA. In the joint noise setting, the RADT-DARA still performs well with the large joint noise. The performance is only inferior to the RADT-DARA and DT in the small joint noise. This phenomenon may be caused by the small dynamics shift and randomness introduced by our augment methods. In the kinematic setting, the performance of our methods remains consistent with the body mass and joint noise setting. In the morphology settings, the RADT-DARA fails, while the RADT-MV still performs well under this hard dynamic shift problem. In the \\Cref{fig:Abla_Clip_halfcheetah}, we demonstrate the clip performance in RADT-MV in HalfCheetah. It shows that the clip method can improve the performance and robustness of our method.\n%         \\includegraphics[width=0.9\\textwidth]{figure/Abla_shift_halfcheetah.png} %         \\vspace{-0.1in}         \\caption{\\footnotesize {Performance of RADT methods under varying environment shift settings in the HalfCHeetah Medium environment.  \\textbf{\"B-x\"} denotes that the body mass in the simulator is set to \\textbf{x}. \\textbf{\"N-x\"} denotes the addition of random noise in the range \\textbf{(-x, +x)} to the action.  Refer \\Cref{sec:environment_dataset} for details about Kinematic and Morphology Shift settings in the Medium HalfCheetah environment.}}         \\label{fig:Abla_shift_halfcheetah}         \\vspace{-0.2in}     %         \\centering         \\includegraphics[width=0.75\\textwidth]{figure/Abla_Clip_halfcheetah.png} %         \\caption{\\footnotesize {Performance comparison of \\textbf{RADT-MV} and \\textbf{RADT-MV without the clipping technique} across BodyMass and JointNoise shift settings in Random(R), Medium(M), Medium-Replay(M-R), and Medium-Expert(M-E) environments in HalfCheetah.}}         \\label{fig:Abla_Clip_halfcheetah}     \\vspace{-0.2in}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the performance of RADT methods under varying environment shift settings in the HalfCheetah Medium environment. It includes three subplots for BodyMass, JointNoise, and Medium shift settings. In BodyMass, the horizontal axis represents different body mass settings in the simulator (B-0.25, B-0.50, B-0.75), with RADT-MV showing the best performance across all settings. For JointNoise, the horizontal axis indicates the range of random noise added to the action (N-0.05, N-0.25, N-0.50, N-0.75), where RADT-DARA performs well at various noise levels, slightly below RADT-MV and DT at small noise levels. In the Medium setting, the horizontal axis represents Kinematic and Morphology shifts, with RADT-MV and 1T10S performing similarly in Kinematic shift, but RADT-DARA's performance significantly drops in Morphology shift, while RADT-MV maintains good performance."
    },
    "27": {
        "figure1": "2410.23450v1_Abla_Clip_halfcheetah",
        "label1": "fig:Abla_Clip_halfcheetah",
        "caption1": "\\footnotesize {Performance comparison of \\textbf{RADT-MV} and \\textbf{RADT-MV without the clipping technique} across BodyMass and JointNoise shift settings in Random(R), Medium(M), Medium-Replay(M-R), and Medium-Expert(M-E) environments in HalfCheetah.}}  \\vspace{-0.2in",
        "figure2": "2410.23450v1_Abla_shift_halfcheetah",
        "label2": "fig:Abla_shift_halfcheetah",
        "caption2": "\\footnotesize {Performance of RADT methods under varying environment shift settings in the HalfCHeetah Medium environment.  \\textbf{\"B-x\"} denotes that the body mass in the simulator is set to \\textbf{x}. \\textbf{\"N-x\"} denotes the addition of random noise in the range \\textbf{(-x, +x)} to the action.  Refer \\Cref{sec:environment_dataset} for details about Kinematic and Morphology Shift settings in the Medium HalfCheetah environment.}}  \\vspace{-0.2in",
        "figure3": "2410.23450v1_Abla_shift_halfcheetah",
        "label3": "fig:Abla_shift_halfcheetah",
        "caption3": "\\footnotesize {Performance of RADT methods under varying environment shift settings in the HalfCHeetah Medium environment.  \\textbf{\"B-x\"} denotes that the body mass in the simulator is set to \\textbf{x}. \\textbf{\"N-x\"} denotes the addition of random noise in the range \\textbf{(-x, +x)} to the action.  Refer \\Cref{sec:environment_dataset} for details about Kinematic and Morphology Shift settings in the Medium HalfCheetah environment.}}  \\vspace{-0.2in",
        "text": "We conduct ablation studies in the HalfCheetah environment to verify the effectiveness of the RADT methods. These studies include evaluating the performance of RADT methods under other shift settings in the HalfCheetah environment and assessing the impact of the clipping technique on RADT performance. The experiment results for the two ablation studies are shown in the \\Cref{fig:Abla_shift_halfcheetah} and \\Cref{fig:Abla_Clip_halfcheetah}. In the body mass shift settings, our proposed method RADT-DARA and RADT-MV can both mitigate the off-dynamics problem and the RADT-MV performs better than the RADT-DARA. In the joint noise setting, the RADT-DARA still performs well with the large joint noise. The performance is only inferior to the RADT-DARA and DT in the small joint noise. This phenomenon may be caused by the small dynamics shift and randomness introduced by our augment methods. In the kinematic setting, the performance of our methods remains consistent with the body mass and joint noise setting. In the morphology settings, the RADT-DARA fails, while the RADT-MV still performs well under this hard dynamic shift problem. In the \\Cref{fig:Abla_Clip_halfcheetah}, we demonstrate the clip performance in RADT-MV in HalfCheetah. It shows that the clip method can improve the performance and robustness of our method.\n%         \\includegraphics[width=0.9\\textwidth]{figure/Abla_shift_halfcheetah.png} %         \\vspace{-0.1in}         \\caption{\\footnotesize {Performance of RADT methods under varying environment shift settings in the HalfCHeetah Medium environment.  \\textbf{\"B-x\"} denotes that the body mass in the simulator is set to \\textbf{x}. \\textbf{\"N-x\"} denotes the addition of random noise in the range \\textbf{(-x, +x)} to the action.  Refer \\Cref{sec:environment_dataset} for details about Kinematic and Morphology Shift settings in the Medium HalfCheetah environment.}}         \\label{fig:Abla_shift_halfcheetah}         \\vspace{-0.2in}     %         \\centering         \\includegraphics[width=0.75\\textwidth]{figure/Abla_Clip_halfcheetah.png} %         \\caption{\\footnotesize {Performance comparison of \\textbf{RADT-MV} and \\textbf{RADT-MV without the clipping technique} across BodyMass and JointNoise shift settings in Random(R), Medium(M), Medium-Replay(M-R), and Medium-Expert(M-E) environments in HalfCheetah.}}         \\label{fig:Abla_Clip_halfcheetah}     \\vspace{-0.2in}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the performance of RADT methods under varying environment shift settings in the HalfCheetah Medium environment. In the BodyMass shift setting, the horizontal axis represents the body mass in the simulator set to x (B-x), where x is 0.25, 0.50, and 0.75; the vertical axis represents performance. RADT-MV outperforms 1T10S and RADT-DARA across all body mass settings. In the JointNoise shift setting, the horizontal axis represents the addition of random noise in the range (-x, +x) to the action (N-x), where x is 0.05, 0.25, 0.50, and 0.75; the vertical axis represents performance. RADT-MV outperforms 1T10S and RADT-DARA across all noise settings. In the Medium setting, the horizontal axis represents Kinematic and Morphology shift settings; the vertical axis represents performance. In the Kinematic setting, the three methods perform similarly, but in the Morphology setting, RADT-MV significantly outperforms the other two methods."
    },
    "28": {
        "figure1": "2410.23495v2_imagenet_Intro",
        "label1": "fig:tiny-imagenet",
        "caption1": "Performance comparison of various methods on Tiny-ImageNet using ResNet-18. The same hyperparameters are used across all methods. The dataset is divided into 50 chunks, with a constant number of data points added to the training dataset in each experiment (x-axis), reaching the full dataset at the 50th experiment. Models are trained until achieving 99.9\\% train accuracy before proceeding to the next experiment; the plot on the right reports the number of update steps executed in each experiment. Results are averaged over three random seeds. ``Cold'' refers to cold-starting and ``Warm'' refers to warm-starting. The Shrink \\& Perturb (S\\&P) method involves shrinking the model weights by a constant factor and adding noise \\citep{ash2020warm}. Notably, DASH, our proposed method, achieves better generalization performance compared to both training from scratch and S\\&P, while requiring fewer steps to converge.}  \\vspace{-7pt",
        "text": "\\centering     \\includegraphics[width=0.8\\textwidth]{Figures/imagenet_Intro.pdf}     \\caption{Performance comparison of various methods on Tiny-ImageNet using ResNet-18. The same hyperparameters are used across all methods. The dataset is divided into 50 chunks, with a constant number of data points added to the training dataset in each experiment (x-axis), reaching the full dataset at the 50th experiment. Models are trained until achieving 99.9\\% train accuracy before proceeding to the next experiment; the plot on the right reports the number of update steps executed in each experiment. Results are averaged over three random seeds. ``Cold'' refers to cold-starting and ``Warm'' refers to warm-starting. The Shrink \\& Perturb (S\\&P) method involves shrinking the model weights by a constant factor and adding noise \\citep{ash2020warm}. Notably, DASH, our proposed method, achieves better generalization performance compared to both training from scratch and S\\&P, while requiring fewer steps to converge.}     \\label{fig:tiny-imagenet}     \\vspace{-7pt}\nInspired by this finding, we propose Direction-Aware SHrinking (DASH), which aims to encourage the model to forget memorized noise without affecting previously learned features. This enables the model to learn features that cannot be acquired through warm-starting alone, enhancing the model's generalization ability. We validate DASH using an expanding dataset setting, similar to the approach in \\citet{ash2020warm}, employing various models, datasets, and optimizers. As an example, Figure~\\ref{fig:tiny-imagenet} shows promising results in terms of both test accuracy and training time.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates a performance comparison of various methods on the Tiny-ImageNet dataset using ResNet-18. The x-axis represents the number of experiments. The left plot displays the test accuracy versus the number of experiments. The DASH method demonstrates the best generalization performance, significantly outperforming Cold-starting and Shrink and Perturb (S and P) methods. The Warm-starting method exhibits the lowest test accuracy."
    },
    "29": {
        "figure1": "2410.23495v2_Train_Test_Overfit",
        "label1": "fig:overfit_acc",
        "caption1": "The plot shows the test accuracy (left y-axis) when the model is pretrained for varying epochs (x-axis) and then fine-tuned on the full data, along with the pretrain accuracy (right y-axis) plotted in brown. We trained three-layer MLP (left) and ResNet-18 (right). Each transparent line and point corresponds to a specific random seed, and the median values are highlighted with opaque markers and solid lines. The `Random' corresponds to training from random initialization (cold-start).}  \\vspace{-7pt",
        "text": "Figure~\\ref{fig:overfit_acc} shows the change in the model's performance based on the duration of pre-training. When pre-training is stopped at a certain epoch and the model is then trained on the full dataset, test accuracy is maintained. However, if pre-training continues beyond a specific threshold (approximately 50\\% pre-training accuracy in this case), warm-starting significantly impairs the model's performance as it increasingly memorizes training data points. We attribute this phenomenon to the neural network's memorization process after learning features. This is consistent with reports of a critical learning period where neural networks learn useful features in the early phase of learning \\citep{achille2018critical, frankle2020early, kleinman2024critical}, and with findings that neural networks tend to learn features followed by memorizing noises \\citep{arpit2017closer, jiang2020characterizing}. Using the same experimental settings as in Figure~\\ref{fig:overfit_acc}, we tested with a large-scale dataset, ImageNet-1k, and observed similar trends (see Figure~\\ref{fig:imagenet_50100} in Appendix~\\ref{sec:appendix_justification}).\n\\centering     \\includegraphics[width=0.77\\textwidth]{Figures/Train_Test_Overfit.pdf}     \\vspace{-5pt}     \\caption{The plot shows the test accuracy (left y-axis) when the model is pretrained for varying epochs (x-axis) and then fine-tuned on the full data, along with the pretrain accuracy (right y-axis) plotted in brown. We trained three-layer MLP (left) and ResNet-18 (right). Each transparent line and point corresponds to a specific random seed, and the median values are highlighted with opaque markers and solid lines. The `Random' corresponds to training from random initialization (cold-start).}     \\label{fig:overfit_acc}      \\vspace{-7pt}",
        "summarize_figure": "figure1",
        "summarization": "The image illustrates the test accuracy variations of MLP under different pre-training epochs. The x-axis represents the number of pre-training epochs, the left y-axis represents the test accuracy (percentage), and the right y-axis represents the pre-training accuracy (percentage). The figure includes three curves: a blue dashed line representing the test accuracy of random initialization (cold-start), a pink solid line representing the test accuracy of warm-start, and a brown solid line representing the pre-training accuracy of warm-start. It can be observed that as the number of pre-training epochs increases, the pre-training accuracy continuously rises, while the test accuracy initially remains stable and then begins to decline after the epoch reaches 30."
    },
    "30": {
        "figure1": "2410.23495v2_Synthetic_Experiment_All",
        "label1": "fig:learning_framework_experiment",
        "caption1": "Comparison of random, warm, and ideal methods across 10 random seeds (mean ± std dev). The test accuracy (left) and the number of learned features across all classes (middle) are nearly identical for random and ideal initializations, causing their plots to overlap. Warm initialization, however, exhibits lower test accuracy compared to both methods. Regarding training time (right), there is a significant gap between random and warm initialization, which the ideal method addresses.}  \\vspace{-5pt",
        "text": "As shown in Figure \\ref{fig:learning_framework_experiment}, the results align with the above theorems. Random initialization, i.e. cold-starting, and ideal initialization achieve almost identical generalization performance, outperforming warm initialization. However, with warm initialization, the model converges faster, as evidenced by the number of non-zero gradient data points, which serves as a proxy for training time. Ideal initialization requires less time compared to cold-starting, which is also consistent with Theorem \\ref{thm:ideal}. Due to the sampling process in our experiment, we observe a gradual increase in the number of learned features and test accuracy in warm-starting, mirroring real-world observations.  These findings remained robust across diverse hyperparameter settings (see Figures~\\ref{fig:synthetic_C_varying}--\\ref{fig:synthetic_tau_varying} in the Appendix~\\ref{sec:appendix_justification}).",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the test accuracy versus the number of experiments for Random, Warm, and Ideal initialization methods. The test accuracy of Ideal and Random initialization almost overlaps and is significantly higher than Warm initialization. As the number of experiments increases, the test accuracy of Ideal and Random initialization rises rapidly and stabilizes, eventually around 95%. Warm initialization also increases, but stabilizes around 75%, which is significantly lower than the former two."
    },
    "31": {
        "figure1": "2410.23495v2_LN_BN_WarmvsRandom",
        "label1": "fig:LN_BN",
        "caption1": "The figure shows the results of training ResNet-18 on CIFAR-10 with three random seeds. Layer normalization (dashed lines) is applied in place of batch normalization in ResNet-18, while solid lines represent the use of standard batch normalization. The red lines denote warm-starting, and the blue lines denote cold-starting. The figure demonstrates that the layer normalization technique cannot serve as a solution for plasticity loss. Moreover, the gap between warm-starting and cold-starting performance increases when layer normalization is employed.",
        "figure2": "2410.23495v2_Dead_Neuron_Ratio",
        "label2": "fig:Dead_Neuron_Ratio",
        "caption2": "The figure presents the results of training ResNet-18 on CIFAR-10 with three random seeds. The presence of dead neurons is assessed after each block of ResNet-18 with training dataset, and the analysis reveals that there are no dead neurons. This finding suggests that techniques designed to revive dead neurons in non-stationary data distributions cannot effectively address the plasticity loss observed in the incremental learning setting with stationary data, which is the primary focus of our study.",
        "text": "Furthermore, applying layer normalization cannot close the gap between cold-starting and warm-starting; rather, the gap increases, as shown in Figure~\\ref{fig:LN_BN}. Also, \\citet{nikishin2022primacy} and \\citet{sokar2023dormant} state that loss of plasticity in non-stationary data distributions arises from inactive neurons in the model. However, this is not the case in our setting, as demonstrated in Figure~\\ref{fig:Dead_Neuron_Ratio}.     \\centering     \\includegraphics[width=0.5\\textwidth]{Figures/LN_BN_WarmvsRandom.pdf}     \\caption{The figure shows the results of training ResNet-18 on CIFAR-10 with three random seeds. Layer normalization (dashed lines) is applied in place of batch normalization in ResNet-18, while solid lines represent the use of standard batch normalization. The red lines denote warm-starting, and the blue lines denote cold-starting. The figure demonstrates that the layer normalization technique cannot serve as a solution for plasticity loss. Moreover, the gap between warm-starting and cold-starting performance increases when layer normalization is employed.}     \\label{fig:LN_BN}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the training results of ResNet-18 on the CIFAR-10 dataset with three random seeds. It compares the effects of layer normalization (LN, dashed lines) and batch normalization (BN, solid lines), with red lines indicating warm-starting and blue lines indicating cold-starting. The results show that layer normalization does not effectively address the plasticity loss issue; instead, it increases the performance gap between warm-starting and cold-starting. Specifically, with layer normalization, the performance improvement of cold-starting is more significant, while the performance improvement of warm-starting is relatively smaller, leading to a larger gap between the two."
    },
    "32": {
        "figure1": "2410.23495v2_Dead_Neuron_Ratio",
        "label1": "fig:Dead_Neuron_Ratio",
        "caption1": "The figure presents the results of training ResNet-18 on CIFAR-10 with three random seeds. The presence of dead neurons is assessed after each block of ResNet-18 with training dataset, and the analysis reveals that there are no dead neurons. This finding suggests that techniques designed to revive dead neurons in non-stationary data distributions cannot effectively address the plasticity loss observed in the incremental learning setting with stationary data, which is the primary focus of our study.",
        "figure2": "2410.23495v2_LN_BN_WarmvsRandom",
        "label2": "fig:LN_BN",
        "caption2": "The figure shows the results of training ResNet-18 on CIFAR-10 with three random seeds. Layer normalization (dashed lines) is applied in place of batch normalization in ResNet-18, while solid lines represent the use of standard batch normalization. The red lines denote warm-starting, and the blue lines denote cold-starting. The figure demonstrates that the layer normalization technique cannot serve as a solution for plasticity loss. Moreover, the gap between warm-starting and cold-starting performance increases when layer normalization is employed.",
        "text": "Furthermore, applying layer normalization cannot close the gap between cold-starting and warm-starting; rather, the gap increases, as shown in Figure~\\ref{fig:LN_BN}. Also, \\citet{nikishin2022primacy} and \\citet{sokar2023dormant} state that loss of plasticity in non-stationary data distributions arises from inactive neurons in the model. However, this is not the case in our setting, as demonstrated in Figure~\\ref{fig:Dead_Neuron_Ratio}.     \\centering     \\includegraphics[width=0.5\\textwidth]{Figures/LN_BN_WarmvsRandom.pdf}     \\caption{The figure shows the results of training ResNet-18 on CIFAR-10 with three random seeds. Layer normalization (dashed lines) is applied in place of batch normalization in ResNet-18, while solid lines represent the use of standard batch normalization. The red lines denote warm-starting, and the blue lines denote cold-starting. The figure demonstrates that the layer normalization technique cannot serve as a solution for plasticity loss. Moreover, the gap between warm-starting and cold-starting performance increases when layer normalization is employed.}     \\label{fig:LN_BN}\n\\centering     \\includegraphics[width=\\textwidth]{Figures/Dead_Neuron_Ratio.pdf}     \\caption{The figure presents the results of training ResNet-18 on CIFAR-10 with three random seeds. The presence of dead neurons is assessed after each block of ResNet-18 with training dataset, and the analysis reveals that there are no dead neurons. This finding suggests that techniques designed to revive dead neurons in non-stationary data distributions cannot effectively address the plasticity loss observed in the incremental learning setting with stationary data, which is the primary focus of our study.}     \\label{fig:Dead_Neuron_Ratio}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the results of training ResNet-18 on CIFAR-10 with three random seeds. The graph compares the test accuracy of layer normalization (LN, dashed lines) and batch normalization (BN, solid lines) in both warm-starting (red lines) and cold-starting (blue lines) scenarios. The results indicate that layer normalization cannot effectively close the gap between cold-starting and warm-starting; instead, it widens the gap. With layer normalization, the performance difference between warm-starting and cold-starting becomes more pronounced."
    },
    "33": {
        "figure1": "2410.23495v2_Heatmap_CIFAR10",
        "label1": "fig:Heatmap_CIFAR10",
        "caption1": "The performance of DASH with various hyperparameter values on the CIFAR-10 dataset using a ResNet-18 architecture. Three runs averaged with standard deviation. Darker colors indicate higher values. The first two heatmaps show that higher values are preferable, while the last heatmap demonstrates that lower values (brighter colors) are preferable.",
        "text": "In this section, we provide the details of the hyperparameters used in our experiments from Section~\\ref{sec:experiments}, as well as Appendix~\\ref{sec:appendix_experiments_nonstationary} and \\ref{sec:appendix_omitted_experiments}. Additionally, we present heatmaps illustrating the results for a wide range of two hyperparameters, $\\alpha$ and $\\lambda$, in DASH. The heatmaps in Figure~\\ref{fig:Heatmap_CIFAR10} suggest that DASH exhibits robustness to hyperparameter variations, indicating that its performance is less affected by the choice of hyperparameter values.\n\\centering     \\includegraphics[width=\\textwidth]{Figures/Heatmap_CIFAR10.pdf}     \\caption{The performance of DASH with various hyperparameter values on the CIFAR-10 dataset using a ResNet-18 architecture. Three runs averaged with standard deviation. Darker colors indicate higher values. The first two heatmaps show that higher values are preferable, while the last heatmap demonstrates that lower values (brighter colors) are preferable.}     \\label{fig:Heatmap_CIFAR10}",
        "summarize_figure": "figure1",
        "summarization": "This image presents the performance of the DASH method using a ResNet-18 architecture on the CIFAR-10 dataset with varying alpha and lambda hyperparameters. The first heatmap displays the test accuracy at the last experiment, where darker colors indicate higher accuracy. The accuracies across different hyperparameter settings show limited variation, with the highest accuracy of 84.28 achieved at alpha=0.3 and lambda=0.05, while other combinations also exceed 83 percent. The second heatmap shows the average test accuracy across all experiments, further illustrating the relative insensitivity of accuracy to hyperparameter changes. The third heatmap depicts the average number of steps across all experiments, with brighter colors indicating fewer steps, which is preferable. Overall, the stable accuracy shown in the first two maps, contrasting with varying steps in the third, supports the text's assertion that DASH demonstrates robustness to hyperparameter variations."
    },
    "34": {
        "figure1": "2410.23495v2_Heatmap_CIFAR10_SP",
        "label1": "fig:Heatmap_CIFAR10_SP",
        "caption1": "The performance of S\\&P on CIFAR-10 using ResNet-18 with varying $\\sigma$ values. While \\citet{ash2020warm} reported better test accuracy when $\\sigma=0.1$ compared to $\\sigma=0.01$, we exhibited significantly lower performance compared to $\\sigma=0.01$.",
        "text": "We fixed the momentum to 0.9 and the batch size to 128. The learning rate is set to 0.001 for training ResNet-18, and for other models, a learning rate of 0.01 is used. The value of $\\rho$ for SAM is chosen based on the performance of cold-starting. The default value of $\\alpha=0.3$ is used, and we did not change this value frequently. The perturbation parameter $\\sigma$ used in the Shrink \\& Perturb (S\\&P) procedure is set to $0.01$, as this value is considered optimal for perturbation, as described in \\citet{ash2020warm}. Initially, we tested $\\sigma=0.1$ as the perturbation parameter, since \\citet{ash2020warm} reported slightly better test accuracy compared to $\\sigma=0.01$ in some cases. However, we experienced significantly poorer generalization performance with $\\sigma=0.1$ compared to $\\sigma=0.01$, as shown in Figure~\\ref{fig:Heatmap_CIFAR10_SP}. The hyperparameters used in our experiments are described in Table~\\ref{tbl:hyperparameters}.\n\\centering     \\includegraphics[width=\\textwidth]{Figures/Heatmap_CIFAR10_SP.pdf}     \\caption{The performance of S\\&P on CIFAR-10 using ResNet-18 with varying $\\sigma$ values. While \\citet{ash2020warm} reported better test accuracy when $\\sigma=0.1$ compared to $\\sigma=0.01$, we exhibited significantly lower performance compared to $\\sigma=0.01$.}     \\label{fig:Heatmap_CIFAR10_SP}",
        "summarize_figure": "figure1",
        "summarization": "The image presents the final test accuracy of the S&P method on the CIFAR-10 dataset using the ResNet-18 model, for varying sigma and lambda values. As shown in the image, when sigma is set to 0.1, the final test accuracy across different lambda values ranges from approximately 58.52% to 59.16%. In contrast, when sigma is set to 0.01, the final test accuracy is significantly higher, ranging from approximately 79.65% to 81.27%. This indicates that reducing sigma from 0.1 to 0.01 results in a substantial improvement in the model's final test accuracy, while accuracy changes relatively less with varying lambda within the tested range, highlighting sigma's dominant impact."
    },
    "35": {
        "figure1": "2410.23495v2_Imagenet_dash_comparison",
        "label1": "fig:imagenet_dash",
        "caption1": "Evaluation of DASH on the ImageNet-1k dataset using ResNet18. We pretrained for 150 epochs using 50\\% of the dataset (shown to the left of the dashed line) and then continued training for another 150 epochs with the full dataset for `Warm', `S\\&P' and `DASH' methods. For cold initialization, we trained for 150 epochs on the full dataset starting from random initialization. DASH outperforms all baseline methods in terms of test accuracy and convergence speed.",
        "text": "We validate the scalability of DASH for larger datasets such as ImageNet-1k. However, conducting experiments for such datasets is challenging, as it would require repeating the training process 50 times until convergence—an extremely time-consuming process. As an alternative, we trained ImageNet-1K on ResNet18 using a setup similar to Figure~\\ref{fig:overfit_acc} in Section~\\ref{sec:warm_vs_cold}. Our setup involved pretraining on 50\\% of the data before fine-tuning on the complete dataset. We used shrinkage parameter $\\lambda=0.3$ for both DASH and S\\&P. Results shown in Figure~\\ref{fig:imagenet_dash} demonstrate that DASH achieves superior performance compared to all baseline methods—including cold initialization, warm initialization, and S\\&P—both in terms of test accuracy and convergence speed. Notably, DASH achieves faster convergence and marginally better test accuracy than S\\&P, demonstrating its effectiveness even on challenging large-scale datasets.\n\\centering     \\includegraphics[width=0.85\\linewidth]{Figures/Imagenet_dash_comparison.pdf}     \\caption{Evaluation of DASH on the ImageNet-1k dataset using ResNet18. We pretrained for 150 epochs using 50\\% of the dataset (shown to the left of the dashed line) and then continued training for another 150 epochs with the full dataset for `Warm', `S\\&P' and `DASH' methods. For cold initialization, we trained for 150 epochs on the full dataset starting from random initialization. DASH outperforms all baseline methods in terms of test accuracy and convergence speed.}     \\label{fig:imagenet_dash}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the evaluation of DASH on the ImageNet-1k dataset using ResNet18. The left subplot shows the train accuracy, and the right subplot shows the test accuracy. The area to the left of the dashed line represents pretraining for 150 epochs using 50% of the dataset, and the area to the right of the dashed line represents continuing training for 150 epochs with the full dataset for \"Warm\" initialization, S and P, and DASH methods. Cold initialization involves training directly on the full dataset for 150 epochs starting from random initialization. DASH outperforms all baseline methods in terms of test accuracy and convergence speed."
    },
    "36": {
        "figure1": "2410.23495v2_online_setup",
        "label1": "fig:online_setup",
        "caption1": "Performance comparison between warm-starting, S\\&P, and DASH in a scenario without access to previous data, using optimal hyperparameters for each method. For S\\&P, the shrinkage parameter $\\lambda=0.3$ with shrinkage interval 20. For DASH, the shrinkage parameter $\\lambda=0.3$ with shrinkage interval 15. DASH significantly outperforms warm-starting, while S\\&P performs even worse than the warm-starting baseline in terms of test accuracy. Since there are no previous data available, we observe sharp drops in test accuracy during shrinkage events, but DASH quickly recovers while S\\&P struggles to regain performance.",
        "figure2": "2410.23495v2_heatmap_online",
        "label2": "fig:heatmap_online",
        "caption2": "Heatmaps comparing S\\&P (top row) and DASH (bottom row) performance without access to previous data. The x-axis shows results for different shrinkage parameters $\\lambda$ (0.1, 0.3, 0.5), while the y-axis shows different shrinkage periods (10, 15, 20). Left heatmaps display final test accuracy, where DASH outperforms S\\&P across all hyperparameter configurations. Middle heatmaps show average test accuracy throughout all experiments, with DASH consistently maintaining superior performance across all configurations. Lastly, right heatmaps show average number of steps to converge across all experiments.",
        "figure3": "2410.23495v2_heatmap_online",
        "label3": "fig:heatmap_online",
        "caption3": "Heatmaps comparing S\\&P (top row) and DASH (bottom row) performance without access to previous data. The x-axis shows results for different shrinkage parameters $\\lambda$ (0.1, 0.3, 0.5), while the y-axis shows different shrinkage periods (10, 15, 20). Left heatmaps display final test accuracy, where DASH outperforms S\\&P across all hyperparameter configurations. Middle heatmaps show average test accuracy throughout all experiments, with DASH consistently maintaining superior performance across all configurations. Lastly, right heatmaps show average number of steps to converge across all experiments.",
        "text": "We tested DASH and S\\&P with intervals of 10, 15, and 20 epochs. For both methods, we explored shrinkage parameter ($\\lambda$) of 0.05, 0.1, and 0.3. We plotted the results using the best hyperparameters for each method in Figure~\\ref{fig:online_setup}. Notably, DASH’s test accuracy consistently exceeded that of S\\&P across all hyperparameter configurations, as shown in Figure~\\ref{fig:heatmap_online}. These findings demonstrate that DASH outperforms both the warm-starting baseline and the S\\&P method in terms of test accuracy. Based on this evidence, we can conclude that DASH is well-suited for data-discarding scenarios.\n\\centering     \\includegraphics[width=1\\linewidth]{Figures/online_setup.pdf}     \\caption{Performance comparison between warm-starting, S\\&P, and DASH in a scenario without access to previous data, using optimal hyperparameters for each method. For S\\&P, the shrinkage parameter $\\lambda=0.3$ with shrinkage interval 20. For DASH, the shrinkage parameter $\\lambda=0.3$ with shrinkage interval 15. DASH significantly outperforms warm-starting, while S\\&P performs even worse than the warm-starting baseline in terms of test accuracy. Since there are no previous data available, we observe sharp drops in test accuracy during shrinkage events, but DASH quickly recovers while S\\&P struggles to regain performance.}     \\label{fig:online_setup}     \\centering     \\includegraphics[width=1\\linewidth]{Figures/heatmap_online.pdf}     \\caption{Heatmaps comparing S\\&P (top row) and DASH (bottom row) performance without access to previous data. The x-axis shows results for different shrinkage parameters $\\lambda$ (0.1, 0.3, 0.5), while the y-axis shows different shrinkage periods (10, 15, 20). Left heatmaps display final test accuracy, where DASH outperforms S\\&P across all hyperparameter configurations. Middle heatmaps show average test accuracy throughout all experiments, with DASH consistently maintaining superior performance across all configurations. Lastly, right heatmaps show average number of steps to converge across all experiments.}     \\label{fig:heatmap_online}\nWe designed an experiment mimicking real-world scenarios where new data arrives continuously during training. Following \\citet{igl2020transient}'s approach, we sampled new data randomly each epoch for the first 500 epochs, combining it with the existing dataset, then continued training for another 500 epochs. This setting assumes a scenario where data continuously arrives before the model has fully converged, as it often does in real-world situations. Cold-started models were trained for 1000 epochs using the entire dataset from the beginning. We used the CIFAR-10 dataset with ResNet18 across five random seeds. We applied both DASH and S\\&P every 50 epochs through the first 500 epochs with shrinkage parameter $\\lambda = 0.3$ for both methods, following a similar setup to Figure~\\ref{fig:online_setup}.",
        "summarize_figure": "figure1",
        "summarization": "In the figure, the performance of warm-starting, S and P, and DASH methods is compared in a scenario without access to previous data, using the optimal hyperparameters for each method. DASH significantly outperforms warm-starting in terms of test accuracy, while the performance of S and P is even worse than the warm-starting baseline. Due to the absence of available previous data, sharp drops in test accuracy are observed during shrinkage events, but DASH recovers quickly, while S and P struggle to regain performance. The shrinkage parameter lambda for S and P is 0.3 with a shrinkage interval of 20, and the shrinkage parameter lambda for DASH is 0.3 with a shrinkage interval of 15."
    },
    "37": {
        "figure1": "2410.23495v2_heatmap_online",
        "label1": "fig:heatmap_online",
        "caption1": "Heatmaps comparing S\\&P (top row) and DASH (bottom row) performance without access to previous data. The x-axis shows results for different shrinkage parameters $\\lambda$ (0.1, 0.3, 0.5), while the y-axis shows different shrinkage periods (10, 15, 20). Left heatmaps display final test accuracy, where DASH outperforms S\\&P across all hyperparameter configurations. Middle heatmaps show average test accuracy throughout all experiments, with DASH consistently maintaining superior performance across all configurations. Lastly, right heatmaps show average number of steps to converge across all experiments.",
        "figure2": "2410.23495v2_online_setup",
        "label2": "fig:online_setup",
        "caption2": "Performance comparison between warm-starting, S\\&P, and DASH in a scenario without access to previous data, using optimal hyperparameters for each method. For S\\&P, the shrinkage parameter $\\lambda=0.3$ with shrinkage interval 20. For DASH, the shrinkage parameter $\\lambda=0.3$ with shrinkage interval 15. DASH significantly outperforms warm-starting, while S\\&P performs even worse than the warm-starting baseline in terms of test accuracy. Since there are no previous data available, we observe sharp drops in test accuracy during shrinkage events, but DASH quickly recovers while S\\&P struggles to regain performance.",
        "figure3": "2410.23495v2_online_setup",
        "label3": "fig:online_setup",
        "caption3": "Performance comparison between warm-starting, S\\&P, and DASH in a scenario without access to previous data, using optimal hyperparameters for each method. For S\\&P, the shrinkage parameter $\\lambda=0.3$ with shrinkage interval 20. For DASH, the shrinkage parameter $\\lambda=0.3$ with shrinkage interval 15. DASH significantly outperforms warm-starting, while S\\&P performs even worse than the warm-starting baseline in terms of test accuracy. Since there are no previous data available, we observe sharp drops in test accuracy during shrinkage events, but DASH quickly recovers while S\\&P struggles to regain performance.",
        "text": "We tested DASH and S\\&P with intervals of 10, 15, and 20 epochs. For both methods, we explored shrinkage parameter ($\\lambda$) of 0.05, 0.1, and 0.3. We plotted the results using the best hyperparameters for each method in Figure~\\ref{fig:online_setup}. Notably, DASH’s test accuracy consistently exceeded that of S\\&P across all hyperparameter configurations, as shown in Figure~\\ref{fig:heatmap_online}. These findings demonstrate that DASH outperforms both the warm-starting baseline and the S\\&P method in terms of test accuracy. Based on this evidence, we can conclude that DASH is well-suited for data-discarding scenarios.\n\\centering     \\includegraphics[width=1\\linewidth]{Figures/online_setup.pdf}     \\caption{Performance comparison between warm-starting, S\\&P, and DASH in a scenario without access to previous data, using optimal hyperparameters for each method. For S\\&P, the shrinkage parameter $\\lambda=0.3$ with shrinkage interval 20. For DASH, the shrinkage parameter $\\lambda=0.3$ with shrinkage interval 15. DASH significantly outperforms warm-starting, while S\\&P performs even worse than the warm-starting baseline in terms of test accuracy. Since there are no previous data available, we observe sharp drops in test accuracy during shrinkage events, but DASH quickly recovers while S\\&P struggles to regain performance.}     \\label{fig:online_setup}     \\centering     \\includegraphics[width=1\\linewidth]{Figures/heatmap_online.pdf}     \\caption{Heatmaps comparing S\\&P (top row) and DASH (bottom row) performance without access to previous data. The x-axis shows results for different shrinkage parameters $\\lambda$ (0.1, 0.3, 0.5), while the y-axis shows different shrinkage periods (10, 15, 20). Left heatmaps display final test accuracy, where DASH outperforms S\\&P across all hyperparameter configurations. Middle heatmaps show average test accuracy throughout all experiments, with DASH consistently maintaining superior performance across all configurations. Lastly, right heatmaps show average number of steps to converge across all experiments.}     \\label{fig:heatmap_online}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the performance comparison of warm-starting, S and P, and DASH in terms of test accuracy and number of steps in a scenario without access to previous data, using the optimal hyperparameters for each method. For S and P, the shrinkage parameter lambda is 0.3 with a shrinkage interval of 20. For DASH, the shrinkage parameter lambda is 0.3 with a shrinkage interval of 15. In terms of test accuracy, DASH significantly outperforms warm-starting, while S and P performs even worse than the warm-starting baseline. Due to the absence of previous data, sharp drops in test accuracy are observed during shrinkage events, but DASH recovers quickly, while S and P struggles to regain performance."
    },
    "38": {
        "figure1": "2410.23495v2_continuously_added",
        "label1": "fig:continuously_added",
        "caption1": "Comparison of training accuracy (left) and test accuracy (right) for ResNet18 on CIFAR-10 over 1000 epochs. During the first 500 epochs, new samples of equal size are added in an i.i.d. manner, followed by 500 epochs of training on the complete dataset. Training accuracy is calculated using only the data available at each epoch during training.",
        "text": "\\centering     \\includegraphics[width=1\\linewidth]{Figures/continuously_added.pdf}     \\caption{Comparison of training accuracy (left) and test accuracy (right) for ResNet18 on CIFAR-10 over 1000 epochs. During the first 500 epochs, new samples of equal size are added in an i.i.d. manner, followed by 500 epochs of training on the complete dataset. Training accuracy is calculated using only the data available at each epoch during training.}     \\label{fig:continuously_added} As shown in Figure~\\ref{fig:continuously_added}, while there is a clear performance gap between warm-starting and cold-starting approaches, both DASH and S\\&P aim to address this difference. Despite using the same shrinkage value of 0.3, DASH successfully preserved learned features while S\\&P did not. This difference is reflected in the training accuracy: DASH showed steady improvement as training progressed, while S\\&P struggled during the shrinking phases. As a result, DASH not only converged faster but also achieved higher test accuracy, while S\\&P lost the convergence advantage that warm-starting provides.",
        "summarize_figure": "figure1",
        "summarization": "This figure compares the training accuracy (left) and test accuracy (right) of ResNet18 on CIFAR-10 over 1000 epochs. During the first 500 epochs, new samples of equal size are added in an independent and identically distributed manner, followed by 500 epochs of training on the complete dataset. Training accuracy is calculated using only the data available at each epoch during training. Regarding training accuracy, the Cold method has lower accuracy in the initial epochs but quickly improves to near 100 percent. The Warm method improves more rapidly, also approaching 100 percent after fewer epochs. The accuracy of the S and P method is initially low, gradually increasing after 500 epochs and eventually approaching 100 percent. The DASH method fluctuates greatly in the early stages of training, but as training progresses, the accuracy steadily increases to nearly 100 percent."
    },
    "39": {
        "figure1": "2410.23495v2_CIL_setup",
        "label1": "fig:CIL",
        "caption1": "Performance on CIFAR-10 using ResNet-18, averaged over five random seeds. Left: Test accuracy evaluated on classes encountered so far. Right: Number of steps required for convergence.",
        "text": "This revised approach yielded more promising results, as shown in Figure~\\ref{fig:CIL}. DASH surpasses other baselines in terms of test accuracy despite requiring longer convergence times. While these results demonstrate DASH’s potential in certain non-stationary environments, we recognize that our modified setup simplifies true non-stationarity, such as in continual learning scenarios.\n\\centering     \\includegraphics[width=1\\linewidth]{Figures/CIL_setup.pdf}     \\caption{Performance on CIFAR-10 using ResNet-18, averaged over five random seeds. Left: Test accuracy evaluated on classes encountered so far. Right: Number of steps required for convergence.}     \\label{fig:CIL}",
        "summarize_figure": "figure1",
        "summarization": "The left plot of the figure illustrates the test accuracy on the CIFAR-10 dataset using the ResNet-18 model, averaged over five random seeds, evaluating the accuracy of classes encountered so far. The horizontal axis represents the number of experiments, ranging from 1 to 10. There are four curves representing the Cold, Warm, S and P, and DASH methods. The DASH method achieves the highest test accuracy, above 80 percent, and remains relatively stable. The S and P method is the second best, stabilizing around 80 percent after the number of experiments exceeds 6. The Cold and Warm methods have lower accuracy, with the Warm method having the lowest accuracy, around 60 percent."
    },
    "40": {
        "figure1": "2410.23495v2_prevGN_vs_NS_All",
        "label1": "fig:N_vs_training_time",
        "caption1": "Trained on ResNet-18 with five random seeds, where CIFAR-10 is divided into 50 chunks and incrementally increased by adding new chunks at each experiment. Each point represents an individual experiment. The gradient norm is used as a proxy for the number of non-zero gradient data points, which in turn serves as a proxy for the training time. A larger gradient norm indicates the model needs to learn more features or memorize more data points to correctly classify all training data points.",
        "figure2": "2410.23495v2_Random_Step_RealEXP",
        "label2": "fig:real-world_numstep_random",
        "caption2": "Figure trained on ResNet-18 with three random seeds. The dataset is divided into 50 chunks, and new chunks are incrementally added for each experiment. The number of steps required for convergence increases with the amount of data when training a cold-started neural network, which is a standard training.",
        "text": "This section demonstrates how our theoretical framework relates to real-world scenarios. Section~\\ref{sec:appendix_justifcation_framework} presents real-world experimental evidence supporting our theoretical framework. Section~\\ref{sec:appendix_various_synthetic} explores the robustness of our synthetic experiments across various hyperparameters. Finally, Section~\\ref{sec:appendix_supporting_experiment_dash} provides empirical validation of the key intuitions behind DASH. Figure~\\ref{fig:N_vs_training_time} shows that the initial gradient norm of training data, $|\\gN^{(j, 0)}|$, can be a proxy for the training time until convergence. As the initial gradient norm increases, the number of steps required for convergence also increases. While this figure uses the gradient norm instead of the number of non-zero gradient data points due to the continuous nature of real-world neural network training, we believe it resembles the behavior of non-zero gradient data points. Additionally, Figure \\ref{fig:real-world_numstep_random} demonstrates that the number of steps required for convergence increases as the number of data points increases in various datasets.     \\centering     \\includegraphics[width=0.9\\textwidth]{Figures/prevGN_vs_NS_All.pdf}     \\caption{Trained on ResNet-18 with five random seeds, where CIFAR-10 is divided into 50 chunks and incrementally increased by adding new chunks at each experiment. Each point represents an individual experiment. The gradient norm is used as a proxy for the number of non-zero gradient data points, which in turn serves as a proxy for the training time. A larger gradient norm indicates the model needs to learn more features or memorize more data points to correctly classify all training data points.}     \\label{fig:N_vs_training_time}     \\centering     \\includegraphics[width=0.9\\textwidth]{Figures/Random_Step_RealEXP.pdf}     \\caption{Figure trained on ResNet-18 with three random seeds. The dataset is divided into 50 chunks, and new chunks are incrementally added for each experiment. The number of steps required for convergence increases with the amount of data when training a cold-started neural network, which is a standard training.}     \\label{fig:real-world_numstep_random}",
        "summarize_figure": "figure1",
        "summarization": "This figure visualizes the training results on the CIFAR-10 dataset using the ResNet-18 model. The dataset is divided into 50 chunks, with new chunks added incrementally in each experiment. Each point in the figure represents an individual experiment, using the gradient norm as a proxy for the number of non-zero gradient data points, which in turn serves as a proxy for training time. The horizontal axis represents the initial gradient norm of the training data, the vertical axis represents the number of steps required for convergence, and the color of the points represents the number of experiments. The left plot (Warm) displays the training results for models warm-started from a previous task, and the right plot (Random) shows the training results for randomly initialized models. It can be observed that as the initial gradient norm increases, the number of steps required for convergence also increases."
    },
    "41": {
        "figure1": "2410.23495v2_Random_Step_RealEXP",
        "label1": "fig:real-world_numstep_random",
        "caption1": "Figure trained on ResNet-18 with three random seeds. The dataset is divided into 50 chunks, and new chunks are incrementally added for each experiment. The number of steps required for convergence increases with the amount of data when training a cold-started neural network, which is a standard training.",
        "figure2": "2410.23495v2_prevGN_vs_NS_All",
        "label2": "fig:N_vs_training_time",
        "caption2": "Trained on ResNet-18 with five random seeds, where CIFAR-10 is divided into 50 chunks and incrementally increased by adding new chunks at each experiment. Each point represents an individual experiment. The gradient norm is used as a proxy for the number of non-zero gradient data points, which in turn serves as a proxy for the training time. A larger gradient norm indicates the model needs to learn more features or memorize more data points to correctly classify all training data points.",
        "text": "This section demonstrates how our theoretical framework relates to real-world scenarios. Section~\\ref{sec:appendix_justifcation_framework} presents real-world experimental evidence supporting our theoretical framework. Section~\\ref{sec:appendix_various_synthetic} explores the robustness of our synthetic experiments across various hyperparameters. Finally, Section~\\ref{sec:appendix_supporting_experiment_dash} provides empirical validation of the key intuitions behind DASH. Figure~\\ref{fig:N_vs_training_time} shows that the initial gradient norm of training data, $|\\gN^{(j, 0)}|$, can be a proxy for the training time until convergence. As the initial gradient norm increases, the number of steps required for convergence also increases. While this figure uses the gradient norm instead of the number of non-zero gradient data points due to the continuous nature of real-world neural network training, we believe it resembles the behavior of non-zero gradient data points. Additionally, Figure \\ref{fig:real-world_numstep_random} demonstrates that the number of steps required for convergence increases as the number of data points increases in various datasets.     \\centering     \\includegraphics[width=0.9\\textwidth]{Figures/prevGN_vs_NS_All.pdf}     \\caption{Trained on ResNet-18 with five random seeds, where CIFAR-10 is divided into 50 chunks and incrementally increased by adding new chunks at each experiment. Each point represents an individual experiment. The gradient norm is used as a proxy for the number of non-zero gradient data points, which in turn serves as a proxy for the training time. A larger gradient norm indicates the model needs to learn more features or memorize more data points to correctly classify all training data points.}     \\label{fig:N_vs_training_time}     \\centering     \\includegraphics[width=0.9\\textwidth]{Figures/Random_Step_RealEXP.pdf}     \\caption{Figure trained on ResNet-18 with three random seeds. The dataset is divided into 50 chunks, and new chunks are incrementally added for each experiment. The number of steps required for convergence increases with the amount of data when training a cold-started neural network, which is a standard training.}     \\label{fig:real-world_numstep_random}",
        "summarize_figure": "figure1",
        "summarization": "Summary:The figure illustrates the results of training on ResNet-18 with five random seeds, where the CIFAR-10 dataset is divided into 50 chunks and new chunks are incrementally added in each experiment. Each point represents an individual experiment. The horizontal axis is the initial gradient norm of training data, and the vertical axis is the number of steps to convergence, with the color of the points representing the number of experiments. The figure is divided into two parts: \"Warm\" indicates warm start, and \"Random\" indicates random start. It can be observed that as the initial gradient norm increases, the number of steps required for convergence also increases. The gradient norm is used as a proxy for the number of non-zero gradient data points, and in turn, as a proxy for the training time. A larger gradient norm indicates that the model needs to learn more features or memorize more data points to correctly classify all training data points."
    },
    "42": {
        "figure1": "2410.23495v2_Imagenet_pre_comparison",
        "label1": "fig:imagenet_50100",
        "caption1": "Effect of pretraining epochs on warm-starting with the large-scale ImageNet-1k dataset using ResNet-18. The plots to the left of the dashed line represent the pretraining results, and the dotted lines indicate the starting points for each pretraining run. Test accuracy declines as the number of pretraining epochs increases, particularly beyond 50 epochs, which aligns with the observations in Figure~\\ref{fig:overfit_acc} in Section~\\ref{sec:warm_vs_cold}.}  \\vspace{-5pt",
        "text": "\\centering     \\includegraphics[width=0.8\\linewidth]{Figures/Imagenet_pre_comparison.pdf}     \\caption{Effect of pretraining epochs on warm-starting with the large-scale ImageNet-1k dataset using ResNet-18. The plots to the left of the dashed line represent the pretraining results, and the dotted lines indicate the starting points for each pretraining run. Test accuracy declines as the number of pretraining epochs increases, particularly beyond 50 epochs, which aligns with the observations in Figure~\\ref{fig:overfit_acc} in Section~\\ref{sec:warm_vs_cold}.}     \\label{fig:imagenet_50100}     \\vspace{-5pt}",
        "summarize_figure": "figure1",
        "summarization": "The image presents the effect of varying pretraining epochs on warm-starting performance using ResNet-18 on the large-scale ImageNet-1k dataset. The left plot shows the training accuracy evolution with epochs, while the right plot displays the test accuracy. Epochs to the left of the dashed line represent the pretraining phase, and dotted lines mark the starting points for warm-starting runs. The right plot indicates that while all warm-starting configurations show rapid initial test accuracy improvement during the warm-starting phase and reach similar peak accuracy as the cold start, the final test accuracy tends to decline with increased pretraining epochs, particularly beyond 50 epochs. Models with cold start or fewer pretraining epochs (e.g., 30 or 50) achieve slightly higher final test accuracy compared to those with more pretraining epochs (e.g., 70 or 150)."
    },
    "43": {
        "figure1": "2410.23495v2_prev_training_acc_13",
        "label1": "fig:prev_training_acc",
        "caption1": "The results are averaged over 10 random seeds. The x-axis represents the number of experiments, while the y-axis represents the training accuracy on previous datasets. Warm-starting can retain previously learned data points when further trained with an incremented dataset. Additionally, DASH, plotted in green, can retain more information compared to other methods.}  \\vspace{-5pt",
        "figure2": "2410.23495v2_Warm_Random_Comparison_TestAcc_RealEXP",
        "label2": "fig:real-world_testacc_warmvsrandom",
        "caption2": "The same hyperparameters are used described in Section~\\ref{sec:experimental_details} with three random seeds. The gap in test accuracy between the two initialization methods increases as dataset complexity increases.",
        "text": "\\centering     \\includegraphics[width=0.85\\textwidth]{Figures/Warm_Random_Comparison_TestAcc_RealEXP.pdf}     \\caption{The same hyperparameters are used described in Section~\\ref{sec:experimental_details} with three random seeds. The gap in test accuracy between the two initialization methods increases as dataset complexity increases.}     \\label{fig:real-world_testacc_warmvsrandom} To validate whether previously learned/memorized data points do not have large gradients when further trained with a combined dataset (existing + newly introduced data) using warm-starting, we plotted the train accuracy on the previous dataset for the first few epochs. Using ResNet-18 on CIFAR-10 (Figure~\\ref{fig:prev_training_acc}), we found that warm-starting preserves performance on previously learned data even when training continues with combined datasets, supporting the main idea of Theorem~\\ref{thm:warm_vs_random}.\nTo verify whether DASH truly captures our intuitions from the ideal algorithms, we conducted an experiment using CIFAR-10 trained on ResNet-18, with the same experimental settings. Figure \\ref{fig:prev_training_acc} demonstrates that when applying DASH, the train accuracy on previous datasets increases more rapidly after a few epochs compared to other methods. We argue that this behavior stems from our algorithm's ability to forget memorized noise while preserving learned features. As the number of experiments increases, the number of learned features also grows. For a fair comparison, we used $\\lambda = 0.05$ for DASH, and when performing S\\&P and shrink, we shrank each weight by multiplying $0.05$. In the case of S\\&P, after shrinking, we added noise sampled from $\\gN(0, 0.01^{2})$.     \\centering     \\includegraphics[width=0.8\\textwidth]{Figures/prev_training_acc_13.pdf}     \\caption{The results are averaged over 10 random seeds. The x-axis represents the number of experiments, while the y-axis represents the training accuracy on previous datasets. Warm-starting can retain previously learned data points when further trained with an incremented dataset. Additionally, DASH, plotted in green, can retain more information compared to other methods.}     \\label{fig:prev_training_acc}     \\vspace{-5pt}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the experimental results of using random initialization and warm-starting on four datasets: F-MNIST, CIFAR-10, CIFAR-100, and T-ImageNet. The x-axis represents the number of experiments, and the y-axis represents the test accuracy (percentage). It can be observed that as the complexity of the dataset increases, the gap in test accuracy between the two initialization methods also increases. Specifically, the test accuracy of the two methods is similar on the F-MNIST dataset, while on the CIFAR-10, CIFAR-100, and T-ImageNet datasets, the test accuracy of the warm-starting method is significantly higher than that of the random initialization method."
    },
    "44": {
        "figure1": "2410.23495v2_CIFAR10_Cos_sim",
        "label1": "fig:cos_sim_plot",
        "caption1": "We trained a 3-layer CNN on CIFAR-10 and plotted cosine similarity values greater than 0.1. The left y-axis (black) represents the cosine similarity between the negative loss gradient from the training data and the model filters, while the right y-axis (green) shows the test accuracy.",
        "text": "We further validate our intuition that cosine similarity between the negative loss gradient and model weights can indicate whether the model has indeed learned features. We trained a 3-layer CNN on CIFAR-10, varying the size of the training dataset. We observed that the cosine similarity between the negative gradient from the test data and the learned filters increases as the training dataset size grows. The model trained with more data appears to learn more features, as evidenced by the rising trend in test accuracy, shown by the dashed line in Figure~\\ref{fig:cos_sim_plot}. This suggests that cosine similarity can capture whether the weights have learned features, which aligns with our expectations.     \\centering     \\includegraphics[width=0.65\\linewidth]{Figures/CIFAR10_Cos_sim.pdf}     \\caption{We trained a 3-layer CNN on CIFAR-10 and plotted cosine similarity values greater than 0.1. The left y-axis (black) represents the cosine similarity between the negative loss gradient from the training data and the model filters, while the right y-axis (green) shows the test accuracy.}     \\label{fig:cos_sim_plot}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the results of a 3-layer CNN trained on the CIFAR-10 dataset. The horizontal axis represents the size of the training dataset, ranging from 1000 to 50000. The left y-axis represents the cosine similarity between the negative loss gradient and the model filters, while the right y-axis shows the test accuracy. The box plots display the distribution of cosine similarity values for different training dataset sizes, and the dashed line shows the trend of test accuracy as the amount of training data varies. As the size of the training dataset increases, both the median cosine similarity and the test accuracy show an upward trend, indicating that models trained with more data are able to learn more features."
    },
    "45": {
        "figure1": "2410.23496v1_Specificity",
        "label1": "fig:specificity",
        "caption1": "\\small The self-correction performance with instructions of various specificity levels on the Winogender benchmark. From specificity-1 to specificity-3, the specificity level increases.",
        "text": "For the \\textbf{CoT} setting, the 70B model demonstrates a positive gain with the CoT approach across all evaluated tasks, with CoT performance notably surpassing self-correction. Nonetheless, other scales of LLMs have varying performance given CoT explanations. For the 13B model, CoT cause a performance decrease compared to self-correction, but CoT helps 7B model acquire better performance among religion and physical bias dimensions, similar phenomenon is observed for the 1B model as well. The 3.8B model only has better performance with CoT on the physical bias but the CoT performance is marginally better than that of self-correction. Therefore, we can conclude that \\textit{LLMs, with less than 70B parameters, can not give informative explanations based on their CoT capability w.r.t. morality-relevant questions.} In the Appendix~\\ref{app:cotexample}, we show an example about the CoT explanation from llama2-7B.     \\centering     \\small     \\includegraphics[width=0.75\\linewidth]{Specificity.png}     \\caption{\\small The self-correction performance with instructions of various specificity levels on the Winogender benchmark. From specificity-1 to specificity-3, the specificity level increases.}     \\label{fig:specificity}\nPer the dimension of \\textbf{specificity} shown in Figure~\\ref{fig:specificity}, the least specific instruction does help all model scales improve significantly, and the improvement is more apparent for the 3.8B and 7B models.  This indicates that \\textit{smaller models, with no less than 3.8B parameters, can understand abstract social norms of stereotyping.}  By increasing the specificity level from 1 to 2, the fairness performance of smaller models is further improved, while the change of the 70B version is slight since it is already very unbiased. This demonstrates that \\textit{more specific social norms in instructions can indeed help both small and large LLMs perform better self-correction}. Given the instruction (specificity-3) clearly containing a correct answer, all scales, except those less than 3.8B, can achieve a perfect fairness performance. This aligns with the conclusion from~\\citet{huang2023large} about \\textit{the significant effect of ground-truth answers in instructions}. Remarkably, the 70B model demonstrates a propensity to approach optimal fairness with regard to instruction of Specificity-2 (in the absence of access to the correct answer), thereby underscoring its proficiency in instruction following and understanding of social norms. Overall, \\textit{LLMs with scales no less than 3.8B can understand abstract social norms in the instruction and instructions with higher specificity levels indeed benefit intrinsic self-correction.}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the self-correction performance of language models of varying sizes on the Winogender benchmark with different specificity levels of instructions. The x-axis represents model size, ranging from 355M to 70B, and the y-axis represents fairness. There are four groups of bars, representing the baseline performance and the performance under three different specificity levels (Specificity-1, Specificity-2, Specificity-3). From Specificity-1 to Specificity-3, the specificity level of the instructions gradually increases. As the specificity level increases, the fairness performance of all models improves, with more apparent improvements for the 3.8B and 7B models. When using instructions containing the correct answer (Specificity-3), all models except those smaller than 3.8B achieve perfect fairness."
    },
    "46": {
        "figure1": "2410.23506v1_stargraph_results",
        "label1": "fig:stargraph_results",
        "caption1": "The Belief State Transformer outperforms baselines in all star graph navigation tasks.",
        "text": "\\centering     \\includegraphics[width=\\textwidth]{figures/stargraph_results.png}  \\caption{The Belief State Transformer outperforms baselines in all star graph navigation tasks.}     \\label{fig:stargraph_results}\nHere, we provide details on the Stargraph results in  \\autoref{fig:stargraph_results}. We run experiments on three types of graphs: $G({2, 5})$, $G({5,5})$, $G({2,20})$. In each experiment, we choose one graph topology and generate many example graph sequences, with all methods receiving the same amount of data and every baseline receiving at least as much computation as the Belief State Transformer uses.  For evaluation, the models are conditioned on the edge list, start, goal, and current path, with the task of next node prediction: $p(n_i \\mid \\mathcal{E}, n_1, n_l, n_{1:i-1})$. We report the path accuracy, which is the percentage of correct path generations during the test time, over 10,000 evaluation graphs.\nAs seen in \\cref{fig:stargraph_results}, the Belief State Transformer successfully learns to solve all the graphs.  Since the Belief State Transformer uses an empty suffix, the parameter counts at inference time are very similar with minor variations driven by variations in output heads.",
        "summarize_figure": "figure1",
        "summarization": "The image illustrates a comparison of success rates for four different methods (Forward-only, Data Aug, Teacherless, and Belief State) in star graph navigation tasks. The horizontal axis represents three different star graph structures: G(2,5), G(2,20), and G(5,5), while the vertical axis represents the success rate. The image shows that the Belief State method achieves the highest success rate of 100% across all three star graph structures. In the G(2,5) graph, the Teacherless method also achieves a 100% success rate, while the success rates of the Forward-only and Data Aug methods are close, at approximately 50%. In the G(2,20) graph, the success rates of the Forward-only and Data Aug methods are close to 50%. In the G(5,5) graph, the success rates of the Forward-only and Data Aug methods are lower, with Forward-only being slightly lower than Data Aug."
    },
    "47": {
        "figure1": "2410.23506v1_stargraph_ablations",
        "label1": "fig:stargraph_variants",
        "caption1": "Ablations of the Belief State Transformer on the star graph. Both the belief state objective and the backward encoder are crucial.",
        "text": "\\centering     \\includegraphics[width=\\textwidth]{figures/stargraph_ablations.png}  \\caption{Ablations of the Belief State Transformer on the star graph. Both the belief state objective and the backward encoder are crucial.}     \\label{fig:stargraph_variants}\nThe results in \\cref{fig:stargraph_variants} show that both the belief state objective and backward encoder are important components of the Belief State Transformer, and removing either drops performance back down to randomly guessing amongst valid paths. Prediction of the Prev token forces the Transformer to learn to represent long term dependencies which ends up being useful for goal-conditioned navigation.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates ablation study results of the Belief State Transformer on the star graph. The x-axis represents different graph structures: G(2,5), G(2,20), and G(5,5), and the y-axis represents the success rate. There are three groups of bar graphs, representing three different model configurations: Belief w/o Prev, Belief w/o Backward, and the complete Belief State model. For G(2,5) and G(2,20), the complete Belief State model achieves 100% success rate, Belief w/o Prev achieves about 50%, and Belief w/o Backward achieves about 45%-50%. For G(5,5), the complete Belief State model achieves 100% success rate, Belief w/o Prev achieves about 10%, and Belief w/o Backward achieves about 16%. The results indicate that the complete Belief State model shows the highest success rate on all graph structures, while removing Prev or Backward leads to a significant decrease in performance."
    },
    "48": {
        "figure1": "2410.23506v1_nextprev-probing-graph",
        "label1": "fig:stargraph_probing_graph",
        "caption1": "We probe the representations for the graph description tokens, and find that the Belief State embeddings contain more information.",
        "text": "\\centering     \\includegraphics[width=0.95\\textwidth]{figures/nextprev-probing-graph.pdf}  \\caption{We probe the representations for the graph description tokens, and find that the Belief State embeddings contain more information.}     \\label{fig:stargraph_probing_graph} The same probing technique can understand how well the Belief State successfully captures information about the graph description.   We found that the Belief State Transformer captures more information about the graph description than the Forward-Only model (\\autoref{fig:stargraph_probing_graph}).   Capturing the graph description completely is sufficient but not necessary for learning the belief state, since all the information in the graph description might not be used for predicting the future tokens.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the accuracy of graph description prediction. It compares the prediction accuracy under three conditions: without context (w/o context), using the next token (Next-token), and using the Belief State. The horizontal axis represents different token positions (from 1 to 16), and the vertical axis represents the prediction accuracy. It can be observed that the accuracy of prediction using the Belief State is significantly higher than that using the Next-token at most token positions, especially at token positions 1, 2, 5, 6, 7, 8, 15, and 16. This indicates that the Belief State embeddings contain more information about the graph description. In all cases, the accuracy without context information is close to 0."
    },
    "49": {
        "figure1": "2410.23507v1_t5-base",
        "label1": "fig:t5-base",
        "caption1": "$F_{0.5}$ scores of a T5-v1.1-Base model on the six most frequent error types and all error types (\\texttt{ALL}) in the BEA-2019 development set at different numbers of training steps.",
        "text": "It would be desirable to just have one model with multiple sub-networks that specialize in different aspects, such as which error types it can correct more accurately. The capability of a neural network to correct one error type is not necessarily transferable to correcting other error types, as \\citet{qorib-etal-2022-frustratingly_short} report that state-of-the-art GEC models are often less accurate in correcting certain error types than weaker GEC models. This could be caused by task interference during the training of the model. When fine-tuning T5-v1.1-Base for grammatical error correction, we observe that the model becomes less accurate at correcting punctuation (\\texttt{PUNCT}) and preposition (\\texttt{PREP}) error types when it has the best overall (\\texttt{ALL}) performance (Figure~\\ref{fig:t5-base}). We also notice that the model's accuracy in correcting preposition errors decreases when its accuracy in correcting punctuation errors increases (e.g., training step 1600 to 2000 in Figure~\\ref{fig:t5-base}).",
        "summarize_figure": "figure1",
        "summarization": "The image illustrates the F0.5 scores of a T5-v1.1-Base model on the BEA-2019 development set for six frequent error types (PUNCT, VERB, OTHER, DET, PREP, NOUN) and all error types (ALL) at different training steps. As shown, the F0.5 scores for various error types fluctuate with increasing training steps. DET performs well early in training, peaking around 500 steps. PUNCT maintains a high score throughout training, peaking around 1200 steps. PREP and NOUN scores are relatively stable but lower than PUNCT and DET. The ALL score also fluctuates during training and decreases slightly later. Notably, the model's accuracy in correcting PUNCT and PREP errors decreases when its overall ALL performance is optimal. Additionally, the accuracy in correcting PREP errors decreases as the accuracy in correcting PUNCT errors increases."
    },
    "50": {
        "figure1": "2410.23507v1_t5-large",
        "label1": "fig:t5-large",
        "caption1": "$F_{0.5}$ scores of a T5-v1.1-Large model on different error types in the BEA-2019 development set at different numbers of training steps.",
        "text": "We observe a similar indication of task interference that we explain in the introduction on the training of T5-v1.1-Large (Figure~\\ref{fig:t5-large}).",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the F0.5 scores of a T5-v1.1-Large model on different error types in the BEA-2019 development set at different numbers of training steps. There are seven curves in the figure, representing punctuation(PUNCT), verb(VERB), other(OTHER), determiner(DET), preposition(PREP), noun(NOUN), and all types(ALL) respectively. It can be seen that the F0.5 scores of punctuation, determiners and prepositions are relatively high and remain stable during the training process. The F0.5 scores of verbs, nouns and all types fluctuate in the initial stage of training, but then tend to stabilize. The F0.5 scores of other types are relatively low and do not change much during the training process."
    },
    "51": {
        "figure1": "2410.23510v1_chars",
        "label1": "figure:chars",
        "caption1": "The distribution of sentence lengths in characters in the \\textit{combined} corpus. The horizontal axis shows the sentence length in characters, the vertical axis shows the number of sentences in the resulting corpus having that length.",
        "text": "\\centering     \\includegraphics[width=1\\linewidth]{figures/chars.png}     \\caption{The distribution of sentence lengths in characters in the \\textit{combined} corpus. The horizontal axis shows the sentence length in characters, the vertical axis shows the number of sentences in the resulting corpus having that length.}     \\label{figure:chars}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the distribution of sentence lengths (in characters) in the combined corpus. The horizontal axis represents the sentence length (number of characters), and the vertical axis represents the number of sentences in the corpus with that length. The distribution shows that the sentence length is mainly concentrated in a short range, peaking at approximately 1.4 million sentences between 51 and 101 characters. As the sentence length increases, the number of sentences decreases rapidly. The number of sentences with a length of more than 201 characters decreases significantly, and the number of sentences with more than 301 characters is very small."
    },
    "52": {
        "figure1": "2410.23510v1_words",
        "label1": "figure:words",
        "caption1": "The distribution of sentence lengths in NLTK words in the \\textit{combined} corpus. The horizontal axis shows the sentence length in words after tokenising with NLTK, the vertical axis shows the number of sentences in the resulting corpus having that length.",
        "text": "\\centering     \\includegraphics[width=1\\linewidth]{figures/words.png}     \\caption{The distribution of sentence lengths in NLTK words in the \\textit{combined} corpus. The horizontal axis shows the sentence length in words after tokenising with NLTK, the vertical axis shows the number of sentences in the resulting corpus having that length.}     \\label{figure:words}",
        "summarize_figure": "figure1",
        "summarization": "Here is the summary of the figure:The image illustrates the distribution of sentence lengths in NLTK words within the combined corpus. The horizontal axis represents the sentence length in words after tokenization with NLTK, while the vertical axis represents the number of sentences in the corpus having that length. The distribution of sentence lengths shows a skewed distribution. The number of sentences peaks when the sentence length is around 10 to 20 words, reaching approximately 8.5 million sentences. As the sentence length increases, the number of sentences decreases rapidly. When the sentence length exceeds 50 words, the number of sentences approaches 0. This indicates that most sentences in the combined corpus are relatively short, and long sentences occur less frequently."
    },
    "53": {
        "figure1": "2410.23510v1_tokens",
        "label1": "figure:tokens",
        "caption1": "The distribution of sentence lengths in uncased BERT tokens in the \\textit{combined} corpus. The horizontal axis shows the sentence length in words after tokenising with the BERT tokeniser, the vertical axis shows the number of sentences in the resulting corpus having that length.",
        "text": "\\centering     \\includegraphics[width=1\\linewidth]{figures/tokens.png}     \\caption{The distribution of sentence lengths in uncased BERT tokens in the \\textit{combined} corpus. The horizontal axis shows the sentence length in words after tokenising with the BERT tokeniser, the vertical axis shows the number of sentences in the resulting corpus having that length.}     \\label{figure:tokens}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the distribution of sentence lengths in uncased BERT tokens within a combined corpus. The horizontal axis represents the sentence length in words after tokenization with the BERT tokenizer, while the vertical axis indicates the number of sentences in the corpus with that specific length. The curve shows that sentence lengths are mainly concentrated in a shorter range, with the number of sentences decreasing rapidly as the sentence length increases. The distribution peaks between approximately 10 and 30 token length, followed by a long-tail distribution, indicating that long sentences are less frequent in the corpus."
    },
    "54": {
        "figure1": "2410.23510v1_rasl_1",
        "label1": "figure:rasl1",
        "caption1": "The reconstruction accuracy of a model with $\\ell=1,m=1,d=768$ on the test set plotted against the token length of test sentences. The horizontal axis shows the sentence length in tokens, the vertical axis shows the mean reconstruction accuracy for that length.",
        "text": "\\centering     \\includegraphics[width=1\\linewidth]{figures/rasl_1.png}     \\caption{The reconstruction accuracy of a model with $\\ell=1,m=1,d=768$ on the test set plotted against the token length of test sentences. The horizontal axis shows the sentence length in tokens, the vertical axis shows the mean reconstruction accuracy for that length.}     \\label{figure:rasl1}",
        "summarize_figure": "figure1",
        "summarization": "The picture illustrates the relationship between the reconstruction accuracy of a model on the test set and the token length of test sentences, where l=1, m=1, and d=768. The horizontal axis represents the sentence token length, and the vertical axis represents the mean reconstruction accuracy. As shown, the reconstruction accuracy decreases as the sentence length increases. When the sentence length is short, the reconstruction accuracy is close to 100 percent, but as the sentence length increases, the accuracy gradually decreases, dropping to around 30 percent when the sentence length reaches 121."
    },
    "55": {
        "figure1": "2410.23510v1_rasl_2",
        "label1": "figure:rasl2",
        "caption1": "The reconstruction accuracy of a model with $\\ell=1,m=1,d=768$ on the test set plotted against the token length of test sentences. The horizontal axis shows the sentence length in tokens, the vertical axis shows the mean reconstruction accuracy for that length.",
        "text": "\\centering     \\includegraphics[width=1\\linewidth]{figures/rasl_2.png}     \\caption{The reconstruction accuracy of a model with $\\ell=1,m=1,d=768$ on the test set plotted against the token length of test sentences. The horizontal axis shows the sentence length in tokens, the vertical axis shows the mean reconstruction accuracy for that length.}     \\label{figure:rasl2}",
        "summarize_figure": "figure1",
        "summarization": "English:The image shows the reconstruction accuracy of a model on the test set plotted against sentence token length. For the model with l=2, m=1, d=768, the reconstruction accuracy is close to 100% for very short sentences, such as a sentence length of 1 token. However, as the sentence length increases, the reconstruction accuracy drops significantly. When the sentence length reaches approximately 121 tokens, the accuracy falls to between about 38% and 40%. This indicates that the reconstruction performance of this model decreases as the input sequence becomes longer."
    },
    "56": {
        "figure1": "2410.23510v1_rasl_3",
        "label1": "figure:rasl3",
        "caption1": "The reconstruction accuracy of a model with $\\ell=1,m=1,d=768$ on the test set plotted against the token length of test sentences. The horizontal axis shows the sentence length in tokens, the vertical axis shows the mean reconstruction accuracy for that length.",
        "text": "\\centering     \\includegraphics[width=1\\linewidth]{figures/rasl_3.png}     \\caption{The reconstruction accuracy of a model with $\\ell=1,m=1,d=768$ on the test set plotted against the token length of test sentences. The horizontal axis shows the sentence length in tokens, the vertical axis shows the mean reconstruction accuracy for that length.}     \\label{figure:rasl3}",
        "summarize_figure": "figure1",
        "summarization": "The picture illustrates the reconstruction accuracy of a model on the test set versus the token length of test sentences. The horizontal axis represents the sentence token length, and the vertical axis represents the mean reconstruction accuracy. It can be observed that the reconstruction accuracy is close to 100 percent when the sentence token length is short. As the sentence length increases, the reconstruction accuracy gradually decreases. When the sentence length exceeds 100 tokens, the reconstruction accuracy drops to approximately 50 percent."
    },
    "57": {
        "figure1": "2410.23511v1_decision-stacked-plot-2wiki",
        "label1": "fig:decision-analysis",
        "caption1": "Comparing the strategy planning distribution of various techniques with optimal policy for 2WikiQA.",
        "text": "\\centering     \\includegraphics[width=\\linewidth]{figures/decision-stacked-plot-2wiki.pdf}     \\caption{Comparing the strategy planning distribution of various techniques with optimal policy for 2WikiQA.}     \\label{fig:decision-analysis}\nWe compare the strategy planning distribution of various techniques with the optimal policy for 2WikiMultihopQA in Figure~\\ref{fig:decision-analysis}. The major difference is the usage of Plan and Reason which are nearly 0\\% for Fixed/Classifier approaches. On the other hand, \\modelName-base and \\modelVerifyName{} are closer to the optimal distribution. We quantify this proximity of the probability distributions in terms of KL divergence. This highlights the better strategy planning and stronger calibration of \\modelName{}.\nChoosing the right strategy in a 0-shot way is difficult, as models don't surely know what they know and don't know \\cite{yin2023largelanguagemodelsknow}. We analyze the 0-shot \\modelName{} strategy planning in Figure~\\ref{fig:decision-analysis} and note how the 0-shot model mostly resorts to the most expensive strategy, while fine-tuning helps to learn the patterns between questions and model capabilities. We also compare the model performance of 0-shot / few-shot \\modelName{} with fine-tuned \\modelName{} in Table~\\ref{tab:ablation-finetuning} for 2WikiMultihopQA. Clearly, the non-fine-tuned models fail to improve over the fixed strategy baseline, but fine-tuning provides strong performance gains - demonstrating how fine-tuning strongly improves calibration for strategy planning.",
        "summarize_figure": "figure1",
        "summarization": "This figure compares the strategy planning distribution of various techniques with the optimal policy for 2WikiQA. The figure shows the percentage distribution of strategy selection for different techniques (Optimal, DyPlan-verify, DyPlan-base, Classifier, 0-shot DyPlan, and Fixed Retrieval), including Direct, Plan, Reason, and Retrieval. The Optimal strategy has a certain percentage in Direct, Plan, and Reason, while Retrieval accounts for a small proportion. The strategy distribution of DyPlan-verify and DyPlan-base is closer to the Optimal strategy. The Classifier strategy is mainly concentrated on the Direct strategy. 0-shot DyPlan and Fixed Retrieval rely almost entirely on the Retrieval strategy."
    },
    "58": {
        "figure1": "2410.23511v1_generalizability-f1",
        "label1": "fig:combined-data-ablation",
        "caption1": "Assessing the generalizability of \\modelName{} by comparing the performance (F1 score) of combined-data training with individal-data training.",
        "text": "\\centering     \\includegraphics[width=\\linewidth]{figures/generalizability-f1.pdf}     \\caption{Assessing the generalizability of \\modelName{} by comparing the performance (F1 score) of combined-data training with individal-data training.}     \\label{fig:combined-data-ablation}\nTo assess the generalization of \\modelName, we fine-tune it on combined data from the three benchmark datasets. To ensure a fair comparison, the combined data comprises 20k datapoints (same as individual data) with equal shares from the three datasets. We compare the performance of this combined-data fine-tuned model with the individual-data fine-tuned model in Figure~\\ref{fig:combined-data-ablation} and note how the performances are nearly similar for both models. The cost analysis for the combined data model (Table~\\ref{tab:combined-data-cost-ablation} in \\S~\\ref{sec:appendix-combined-data}) reveals at-par levels of cost as well. Thus, this study reveals how the gains provided by \\modelName/\\modelVerifyName{} are generalizable and not overfitting to a single dataset.",
        "summarize_figure": "figure1",
        "summarization": "The figure compares the F1 scores of DyPlan and DyPlan-verify models trained on individual datasets versus combined datasets across three test datasets: HotpotQA, 2WikiQA, and Musique. For the HotpotQA dataset, the performance of models trained individually and with combined data is nearly the same. In the 2WikiQA dataset, the combined-data-trained DyPlan model performs slightly lower than the individually-trained model, while the DyPlan-verify model shows a slight improvement. On the Musique dataset, the combined-data-trained models perform slightly better than their individually-trained counterparts. Overall, the performance of combined-trained and individually-trained models is very close across all datasets, indicating strong generalization ability and no overfitting to a single dataset."
    },
    "59": {
        "figure1": "2410.23511v1_hotpotqa-venn",
        "label1": "fig:hotpotqa-venn",
        "caption1": "Breaking the contribution of each strategy combination to HotpotQA model performance. A strategy's inclusion in the set is indicated by the colored dot. \\textcolor{red}{Red} dots and bars indicate the hierarchy violations.",
        "figure2": "2410.23511v1_2wikimultihopqa-venn",
        "label2": "fig:2wiki-venn",
        "caption2": "Breaking the contribution of each strategy combination to 2WikiMultihopQA model performance. A strategy's inclusion in the set is indicated by the colored dot. \\textcolor{red}{Red} dots and bars indicate the hierarchy violations.",
        "figure3": "2410.23511v1_musique-venn",
        "label3": "fig:musique-venn",
        "caption3": "Breaking the contribution of each strategy combination to Musique model performance. A strategy's inclusion in the set is indicated by the colored dot. \\textcolor{red}{Red} dots and bars indicate the hierarchy violations.",
        "text": "\\centering     \\includegraphics[width=\\linewidth]{figures/hotpotqa-venn.pdf}     \\caption{Breaking the contribution of each strategy combination to HotpotQA model performance. A strategy's inclusion in the set is indicated by the colored dot. \\textcolor{red}{Red} dots and bars indicate the hierarchy violations.}     \\label{fig:hotpotqa-venn}\nSimilar to the study in \\S~\\ref{sec:methodology-motivation}, we quantify the F1 performance contribution of the hierarchy violations for all the four strategies using Llama3-8B-Instruct for the three datasets of HotpotQA, 2WikiMultihopQA and Musique in Figures~\\ref{fig:hotpotqa-venn}, \\ref{fig:2wiki-venn} and \\ref{fig:musique-venn}. These upset plots are a way of visualizing Venn diagrams, wherein each column is a unique combination of strategies, and the bar heights indicate its F1 contribution. The colored dots (black/red) indicate the presence of the corresponding strategy in the strategy combination set, while the grey dots indicate the absence. For example, in Figure~\\ref{fig:hotpotqa-venn}, the first bar indicates that there are about 9.5\\% questions that only Retrieval can correctly answer while all other methods fail. Similarly, the second bar indicates that more than 5\\% questions can only be answered by Direct and no other strategy. To distinctively show the hierarchy violations, we color-code them in \\textcolor{red}{red} in these plots.",
        "summarize_figure": "figure1",
        "summarization": "This is a graph showing the contribution of strategy combinations to the HotpotQA model performance. The figure displays the percentage contribution of various strategy combinations to the model's F1 performance. Each column represents a unique strategy combination, and the height of the bar indicates its F1 contribution. The dots in the figure indicate the inclusion of strategies, with red dots and bars indicating hierarchy violations. The leftmost bar indicates that about 12.5% of the questions can only be answered correctly by the Retrieval strategy, while all other methods fail. The rightmost bar indicates that using all strategies simultaneously can correctly answer about 15% of the questions."
    },
    "60": {
        "figure1": "2410.23511v1_2wikimultihopqa-venn",
        "label1": "fig:2wiki-venn",
        "caption1": "Breaking the contribution of each strategy combination to 2WikiMultihopQA model performance. A strategy's inclusion in the set is indicated by the colored dot. \\textcolor{red}{Red} dots and bars indicate the hierarchy violations.",
        "figure2": "2410.23511v1_hotpotqa-venn",
        "label2": "fig:hotpotqa-venn",
        "caption2": "Breaking the contribution of each strategy combination to HotpotQA model performance. A strategy's inclusion in the set is indicated by the colored dot. \\textcolor{red}{Red} dots and bars indicate the hierarchy violations.",
        "figure3": "2410.23511v1_musique-venn",
        "label3": "fig:musique-venn",
        "caption3": "Breaking the contribution of each strategy combination to Musique model performance. A strategy's inclusion in the set is indicated by the colored dot. \\textcolor{red}{Red} dots and bars indicate the hierarchy violations.",
        "text": "\\centering     \\includegraphics[width=\\linewidth]{figures/2wikimultihopqa-venn.pdf}     \\caption{Breaking the contribution of each strategy combination to 2WikiMultihopQA model performance. A strategy's inclusion in the set is indicated by the colored dot. \\textcolor{red}{Red} dots and bars indicate the hierarchy violations.}     \\label{fig:2wiki-venn}\nSimilar to the study in \\S~\\ref{sec:methodology-motivation}, we quantify the F1 performance contribution of the hierarchy violations for all the four strategies using Llama3-8B-Instruct for the three datasets of HotpotQA, 2WikiMultihopQA and Musique in Figures~\\ref{fig:hotpotqa-venn}, \\ref{fig:2wiki-venn} and \\ref{fig:musique-venn}. These upset plots are a way of visualizing Venn diagrams, wherein each column is a unique combination of strategies, and the bar heights indicate its F1 contribution. The colored dots (black/red) indicate the presence of the corresponding strategy in the strategy combination set, while the grey dots indicate the absence. For example, in Figure~\\ref{fig:hotpotqa-venn}, the first bar indicates that there are about 9.5\\% questions that only Retrieval can correctly answer while all other methods fail. Similarly, the second bar indicates that more than 5\\% questions can only be answered by Direct and no other strategy. To distinctively show the hierarchy violations, we color-code them in \\textcolor{red}{red} in these plots.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the contribution of various strategy combinations to the 2WikiMultihopQA model performance. Each bar represents a unique combination of strategies, and the bar height indicates the percentage of F1 contribution for that combination. The strategies included in the combination are indicated by colored dots: black dots indicate the inclusion of the strategy, red dots indicate hierarchy violations, and gray dots indicate the absence of the strategy. For example, the first bar shows that approximately 9.5% of the questions can be correctly answered using only the Retrieval strategy, while all other methods fail. The second bar indicates that over 5% of the questions can only be answered by the Direct strategy. Red bars and dots denote hierarchy violations."
    },
    "61": {
        "figure1": "2410.23511v1_musique-venn",
        "label1": "fig:musique-venn",
        "caption1": "Breaking the contribution of each strategy combination to Musique model performance. A strategy's inclusion in the set is indicated by the colored dot. \\textcolor{red}{Red} dots and bars indicate the hierarchy violations.",
        "figure2": "2410.23511v1_hotpotqa-venn",
        "label2": "fig:hotpotqa-venn",
        "caption2": "Breaking the contribution of each strategy combination to HotpotQA model performance. A strategy's inclusion in the set is indicated by the colored dot. \\textcolor{red}{Red} dots and bars indicate the hierarchy violations.",
        "figure3": "2410.23511v1_2wikimultihopqa-venn",
        "label3": "fig:2wiki-venn",
        "caption3": "Breaking the contribution of each strategy combination to 2WikiMultihopQA model performance. A strategy's inclusion in the set is indicated by the colored dot. \\textcolor{red}{Red} dots and bars indicate the hierarchy violations.",
        "text": "\\centering     \\includegraphics[width=\\linewidth]{figures/musique-venn.pdf}     \\caption{Breaking the contribution of each strategy combination to Musique model performance. A strategy's inclusion in the set is indicated by the colored dot. \\textcolor{red}{Red} dots and bars indicate the hierarchy violations.}     \\label{fig:musique-venn}\nSimilar to the study in \\S~\\ref{sec:methodology-motivation}, we quantify the F1 performance contribution of the hierarchy violations for all the four strategies using Llama3-8B-Instruct for the three datasets of HotpotQA, 2WikiMultihopQA and Musique in Figures~\\ref{fig:hotpotqa-venn}, \\ref{fig:2wiki-venn} and \\ref{fig:musique-venn}. These upset plots are a way of visualizing Venn diagrams, wherein each column is a unique combination of strategies, and the bar heights indicate its F1 contribution. The colored dots (black/red) indicate the presence of the corresponding strategy in the strategy combination set, while the grey dots indicate the absence. For example, in Figure~\\ref{fig:hotpotqa-venn}, the first bar indicates that there are about 9.5\\% questions that only Retrieval can correctly answer while all other methods fail. Similarly, the second bar indicates that more than 5\\% questions can only be answered by Direct and no other strategy. To distinctively show the hierarchy violations, we color-code them in \\textcolor{red}{red} in these plots.",
        "summarize_figure": "figure1",
        "summarization": "The figure shows the F1 performance contribution of various strategy combinations in the Musique model. The bottom of the chart uses dots to represent the strategy combinations: black dots indicate that the strategy is included in the combination, and gray dots indicate that it is not. The height of the bar indicates the percentage of the F1 value contributed by the strategy combination. Red bars and red dots indicate strategy combinations that violate the hierarchical structure. The first bar in the figure indicates that the contribution rate of the \"Retrieval\" strategy alone is about 4.8%. If the strategy combination includes the \"Direct\" strategy and there is a violation of the hierarchical structure, the contribution rate is about 2.4%."
    },
    "62": {
        "figure1": "2410.23511v1_decision-stacked-plot-hotpotqa",
        "label1": "fig:decision-analysis-hotpotqa",
        "caption1": "Comparing the strategy usage distribution of various techniques with the optimal policy distribution for HotpotQA.",
        "figure2": "2410.23511v1_decision-stacked-plot-musique",
        "label2": "fig:decision-analysis-musique",
        "caption2": "Comparing the strategy usage distribution of various techniques with the optimal policy distribution for Musique.",
        "text": "\\centering     \\includegraphics[width=\\linewidth]{figures/decision-stacked-plot-hotpotqa.pdf}     \\caption{Comparing the strategy usage distribution of various techniques with the optimal policy distribution for HotpotQA.}     \\label{fig:decision-analysis-hotpotqa}\nWe discussed how \\modelName{} helps to better calibrate decision-making in terms of strategy selection for the 2WikiMultihopQA dataset in \\S~\\ref{sec:decision-making-analysis}. Here, we show similar analysis for the other datasets of HotpotQA and Musique in Figures~\\ref{fig:decision-analysis-hotpotqa} and \\ref{fig:decision-analysis-musique}. Similar to our earlier findings, we notice the \\modelName{} helps the strategy usage to be more similar to the optimal policy, in turn helping to improve model performance. We notice for HotpotQA, \\modelName{} is similar to the external classifier.",
        "summarize_figure": "figure1",
        "summarization": "The picture illustrates the strategy usage distribution of various techniques on the HotpotQA dataset, comparing them with the optimal policy distribution. The techniques include Optimal, DyPlan-verify, DyPlan-base, Classifier, 0-shot DyPlan, and Fixed Retrieval. The horizontal axis represents the percentage of strategy selection, and the vertical axis represents different techniques. It can be seen that the Fixed Retrieval technique relies almost entirely on the retrieval strategy, while other techniques combine direct answering, planning, reasoning, and retrieval strategies to varying degrees. In the optimal policy distribution, the direct answering strategy occupies a large proportion, followed by the retrieval strategy, and the planning and reasoning strategies account for a smaller proportion. The DyPlan-verify strategy is closer to the optimal strategy in strategy selection."
    },
    "63": {
        "figure1": "2410.23511v1_hotpotqa-direct-cot-venn",
        "label1": "fig:2wiki-direct-reason-venn",
        "caption1": "Venn Diagram representing the F1 contribution of Direct and Reason strategies for HotpotQA.",
        "text": "\\centering     \\includegraphics[width=0.8\\linewidth]{figures/hotpotqa-direct-cot-venn.pdf}     \\caption{Venn Diagram representing the F1 contribution of Direct and Reason strategies for HotpotQA.}     \\label{fig:2wiki-direct-reason-venn}\nDynamic strategy planning can help reduce inference costs by simply selecting lower-cost strategies to answer simpler questions. In our work, we additionally posit that it can improve model performance as strategy selection can act as an ensembling method. We verify this hypothesis through a simple analysis using two strategies of \\textit{Direct} and \\textit{Reason}. Direct prompts the LLM to directly provide the answer to the question while Reason utilizes Chain-of-Thought \\cite{chain-of-thought} to answer step-by-step. Utilizing these strategies, we prompt Llama3-8B-Instruct \\cite{llama3}, evaluate using F1 score on 1000 samples from 2WikiMultihopQA \\cite{ho-etal-2020-constructing} dataset, and show their performances as a Venn diagram in Figure~\\ref{fig:2wiki-direct-reason-venn}. Generally, one can assume that adding reasoning should only allow us to further answer harder questions while still being able to answer all questions that Direct strategy answers correctly. However, there is a significant contribution of 7.1\\% F1 where Reason is incorrect, but Direct is correct (for example, a mistake or hallucination can lead to an incorrect result). We notice similar patterns across datasets and strategies (shown in \\S~\\ref{sec:appendix-hierarchy-violations}). Such patterns throw light on how the choice of an appropriate strategy can also improve model performance.",
        "summarize_figure": "figure1",
        "summarization": "Here is the summary of the figure:The figure presents a Venn diagram illustrating the F1 score contribution of Direct and Reason strategies on the HotpotQA dataset. The unique contribution of the Direct strategy is 7.1, while the unique contribution of the Reason strategy is 13.2. The shared contribution of both strategies, meaning the questions they both answer correctly, is 23.7. Additionally, there are 56.0 questions that neither strategy can answer correctly."
    },
    "64": {
        "figure1": "2410.23511v1_main-results-avg",
        "label1": "fig:main-avg-plot",
        "caption1": "Performance v/s Inference Efficiency for various techniques. Our technique \\modelName{} (\\textcolor{darkgreen}{in green}) provides the best performance while also reducing the inference costs relative to Fixed-sft Retrieval baseline.",
        "text": "\\centering     \\includegraphics[width=\\linewidth]{figures/main-results-avg.pdf}     \\caption{Performance v/s Inference Efficiency for various techniques. Our technique \\modelName{} (\\textcolor{darkgreen}{in green}) provides the best performance while also reducing the inference costs relative to Fixed-sft Retrieval baseline.}     \\label{fig:main-avg-plot}\nWe present our main results comparing \\modelName{} utilizing all the strategies with other baselines in Table~\\ref{tab:main-results}. We utilize the best-performing Fixed-sft Retrieval model as the reference baseline for comparisons. We also aggregate these metrics across datasets and plot Performance (F1 score) v/s Efficiency (weighted sum of \\# T and \\# R)\\footnote{Weights are determined based on pricing of input and output tokens for GPT4o-mini.} in Figure~\\ref{fig:main-avg-plot}.",
        "summarize_figure": "figure1",
        "summarization": "English:The figure plots performance measured by F1 score against inference efficiency for various techniques, with the efficiency axis on a log scale. The green stars in the plot represent the DyPlan and DyPlan-verify techniques. Fixed-sft Retrieval serves as a reference baseline. DyPlan-verify achieves the highest F1 score among all techniques shown. Compared to the Fixed-sft Retrieval baseline, both DyPlan and DyPlan-verify demonstrate superior performance; specifically, DyPlan achieves higher performance at lower efficiency than the baseline, while DyPlan-verify reaches the highest performance at an efficiency level similar to the baseline. Other techniques display different trade-offs between performance and efficiency."
    },
    "65": {
        "figure1": "2410.23522v1_fig_noise",
        "label1": "fig_noisematch",
        "caption1": "Mean matching performance for scenes with varying viewpoint and illumination. Operating R2D2 on the original HPatches dataset with high SNR (black) acts as a baseline. Operating R2D2 on the generated HPatches robotic burst common images fails to produce accurate matches for moderate (orange) and strong (red) noise. Our method outperforms R2D2 in producing improved feature matches in both moderate (cyan) and strong (blue) noise.",
        "text": "\\centering     \\includegraphics[width=\\linewidth]{Figures/fig_noise.pdf}     \\caption{Mean matching performance for scenes with varying viewpoint and illumination. Operating R2D2 on the original HPatches dataset with high SNR (black) acts as a baseline. Operating R2D2 on the generated HPatches robotic burst common images fails to produce accurate matches for moderate (orange) and strong (red) noise. Our method outperforms R2D2 in producing improved feature matches in both moderate (cyan) and strong (blue) noise.}     \\label{fig_noisematch}\nMatching accuracy demonstrates the ability of a model to correctly match pixel correspondences between images of the same scene. We compare the mean matching accuracy (MMA) for various pixel error thresholds \\cite{dusmanu2019d2net} using our feature finder on generated low-light robotic bursts and single-image based R2D2 on corresponding common images for all 108 scenes as in \\autoref{fig_noisematch}. Ground truth is computed by running R2D2 on high-SNR images (black). Our method outperforms R2D2 in matching performance at all thresholds under moderate and strong noise, though performance declines with increasing noise.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the mean matching performance for scenes with varying viewpoint and illumination. The black line represents the baseline performance of R2D2 on the original HPatches dataset (high SNR). The orange and red lines show the performance of R2D2 on generated HPatches robotic burst common images with moderate (sigma noise=0.2) and strong (sigma noise=0.8) noise, respectively, indicating a significant drop in matching accuracy. The cyan and blue lines represent the performance of our method under moderate and strong noise, outperforming R2D2. Our method exhibits better matching performance at all thresholds under both noise levels, although performance decreases with increasing noise."
    },
    "66": {
        "figure1": "2410.23526v1_frequency_and_factcheck_score_samples",
        "label1": "fig:fact_check_results",
        "caption1": "Frequency of selected answer options and their corresponding fact-check scores across multiple samples from the USMLE-MedQA dataset using the Llama 3 70B Instruct model. For each question, ten responses were generated with a temperature setting of 1.2. The fact-check system assigned scores to each option, with higher scores indicating higher factual accuracy. The correct answers, highlighted in gold, consistently received higher fact-check scores. ",
        "text": "To evaluate the effectiveness of our fact-checking system, we conducted experiments using the Llama 3 70B Instruct model on several samples of the USMLE-MedQA dataset. For each question, ten responses were generated with a temperature setting of 1.2. These responses were subsequently evaluated using our fact-checking system. The figure \\ref{fig:fact_check_results} displays the frequency of each answer option along with the average fact-check score assigned to those options. Notably, the fact-check scores tend to be higher for the correct answers, which are highlighted in gold. This visualization illustrates the correlation between the frequency of selected options and their factual accuracy, as determined by the fact-checking system. The results demonstrate that the fact-checking system can reliably identify and score correct responses, supporting its utility in enhancing the factual accuracy of model outputs.\nTo demonstrate the effectiveness of our fact-checking system, we conducted experiments using the Llama 3 70B Instruct model on multiple samples from the USMLE-MedQA dataset. Figure \\ref{fig:fact_check_results} illustrates the results of these experiments, showing the frequency of selected answer options and their corresponding fact-check scores.\n\\centering     \\includegraphics[width=0.7\\textwidth]{figs/frequency_and_factcheck_score_samples.pdf}     \\caption{Frequency of selected answer options and their corresponding fact-check scores across multiple samples from the USMLE-MedQA dataset using the Llama 3 70B Instruct model. For each question, ten responses were generated with a temperature setting of 1.2. The fact-check system assigned scores to each option, with higher scores indicating higher factual accuracy. The correct answers, highlighted in gold, consistently received higher fact-check scores. }     \\label{fig:fact_check_results}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the frequency of selected answer options and their corresponding fact-check scores across multiple samples from the USMLE-MedQA dataset using the Llama 3 70B Instruct model. For each question, ten responses were generated with a temperature setting of 1.2. The figure shows the results for Sample 1, where the x-axis represents answer options (A, B, C, D, and E), the left y-axis represents frequency, and the right y-axis represents the fact-check score. The correct answer E, highlighted in gold, has a fact-check score of 1.0 and a frequency of 1. The other options (A, B, C, and D) have varying frequencies, but all have lower fact-check scores."
    },
    "67": {
        "figure1": "2410.23530v1_samples_cifar10",
        "label1": "fig:training_sim_cifar",
        "caption1": "Similarity of the generations sampled from the same random noise at different stages of diffusion model's training to the final outputs for DDPM (CIFAR-10). We plot CKA, SVCCA, SSIM, and DINO image alignment metrics and show that the diffusion model already learns the mapping between Gaussian noise and generations at the beginning of the training.",
        "text": "In~\\Cref{fig:training_sim_cifar}, we visualize how the diffusion model learns the low-frequency features of the image already at the beginning of the fine-tuning when comparing generations from the next training steps against the generations after finished fine-tuning using four image-alignment metrics: CKA, DINO, SSIM, and SVCCA for the DDPM model trained on the CIFAR-10 dataset.     \\centering     \\includegraphics[width=0.74\\linewidth]{Figures/samples_cifar10.pdf}",
        "summarize_figure": "figure1",
        "summarization": "Here is a summary generated based on the chart information you provided:The figure shows the similarity between generations sampled from the same random noise at different stages of diffusion model's training and the final outputs. The graph plots four image alignment metrics: CKA, SVCCA, SSIM, and DINO. The horizontal axis represents the training steps, and the vertical axis represents the degree of image alignment. It can be seen that the diffusion model has already learned the mapping between Gaussian noise and the generated results in the early stage of training. The CKA metric has the highest image alignment, followed by SSIM, while SVCCA and DINO have relatively lower image alignment. As the number of training steps increases, the image alignment of all metrics shows an upward trend and stabilizes in the later stage."
    },
    "68": {
        "figure1": "2410.23535v1_exp2_more_final",
        "label1": "fig:da-analysis",
        "caption1": "Distribution of F-1 Scores across different Dialogue Acts. Robot-only dialogue acts are omitted.}  \\vspace*{-2ex",
        "text": "The distribution of F1 scores across dialogue acts as shown in Figure \\ref{fig:da-analysis} reveals notable patterns in the performance of GPT-4 and Llama 3.1 models under zero-shot (ZS) and few-shot (FS) approaches. GPT-4 consistently outperforms Llama 3.1, with its few-shot implementation showing the highest F1 scores across most dialogue acts. This suggests that GPT-4 benefits significantly from task-specific examples. Both models demonstrate strengths in common dialogue acts such as 'Instruction' and 'InformationOnObjectDetails', indicating their proficiency in task-oriented communication. However, there's a marked performance drop for more nuanced or less frequent acts. 'AlternateQuestions', 'RequestForInstruction', and 'RequestForObjectLocationAndOtherDetails' prove particularly challenging, especially for Llama 3.1, highlighting the difficulty in capturing complex query structures. Interestingly, 'Acknowledge' and 'Greetings/Salutations' show high variance across models and training approaches, suggesting that seemingly simple acts can be context-dependent and thus harder to predict consistently. The acts 'RequestMore', 'RequestOtherInfo', and 'OtherInterfaceComment' have near-zero F1 scores for all models, as expected because these dialogue acts are typically reserved for the robot, as shown in Section \\ref{app:ex:da-exp}. The zero-shot performance of GPT-4 is notably robust, often rivaling its few-shot counterpart, which underscores its strong pre-training and generalization capabilities. This variance in performance across dialogue acts suggests that future improvements in dialogue systems may benefit from act-specific optimization strategies, particularly focusing on the more challenging and nuanced dialogue acts.",
        "summarize_figure": "figure1",
        "summarization": "This is a distribution chart of F1 scores across various dialogue acts, showing the performance of GPT 4o and Llama 3.1 models under zero-shot (ZS) and few-shot (FS) approaches. GPT 4o outperforms Llama 3.1 in most dialogue acts, with its few-shot implementation showing the highest F1 scores. Both models demonstrate strengths in common dialogue acts like \"Instruction\" and \"InformationOnObjectDetails\". However, there's a marked performance drop for more nuanced or less frequent acts. \"Acknowledge\" and \"Greetings/Salutations\" show high variance across models and training approaches."
    },
    "69": {
        "figure1": "2410.23535v1_move_actions_impact_cr",
        "label1": "fig:move",
        "caption1": "Impact of Move Actions on GPT-4 Performance. The figure illustrates the effect of including move actions user behavior modeled by GPT-4 in this case. The results are evaluated using Speak-F1 (blue) and DA Accuracy (green) metrics.  ",
        "text": "The impact of move actions on GPT-4's performance is strikingly illustrated in Figure \\ref{fig:move}. When move actions are included, the Speak F1 scores remain relatively low and consistent between zero-shot (27.04\\%) and few-shot (26.79\\%) approaches, indicating that the presence of move actions significantly hampers the model's ability to accurately predict speaking turns. However, the exclusion of move actions leads to a dramatic improvement in Speak F1 scores, jumping to 42.03\\% for zero-shot and 43.39\\% for few-shot scenarios. This substantial increase suggests that move actions introduce noise that confuses the model's decision-making process for speech prediction. The impact on Dialogue Act (DA) Accuracy is even more pronounced, with few-shot learning showing considerable advantages. In the presence of move actions, few-shot learning improves DA Accuracy from 29.04\\% to 41.68\\%, while in their absence, it rises from 40.82\\% to an impressive 54.06\\%. We also looked at the selective removal of move acts in a heuristic fashion; for more analysis of the impact of move actions and their selective removal \\ref{app:ex:speakornot}",
        "summarize_figure": "figure1",
        "summarization": "The picture illustrates the impact of move actions on GPT-4 performance, with Speak F1 (blue) and DA Accuracy (green) as evaluation metrics. When move actions are included, the Speak F1 scores for Zero-Shot and Few-Shot are relatively low and consistent, at 27.04% and 26.79% respectively. Excluding move actions leads to a significant increase in Speak F1 scores, reaching 42.03% for Zero-Shot and 43.39% for Few-Shot. With the inclusion of move actions, Few-Shot learning improves DA Accuracy from 29.04% to 41.68%, and with the exclusion of move actions, DA Accuracy improves from 40.82% to 54.06%."
    },
    "70": {
        "figure1": "2410.23537v1_motivation",
        "label1": "fig:motivation",
        "caption1": " End-to-end performance comparison of existing FCFS scheduling and speculative scheduling in ALISE on the ShareGPT dataset. ",
        "text": "To showcase such HoL blocking issues, we run the OPT-13B~\\cite{opt} model on a single NVIDIA V100 GPU on the ShareGPT dataset~\\cite{shareGPT} with varying sequence lengths to benchmark the baseline FCFS and \\ourmethod. Figure~\\ref{fig:motivation} shows the end-to-end latency comparison of both methods. As the request rate continually increases, FCFS induces much higher response latency due to inflexible non-preemptive scheduling. In contrast, \\ourmethod, with accurate and timely information about sequence length and execution time, can schedule the workloads appropriately, maximizing device usage, and providing up to $2\\times$ better throughput under the same latency constraints.",
        "summarize_figure": "figure1",
        "summarization": "Here is a summary generated based on the chart information you provided:The picture compares the end-to-end performance of FCFS (first-come, first-served) scheduling and ALISE (a speculative scheduling method) on the ShareGPT dataset. The horizontal axis of the graph is the request rate (req/s), and the vertical axis is the normalized latency (ms/token). The latency of the FCFS method increases rapidly with the increase of the request rate, and the latency exceeds 200 ms/token when the request rate reaches 0.72 req/s. In contrast, the latency of the ALISE method increases more slowly, and the latency does not exceed 200 ms/token even when the request rate reaches 1.52 req/s. This shows that the ALISE method can schedule workloads more efficiently when dealing with high request rates, thereby reducing latency."
    },
    "71": {
        "figure1": "2410.23537v1_profile_new",
        "label1": "fig:breakdown",
        "caption1": " Execution breakdown of the OPT-13B. The left figure shows the prefill execution time with different input lengths ($s$), and the right figure shows the execution time for different decoding steps.",
        "text": "To further study the relationship between the sequence length and the execution latency of each stage, we perform an inference benchmark for OPT-13B under different input sequence settings. As shown in Figure~\\ref{fig:breakdown}, we can see that 1) the prefilling execution time $T_{pre}(s)$ increases almost linearly with input length $s$. Coincidentally, the decoding latency exhibits a similar linear relationship concerning input length $s$, and the decoding latency $T_{dec}(s,n)$ for different iterations does not exhibit any noticeable changes, largely due to the KV cache.  Based on these observations, we propose to estimate the prefilling and decoding latency as the following:      T_{pre}(s) &\\approx s \\cdot T_{0} \\label{eq:4} \\\\     T_{dec}(s,n)  &\\approx n \\cdot ( \\alpha s + \\beta) \\label{eq:5} where $\\{T_{0}, \\alpha, \\beta\\}$ are the linear regression coefficients that can be determined by profiling numerous samples of prefilling latency of different input and output sequence lengths.",
        "summarize_figure": "figure1",
        "summarization": "The left figure shows the prefill execution time of the OPT-13B model with different input lengths. The horizontal axis represents the input length, with values of 64, 128, 256, 512, 768, and 1024, and the vertical axis represents the execution time in seconds. It can be seen that the execution time increases monotonically with the increase of input length, especially after the input length exceeds 512, the increasing trend of execution time is more obvious."
    },
    "72": {
        "figure1": "2410.23537v1_main_results",
        "label1": "fig:main",
        "caption1": " End-to-end performance comparison of \\ourmethod and baselines, including ORCA~\\cite{orca}, vLLM~\\cite{vLLM}, and the Oracle on the Alpaca~\\cite{alpaca} and ShareGPT dataset~\\cite{shareGPT}. ",
        "text": "The first row of Figure~\\ref{fig:main} shows the results on the Alpaca dataset. We can see that as the request rate increases, the normalized latency gradually increases but then explodes after a certain threshold.  Such observation aligns with previous works~\\cite{orca,vLLM,flexgen, alisa}, where LLM inference is often \\textit{memory-bound}, and once the request rate reaches the memory capacity of the serving system, the response latency grows infinitely as the request queue pool continues to expand. On the Alpaca dataset, \\ourmethod obtains $1.3\\sim1.8\\times$ higher throughput compared to vLLM while maintaining similar latency results. This is due to \\ourmethod's speculative scheduling strategy that dynamically adjusts the priority order of each request thus reducing the response delay. For instance, as shown in Figure~\\ref{fig:main}(b), under the latency constraints of 200 ms, \\ourmethod processes $1.5\\times$ more requests than vLLM. When compared against ORCA, \\ourmethod sustains up to $3.1\\times$ higher request rates. The advantage of \\ourmethod is more pronounced on the ShareGPT dataset, as shown in the second row of Figure~\\ref{fig:main}.  This is because the ShareGPT dataset contains longer input prompts and outputs on average than the Alpaca dataset, with much higher variance. Compared to ORCA, \\ourmethod also obtains up to $4.3\\times$ high throughput under similar latency constraints.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the end-to-end performance comparison of ALISE, ORCA, vLLM, and Oracle on the Alpaca dataset for OPT-2.7B, OPT-6.7B, and OPT-13B models. The x-axis represents the request rate (req/s), and the y-axis represents the normalized latency (ms/token). As the request rate increases, the normalized latency gradually increases, exploding after a certain threshold. ALISE achieves approximately 1.3x to 1.8x higher throughput compared to vLLM while maintaining similar latency."
    },
    "73": {
        "figure1": "2410.23537v1_dist",
        "label1": "fig:dist",
        "caption1": "Input and output length distribution of the Alpaca and ShareGPT datasets.",
        "text": "To emulate the LLM workloads, we synthesize traces of user requests based on the Alpaca~\\cite{alpaca} and ShareGPT~\\cite{shareGPT} dataset since there is no publicly available request trace for LLMs.  The Alpaca dataset is a GPT-3.5 generated instruction dataset and is widely used for LLM inference evaluation~\\cite{it1,vLLM}.  The ShareGPT dataset is a public dataset that collects the chatbot conversation from ChatGPT users, which exhibit greater variance in contents and sequence length, as shown in Figure~\\ref{fig:dist}. We tokenize each dataset and use their input and output lengths to simulate real-world user requests. In terms of request timestamps, we generate the arrival time based on the Poisson distribution.",
        "summarize_figure": "figure1",
        "summarization": "The image presents the input and output sequence length distributions for the Alpaca and ShareGPT datasets. For the Alpaca dataset, the average input and output sequence lengths are 59.8 and 267.5 respectively, with sequence length distributions primarily concentrated in shorter ranges. In contrast, the ShareGPT dataset shows average input and output lengths of 137.8 and 956.2 respectively. The distributions for ShareGPT's input and output sequences are notably longer, particularly the output sequence distribution which exhibits a wider range, indicating the ShareGPT dataset demonstrates greater variance in content and sequence length."
    },
    "74": {
        "figure1": "2410.23537v1_kv_ab",
        "label1": "fig:kv_ab",
        "caption1": " Latency comparison of different memory management protocols in \\ourmethod on the Alpaca dataset. ",
        "text": "We perform the experiments on the OPT models with varying request rates on the Alpaca dataset.  As shown in Figure~\\ref{fig:kv_ab}, when the request rate is relatively low, the gap between \\ourmethod and the other two methods is not significant, as the KV cache for most jobs is in the GPU. However, as the request rate increases, the gap between these solutions is widened: For the \\textit{Defer} strategy, with a higher request rate, the HoL blocking issues resurface, as more newly arrived jobs with shorter execution times are delayed, thus leading to increased response latency; for the \\textit{Recompute} strategy, the increased latency is largely attributed to the increased execution time for token generation, since for scheduled preempted jobs, the system has to recompute the KV cache of previous iterations to generate the subsequent tokens.",
        "summarize_figure": "figure1",
        "summarization": "Here is a summary of the figure:The image shows a latency comparison of different memory management protocols (Recompute, Defer, ALISE) for the OPT model (OPT-2.7B) on the Alpaca dataset. The x-axis represents the request rate (req/s), and the y-axis represents the normalized latency (ms/token). As the graph shows, when the request rate is relatively low, the gap between ALISE and the other two methods is not significant. As the request rate increases, the latency of all three methods increases, but the increase in ALISE is significantly less than that of Recompute and Defer. ALISE exhibits the lowest latency at high request rates."
    },
    "75": {
        "figure1": "2410.23556v1_fig07_forgery_attribution",
        "label1": "tab_fg_performance",
        "caption1": "The forgery attribute classification results.",
        "text": "\\centering     \\includegraphics[width=0.45\\textwidth]{fig07_forgery_attribution.pdf}     \\caption{The forgery attribute classification results.}     \\label{tab_fg_performance}\nWe perform the fine-grained classification among real images and $13$ forgery categories on $4$ different levels, and the most challenging scenario is the fine-grained classification on the $4$-th level. The result is reported in Fig.~\\ref{tab_fg_performance}. Specifically, we train HiFi-Net $4$ times, and at each time, only classifies the fine-grained forgery attributes at one level, denoted as \\textit{Baseline}.  Then, we train a HiFi-Net~\\cite{guo2023hierarchical} to classify all $4$ levels but without the hierarchical dependency via Eq.~\\ref{eq_conditional_prob}, denoted as \\textit{multi-scale}.  Also, we compare the pre-trained image attribution works~\\cite{asnani2021reverse,yu2019attributing_image_attribute} and the new proposed HiFi-Net++. Both HiFi-Net and HiFi-Net++ have improvements over previous works~\\cite{yu2019attributing_image_attribute,asnani2021reverse}, which we believe is because the previous works only learn to attribute CNN-synthesized images, yet do not consider attributing image editing methods.  Again, the performance difference between HiFi-Net and HiFi-Net++ is not large since the language-guided forgery localization enhancer mainly improves the localization performance. In this work, we develop an IFDL method for both CNN-synthesized and image-editing forgery domains. We formulate the IFDL as a hierarchical fine-grained classification problem that requires the algorithm to classify the individual forgery method of given images via predicting the entire hierarchical path. Moreover, we improve our algorithm generalization ability and performance on localizing small-region manipulations via a language-guided forgery localization enhancer. Lastly, HiFi-IFDL dataset is proposed to further help the community in developing forgery detection algorithms.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the forgery attribute classification results, using the F1 score as the evaluation metric. The F1 score for the Attribute method is 31.11, for FEN-Net it is 48, for the Baseline method it is 75, for the Multi-scale method it is 80, for HiFi-Net it is 87.63, and for HiFi-Net++ it is 88.02. The results indicate that HiFi-Net++ performs the best in the forgery attribute classification task, slightly outperforming HiFi-Net, while the Attribute method has relatively low performance."
    },
    "76": {
        "figure1": "2410.23558v1_judges",
        "label1": "fig:judges",
        "caption1": "Evaluation scores of adopting different judgment models. We use the same target model and evaluation instructions.",
        "text": "In our experiments, we observed that the prompt designed for configuring a LLM as a judge, as provided by the competition organizers, exhibits strong transferability.  We applied this identical judgment prompt across various commercial and open-source LLMs.  As illustrated in Figure~\\ref{fig:judges}, the ratings generated by different judge LLMs were both stable and closely correlated, thereby validating the efficacy of the official judgment process.",
        "summarize_figure": "figure1",
        "summarization": "The image shows a comparison of evaluation scores across different models. GLM-4-Plus, Qwen-Max-Latest, and Llama-3-8B-Instruct models achieve the highest scores, all at 0.90. DeepSeek-V2.5 scores 0.87, slightly lower than the top three. The GLM-4-Flash model has the lowest score, at 0.83."
    },
    "77": {
        "figure1": "2410.23558v1_Score_diff",
        "label1": "fig:Score_diff",
        "caption1": "Diagram of Jailbreak Score distribution",
        "text": "Through our intensive local experiments, we have observed that not all of the provided malicious instructions exhibit the same difficulty in jailbreaking the target LLM.  To illustrate this effect, we present the distribution of jailbreak scores for all malicious instructions, as evaluated by the Llama3-8B-Instruct model, in Figure~\\ref{fig:Score_diff}.  To further enhance jailbreak performance within limited computational budgets, we propose to iteratively optimize the instructions with lower scores, allocating additional computational resources to them.",
        "summarize_figure": "figure1",
        "summarization": "The figure shows the distribution of jailbreak scores for malicious instructions using TAP, PAP, and Ours methods, with score range on the x-axis and frequency on the y-axis. In the score range of 0.8-1.0, the frequency of the Ours method is significantly higher than that of the TAP and PAP methods, indicating that the Ours method performs better in this score range. In the score range of 0-0.2, the frequency of the PAP method is significantly higher than that of the TAP and Ours methods. In the score range of 0.6-0.8, the TAP method is slightly higher than the PAP method, while the frequency of the Ours method is very low in this range."
    },
    "78": {
        "figure1": "2410.23570v1_9",
        "label1": "Fig9",
        "caption1": "Influence of the number of hierarchies on model performance.",
        "text": "\\centering \t\\includegraphics[width=0.75\\linewidth]{9.png}  \t\\caption{Influence of the number of hierarchies on model performance.} \t\\label{Fig9}\nIn the proposed method, the number of decoupling phrases in the text corresponds to the number of hierarchies. On the test and validation sets of the ReferItGame and RefCOCOg datasets, we analyze the influence of the number of hierarchies on the prediction accuracy of the proposed method. Fig.~\\ref{Fig9} describes the effect of the number of hierarchies on model performance. As shown in Fig.~\\ref{Fig9}, as the number of hierarchies increases, the prediction accuracy of the model gradually improves. It indicates that the information extracted from each hierarchical phrase has been successfully leveraged, thereby improving the model performance.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the influence of the number of hierarchies on model performance. The horizontal axis represents the number of hierarchical phrases, and the vertical axis represents the precision (Prec@0.5) in percentage. The figure includes four curves, representing the performance of the ReferItGame dataset on the test and validation sets, and the RefCOCOg dataset on the test and validation sets. As shown in the figure, as the number of hierarchical phrases increases, the precision of the model on the four datasets generally shows an upward trend. Specifically, the ReferItGame dataset has the highest precision on the validation set, and it tends to stabilize after the number of hierarchies reaches 5. The RefCOCOg dataset has the second highest precision on the validation set, and it also tends to stabilize after the number of hierarchies reaches 9. The precision of the ReferItGame dataset on the test set tends to stabilize after the number of hierarchies reaches 3. The precision of the RefCOCOg dataset on the test set is relatively low, but still shows an upward trend."
    },
    "79": {
        "figure1": "2410.23578v1_framework",
        "label1": "fig:diagram",
        "caption1": "Architecture of the automated framework for quantum flakiness detection.",
        "text": "\\centering     \\includegraphics[width=1.0\\linewidth]{framework.png}     \\caption{Architecture of the automated framework for quantum flakiness detection.}     \\label{fig:diagram}\nOur experiments demonstrate an automated LLM-based framework (see Figure~\\ref{fig:diagram}) that efficiently gathers resources, configures inputs, and classifies bugs by streamlining the standard software development workflow with GitHub as the version control system. This framework simulates a typical software engineering process for bug resolution.",
        "summarize_figure": "figure1",
        "summarization": "The picture illustrates the architecture of an automated framework for quantum flakiness detection. Starting from harvested quantum repositories, the framework undergoes repository extract preprocess, including data extraction. The preprocess involves two main steps: segmentation and structured conversations. The segmentation step divides the repository into PR Reports, Issue Reports, and Source Code, distinguishing between partial code and full code for the source code. After Non-Textual Filtering, it enters the structured conversation stage, organizing information into structured forms such as Rp, RQ1/2/3, and Cp. Finally, through Conversation Submission, it enters the Results Extraction stage, processes the information using LLM to obtain results, and performs Manual Validation."
    },
    "80": {
        "figure1": "2410.23603v1_figure2",
        "label1": "figure:decoding-results",
        "caption1": "\\textbf{A} Schematic of our controlled modeling experiment using the SLIP model family \\citep{mu2021slip}. `Controlled' in this case refers to the isolation of singular axes of interest across distinct sets of model that vary exclusively along these axes (with other possible variations held constant). In SLIP, both the training dataset (YFCC15M) and architecture (ViT-[S,B,L]) are held constant across 3 variants of model (SimCLR, CLIP, and SLIP). The difference between SimCLR and SLIP (a combination of SimCLR's visual augmentation regime with CLIP's language alignment in a unified contrastive learning pipeline) are a direct empirical instantiation of variation in the presence or absence of training provided by language. \\textbf{B} Results from our feature regression pipeline as applied to SimCLR (a unimodal vision model), CLIP (a language-aligned model) and SLIP (a model that combines unimdal vision training and language alignment) -- holding dataset and architecture constant. \\textbf{B\\textsubscript{1}} In the top plot, we see results across layers (the semitransparent jagged lines are individual layer scores; the curves are the output of a generalized additive smoother across layers; the SLIP models each have 3 variants: ViT-[Small, Base, Large]). The takeaway here is that for all models, predictive accuracy is generally higher in deeper layers (with the final embedding layer often the highest). \\textbf{B\\textsubscript{2}} In the bottom plot, we see the results from the maximally predictive layers of each model. Error bars are 95\\% confidence intervals across 1000 bootstrap resamples of the human subject pool. The takeaway here is that adding language alignment (without taking away unimodal vision training) in the form of the SLIP objective does significantly increase downstream readout of aesthetic information.} %\\vspace{-2em",
        "text": "The problem, however, with comparing the CLIP model directly to other models is that CLIP is trained on a proprietary dataset of 400 million image-text pairs not yet available to the public. To address this discrepancy, we use the SLIP models \\citep{mu2021slip} -- a series of Vision Transformers (Small [ViT-S], Base [ViT-B], \\& Large [ViT-L]), all trained on the YFCC15M dataset (15 million image-text pairs), but only on 1 of 3 tasks: pure SimCLR-style self-supervision; pure CLIP-style language alignment; or the eponymous SLIP -- a combination of self-supervision and language alignment. The SLIP models allow us to control for the influence of language, holding architecture and dataset constant. (A schematic of this controlled modeling procedure involving the SLIP models may be found in Figure \\ref{figure:decoding-results}A).\n\\centering   \\includegraphics[width=\\linewidth]{figures/figure2.png}   \\label{figure:decoding-results}   \\caption{\\textbf{A} Schematic of our controlled modeling experiment using the SLIP model family \\citep{mu2021slip}. `Controlled' in this case refers to the isolation of singular axes of interest across distinct sets of model that vary exclusively along these axes (with other possible variations held constant). In SLIP, both the training dataset (YFCC15M) and architecture (ViT-[S,B,L]) are held constant across 3 variants of model (SimCLR, CLIP, and SLIP). The difference between SimCLR and SLIP (a combination of SimCLR's visual augmentation regime with CLIP's language alignment in a unified contrastive learning pipeline) are a direct empirical instantiation of variation in the presence or absence of training provided by language. \\textbf{B} Results from our feature regression pipeline as applied to SimCLR (a unimodal vision model), CLIP (a language-aligned model) and SLIP (a model that combines unimdal vision training and language alignment) -- holding dataset and architecture constant. \\textbf{B\\textsubscript{1}} In the top plot, we see results across layers (the semitransparent jagged lines are individual layer scores; the curves are the output of a generalized additive smoother across layers; the SLIP models each have 3 variants: ViT-[Small, Base, Large]). The takeaway here is that for all models, predictive accuracy is generally higher in deeper layers (with the final embedding layer often the highest). \\textbf{B\\textsubscript{2}} In the bottom plot, we see the results from the maximally predictive layers of each model. Error bars are 95\\% confidence intervals across 1000 bootstrap resamples of the human subject pool. The takeaway here is that adding language alignment (without taking away unimodal vision training) in the form of the SLIP objective does significantly increase downstream readout of aesthetic information.}   %\\vspace{-2em}\nThe pattern of results across the SLIP models (Figure \\ref{figure:decoding-results}B) (and in particular the comparison between SimCLR and SLIP) suggests \\textit{adding language} to purely visual learning does indeed increase the downstream predictive accuracy of aesthetic ratings. Specifically, while pure CLIP-training shows discrepant gains over pure SIMCLR-training across the 3 vision transformer sizes (performing slightly better in ViT-S and ViT-B, and slightly worse in ViT-L), SLIP-training outperforms its pure SimCLR counterpart across all 3 transformer sizes by a significant, at least midsize margin. A bootstrapping analysis using 1000 resamples of the human subject pool (averaging across model size) shows the difference between SimCLR and CLIP to be nonsignificant, with a bootstrapped mean of 0.0098 [-0.027, 0.041] ($p$ = 0.67), while the difference between SimCLR and SLIP is significant, with a bootstrapped mean of 0.067 [0.037, 0.096] ($p$ < 0.001).",
        "summarize_figure": "figure1",
        "summarization": "Figure A illustrates a schematic of controlled modeling experiments using the SLIP model family. \"Controlled\" refers to variations along a single axis (e.g., sensory diet, architecture, task) across distinct model sets, while keeping other potential variations constant. In SLIP, the training dataset (YFCC15M) and architecture (ViT-[S,B,L]) are held constant across three model variants (SimCLR, CLIP, and SLIP). The difference between SimCLR and SLIP lies in the combination of SimCLR's visual augmentation and CLIP's language alignment in a unified contrastive learning pipeline, directly demonstrating the impact of the presence or absence of language training."
    },
    "81": {
        "figure1": "2410.23603v1_figure3",
        "label1": "figure:clipcap-results",
        "caption1": "\\textbf{A} Schematic of our experiment using CLIPCap \\citep{mokady2021clipcap} to translate the visual embeddings of CLIP into natural language by way of a GPT2 text decoder: The process begins with the embedding of an image (red line) into the latent space of a CLIP-ViT-B32 model. These embeddings contain only feedforward visual information. CLIP's latent visual embedding is then piped into GPT2 by way of CLIPCap's MLP adapter, and in the first pass through GPT (blue line), the only context available to GPT2 for next token generation is the visual information instantiated in CLIPCap's `prefix' tokens. Once a caption is produced, we concatenate (purple line) this caption with the original visual prefix and pipe it once again through GPT2 to extract embeddings that instantiate both the original visual information in the prefix, as well as any added information instantiated in the caption. Finally, we remove the visual prefix from the caption, and extract the GPT2 embeddings for the generated caption alone, effectively extracting the pure linguistic context provided by this caption. \\textbf{B} Results of the CLIPCap translation experiment: The red line in the facet on the left are the scores across the layers of the CLIP visual encoder used to generate an image `prefix' embedding that is subsequently passed to GPT2 for captioning. The line in blue in the facet on the right is the predictive power of that prefix embedding as it is processed across the layers of GPT2. In other words, this blue line tracks the potential of GPT2 to facilitate better aesthetic decoding by extracting further information from the visual prefix. The line in green is the predictive power of the generated caption passed back through GPT2 \\textit{without} the prefix embedding. This line tracks how well (machine-generated, image-conditioned) language alone might predict aesthetic ratings.  The line in purple is the predictive power of the generated caption passed back through GPT2 \\textit{with} the prefix embedding. This line tracks whether visual embeddings and image-conditioned language together might outperform either one alone. The difference between the blue line and the green line represents the difference in predictive power between CLIP's visual features and GPT2's linguistic features  -- the difference, in other words, between language-aligned perception and language alone. This gap is substantial. The negative slope on the purple line seems to be an artifact of the feature regression overfitting to the embedding complexity added by the caption. Each line in this plot may be thought of as instantiating a form of `context window' -- a term used in natural language processing to describe one information provides precedent for any given `next token' prediction in the language-generating process.",
        "text": "For this experiment (summarized with detail in Figure \\ref{figure:clipcap-results}), we use CLIP-Cap's MLP method of projection, which defaults to a prefix embedding length of 10 and uses CLIP-ViT-B/32 as its visual backbone. In the same way we decode aesthetics from features evoked by images in visual models, here we decode aesthetics from features evoked by the `embeddings' (for prefix and caption alike) in the language model: that is to say, layer by layer, and using the same regression method. We find first and foremost that while the projected visual prefix embedding preserves all the information necessary to decode aesthetics as accurately as in the CLIP visual encoder, the hierarchical language processing of GPT2 facilitates no additional decoding. (The accuracy of CLIP's visual encoder is 84.8\\% [83.2\\%, 85.6\\%] explainable variance explained; the accuracy of GPT2 operating over the prefix embedding never exceeds 85.3\\%.).\n\\centering   \\includegraphics[width=\\linewidth]{figures/figure3.png}   \\label{figure:clipcap-results}   \\caption{\\textbf{A} Schematic of our experiment using CLIPCap \\citep{mokady2021clipcap} to translate the visual embeddings of CLIP into natural language by way of a GPT2 text decoder: The process begins with the embedding of an image (red line) into the latent space of a CLIP-ViT-B32 model. These embeddings contain only feedforward visual information. CLIP's latent visual embedding is then piped into GPT2 by way of CLIPCap's MLP adapter, and in the first pass through GPT (blue line), the only context available to GPT2 for next token generation is the visual information instantiated in CLIPCap's `prefix' tokens. Once a caption is produced, we concatenate (purple line) this caption with the original visual prefix and pipe it once again through GPT2 to extract embeddings that instantiate both the original visual information in the prefix, as well as any added information instantiated in the caption. Finally, we remove the visual prefix from the caption, and extract the GPT2 embeddings for the generated caption alone, effectively extracting the pure linguistic context provided by this caption. \\textbf{B} Results of the CLIPCap translation experiment: The red line in the facet on the left are the scores across the layers of the CLIP visual encoder used to generate an image `prefix' embedding that is subsequently passed to GPT2 for captioning. The line in blue in the facet on the right is the predictive power of that prefix embedding as it is processed across the layers of GPT2. In other words, this blue line tracks the potential of GPT2 to facilitate better aesthetic decoding by extracting further information from the visual prefix. The line in green is the predictive power of the generated caption passed back through GPT2 \\textit{without} the prefix embedding. This line tracks how well (machine-generated, image-conditioned) language alone might predict aesthetic ratings.  The line in purple is the predictive power of the generated caption passed back through GPT2 \\textit{with} the prefix embedding. This line tracks whether visual embeddings and image-conditioned language together might outperform either one alone. The difference between the blue line and the green line represents the difference in predictive power between CLIP's visual features and GPT2's linguistic features  -- the difference, in other words, between language-aligned perception and language alone. This gap is substantial. The negative slope on the purple line seems to be an artifact of the feature regression overfitting to the embedding complexity added by the caption. Each line in this plot may be thought of as instantiating a form of `context window' -- a term used in natural language processing to describe one information provides precedent for any given `next token' prediction in the language-generating process.}",
        "summarize_figure": "figure1",
        "summarization": "Figure A illustrates the experimental workflow of using CLIPCap to translate CLIP's visual embeddings into natural language. The process begins with embedding an image into the latent space of a CLIP-ViT-B32 model, which contains feedforward visual information. CLIP's latent visual embedding is then fed into GPT2 via CLIPCap's MLP adapter. During the first pass through GPT2, the only available context is the visual information instantiated in CLIPCap's \"prefix\" tokens. After a caption is generated, it is concatenated with the original visual prefix and passed through GPT2 again to extract embeddings that instantiate both the original visual information in the prefix and any added information instantiated in the caption. Finally, the visual prefix is removed from the caption, and the GPT2 embeddings for the generated caption alone are extracted, effectively extracting the pure linguistic context provided by the caption."
    },
    "82": {
        "figure1": "2410.23608v2_ablation",
        "label1": "fig:gt",
        "caption1": "Under ground truth (GT) supervision, attending to only informative tokens can achieve better performance and efficiency.",
        "text": "\\centering     \\includegraphics[width=0.95\\columnwidth]{ablation.png}     \\caption{Under ground truth (GT) supervision, attending to only informative tokens can achieve better performance and efficiency.}     \\label{fig:gt}\nTo illustrate the effectiveness of informative token selection, we designed experiments where all informative tokens were selected based on ground truth (GT) selection. As illustrated in \\cref{fig:gt}, for both plain ViT and window-based attention mechanisms, selecting tokens to disregard background information improves both accuracy and efficiency, as confirmed across two different tasks.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates that attending to only informative tokens under ground truth supervision can achieve better performance and efficiency. On the VOC-S dataset, using both Swin and ViT models, GT-SPA (ground truth-based selection of informative tokens) shows an increase in mAP compared to MSA (multi-head self-attention). On the IN-S dataset, using both Swin and ViT models, GT-SPA also demonstrates an increase in Top-1 accuracy compared to MSA. Furthermore, on the VOC-S dataset, GT-SPA exhibits a decrease in computational cost (FLOPs) compared to MSA for both Swin and ViT models. Similarly, on the IN-S dataset, GT-SPA shows a reduction in computational cost (FLOPs) compared to MSA for both models. This suggests that selectively attending to informative tokens and disregarding background information can improve both accuracy and efficiency."
    },
    "83": {
        "figure1": "2410.23609v1_teaser",
        "label1": "fig:teaser",
        "caption1": "Positional bias in long-form summarization: On two representative models and datasets, summaries are less faithful to the documents in the middle.",
        "text": "\\centering     \\includegraphics[width=0.9\\columnwidth]{figures/teaser.pdf}     \\caption{Positional bias in long-form summarization: On two representative models and datasets, summaries are less faithful to the documents in the middle.}     \\label{fig:teaser}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the positional bias in long-form summarization. The horizontal axis represents the document index (from 1st to 5th), and the vertical axis represents  in percentage). The figure includes four curves, representing the faithfulness performance of the ArXiv and PubMed datasets and the GPT-4o and LLaMA-70B models at different document positions. The faithfulness of the ArXiv dataset decreases significantly from the 1st to the 3rd document position, reaching the lowest point at the 3rd position, and then rises. The faithfulness of the PubMed dataset is generally high and relatively stable, but there is a slight decrease in the middle position. The curves of GPT-4o and LLaMA-70B show the difference in faithfulness when the model processes documents at different positions."
    },
    "84": {
        "figure1": "2410.23609v1_faithfulness",
        "label1": "fig:direct_analysis",
        "caption1": "Faithfulness analysis across different positions of documents.",
        "figure2": "2410.23609v1_faithfulness_superpal",
        "label2": "fig:direct_analysis_superpal",
        "caption2": "Faithfulness analysis across different positions with SuperPal as alignment.",
        "text": "\\centering     \\includegraphics[width=0.9\\textwidth]{figures/faithfulness}     \\caption{Faithfulness analysis across different positions of documents.}     \\label{fig:direct_analysis}\nWe evaluate whether the faithfulness score can be used for alignment. Specifically, we use SuperPal \\cite{ernst-etal-2021-summary}, the state-of-the-art document-summary sentence alignment model, to test whether it reaches similar conclusions in terms of faithfulness. SuperPal operates at the sentence level, aligning summary sentences to document sentences. We use the indices of the aligned document sentences to compare alignment positions. The correlation between the two metrics is high, with a Spearman correlation of 0.74. We also verify the alignment visually, as shown in \\autoref{fig:direct_analysis_superpal}. We observe the same trends as shown in \\autoref{fig:direct_analysis} and discussed in Section~\\ref{sec:faithfulness_analysis}: A dominant U-shaped curve for ArXiv, PubMed, and MultiNews, and a strong lead bias for MultiXScience. This demonstrates that using the maximum faithfulness score as an attribution method provides similar conclusions to those obtained from a trained document-summary sentence alignment model.",
        "summarize_figure": "figure1",
        "summarization": "The picture shows the faithfulness analysis across different document positions, specifically analyzing the faithfulness performance on the ArXiv dataset. As seen in the picture, for the ArXiv dataset, models such as ChatGPT, LLaMA-8B, LLaMA-70B, Mixtral, Mixtral-Large, GPT-4o and Gemini-1.5-Pro exhibit a U-shaped trend in faithfulness across different positions of the document. The beginning of the document (1st document index) usually has higher faithfulness, while the middle positions (2nd to 4th document index) have lower faithfulness, which then rises again. For example, ChatGPT has a faithfulness of nearly 90% at the first document index, drops to about 60% at the third document index, and then rises back to nearly 80% at the fifth document index. The remaining models also show a similar trend."
    },
    "85": {
        "figure1": "2410.23609v1_subset",
        "label1": "fig:subset",
        "caption1": "Faithfulness analysis by increasing the context length by adding documents.",
        "text": "\\centering     \\includegraphics[width=\\linewidth]{figures/subset.pdf}     \\caption{Faithfulness analysis by increasing the context length by adding documents.}     \\label{fig:subset} So far, our analyses have been post-hoc, where we attempt to analyze the faithfulness scores when the input is fixed. Here, we try to analyze how faithfulness correlates with length by incrementally increasing the number of documents. For all datasets, we start with summarizing one document, and then we add one more document and summarize again. We then calculate the faithfulness scores of the generated summaries using the corresponding set of documents. We exclude SummHay here, as it is computationally expensive to run the subsets for all 100 documents incrementally.\nResults are in \\autoref{fig:subset}. For ArXiv, PubMed, and DiverseSumm, we observe the same U-shape occurring. This indicates that the faithfulness of the summary decreases as more documents are included up to a certain point, after which it begins to improve. This suggests that the model may switch modes at a certain context length and focus more on the documents introduced later. On MultiNews, we can similarly observe this trend, as the faithfulness steadily improves for most models as we include more input documents. This phenomenon is consistent with prior research on LLMs' behavior of summarizing extremely long input, where models focus a large portion of their attention on the later part \\cite{kim2024fables, laban2024SummHay}.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the faithfulness analysis by increasing the number of documents. On the ArXiv dataset, ChatGPT exhibits the highest faithfulness, consistently outperforming other models. However, the faithfulness of all models shows a U-shaped trend: faithfulness decreases initially and then increases with the document index. GPT-4o experiences the most significant drop in faithfulness, particularly from the first to the fourth document. LLaMA-8B and LLaMA-70B have relatively close faithfulness scores, but both are lower than ChatGPT and GPT-4o. Mixtral and Mixtral-Large also demonstrate a similar U-shaped trend with close values. This suggests that the faithfulness of the models does not continuously improve with the number of input documents, but rather decreases within a certain range before gradually recovering."
    },
    "86": {
        "figure1": "2410.23609v1_faithfulness_superpal",
        "label1": "fig:direct_analysis_superpal",
        "caption1": "Faithfulness analysis across different positions with SuperPal as alignment.",
        "figure2": "2410.23609v1_faithfulness",
        "label2": "fig:direct_analysis",
        "caption2": "Faithfulness analysis across different positions of documents.",
        "text": "\\centering     \\includegraphics[width=\\textwidth]{figures/faithfulness_superpal.pdf}     \\caption{Faithfulness analysis across different positions with SuperPal as alignment.}     \\label{fig:direct_analysis_superpal}\nWe evaluate whether the faithfulness score can be used for alignment. Specifically, we use SuperPal \\cite{ernst-etal-2021-summary}, the state-of-the-art document-summary sentence alignment model, to test whether it reaches similar conclusions in terms of faithfulness. SuperPal operates at the sentence level, aligning summary sentences to document sentences. We use the indices of the aligned document sentences to compare alignment positions. The correlation between the two metrics is high, with a Spearman correlation of 0.74. We also verify the alignment visually, as shown in \\autoref{fig:direct_analysis_superpal}. We observe the same trends as shown in \\autoref{fig:direct_analysis} and discussed in Section~\\ref{sec:faithfulness_analysis}: A dominant U-shaped curve for ArXiv, PubMed, and MultiNews, and a strong lead bias for MultiXScience. This demonstrates that using the maximum faithfulness score as an attribution method provides similar conclusions to those obtained from a trained document-summary sentence alignment model.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the faithfulness analysis across different positions using SuperPal alignment. Specifically, the figure shows the faithfulness scores of the Mixtral-Large model at different positions in the ArXiv dataset. As observed, the faithfulness score changes with document position, showing a trend. The faithfulness score is high at the first position but drops rapidly, reaching a low point at the second position, then gradually rises, and decreases again at the fourth and fifth positions. The difference in faithfulness scores of different models at each position indicates that the faithfulness performance of the models varies when processing document content at different positions."
    },
    "87": {
        "figure1": "2410.23609v1_coverage",
        "label1": "fig:coverage_analysis",
        "caption1": "Coverage analysis across different positions.",
        "text": "\\centering     \\includegraphics[width=\\textwidth]{figures/coverage}     \\caption{Coverage analysis across different positions.}     \\label{fig:coverage_analysis}\nWe also provide the coverage analysis, by using the faithfulness score of all documents. We show the figure in \\autoref{fig:coverage_analysis}. The trend generally follows the observation of \\citet{ravaut-etal-2024-context}, who uses bigram matching between the summary and documents, observing either a U-shape or a lead bias.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the coverage analysis of different models on the ArXiv dataset across various document positions. The coverage rates of all models exhibit a clear decreasing trend, with the most significant drop occurring from the first to the second document position. After the second position, the coverage stabilizes and slightly increases in subsequent document positions. The ChatGPT model demonstrates relatively high coverage across all positions, while the Llama-70B model shows relatively low coverage. The difference in coverage between different models is most prominent in the first and second document positions, with the differences gradually diminishing in later positions. The overall trend aligns with the U-shape or lead bias observed by Ravaut et al. in 2024."
    },
    "88": {
        "figure1": "2410.23609v1_faithfulness_gpt4o",
        "label1": "fig:faithfulness_gpt4o",
        "caption1": "Faithfulness analysis across different positions using GPT-4o faithfulness metric.",
        "text": "\\centering     \\includegraphics[width=\\textwidth]{figures/faithfulness_gpt4o.pdf}     \\caption{Faithfulness analysis across different positions using GPT-4o faithfulness metric.}     \\label{fig:faithfulness_gpt4o} Here, we now evaluate faithfulness using our best faithfulness metrics on a subset that excludes Llama-based models and includes only ArXiv, PubMed, MultiNews, and MultiXScience. Compared to \\autoref{fig:faithfulness_gpt4o}, which uses MiniCheck, we observe a smoother graph, but the key findings remain the same. For example, we observe the same U-shape occurring for ArXiv, PubMed, and MultiNews, while MultiXScience exhibits a strong lead bias. As mentioned in Section~\\ref{sec:faithfulness_experimental_setup}, since the computation is expensive, we still use MiniCheck for the remaining analyses.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the faithfulness analysis of different models across various document positions on four datasets: ArXiv, PubMed, MultiNews, and MultiXScience, using the GPT-4o faithfulness metric. The horizontal axis represents the document index (from 1st to 5th), and the vertical axis indicates the faithfulness percentage. The image reveals varying performances of ChatGPT, GPT-4o, Mixtral, and Mixtral-Large across different datasets. For instance, a U-shaped trend is observed for ArXiv, PubMed, and MultiNews datasets, indicating lower faithfulness in the middle positions. MultiXScience exhibits a strong lead bias, with higher faithfulness for the first document and a decrease as the document position increases. GPT-4o generally demonstrates the highest faithfulness across the four datasets, while ChatGPT shows relatively lower faithfulness, especially on the ArXiv dataset."
    },
    "89": {
        "figure1": "2410.23629v2_fig_supple_2",
        "label1": "fig:pressuresensor",
        "caption1": "skip}{-1pt} \\setlength{\\belowcaptionskip}{-15pt} \\caption{FSR sensor calibration and response curves. \\textbf{(a)} We calibrated each FSR pressure sensor with the calibration setup using the precise push-pull gauge. \\textbf{(b) \\& (c)} The FSR's resistance and conductance characteristics as a function of applied force, respectively, illustrating the sensor's calibration curve derived from multiple trials for data collection.",
        "text": "\\centering   \\includegraphics[width=0.99\\textwidth]{fig/fig_supple_2.png}   \\setlength{\\abovecaptionskip}{-1pt}    \\setlength{\\belowcaptionskip}{-15pt}   \\caption{FSR sensor calibration and response curves. \\textbf{(a)} We calibrated each FSR pressure sensor with the calibration setup using the precise push-pull gauge. \\textbf{(b) \\& (c)} The FSR's resistance and conductance characteristics as a function of applied force, respectively, illustrating the sensor's calibration curve derived from multiple trials for data collection.}   \\label{fig:pressuresensor}\nFor fingertip pressure measurements, we utilized a Force-Sensing Resistor~(FSR) type pressure sensor~(RA18DIY, Marveldex), characterized by a force range of $0\\sim40$~N/cm$^2$ and a thin profile of 0.7~mm with an 8~mm diameter sensing area. Figure~\\ref{fig:pressuresensor} presents the typical response behavior of the FSR, highlighting the force-resistance relationship. In our work, we derived a precise fitting model for the sensor's output through repetitive calibrations using a push-pull gauge. The calibration process involved conducting 30 load-unload cycles~(ranging from $0\\sim30$ N, incremented by 1~N) and meticulously recording the resistance values to ensure accuracy.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the calibration and response curves of the Force-Sensing Resistor (FSR) sensor. The picture shows the calibration setup used to calibrate the FSR pressure sensor, which utilizes a precise push-pull gauge. A precise fitting model for the sensor's output can be derived through repetitive calibrations. The calibration process involved conducting 30 load-unload cycles (ranging from 0 to 30 N, incremented by 1 N) and meticulously recording the resistance values to ensure accuracy."
    },
    "90": {
        "figure1": "2410.23656v1_productivity",
        "label1": "productivity",
        "caption1": "Productivity scores per language after averaging results for 300, 400, and 500 merge operations. Measurements were performed using the PBC parallel corpora. The error bars indicate the standard deviation between rounds of merge operations.",
        "figure2": "2410.23656v1_top_tokens_zoom",
        "label2": "top_tokens",
        "caption2": "Frequencies of the top $n$-th most repeated subword. As observed in the graph, as further tokens are analyzed, the tokens in synthetic languages show higher frequencies.",
        "text": "The implications of these results extended to productivity scores (see Figure \\ref{productivity}). All analytic languages displayed lower productivity compared to the synthetic ones. The reliance of analytic languages on word order and function words results in fewer unique subwords, thereby reducing their scores. This coincides with the results shown in Figure \\ref{trends}: the languages that show the highest repetition tendencies are also the most productive.\n\\centering     \\includegraphics[width=0.4\\linewidth]{figures/productivity.png}     \\caption{Productivity scores per language after averaging results for 300, 400, and 500 merge operations. Measurements were performed using the PBC parallel corpora. The error bars indicate the standard deviation between rounds of merge operations.}     \\label{productivity}\nEquation \\ref{formalism} is supported by visual evidence shown in Figures \\ref{trends}, \\ref{productivity}, and \\ref{top_tokens}, and the statistical results from the $t$-tests, ANOVA, and the slopes in Table \\ref{slopes}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the productivity scores of subwords across different languages. Hungarian shows the highest productivity at 1.65, followed by Finnish and Basque, both at 1.63. French has a productivity of 1.58, Spanish 1.56, and English has the lowest productivity at 1.51. The error bars in the figure indicate the standard deviation between rounds of merge operations. Overall, analytic languages (English, Spanish, French) show lower productivity compared to synthetic languages (Basque, Finnish, Hungarian)."
    },
    "91": {
        "figure1": "2410.23656v1_top_tokens_zoom",
        "label1": "top_tokens",
        "caption1": "Frequencies of the top $n$-th most repeated subword. As observed in the graph, as further tokens are analyzed, the tokens in synthetic languages show higher frequencies.",
        "figure2": "2410.23656v1_productivity",
        "label2": "productivity",
        "caption2": "Productivity scores per language after averaging results for 300, 400, and 500 merge operations. Measurements were performed using the PBC parallel corpora. The error bars indicate the standard deviation between rounds of merge operations.",
        "text": "Our analysis of subword patterns revealed interesting differences between languages. Initially, no distinctive patterns were observed. However, as additional subwords were analyzed and the sample size increased some languages exhibited stabilization, while others showed a sharper decline (Figures \\ref{trends} and \\ref{top_tokens}). These differences reflect both, (1) that the groupings –synthetic and analytic– seem to behave similarly, and (2) that there is a continuum favoring the richer morphological typologies.\n\\centering     \\includegraphics[width=1\\linewidth]{figures/top_tokens_zoom.png}     \\caption{Frequencies of the top $n$-th most repeated subword. As observed in the graph, as further tokens are analyzed, the tokens in synthetic languages show higher frequencies.}     \\label{top_tokens}\nEquation \\ref{formalism} is supported by visual evidence shown in Figures \\ref{trends}, \\ref{productivity}, and \\ref{top_tokens}, and the statistical results from the $t$-tests, ANOVA, and the slopes in Table \\ref{slopes}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the frequencies of the top 50 most repeated subwords across different languages. Basque, Finnish, and Hungarian show relatively higher frequencies, while English, Spanish, and French exhibit lower frequencies. As more subwords are analyzed, the subword frequencies in synthetic languages are higher. Specifically, Token 1 has the highest frequency across all languages, but the frequency of each language decreases with the increase of the Token number. The frequency of Basque is lower than Finnish and Hungarian in the first few tokens, but it maintains a relatively stable and high level in the subsequent tokens. The frequency of English is relatively low in all tokens."
    },
    "92": {
        "figure1": "2410.23663v1_robustness",
        "label1": "fig:robustness",
        "caption1": "Robustness Evaluation within unseen distortions of various levels in terms of AUC (\\%). Several models trained on \\emph{clean} FF++ are evaluated under several distortions with different levels. \"Average\" denotes the average performance against the same-level distortions.",
        "text": "\\centering     \\includegraphics[width= 0.8\\textwidth]{figures/robustness.pdf}     \\caption{Robustness Evaluation within unseen distortions of various levels in terms of AUC (\\%). Several models trained on \\emph{clean} FF++ are evaluated under several distortions with different levels. \"Average\" denotes the average performance against the same-level distortions.}     \\label{fig:robustness}\nAs shown in Fig. \\ref{fig:robustness}, our proposed DIP is more robust than the other involved methods under various distortion scenarios. Since the spatial forgery artifacts are heavily corrupted under spatial distortions, including Gaussian noise, pixelation, color saturation, and contrast, while the interframe relationships, i.e. temporal artifacts, are relatively well preserved,   the video-based methods,  i.e., LipForensics \\cite{haliassos_lips_2021} and the proposed DIP, are significantly more robust than the image-based methods, e.g., Xception \\cite{rossler_faceforensics_2019}, Face X-ray \\cite{li_face_2020}, and PatchForensics \\cite{chai_what_2020}, which rely heavily on spatial forgery artifacts.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the robustness evaluation of various models under unseen distortions of different levels, measured by AUC (%). Specifically, the figure includes six subplots, showing the change of AUC values of different models with the degree of distortion under six distortion types: color saturation, color contrast, block-wise distortion, Gaussian noise, Gaussian blur, and pixelation. In addition, there is an \"Average\" subplot, which represents the average performance against the same level of distortion. As can be seen, as the degree of distortion increases, the AUC values of all models generally decrease, indicating a reduction in robustness. The performance differences between different models are also obvious. Some models show good robustness under specific distortion types, while others perform poorly under other distortion types."
    },
    "93": {
        "figure1": "2410.23668v1_eval_hbm_util",
        "label1": "fig:eval-hbm-util",
        "caption1": "HBM bandwidth trace from one \\ts \\space socket running Llama3.1-8B. Kernel looping(orange) sustains higher HBM bandwidth utilization over the baseline (blue).",
        "text": "\\centering          \\includegraphics[width=\\linewidth]{figs/eval_hbm_util.pdf}          \\caption{HBM bandwidth trace from one \\ts \\space socket running Llama3.1-8B. Kernel looping(orange) sustains higher HBM bandwidth utilization over the baseline (blue).}          \\label{fig:eval-hbm-util}\nFigure~\\ref{fig:eval-hbm-util} illustrates the benefits of kernel looping with an HBM bandwidth utilization trace obtained from a single socket of a \\ts-16 system executing Llama3.1-8B. Kernel looping (orange) sustains a much higher HBM bandwidth utilization compared to the baseline (blue), which shows periodic drops in HBM bandwidth due to synchronization overheads. Consequently, kernel looping enables executing many more decoder layers during the same time window.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the HBM bandwidth utilization trace from one \\ts socket running the Llama3.1-8B model. It compares the HBM bandwidth utilization of the baseline (blue) and kernel looping (orange). The result shows that kernel looping sustains a much higher HBM bandwidth utilization compared to the baseline. The baseline exhibits periodic drops in HBM bandwidth, while kernel looping enables the execution of more decoder layers within the same time window. The reference line for the max HBM bandwidth is also indicated, which is approximately 1800 GB/s. The baseline HBM bandwidth is close to 1700 GB/s at 0 microseconds and then drops rapidly and stabilizes at around 500 GB/s. The loop kernel HBM bandwidth is maintained above 1500 GB/s."
    },
    "94": {
        "figure1": "2410.23672v1_d_2000_n_300_beta0.1_r1_rho1_0.15_rho2_0.05_sigma1_0.25_sigma2_0.15_m_1_alpha_0.005_eta_1_T_10000",
        "label1": "fig:synthetic",
        "caption1": "Numerical results on our problem setting. We validate our findings on the trends of \\textsf{ERM}, \\textsf{Cutout}, and \\textsf{CutMix} in learning common feature (Left), rare feature (Center), and extremely rare feature (Right). The output of the common feature trained by \\textsf{CutMix} shows non-monotone behavior.",
        "text": "For each feature vector $\\vv$ of the positive label, we plot the output of the learned filters for the feature vector $\\phi (  \\langle \\vw_1^{(t)}, \\vv \\rangle) - \\phi ( \\langle  \\vw_{-1}^{(t)}, \\vv\\rangle) $ throughout training in Figure~\\ref{fig:synthetic}. Our numerical findings confirm that \\textsf{ERM} can only learn common features, \\textsf{Cutout} can learn common and rare features but cannot learn extremely rare features, and \\textsf{CutMix} can learn all types of features. Especially, \\textsf{CutMix} learn common features, rare features, and extremely rare features almost evenly. Also, we observed non-monotone behavior of the output in the case of \\textsf{CutMix}, which motivated our novel proof technique.  The same trends are observed with different architectures, such as a smoothed (leaky) ReLU network with multiple neurons, as detailed in Appendix~\\ref{sec:synthetic_additional_exp}.\n\\centering     \\includegraphics[width = \\textwidth]{Figures/d_2000_n_300_beta0.1_r1_rho1_0.15_rho2_0.05_sigma1_0.25_sigma2_0.15_m_1_alpha_0.005_eta_1_T_10000.pdf}     \\caption{Numerical results on our problem setting. We validate our findings on the trends of \\textsf{ERM}, \\textsf{Cutout}, and \\textsf{CutMix} in learning common feature (Left), rare feature (Center), and extremely rare feature (Right). The output of the common feature trained by \\textsf{CutMix} shows non-monotone behavior.}     \\label{fig:synthetic}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the output trends of ERM, Cutout, and CutMix methods when learning different feature types (common, rare, and extremely rare) over training iterations. For common features, the output of ERM and Cutout increases steadily with iterations, with Cutout growing faster and reaching a higher value, while CutMix shows non-monotone behavior, initially increasing before slightly decreasing and stabilizing at a lower level. For rare features, ERM's output remains near zero, while both Cutout and CutMix's outputs increase rapidly, with CutMix showing a slightly faster rise. In learning extremely rare features, both ERM and Cutout's outputs stay close to zero, and only CutMix's output increases quickly with iterations to a high value. The figure demonstrates that ERM primarily learns common features, Cutout learns common and rare features, and CutMix effectively learns all feature types, exhibiting unique non-monotone behavior for common features."
    },
    "95": {
        "figure1": "2410.23672v1_output_cutpaste_plot",
        "label1": "fig:cifar",
        "caption1": "Histogram of dog prediction output subtracted by cat prediction output evaluated on data points augmented by CutMix data using cat data and dog data with varying mixing ratio $\\lambda$ ($\\text{Dog}:\\text{Cat} = \\lambda : 1-\\lambda$) (Left) $\\lambda = 1$ , (Center) $\\lambda = 0.8$, (Right) $\\lambda = 0.6$",
        "text": "We randomly generate augmented data using CutMix from pairs of cat images and dog images in CIFAR-10 with varying mixing ratios $\\lambda = 1,0.8,0.6$ (Dog:Cat  = $\\lambda : 1-\\lambda$). We randomly make $5,000$ (cat, dot)-pairs in CIFAR-10 training set and apply CutMix randomly $10$ times. By repeating this procedure $10$ times, we generate total $5,000 \\times 10 \\times 10 = 500,000$ augmented samples for each mixing ratio $\\lambda$. We plot a histogram of dog prediction output subtracted by cat prediction output (before applying the softmax function), evaluated on $500,000$ augmented data in Figure~\\ref{fig:cifar}.\n\\centering     \\includegraphics[width = \\textwidth]{Figures/output_cutpaste_plot.pdf}     \\caption{Histogram of dog prediction output subtracted by cat prediction output evaluated on data points augmented by CutMix data using cat data and dog data with varying mixing ratio $\\lambda$ ($\\text{Dog}:\\text{Cat} = \\lambda : 1-\\lambda$) (Left) $\\lambda = 1$ , (Center) $\\lambda = 0.8$, (Right) $\\lambda = 0.6$}     \\label{fig:cifar}",
        "summarize_figure": "figure1",
        "summarization": "Figure Summary:The image presents histograms of the dog prediction output minus the cat prediction output, evaluated on data augmented using CutMix with varying mixing ratios of cat and dog images (lambda = 1, 0.8, 0.6). The leftmost plot (lambda = 1) shows distributions for all three methods (ERM, Cutout, CutMix) primarily concentrated in the positive region, indicating higher dog prediction output than cat. In the center (lambda = 0.8) and right (lambda = 0.6) plots, as the proportion of cat images increases, all distributions shift towards the left. Notably, compared to ERM and Cutout, the CutMix distribution shifts further towards zero and negative values and becomes wider as lambda decreases, suggesting its output difference is more sensitive to the mixing ratio."
    },
    "96": {
        "figure1": "2410.23672v1_d_2000_n_300_beta0.1_r1_rho1_0.15_rho2_0.03_sigma1_0.25_sigma2_0.12_m_10_alpha_0.005_eta_1_T_10000",
        "label1": "fig:multi-leaky",
        "caption1": "Multi-neuron with a smoothed leaky ReLU actiation",
        "figure2": "2410.23672v1_d_2000_n_300_beta0.0_r1_rho1_0.2_rho2_0.05_sigma1_0.15_sigma2_0.1_m_10_alpha_0.005_eta_1_T_10000",
        "label2": "fig:smooth-relu",
        "caption2": "Multi-neuron with a smoothed ReLU",
        "text": "For the multi-neuron with smoothed Leaky ReLU case (Figure~\\ref{fig:multi-leaky}), we use $10$ neurons for each positive/negative output with the slope of negative regime $\\beta = 0.1$ and the length of polynomial regime $r = 1$. We set the strength of dominant noise $\\sigma_\\mathrm{d} = 0.25$, the strength of background noise $\\sigma_\\mathrm{b} = 0.12$, and the strength of feature noise $\\alpha = 0.05$. In addition, frequencies of common features, rare features, and extremely rare features are set to $0.72$, $0.15$, and $0.03$, respectively.\nFor the multi-neuron with smoothed ReLU case i.e., $\\beta = 0$ (Figure~\\ref{fig:smooth-relu}), we set the length of the polynomial regime as $r = 1$, and we use $10$ neurons for each positive/negative output. We set the remaining problem parameters as follows: the strength of dominant noise $\\sigma_\\mathrm{d} = 0.25$, the strength of background noise $\\sigma_\\mathrm{b} = 0.12$, and the strength of feature noise $\\alpha = 0.05$. In addition, frequencies of common features, rare features, and extremely rare features are set to $0.75$, $0.2$, and $0.05$, respectively.      \\centering     \\includegraphics[width = \\textwidth]{Figures/d_2000_n_300_beta0.1_r1_rho1_0.15_rho2_0.03_sigma1_0.25_sigma2_0.12_m_10_alpha_0.005_eta_1_T_10000.pdf}     \\caption{Multi-neuron with a smoothed leaky ReLU actiation}     \\label{fig:multi-leaky}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the impact of different training methods on different types of feature outputs in a multi-neuron model with smoothed Leaky ReLU activation. The x-axis represents iterations, and the y-axis represents feature output. The first subplot focuses on common feature output, where the Cutout method shows the highest output value, which continues to increase with the number of iterations, while the ERM method's output value is relatively low and tends to stabilize, and the CutMix method has the lowest output value. The second subplot focuses on rare feature output, with the Cutout method again showing the highest output value, and the CutMix method's output stabilizing at a low level. The third subplot focuses on extremely rare feature output, with the CutMix method showing the highest output value and the Cutout method's output stabilizing at a low level. The ERM method's output is very low for all three feature types."
    },
    "97": {
        "figure1": "2410.23672v1_d_2000_n_300_beta0.0_r1_rho1_0.2_rho2_0.05_sigma1_0.15_sigma2_0.1_m_10_alpha_0.005_eta_1_T_10000",
        "label1": "fig:smooth-relu",
        "caption1": "Multi-neuron with a smoothed ReLU",
        "figure2": "2410.23672v1_d_2000_n_300_beta0.1_r1_rho1_0.15_rho2_0.03_sigma1_0.25_sigma2_0.12_m_10_alpha_0.005_eta_1_T_10000",
        "label2": "fig:multi-leaky",
        "caption2": "Multi-neuron with a smoothed leaky ReLU actiation",
        "text": "For the multi-neuron with smoothed ReLU case i.e., $\\beta = 0$ (Figure~\\ref{fig:smooth-relu}), we set the length of the polynomial regime as $r = 1$, and we use $10$ neurons for each positive/negative output. We set the remaining problem parameters as follows: the strength of dominant noise $\\sigma_\\mathrm{d} = 0.25$, the strength of background noise $\\sigma_\\mathrm{b} = 0.12$, and the strength of feature noise $\\alpha = 0.05$. In addition, frequencies of common features, rare features, and extremely rare features are set to $0.75$, $0.2$, and $0.05$, respectively.      \\centering     \\includegraphics[width = \\textwidth]{Figures/d_2000_n_300_beta0.1_r1_rho1_0.15_rho2_0.03_sigma1_0.25_sigma2_0.12_m_10_alpha_0.005_eta_1_T_10000.pdf}     \\caption{Multi-neuron with a smoothed leaky ReLU actiation}     \\label{fig:multi-leaky}\n\\centering     \\caption{Multi-neuron with a smoothed ReLU}     \\label{fig:smooth-relu}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the feature output performance of different training methods (ERM, Cutout, CutMix) during training iterations under a multi-neuron smoothed ReLU activation function. The three subplots from left to right show the output of common features, rare features, and extremely rare features, respectively. For common features, the Cutout method performs the best, with the output value increasing significantly with the number of iterations, followed by the ERM method, and the CutMix method showing a lower output value. For rare features, the output value of the Cutout method also increases significantly with the number of iterations, while the output values of the ERM and CutMix methods hardly change. For extremely rare features, the output value of the CutMix method increases rapidly after early iterations and tends to be stable, while the output values of the ERM and Cutout methods are still low."
    },
    "98": {
        "figure1": "2410.23677v1_shifted_Gauss_a_mini",
        "label1": "fig:shifted-gauss-a-mini",
        "caption1": "Accuracy on the mean-shifted Gaussian dataset in Scenario~(a). The blue lines represent accuracy of the classifier $f$ on $\\calD$, i.e., training accuracy. The orange lines represent accuracy of the classifier $g$ on $\\calD$.",
        "figure2": "2410.23677v1_cossim",
        "label2": "fig:cossim",
        "caption2": "Standard training accuracy and cosine similarity. The blue lines represent the accuracy of the classifier $f$ on $\\calD := \\{(\\x_n,y_n)\\}^N_{n=1}$, i.e., training accuracy. The orange lines represent the average cosine similarity between the experimentally calculated adversarial perturbations and the theoretically predicted ones.",
        "text": "A comprehensive set of experiments conducted to validate our theorems can be found in \\cref{sec:experiments-ap}. In this section, we briefly present two results that confirm \\cref{th:PL-a}. As a training dataset $\\calD := \\{(\\x_n, y_n)\\}^N_{n=1}$, we employed a synthetic training dataset to easily change the input dimension, which effectively helps perturbation learning in both scenarios, as predicted by our theorems. Note that the perturbation learning on real-world datasets can be found in the literature~\\cite{NRF,EX}. We generated synthetic data and labels from the mean-shifted Gaussian distribution as follows: $\\{\\x_n\\}^N_{n=1}$ are independently sampled from $\\calN(0.3 \\times y_n \\times \\one, \\I)$, and $y_n$ is set to one if $n \\in [N/2]$ and minus one otherwise. The experimental settings are as follows: $d = 100$, $N = 1,000$, $m = 100$, $\\gamma = 0$, $\\ell(s) := s$, $\\epsilon = 0.01$, and the number of training steps is set to 1,000 for both $f$ and $g$. The experimental results for perturbation learning under Scenario~(a) are shown in \\cref{fig:shifted-gauss-a-mini}. A high input dimension facilitates the alignment between $f$ and $g$. Our theoretical results assume a wide network width, and \\cref{fig:shifted-gauss-a-mini} indicates that a sufficiently large width consistently stabilize the alignment.\nWe used non-stochastic gradient descent~(i.e., each gradient calculation uses the entire dataset) with 0.9 momentum and the learning rate scheduler that multiplies a learning rate by 0.1 when a training loss has stopped improving during 10 epochs. For \\crefrange{fig:shifted-gauss-a-mini}{fig:cossim}, we selected the best accuracy, agreement ratio, and cosine similarity from training with multiple initial learning rates.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the experimental results of perturbation learning under Scenario (a) on the mean-shifted Gaussian dataset. The blue line represents the accuracy of classifier f on dataset D, i.e., training accuracy, and the orange line represents the accuracy of classifier g on dataset D. The figure indicates that a higher input dimension facilitates the alignment between f and g. Specifically, as the input dimension d increases from 10 to 100, the accuracy of the standard classifier f increases from approximately 0.8 to nearly 1.0, while the accuracy of the perturbation classifier g also increases from around 0.5 to nearly 1.0."
    },
    "99": {
        "figure1": "2410.23677v1_Gauss_a",
        "label1": "fig:gauss-a",
        "caption1": "Accuracy and agreement ratio on the zero-mean Gaussian in Scenario~(a). The description is the same as \\cref{fig:shifted-gauss-a}.",
        "figure2": "2410.23677v1_map",
        "label2": "fig:map",
        "caption2": "Prediction matching between the classifiers from standard training and perturbation learning, $f$ and $g$. The two axes are the normalized average vectors of samples from the positive and negative classes, respectively. The blue circles and orange crosses correspond to the projections of positive and negative samples onto these axes. The gray and green areas indicate regions where two predictions are consistent and inconsistent, respectively. The red solid lines represent $\\hatf(\\z) = 0$. The black dashed lines represent $\\hatg_a(\\z) = 0$ in Scenario~(a) and $\\hatg_b(\\z) = 0$ in Scenario~(b).",
        "text": "A common experimental setting for the zero-mean Gaussian dataset and Scenario~(a) in \\cref{fig:gauss-a,fig:cossim} is as follows: $d = 10,000$, $m = 100$, $\\gamma = 0$, $N = 10,000$, $\\ell(s) = s$, $T_f = 1,000$, $T_g = 1,000$, $\\epsilon = \\sqrt{d\\times0.001^2} = 0.1$, $\\eta_f = 1$ or $0.1$, and $\\eta_g = 1$ or $0.1$. However, for the comparison of $T_f$~(i.e., the graph in the fourth row and the first column in \\cref{fig:gauss-a}), we set $\\eta_f$ only to 0.1. For the comparison of $d$~(i.e., the graph in the first row in \\cref{fig:gauss-a}), we employed $\\epsilon = \\sqrt{d\\times0.001^2}$. In \\cref{fig:map}, we used $d = 1,000$, $m = 1,000$, $\\gamma = 0$, $N = 2,000$, $\\ell(s) = s$, $T_f = 1,000$, $T_g = 1,000$, $\\epsilon = \\sqrt{d\\times0.001^2} = 0.031$, $\\eta_f = 1$, and $\\eta_g = 1$.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates how the accuracy and agreement ratio of standard training and perturbation training change with the input dimension d under a zero-mean Gaussian distribution. The horizontal axis represents the input dimension d, ranging from 10 squared to 10 to the power of 4. The vertical axis represents the accuracy and agreement ratio. As the input dimension d increases, the accuracy of both standard training and perturbation training shows an upward trend, but the improvement of perturbation training is more obvious. The agreement ratio decreases as d increases, but stabilizes at high dimensions. This indicates that at high dimensions, perturbation training can better improve the accuracy of the model, but it may also lead to inconsistency in the model's prediction results."
    },
    "100": {
        "figure1": "2410.23677v1_Gauss_b",
        "label1": "fig:gauss-b",
        "caption1": "Accuracy and agreement ratio on the zero-mean Gaussian in Scenario~(b). The description is the same as \\cref{fig:shifted-gauss-a}.",
        "figure2": "2410.23677v1_map",
        "label2": "fig:map",
        "caption2": "Prediction matching between the classifiers from standard training and perturbation learning, $f$ and $g$. The two axes are the normalized average vectors of samples from the positive and negative classes, respectively. The blue circles and orange crosses correspond to the projections of positive and negative samples onto these axes. The gray and green areas indicate regions where two predictions are consistent and inconsistent, respectively. The red solid lines represent $\\hatf(\\z) = 0$. The black dashed lines represent $\\hatg_a(\\z) = 0$ in Scenario~(a) and $\\hatg_b(\\z) = 0$ in Scenario~(b).",
        "text": "A common experimental setting for the zero-mean Gaussian dataset and Scenario~(b) in \\cref{fig:gauss-b,fig:cossim} is as follows: $d = 10,000$, $m = 100$, $\\gamma = 0$, $N = 10,000$, $\\ell(s) = s$, $T_f = 1,000$, $T_g = 1,000$, $\\epsilon = \\sqrt{d\\times0.1^2} = 10$, $\\eta_f = 1$ or $0.1$, and $\\eta_g = 1$ or $0.1$. However, for the comparison of $T_f$~(i.e., the graph in the fourth row and the first column in \\cref{fig:gauss-b}), we set $\\eta_f$ only to 0.1. In addition, for the comparison of $T_g$~(i.e., the graph in the fourth row and the second column in \\cref{fig:gauss-b}), we set $\\eta_g$ only to 0.1. For the comparison of $d$~(i.e., the graph in the first row in \\cref{fig:gauss-b}), we employed $\\epsilon = \\sqrt{d\\times0.1^2}$. In \\cref{fig:map}, we used $d = 1,000$, $m = 1,000$, $\\gamma = 0$, $N = 10,000$, $\\ell(s) = s$, $T_f = 1,000$, $T_g = 1,000$, $\\epsilon = \\sqrt{d\\times0.01^2} = 0.31$, $\\eta_f = 0.1$, and $\\eta_g = 0.1$.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the impact of input dimension d on the accuracy of standard training, perturbation learning, and the agreement ratio between them in the zero-mean Gaussian scenario (b). The horizontal axis represents the input dimension d, ranging from 10 squared to 10 to the power of 4, and the vertical axis represents the accuracy and agreement ratio. As the input dimension d increases, the accuracy of standard training (blue line with circle markers) and perturbation learning (orange line with cross markers) both increase, with standard training slightly outperforming perturbation learning. The agreement ratio between the two (green line with triangle markers) also increases with the input dimension d, gradually approaching the accuracy of standard training."
    },
    "101": {
        "figure1": "2410.23677v1_cossim",
        "label1": "fig:cossim",
        "caption1": "Standard training accuracy and cosine similarity. The blue lines represent the accuracy of the classifier $f$ on $\\calD := \\{(\\x_n,y_n)\\}^N_{n=1}$, i.e., training accuracy. The orange lines represent the average cosine similarity between the experimentally calculated adversarial perturbations and the theoretically predicted ones.",
        "figure2": "2410.23677v1_shifted_Gauss_a_mini",
        "label2": "fig:shifted-gauss-a-mini",
        "caption2": "Accuracy on the mean-shifted Gaussian dataset in Scenario~(a). The blue lines represent accuracy of the classifier $f$ on $\\calD$, i.e., training accuracy. The orange lines represent accuracy of the classifier $g$ on $\\calD$.",
        "figure3": "2410.23677v1_shifted_Gauss_a",
        "label3": "fig:shifted-gauss-a",
        "caption3": "Accuracy and agreement ratio on the mean-shifted Gaussian in Scenario~(a). The blue lines represent the accuracy of the classifier $f$ on $\\calD := \\{(\\x_n,y_n)\\}^N_{n=1}$, i.e., training accuracy. The orange lines represent the accuracy of the classifier $g$ on $\\calD$. The green lines represent the prediction agreement between $f$ and $g$ on the test dataset.",
        "figure4": "2410.23677v1_FMNIST_b",
        "label4": "fig:FMNIST-b",
        "caption4": "Accuracy and agreement ratio on Fashion-MNIST in Scenario~(b). The description is the same as \\cref{fig:shifted-gauss-a}.",
        "text": "We used non-stochastic gradient descent~(i.e., each gradient calculation uses the entire dataset) with 0.9 momentum and the learning rate scheduler that multiplies a learning rate by 0.1 when a training loss has stopped improving during 10 epochs. For \\crefrange{fig:shifted-gauss-a-mini}{fig:cossim}, we selected the best accuracy, agreement ratio, and cosine similarity from training with multiple initial learning rates.\nThe cosine similarity in \\cref{fig:cossim} is the average one between the experimentally calculated adversarial perturbation and the theoretically predicted one~(cf. \\cref{eq:AP}) across all $n$. The blue lines are the same as those in \\crefrange{fig:shifted-gauss-a}{fig:FMNIST-b}.",
        "summarize_figure": "figure1",
        "summarization": "The figure shows the accuracy on the mean-shifted Gaussian dataset in Scenario (a). The blue line represents the accuracy of classifier f on dataset D, which is the training accuracy. The orange line represents the accuracy of classifier g on dataset D. The horizontal axis represents the input dimension d, ranging from 10 to 100. As the input dimension d increases, the accuracy of the standard classifier f gradually rises from approximately 0.85 to nearly 1.0, while the accuracy of the perturbation classifier g slowly increases from 0.5, eventually reaching a level close to 1.0 at approximately d=80."
    },
    "102": {
        "figure1": "2410.23698v1_base2new_gain",
        "label1": "fig:base2new_gain",
        "caption1": "\\textbf{Quantifying the role of LLM knowledge (distilled with $\\mathcal{L}_{\\text{distill}}$) in prompt learning.} $\\mathcal{L}_{\\text{distill}}$ consistently improves the base and new class accuracies on 11 classification datasets.}  \\end{center",
        "figure2": "2410.23698v1_modality_gap",
        "label2": "fig:modality_gap",
        "caption2": "\\textbf{Optimal gap distance obtained by \\method learning} on the few-shot classification task (base-to-new class generalization setting). Y axis indicates the Harmonic mean (H) of base and new class accuracies on each dataset. X axis indicates the gap distance varied by shifting the image and text embeddings following~\\citep{ModalityGap}. We see the optimal gap distance can be increased (on SUN397) or decreased (on ImageNet and EuroSAT) over the default gap obtained from pretraining.}  \\end{center} \\vskip -0.2in",
        "text": "Next we compare with a variant of our approach, with only task loss $\\mathcal{L}_{\\text{task}}$ but no $\\mathcal{L}_{\\text{distill}}$ to distill LLM knowledge. This variant allows decoupling the contribution of LLM knowledge for prompt learning under a fair setting. Fig.~\\ref{fig:base2new_gain} shows that using $\\mathcal{L}_{\\text{distill}}$ leads to consistent gains for both seen and unseen classes from all the considered datasets. The distilled textual knowledge makes an especially large impact for those fine-grained actions (UCF101) and visual classes (StanfordCars, Flowers102 and FGVCAircraft), which can be under-represented during both CLIP pretraining and prompt learning. Large gains are also observed for the special domains of textures (DTD) and satellite images (EuroSAT) with large distribution shift.\nFig.~\\ref{fig:modality_gap} shows the analysis results for few-shot classification on three example datasets. As expected, after \\method-based prompt learning, the optimal gap distance that attains maximum accuracy deviates from the default gap distance of pretraind CLIP. We see the optimal gap is increased on SUN397 dataset, while decreased on ImageNet and EuroSAT datasets. In fact, the optimal gap is reduced on 7 out of a total of 11 datasets and moderately increased on the remaining 4. On the other hand, \\method consistently improves classification accuracy on each dataset, see Table~\\ref{tb:base2new} and Fig.~\\ref{fig:base2new_gain}. This suggests that \\textbf{modality gap is not highly correlated with downstream generalization}, which is in line with one of the main arguments in~\\citep{ModalityGap} that \\textbf{good generalization does not necessarily need a reduced modality gap}.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the comparison of absolute accuracy gain (AAPE) on base classes with and without L(distill) across 11 classification datasets. It is evident that utilizing L(distill) consistently improves accuracy for all datasets considered. Specifically, the highest accuracy gain is observed on the EuroSAT dataset, reaching 13.91%. Significant gains are also noted on the UCF101, SUN397, and DTD datasets, at 7.02%, 9.45%, and 5.46% respectively. Some gains are also observed on other datasets such as Flowers102, StanfordCars, and FGVCAircraft. This indicates that the knowledge distilled through L(distill) effectively enhances the model's classification accuracy on base classes."
    },
    "103": {
        "figure1": "2410.23698v1_modality_gap",
        "label1": "fig:modality_gap",
        "caption1": "\\textbf{Optimal gap distance obtained by \\method learning} on the few-shot classification task (base-to-new class generalization setting). Y axis indicates the Harmonic mean (H) of base and new class accuracies on each dataset. X axis indicates the gap distance varied by shifting the image and text embeddings following~\\citep{ModalityGap}. We see the optimal gap distance can be increased (on SUN397) or decreased (on ImageNet and EuroSAT) over the default gap obtained from pretraining.}  \\end{center} \\vskip -0.2in",
        "figure2": "2410.23698v1_base2new_gain",
        "label2": "fig:base2new_gain",
        "caption2": "\\textbf{Quantifying the role of LLM knowledge (distilled with $\\mathcal{L}_{\\text{distill}}$) in prompt learning.} $\\mathcal{L}_{\\text{distill}}$ consistently improves the base and new class accuracies on 11 classification datasets.}  \\end{center",
        "text": "Fig.~\\ref{fig:modality_gap} shows the analysis results for few-shot classification on three example datasets. As expected, after \\method-based prompt learning, the optimal gap distance that attains maximum accuracy deviates from the default gap distance of pretraind CLIP. We see the optimal gap is increased on SUN397 dataset, while decreased on ImageNet and EuroSAT datasets. In fact, the optimal gap is reduced on 7 out of a total of 11 datasets and moderately increased on the remaining 4. On the other hand, \\method consistently improves classification accuracy on each dataset, see Table~\\ref{tb:base2new} and Fig.~\\ref{fig:base2new_gain}. This suggests that \\textbf{modality gap is not highly correlated with downstream generalization}, which is in line with one of the main arguments in~\\citep{ModalityGap} that \\textbf{good generalization does not necessarily need a reduced modality gap}.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the optimal gap distance obtained by learning in the few-shot classification task (base-to-new class generalization setting). The Y axis represents the Harmonic mean (H) of base and new class accuracies on each dataset, and the X axis represents the gap distance varied by shifting the image and text embeddings. As shown in the figure, the optimal gap distance can be increased (on SUN397) or decreased (on ImageNet and EuroSAT) compared to the default gap distance of the pretrained CLIP model."
    },
    "104": {
        "figure1": "2410.23725v1_box_plot",
        "label1": "fig:boxplot",
        "caption1": "Clinical coding time (sec.) for complex and simple clinical texts.",
        "text": "In terms of the descriptive statistics, the control had a mean coding time of 242.6 sec. (192.3 SD) while the intervention had 147 sec. (94.5 SD). Due to the presence of outliers and the non-parametric nature of the data distribution, we used the Mann-Whitney U test to analyze the data. The Mann-Whitney U test is a non-parametric test that does not require the assumption of normality and is less sensitive to outliers and imbalanced sample sizes. Data distribution and outliers are illustrated in the box plot in Fig~\\ref{fig:boxplot}. The test is specifically designed to assesses whether the median difference between the observations is significantly different from zero. This allowed us to effectively compare the performance metrics between the two interventions for each participant.",
        "summarize_figure": "figure1",
        "summarization": "The image displays the clinical coding time for complex texts (Period 1) and simple texts (Period 2), comparing the control and intervention groups. In the complex texts period, the coding time distribution of the control group is wider, with obvious outliers and a higher average coding time. The coding time of the intervention group is relatively concentrated, with fewer outliers, and the average coding time is lower than that of the control group. In the simple texts period, the coding time of both groups is significantly shortened. The coding time of the intervention group is still lower than that of the control group, but the difference is reduced, and there are outliers in both groups."
    },
    "105": {
        "figure1": "2410.23726v1_KL",
        "label1": "fig:kl_dist",
        "caption1": "\\footnotesize{The trajectories of the KL divergence as a function of training steps are plotted for both methods. Specifically, we plot the mean KL and the standard deviation of the KL for the $40$ independently trained policies for both methods. Green denotes the KL trajectory for the vanilla PPO method, whereas blue indicates the variance-aware method. As can be seen, by the end of the training, with high probability, the KL divergence of the final policy from the reference policy is roughly the same for both methods. In particular, both methods produce policies whose KL divergences from the reference policy lie between $1.2$ and $1.4$.}} %\\vspace{-0.2cm",
        "text": "Figure \\ref{fig:kl_dist} shows the evolution of the KL divergence between the trained and reference policies for both methods. The average and standard deviation of the KL divergence for the $40$ policies for both sets of methods are plotted. As can be seen with high probability, the KL divergence for both methods lies within the $1.2$ and $1.4$ range. Each of the $40$ independent policies was run with an initial random seed of $0$.     \\centering     \\includegraphics[width = 0.5\\textwidth]{Figures/KL.png}     \\caption{\\footnotesize{The trajectories of the KL divergence as a function of training steps are plotted for both methods. Specifically, we plot the mean KL and the standard deviation of the KL for the $40$ independently trained policies for both methods. Green denotes the KL trajectory for the vanilla PPO method, whereas blue indicates the variance-aware method. As can be seen, by the end of the training, with high probability, the KL divergence of the final policy from the reference policy is roughly the same for both methods. In particular, both methods produce policies whose KL divergences from the reference policy lie between $1.2$ and $1.4$.}}     %\\vspace{-0.2cm}     \\label{fig:kl_dist}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the trajectories of KL divergence versus training episodes for two training methods: Variance Aware Training and Vanilla Training. It plots the mean and standard deviation of KL divergence for 40 independently trained policies for each method. The blue line represents the KL divergence trajectory for Variance Aware Training, while the green line represents that of Vanilla PPO. As shown, by the end of training, the KL divergence of the final policy from the reference policy is roughly the same for both methods, with KL divergence values between 1.2 and 1.4."
    },
    "106": {
        "figure1": "2410.23726v1_proxy_reward",
        "label1": "fig:proxy_reward",
        "caption1": "\\footnotesize{The trajectories of the proxy reward as a function of training steps are plotted for both methods. Specifically, we plot the mean proxy reward and the standard deviation of the proxy rewards for the $40$ independently trained policies for both methods. Green denotes the trajectory for the vanilla PPO method, whereas blue indicates the variance-aware method.}} %\\vspace{-0.2cm",
        "text": "Figure \\ref{fig:proxy_reward} shows the evolution of the rewards collected by the policies for both methods. The average and standard deviation of the rewards for the $40$ policies for both sets of methods are plotted.      \\centering     \\includegraphics[width = 0.5\\textwidth]{Figures/proxy_reward.png}     \\caption{\\footnotesize{The trajectories of the proxy reward as a function of training steps are plotted for both methods. Specifically, we plot the mean proxy reward and the standard deviation of the proxy rewards for the $40$ independently trained policies for both methods. Green denotes the trajectory for the vanilla PPO method, whereas blue indicates the variance-aware method.}}     %\\vspace{-0.2cm}     \\label{fig:proxy_reward}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the trajectories of proxy rewards for two training methods (Vanilla Training and Variance Training) as a function of training episode. The green curve represents the trajectory of Vanilla Training, while the blue curve represents the trajectory of Variance Training. It shows the mean proxy reward and the standard deviation of the proxy rewards for 40 independently trained policies. As shown in the figure, the proxy rewards of both methods increase with training. In the early stage of training, the reward values of the two methods are similar. However, as training progresses, the reward value of Variance Training is consistently higher than that of Vanilla Training, and the reward standard deviation of Variance Training is also relatively smaller, indicating a more stable training process."
    },
    "107": {
        "figure1": "2410.23726v1_Bar_2",
        "label1": "fig:reward_dist_2",
        "caption1": "\\footnotesize{The reward distribution for the two methods compared with the reference policy's quality. The distribution marked in indigo represents the reward distribution for the reference policy, based on 100 samples of the average reward determined by the judge reward model on responses generated by \\textbf{GPT-2}. The reward distribution from the reference policy has a mean of $0.15$ and a variance of $0.012$. The reward distribution for the variance-aware method (in red) has a mean of $0.41$ and a variance of $0.016$. The reward distribution for the vanilla PPO method (in cyan) has a mean of $0.43$ and a variance of $0.038$.}}  %\\vspace{-0.2cm",
        "text": "In Figure \\ref{fig:reward_dist_2}, we repeat the experiment of Section \\ref{sec:PPO}, but this time with $100$ sample policies trained using the vanilla and the variance aware method and evaluated using the judge reward model.\n\\centering     \\includegraphics[width = \\textwidth]{Figures/Bar_2.png}     \\caption{\\footnotesize{The reward distribution for the two methods compared with the reference policy's quality. The distribution marked in indigo represents the reward distribution for the reference policy, based on 100 samples of the average reward determined by the judge reward model on responses generated by \\textbf{GPT-2}. The reward distribution from the reference policy has a mean of $0.15$ and a variance of $0.012$. The reward distribution for the variance-aware method (in red) has a mean of $0.41$ and a variance of $0.016$. The reward distribution for the vanilla PPO method (in cyan) has a mean of $0.43$ and a variance of $0.038$.}}     \\label{fig:reward_dist_2}     %\\vspace{-0.2cm}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the reward distribution histograms for a reference policy and two methods (Variance Aware Policy and Vanilla PPO). The reference policy's reward distribution (indigo) is concentrated around lower reward values, with a mean of 0.15. The reward distributions for the Variance Aware Policy (red) and Vanilla PPO (cyan) are shifted towards higher reward values, with means of 0.41 and 0.43, respectively. This indicates that both the Variance Aware Policy and Vanilla PPO achieve higher rewards compared to the reference policy. Also, the distribution of Vanilla PPO is wider, showing it has a higher variance of rewards."
    },
    "108": {
        "figure1": "2410.23745v1_end-to-end-performance",
        "label1": "fig:cifar100-perf",
        "caption1": "End-to-end performance comparison between \\sys and the TVM baseline on CIFAR-100.",
        "text": "\\centering     \\includegraphics[width=\\columnwidth]{figures/end-to-end-performance.pdf}     \\vspace{-2.2em}     \\caption{End-to-end performance comparison between \\sys and the TVM baseline on CIFAR-100.}     \\label{fig:cifar100-perf}",
        "summarize_figure": "figure1",
        "summarization": "The image illustrates the end-to-end performance comparison between Syno and the TVM baseline on the CIFAR-100 dataset. The chart consists of two parts, showing the speedup on CPU and GPU, respectively. For CPU speedup, Syno outperforms TVM on all models, with ResNet-18 and ResNet-34 showing the most significant improvements, with speedups of 3.41 and 3.33, respectively. For GPU speedup, Syno also exceeds TVM on all models, with speedups of 2.66 and 2.45 on ResNet-18 and ResNet-34, respectively, showing a clear advantage. Overall, Syno achieves performance improvements over TVM on both CPU and GPU."
    },
    "109": {
        "figure1": "2410.23745v1_imagenet-performance",
        "label1": "fig:imagenet-result",
        "caption1": "Pareto optimal curves of accuracy vs. inference time between \\sys and the TVM baseline on ImageNet. For each model, the standalone point is the baseline, and the connected points are discovered by \\sys.",
        "text": "\\centering     \\includegraphics[width=\\columnwidth]{figures/imagenet-performance.pdf}     \\vspace{-2.2em}     \\caption{Pareto optimal curves of accuracy vs. inference time between \\sys and the TVM baseline on ImageNet. For each model, the standalone point is the baseline, and the connected points are discovered by \\sys.}     \\label{fig:imagenet-result}\nFor every model, we select some discovered operators that have comparable accuracy with the baseline and re-evaluate them on ImageNet. We plot the Pareto optimal curves of accuracy vs. inference time in \\cref{fig:imagenet-result}.  Most of our operators exhibit a minor 1\\% to 2\\% accuracy loss while enabling $1.10\\times$ to $4.73\\times$ speedups. If more accuracy loss is acceptable, then going along the Pareto curves further boosts performance.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the Pareto optimal curves of accuracy versus inference time between sys and the TVM baseline on ImageNet. For each model, the standalone point represents the baseline performance, and the connected points indicate the optimization results discovered by sys. Specifically, the left plot shows the relationship between CPU inference time (ms) and Top-1 accuracy. It is evident that for all models, sys can significantly reduce CPU inference time with a slight sacrifice in accuracy. For instance, the ResNet18 model, after optimization by sys, experiences a substantial reduction in inference time with a marginal decrease in accuracy. The right plot displays the relationship between GPU inference time (ms) and Top-1 accuracy, similarly demonstrating the trend that sys can optimize inference time on the GPU."
    },
    "110": {
        "figure1": "2410.23745v1_kernel-performance",
        "label1": "fig:layerwise",
        "caption1": "Layer-wise performance comparison between \\sys and NAS-PTE on ResNet-34.",
        "text": "\\centering     \\includegraphics[width=0.9\\textwidth]{figures/kernel-performance.pdf}     \\vspace{-1.2em}     \\caption{Layer-wise performance comparison between \\sys and NAS-PTE on ResNet-34.}     \\label{fig:layerwise}",
        "summarize_figure": "figure1",
        "summarization": "This chart summarizes the layer-wise performance comparison between \\sys and NAS-PTE on ResNet-34.The chart contains two subplots showing CPU and GPU speedups, respectively.For CPU speedup, Syno Operator 2 achieves the highest speedup at layer L1, at 23.20x, while in other layers, NAS-PTE Seq 3 also shows high speedups, such as reaching 14.02x at layer L17.For GPU speedup, Syno Operator 2 achieves the highest speedup at layer L8, at 6.75x, and also reaches 6.79x at layer L30.Overall, Syno Operator 2 demonstrates significant speedups in some layers on both CPU and GPU."
    },
    "111": {
        "figure1": "2410.23745v1_gpt-loss",
        "label1": "fig:gpt-loss-result",
        "caption1": "Comparison of language perplexity vs. training steps between \\sys and the original GPT-2.",
        "text": "\\centering     \\includegraphics[width=\\columnwidth]{figures/gpt-loss.pdf}     \\vspace{-2.2em}     \\caption{Comparison of language perplexity vs. training steps between \\sys and the original GPT-2.}     \\label{fig:gpt-loss-result}\nWe follow Primer~\\cite{primer} to allocate a 30-minute training period on GPT-2 for each searched operator and compare their final language perplexity results.  We then extend the training for the best-performing operator and the original model to reach 100,000 steps as shown in \\cref{fig:gpt-loss-result}. When searching for substitutions for the QKV projection, our best operator achieves a $1.1\\times$ training speedup and reduces the perplexity to $99$, outperforming the original model’s perplexity of $111$. More specifically, our operator constructs the original projections by groups, which allows the QKV matrices used in the attention modules to learn from different features of input tokens, thereby improving the training efficiency.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the comparison of language perplexity versus training steps between Syno and the original GPT-2. As the number of training steps increases, the perplexity of both models shows a decreasing trend. In the early stage of training (approximately between 0 and 20000 steps), Syno's perplexity decreases faster than that of GPT-2. After reaching about 40000 steps, the perplexity of both models tends to flatten out, but Syno's perplexity remains lower than GPT-2's. Finally, at 100000 steps, Syno's perplexity is slightly below 100, while GPT-2's perplexity is slightly above 100. This indicates that Syno outperforms the original GPT-2 in reducing language perplexity."
    },
    "112": {
        "figure1": "2410.23746v1_text_length",
        "label1": "fig:text_length",
        "caption1": "Text length distribution of DetectRL.",
        "text": "In this section, we analyze the textual features of DetectRL samples to provide additional potentially valuable insights.     \\centering     \\includegraphics[width=0.8\\textwidth, trim=0 0 0 0]{figures/text_length.pdf}     \\caption{Text length distribution of DetectRL.}     \\label{fig:text_length}\nWe performed a statistical analysis of text length distribution in DetectRL, as shown in \\autoref{fig:text_length}. Compared to academic writing and social media texts, news writing and creative writing exhibit notably longer average lengths. The distributions for texts generated by Claude-instant, PaLM-2-bison, and Llama-2-70b are similar, whereas GPT-3.5-turbo tends to produce longer texts. Additionally, we observed that samples subjected to attack manipulation show almost no significant difference in length, except in the data mixing setup.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the text length distribution in the DetectRL dataset across Multi-Domain, Multi-LLM, and Multi-Attack categories. In the Multi-Domain section, the text length distribution of the ArXiv dataset is concentrated in a shorter range, while XSum, Writing, and Review datasets have relatively longer text lengths. Regarding Multi-LLM, the text length distributions of texts generated by Claude-instant, PaLM-2-bison, and Llama-2-70b are similar, with GPT-3.5-turbo tending to produce longer texts. In terms of Multi-Attack, the impact of attack methods on text length is not significant, except for data mixing."
    },
    "113": {
        "figure1": "2410.23746v1_n_gram",
        "label1": "fig:n_gram",
        "caption1": "\\textit{N}-gram distribution of DetectRL.",
        "text": "\\centering     \\includegraphics[width=0.8\\textwidth, trim=0 0 0 0]{figures/n_gram.pdf}     \\caption{\\textit{N}-gram distribution of DetectRL.}     \\label{fig:n_gram}\nWe performed statistical analysis of the \\textit{n}-gram distribution in DetectRL, focusing on unigrams, bigrams, and trigrams. The results are presented in \\autoref{fig:n_gram}. Among the four domains, creative writing exhibits the greatest variety of unigrams, bigrams, and trigrams, indicating a higher \\textit{n}-gram diversity. In contrast, academic writing shows the lowest diversity. Among the different LLMs, GPT-3.5-turbo demonstrates the most extensive vocabulary usage, followed by Claude-instant, Llama-2-70b, and PaLM-2-bison, in order of decreasing \\textit{n}-gram richness. Additionally, samples with perturbation attack show the highest \\textit{n}-gram diversity due to the substitution of characters and vocabulary. Notably, in samples involving data mixing, \\textit{n}-gram richness is significantly lower, approximately half that of other sample types.",
        "summarize_figure": "figure1",
        "summarization": "The picture illustrates the statistical analysis results of n-gram distribution in DetectRL, comparing the counts of unigrams, bigrams, and trigrams from three perspectives: Multi-Domain, Multi-LLM, and Multi-Attack. In Multi-Domain, writing exhibits the highest n-gram diversity, while review shows the lowest. Regarding Multi-LLM, GPT-3.5 demonstrates the richest vocabulary usage, followed by Claude, then Llama-2, and lastly PaLM-2. In Multi-Attack, samples with perturb attack show the highest n-gram diversity, while data mixing samples have significantly lower n-gram richness, approximately half that of other sample types."
    },
    "114": {
        "figure1": "2410.23746v1_readability",
        "label1": "fig:legibility",
        "caption1": "Readability distribution of DetectRL.",
        "text": "\\centering     \\includegraphics[width=0.8\\textwidth, trim=0 0 0 0]{figures/readability.pdf}     \\caption{Readability distribution of DetectRL.}     \\label{fig:legibility}",
        "summarize_figure": "figure1",
        "summarization": "Here is a summary generated based on the chart information you provided:This figure shows the readability distribution of DetectRL under Multi-Domain, Multi-LLM, and Multi-Attack settings. In the Multi-Domain aspect, the readability distribution of ArXiv is significantly higher than other domains, while XSum has a relatively lower readability. In the Multi-LLM aspect, the readability distribution of GPT-3.5 is slightly higher than other models. In the Multi-Attack aspect, the readability distribution of Human is relatively high, while Perturb and Mixing have relatively low readability distributions."
    },
    "115": {
        "figure1": "2410.23746v1_lexical_diversity",
        "label1": "fig:lexical_diversity",
        "caption1": "Lexical diversity distribution of DetectRL.",
        "text": "\\centering     \\includegraphics[width=0.8\\textwidth, trim=0 0 0 0]{figures/lexical_diversity.pdf}     \\caption{Lexical diversity distribution of DetectRL.}     \\label{fig:lexical_diversity}\nUpon examining different dimensions within DetectRL, we observed a unique phenomenon in \\autoref{fig:lexical_diversity}: there is almost no significant variation in the distribution of lexical diversity across domains and attacks. This contrasts sharply with the other three features we analyzed. Additionally, we noted that although GPT-3.5-turbo generated samples exhibited slight differences compared to those samples generated by various other LLMs, they generally clustered around a lexical diversity score of 0.7.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the lexical diversity distribution of DetectRL across different dimensions. In the Multi-Domain aspect (arXiv, XSum, Writing, Review), there is almost no significant variation in lexical diversity distribution among different domains. Regarding Multi-LLM (GPT-3.5, Claude, PaLM-2, Llama-2), the samples generated by GPT-3.5 exhibit slight differences compared to those generated by other LLMs, but they generally cluster around a lexical diversity score of approximately 0.7. For Multi-Attack (Direct, Prompt, Paraph., Perturb., Mixing, Human), the lexical diversity distribution shows almost no significant difference among different attack methods."
    },
    "116": {
        "figure1": "2410.23746v1_cross_domains_heatmap",
        "label1": "fig:adaptability_across_domains",
        "caption1": "Generalization in multi-domain.",
        "figure2": "2410.23746v1_cross_models_heatmap",
        "label2": "fig:adaptability_across_llms",
        "caption2": "Generalization in multi-LLM.",
        "figure3": "2410.23746v1_cross_attacks_heatmap",
        "label3": "fig:adaptability_across_attacks",
        "caption3": "Generalization in multi-attack.",
        "text": "We conducted a comprehensive analysis of the generalization of detectors, providing detailed experimental results from three perspectives: Generalization in multi-LLM, Generalization in multi-domain, and Generalization in multi-attack. To visually present these capabilities, we have utilized heatmaps for illustration. The specific heatmaps can be found in \\autoref{fig:adaptability_across_domains}, \\autoref{fig:adaptability_across_llms} and \\autoref{fig:adaptability_across_attacks}.\n\\centering     \\includegraphics[width=1.0\\textwidth, trim=0 0 0 0]{figures/cross_domains_heatmap.pdf}  `    \\caption{Generalization in multi-domain.}     \\label{fig:adaptability_across_domains}",
        "summarize_figure": "figure1",
        "summarization": "The image illustrates the generalization performance of detectors across multiple domains. It presents heatmaps for twelve detectors: Likelihood, Rank, LogRank, LRR, NPR, DetectGPT, Fast-DetectGPT, DNA-GPT, Revise-Detect, Binoculars, Rob-Base, and X-Rob-Base, evaluated on four domains: arxiv, xsum, writing, and review. The X-Rob-Base detector shows high detection performance across all domains, particularly in the review domain, with detection efficacy approaching 1. The Rank detector exhibits relatively low detection performance in all domains, especially in the review domain, with detection efficacy near 0. Other detectors demonstrate varying detection performance across different domains; for instance, the Likelihood detector performs relatively well in the writing domain but comparatively poorly in the arxiv domain."
    },
    "117": {
        "figure1": "2410.23746v1_cross_models_heatmap",
        "label1": "fig:adaptability_across_llms",
        "caption1": "Generalization in multi-LLM.",
        "figure2": "2410.23746v1_cross_domains_heatmap",
        "label2": "fig:adaptability_across_domains",
        "caption2": "Generalization in multi-domain.",
        "figure3": "2410.23746v1_cross_attacks_heatmap",
        "label3": "fig:adaptability_across_attacks",
        "caption3": "Generalization in multi-attack.",
        "text": "We conducted a comprehensive analysis of the generalization of detectors, providing detailed experimental results from three perspectives: Generalization in multi-LLM, Generalization in multi-domain, and Generalization in multi-attack. To visually present these capabilities, we have utilized heatmaps for illustration. The specific heatmaps can be found in \\autoref{fig:adaptability_across_domains}, \\autoref{fig:adaptability_across_llms} and \\autoref{fig:adaptability_across_attacks}.\n\\centering     \\includegraphics[width=1.0\\textwidth, trim=0 0 0 0]{figures/cross_models_heatmap.pdf}     \\caption{Generalization in multi-LLM.}     \\label{fig:adaptability_across_llms}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the generalization ability of detectors in a multi-LLM environment. It contains 12 heatmaps, each corresponding to a detector (Likihood, Rank, LogRank, LRR, NPR, DetectGPT, Fast-DetectGPT, DNA-GPT, Revise-Detect, Binoculars, Rob-Base, X-Rob-Base). The x-axis represents different LLM models (GPT3.5, PaLM2, Claude, Llama2), and the y-axis also represents these LLM models. The color intensity in the heatmaps indicates the detection performance of the detectors on different LLM combinations; darker colors generally indicate better detection performance. For example, for the X-Rob-Base detector, the detection performance on all LLM combinations is close to 1, indicating its strong generalization ability. Other detectors show different detection performances on different LLM combinations, suggesting that their generalization ability may be influenced by the LLM model."
    },
    "118": {
        "figure1": "2410.23746v1_cross_attacks_heatmap",
        "label1": "fig:adaptability_across_attacks",
        "caption1": "Generalization in multi-attack.",
        "figure2": "2410.23746v1_cross_domains_heatmap",
        "label2": "fig:adaptability_across_domains",
        "caption2": "Generalization in multi-domain.",
        "figure3": "2410.23746v1_cross_models_heatmap",
        "label3": "fig:adaptability_across_llms",
        "caption3": "Generalization in multi-LLM.",
        "text": "We conducted a comprehensive analysis of the generalization of detectors, providing detailed experimental results from three perspectives: Generalization in multi-LLM, Generalization in multi-domain, and Generalization in multi-attack. To visually present these capabilities, we have utilized heatmaps for illustration. The specific heatmaps can be found in \\autoref{fig:adaptability_across_domains}, \\autoref{fig:adaptability_across_llms} and \\autoref{fig:adaptability_across_attacks}.\n\\centering     \\includegraphics[width=1.0\\textwidth, trim=0 0 0 0]{figures/cross_attacks_heatmap.pdf}     \\caption{Generalization in multi-attack.}     \\label{fig:adaptability_across_attacks}",
        "summarize_figure": "figure1",
        "summarization": "The image illustrates the generalization performance under multi-attack scenarios, presented as heatmaps showing the performance of different detectors against various attack types. The horizontal axis represents different attack types, including Direct, Prompt, Paraphrase, Perturb, and Mixing. The vertical axis represents different detectors such as Likelihood, Rank, LogRank, LRR, NPR, DetectGPT, Fast-DetectGPT, DNA-GPT, Revise-Detect, Binoculars, Rob-Base, and X-Rob-Base. The color intensity in the heatmaps indicates the detection effectiveness, with darker colors representing better performance. The image reveals significant differences in the generalization performance of different detectors when faced with different attacks. For example, X-Rob-Base shows better performance under various attacks, while other detectors show weaker performance under specific attacks. Overall, this image highlights the varying generalization abilities of detectors in a multi-attack environment, providing an important reference for selecting appropriate detectors and improving their robustness."
    },
    "119": {
        "figure1": "2410.23751v1_average_acc_base_classes",
        "label1": "fig:avg_acc_bc",
        "caption1": "Average test accuracy of classes used for base training across the incremental tasks.",
        "text": "\\begin{figure}     \\centering     \\includegraphics[width=0.9\\linewidth]{images/average_acc_base_classes.png}     \\caption{Average test accuracy of classes used for base training across the incremental tasks.}     \\label{fig:avg_acc_bc}     \\end{figure}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the average test accuracy of classes used for base training across incremental tasks. The x-axis represents the number of tasks, ranging from 0 to 50, and the y-axis represents the accuracy, ranging from 50 to 80. The graph includes two curves, representing the accuracy changes in cases \"With Significance\" and \"Without Significance\". Both curves show a decreasing trend, indicating that the average test accuracy of base training classes gradually decreases as the number of tasks increases. The accuracy decreases faster when the number of tasks is small (0-10), and then the decreasing speed slows down. The two curves are close to each other in most areas, but near tasks 25-30, the \"With Significance\" curve has a significant drop."
    },
    "120": {
        "figure1": "2410.23751v1_best_base_classes",
        "label1": "fig:best_acc_bc",
        "caption1": "\\textbf{Test accuracy of $10$ classes from the set of classes used for base training across the incremental tasks.",
        "text": "\\centering     \\includegraphics[width=0.9\\linewidth]{images/best_base_classes.png}     \\caption{\\textbf{Test accuracy of $10$ classes from the set of classes used for base training across the incremental tasks.}}     \\label{fig:best_acc_bc}",
        "summarize_figure": "figure1",
        "summarization": "Here is a summary of the figure:The figure shows the test accuracy of 10 classes from the set of classes used for base training across incremental tasks. The horizontal axis represents the number of tasks, ranging from 0 to 50, and the vertical axis represents the accuracy, ranging from 50 to 80. The figure includes two curves, representing the test accuracy under two conditions: \"With Significance\" and \"Without Significance\". Both curves show a downward trend, indicating that as the number of tasks increases, the test accuracy of the model gradually decreases. In the early stage of the tasks (0-10), the two curves are relatively close, and the accuracy is high, around 70%. As the tasks progress, both curves fluctuate, but the overall trend remains downward. In the later stage of the tasks (30-50), the two curves gradually converge, and the accuracy decreases to between 50% and 60%."
    },
    "121": {
        "figure1": "2410.23751v1_Memory_Budget",
        "label1": "fig:stability_plot",
        "caption1": "\\textbf{Per-task test accuracies of the model trained on the last incremental task across different memory budgets",
        "text": "\\centering     \\includegraphics[width=0.9\\linewidth]{images/Memory_Budget.png}     \\caption{\\textbf{Per-task test accuracies of the model trained on the last incremental task across different memory budgets}}     \\label{fig:stability_plot}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the per-task test accuracies of a model trained on the last incremental task across different memory budgets. It shows the accuracy variations across five incremental tasks (Task 1 to Task 5) for different memory budgets (100, 20, 50, 5, and 10). With a memory budget of 100, the model achieves the highest accuracy at Task 1, approximately 61percent, followed by a decrease, a rebound at Task 3, and stabilization around 52percent. A memory budget of 20 results in lower overall accuracy compared to a budget of 100, but there is an increase at Task 5. With a memory budget of 50, the accuracy remains relatively stable, fluctuating between 50percent and 55percent. Memory budgets of 5 and 10 yield lower accuracies, but both show a significant increase at Task 5, reaching 65percent and 57percent, respectively."
    },
    "122": {
        "figure1": "2410.23753v1_bayesbad",
        "label1": "fig:bayeselo",
        "caption1": "The difference between BayesElo ratings and the Elo ratings according to our method. We removed to each Elo the average Elo of all players in its respective method, such that the average effective Elo for both BayesElo and our method is $0$",
        "text": "We also used BayesElo\\cite{hutchison_whole-history_2008} to evaluate the Elo ratings of our models using all the PGN files that were generated recording the games played between all our models. In Figure~\\ref{fig:bayeselo}, we plotted a point on $(x, y)$ for each player where $x$ is its Elo rating following our method and $y$ is the difference in predicted Elo between our method and BayesElo.\n\\centering     \\includegraphics[width=0.49\\textwidth]{figures/bayesbad}     \\caption{The difference between BayesElo ratings and the Elo ratings according to our method. We removed to each Elo the average Elo of all players in its respective method, such that the average effective Elo for both BayesElo and our method is $0$}     \\label{fig:bayeselo}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the difference between custom Elo ratings and BayesElo ratings. The horizontal axis represents the custom Elo rating (Elo custom), and the vertical axis represents the difference between the custom Elo rating and the BayesElo rating (Elo custom - Elo Bayes). As shown in the figure, when the custom Elo rating is low (less than 0), the difference is large, close to 300. As the custom Elo rating increases, the difference gradually decreases, and it is close to 0 when the custom Elo rating reaches about 1000. When the custom Elo rating continues to increase, the difference becomes negative, and reaches the lowest point at about -200 when the custom Elo rating reaches about 2000. This indicates that there is a large difference between the two evaluation methods when the Elo rating is low and high, while the two evaluation methods are relatively close when the Elo rating is medium."
    },
    "123": {
        "figure1": "2410.23769v1_Bar-all-question",
        "label1": "Bar-all-question",
        "caption1": " Human evaluation on question generation. Each model generated questions based on the same sampled set of admission reports, and each human expert provided scores for all criteria, with the scoring scale ranging from integers 1 to 5, wherein higher scores denoted better performance. Error bars depict the standard deviation of the mean scores. ",
        "text": "Human evaluation on question generation. Each model generated questions based on the same sampled set of admission reports, and each human expert provided scores for all criteria, with the scoring scale ranging from integers 1 to 5, wherein higher scores denoted better performance. Error bars depict the standard deviation of the mean scores. }\\label{Bar-all-question}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the evaluation results of different models on four metrics for question generation: coherence, sufficiency of key information, information correctness, and professionalism. The evaluation scores range from 1 to 5, with higher scores indicating better performance; error bars represent the standard deviation of the mean scores. In terms of coherence, the ERNIE 4 model scores slightly higher than other models, while the Qwen and Spark 4 models score relatively lower. Regarding the sufficiency of key information, the differences between the models are small, with the Mistral 7B model scoring slightly higher. In terms of information correctness, the ERNIE 4 and ChatGLM 4 models have higher scores, while the Doubao model has a relatively lower score. Regarding professionalism, the differences between the models are small, but overall, the ERNIE 4 model performs slightly better."
    },
    "124": {
        "figure1": "2410.23769v1_Bar-all-answer",
        "label1": "Bar-all-answer",
        "caption1": " Human evaluation on answer generation. Each model generated answers based on the same sampled set of AI-generated questions, and each human expert provided scores for all criteria, with the scoring scale ranging from integers 1 to 5, wherein higher scores denoted better performance. Error bars depict the standard deviation of the mean scores. ",
        "text": "Human evaluation on answer generation. Each model generated answers based on the same sampled set of AI-generated questions, and each human expert provided scores for all criteria, with the scoring scale ranging from integers 1 to 5, wherein higher scores denoted better performance. Error bars depict the standard deviation of the mean scores. }\\label{Bar-all-answer}",
        "summarize_figure": "figure1",
        "summarization": "This is the result of a human evaluation on answer generation.The image displays the scores of different models on several metrics, including coherence, factual consistency, evidence of statement, and professionalism. The scoring scale ranges from 1 to 5, where higher scores indicate better performance, and error bars depict the standard deviation of the mean scores. In terms of coherence, ERNIE 4 scores the highest, followed by Doubao and Spark 4. Regarding factual consistency, Spark 4 is slightly above the other models. In terms of statement evidence, Doubao and Spark 4 have higher scores. In terms of professionalism, Qwen and Spark 4 score the highest."
    },
    "125": {
        "figure1": "2410.23769v1_Bar-all-rectify",
        "label1": "Bar-all-rectify",
        "caption1": " Human evaluation on answer rectification. In the histogram, the original answers are unfilled, while the answers based on AI rectification are filled with diagonal stripes. Each model generated answers based on the same sampled set of AI-generated questions, and each human expert provided scores for all criteria, with the scoring scale ranging from integers 1 to 5, wherein higher scores denoted better performance. Error bars depict the standard deviation of the mean scores. ",
        "text": "Human evaluation on answer rectification. In the histogram, the original answers are unfilled, while the answers based on AI rectification are filled with diagonal stripes. Each model generated answers based on the same sampled set of AI-generated questions, and each human expert provided scores for all criteria, with the scoring scale ranging from integers 1 to 5, wherein higher scores denoted better performance. Error bars depict the standard deviation of the mean scores. }\\label{Bar-all-rectify}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates human evaluation results on various models (ERNIE 4, ChatGLM 4, Doubao, Hunyuan, Llama 3 70B, Mistral 7B, Qwen, Spark 4) across four dimensions: coherence, factual consistency, evidence of statement, and professionalism. It compares original answers with AI-rectified answers in each dimension. The image indicates that AI-rectified answers generally receive higher scores across most models and dimensions. Notably, in terms of coherence, the rectified answers of the ERNIE 4 model show a significant improvement."
    },
    "126": {
        "figure1": "2410.23771v1_longppl-no-lpv",
        "label1": "fig:longppl-nolpv",
        "caption1": "LongPPL without LPV.",
        "text": "\\centering   \\includegraphics[width=0.99\\columnwidth]{figs/longppl-no-lpv.pdf}   \\caption{LongPPL without LPV.}   \\label{fig:longppl-nolpv}",
        "summarize_figure": "figure1",
        "summarization": "The image illustrates the relationship between LongPPL (without LPV) and three different evaluation metrics. The first subplot shows the relationship between LongPPL and LongBench score, exhibiting a slight negative correlation with a correlation coefficient of -0.34. This suggests a slightly decreasing trend in LongPPL values as the LongBench score increases. The second subplot displays the relationship between LongPPL and LongEval accuracy, with a correlation coefficient close to 0 (-0.03), indicating virtually no linear relationship. The third subplot shows the relationship between LongPPL and RULER score, with a correlation coefficient of 0.11, showing a very weak positive correlation. Overall, the linear relationships between LongPPL and these three metrics are quite weak."
    },
    "127": {
        "figure1": "2410.23780v1_pr_curve",
        "label1": "error",
        "caption1": "Overall P-R curves with various random seeds.",
        "text": "We conduct multiple experiments on our method with various random seed, and the experimental results are shown in Figure~\\ref{error}. We repeated all experiments $5$ times with various seeds which are depicted in different colors. We uniformly sampled $100$ points within the range of $0$ to $1$ as the binary classification threshold for association head in correspondence reasoning procedure, and then calculate the $P_{all}$ and $R_{all}$ for each threshold. The mean fitted line is shown in black, demonstrating the stability of our method. Specifically, we calculated the standard deviation of all evaluation metrics at a fixed threshold among different random seeds. For rule extraction sub-task, the standard deviation of $P_{R.E.}$ and $R_{R.E.}$ are $0.32$ and $0.38$. In the rule-lane correspondence reasoning sub-task the standard deviations are $0.07$ and $0.38$ for $P_{C.R.}$ and $R_{C.R.}$. Overall, the standard deviations of  $P_{all}$, $R_{all}$ and $AP$ are $0.18$ $0.10$ and $1.07$, respectively.\n\\centering     \\includegraphics[width = 0.6\\textwidth]{figures/pr_curve.pdf}     \\caption{Overall P-R curves with various random seeds.}     \\label{error}",
        "summarize_figure": "figure1",
        "summarization": "The image illustrates the Precision-Recall curves under different random seeds. It includes five curves in different colors, representing results with random seeds of 0, 42, 123, 1234, and 2024. The black curve represents the mean of these curves. The Precision values are close when the Recall value is less than 0.67, indicating that the model performance is relatively stable. However, when the Recall value is greater than 0.67, the Precision value starts to decrease significantly, and the difference between different random seeds increases. Overall, the method exhibits some stability under different random seeds, but the stability decreases when the Recall value is high."
    },
    "128": {
        "figure1": "2410.23788v1_loss",
        "label1": "fig:loss",
        "caption1": "Comparing the loss changes of different masking training strategies.",
        "text": "\\centering   \\includegraphics[width=1.0\\textwidth]{loss.pdf}   \\caption{Comparing the loss changes of different masking training strategies.} \\label{fig:loss}\nIn Figure~\\ref{fig:loss}, we separately applied the masking training strategies of MDT and EDT to train EDT-S and extracted $L_{masked}$ and $L_{full}$ values at the $300k \\sim 305k$ and $300k \\sim 400k$ training iterations.  The left-top of the figure depicts the loss changes when using MDT's masking training strategy. As $L_{full}$ decreases, $L_{masked}$ increases, and vice versa, illustrating the conflict between these two losses. This conflict arises because $L_{masked}$ in MDT causes the model to focus on masked token reconstruction while ignoring diffusion training. As shown in the bottom-left of the figure, both the $L_{full}$ and $L_{masked}$ hardly decreased during the 300k to 400k training iterations. The right side of the figure shows the loss changes when using EDT's masking training strategy. The $L_{masked}$ and $L_{full}$ exhibit synchronized changes, and the loss values continuously decrease during the 300k to 400k training iterations.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the loss value changes when using the MDT masking training strategy. In the figure, the red line represents the masked input loss (L masked), and the green line represents the full input loss (L full). From 300,000 to 305,000 iterations, it can be observed that as L full decreases, L masked increases, and vice versa, indicating a conflict between these two losses. Moreover, from 300,000 to 400,000 iterations, both L full and L masked hardly decrease."
    },
    "129": {
        "figure1": "2410.23810v1_thresholdComparison",
        "label1": "fig:thresholdComparison",
        "caption1": "\\rebuttal{CALE} comparison with varying $\\tau$ on the 100k (left) and 200m (right) training regimes.} %\\vspace{-0.2cm",
        "text": "As mentioned in \\cref{sec:cale}, the choice of threshold $\\tau$ affects the overall performance of the agents. Consistent with intuition, \\cref{fig:thresholdComparison} demonstrates that higher values of $\\tau$ result in degraded performance. For the remaining experimental evaluations we set $\\tau$ to $0.5$. This choice has consequences for SAC, due to the way its action outputs are initialized, which we discuss in the next subsection.\n\\centering     %\\raisebox{0.2\\height}     {\\includegraphics[width=0.8\\textwidth]{figures/thresholdComparison.pdf}}     %\\vspace{-0.1cm}     \\caption{\\rebuttal{CALE} comparison with varying $\\tau$ on the 100k (left) and 200m (right) training regimes.}     %\\vspace{-0.2cm}     \\label{fig:thresholdComparison}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the impact of different threshold values of tau on agent performance under the 100k training regime. The horizontal axis represents agent steps (in units of 10,000), and the vertical axis represents Human Normalized IQM. The figure shows that the agent performs best when the value of tau is 0.1, and the performance gradually decreases as the value of tau increases. When the value of tau is 0.9, the agent has the worst performance. All performance curves show an upward trend, indicating that the agent's performance is constantly improving as training progresses. The shaded area represents the confidence interval of performance, with the largest confidence interval when the value of tau is 0.1 and the smallest confidence interval when the value of tau is 0.9."
    },
    "130": {
        "figure1": "2410.23810v1_explorationComparison",
        "label1": "fig:explorationComparison",
        "caption1": "\\rebuttal{CALE} comparison of default SAC exploration with the more common $\\epsilon$-greedy exploration used in discrete action agents on the 100k (left) and 200m (right) training regimes.",
        "text": "Due to its objective including entropy maximization and the fact that the actor is parameterized as a Gaussian distribution, SAC induces a natural exploration strategy obtained by sampling from $\\psi_{A}$ (and simply using $\\mu$ when acting greedily). We refer to this as the {\\bf standard} exploration strategy. However, the exploration strategy typically used on the ALE is {\\bf $\\epsilon$-greedy}, where actions are chosen randomly with probability $\\epsilon$; a common choice for ALE experiments is to start $\\epsilon$ at $1.0$ and decay it to $0.01$ over the first million environment frames. For our continuous action setup we sample uniformly randomly in $[0, 1]\\times [-\\pi, \\pi] \\times [0, 1]$ with probability $\\epsilon$. Perhaps surprisingly, {\\bf standard} outperforms {\\bf $\\epsilon$-greedy} exploration in the 200 million training regime, as demonstrated in \\cref{fig:explorationComparison}. This may be due to the way the action outputs are parameterized, and merits further inquiry.\n\\centering     {\\includegraphics[width=0.8\\textwidth]{figures/explorationComparison.pdf}}     %\\vspace{-0.1cm}     \\caption{\\rebuttal{CALE} comparison of default SAC exploration with the more common $\\epsilon$-greedy exploration used in discrete action agents on the 100k (left) and 200m (right) training regimes.}     \\label{fig:explorationComparison}",
        "summarize_figure": "figure1",
        "summarization": "The image compares the performance of two exploration strategies, Standard and epsilon-Greedy, for the SAC reinforcement learning algorithm across two training durations. In the shorter 100k agent steps training, the epsilon-Greedy strategy initially performs better, but the Standard strategy catches up and slightly surpasses it. However, in the longer training regime of up to 200 million environment frames, the Standard strategy consistently and significantly outperforms the epsilon-Greedy strategy throughout, achieving a much higher final performance."
    },
    "131": {
        "figure1": "2410.23810v1_subgames_200m",
        "label1": "fig:subgames200m",
        "caption1": "\\rebuttal{CALE} comparison of SAC with DQN (using the default Dopamine implementation \\citep{castro18dopamine}) on a selection of games. Returns averaged over 5 independent runs, with shaded areas representing 95\\% confidence intervals.} %\\vspace{-0.2cm",
        "text": "Aggregate performance curves can often conceal interesting per-game differences. Indeed, \\cref{fig:subgames200m} demonstrates that SAC can sometimes surpass the performance of DQN (Asteroids, Bowling, Centipede), sometimes have comparable performance (Asterix, Boxing, MsPacman, Pong), and sometimes under-perform (BankHeist, Breakout, SpaceInvaders). Minimal action sets (as discussed in \\cref{sec:cale}) do not appear to correlate with these performance differences (Bowling, Pong and SpaceInvaders all use a minimal set of 6 actions in the ALE); similarly, reward distributions (as we will discuss below) do not appear to correlate with performance differences between these two agents either. The differences may be due to differences in transition dynamics, as well as exploration challenges, which we discuss below.\n\\centering     {\\includegraphics[width=\\textwidth]{figures/subgames_200m.pdf}}     %\\vspace{-0.1cm}     \\caption{\\rebuttal{CALE} comparison of SAC with DQN (using the default Dopamine implementation \\citep{castro18dopamine}) on a selection of games. Returns averaged over 5 independent runs, with shaded areas representing 95\\% confidence intervals.}     %\\vspace{-0.2cm}     \\label{fig:subgames200m}",
        "summarize_figure": "figure1",
        "summarization": "The image presents a performance comparison between SAC and DQN reinforcement learning agents across a selection of games, with Return on the vertical axis and Environment frames in millions on the horizontal axis. Shaded areas indicate 95% confidence intervals. The image shows that the relative performance of the two agents varies significantly across different games. In games such as Asteroids, Bowling, and Centipede, SAC achieves notably higher returns, indicating superior performance. Conversely, in games like Asterix, Boxing, MsPacman, and Pong, the performance of SAC and DQN is comparable, with their return curves showing similar trends and often overlapping confidence intervals. However, in games like BankHeist, Breakout, and SpaceInvaders, DQN achieves significantly higher returns than SAC, demonstrating DQN's advantage. This indicates that the relative effectiveness of SAC and DQN is highly dependent on the specific game environment."
    },
    "132": {
        "figure1": "2410.23810v1_dqnAggregateComparison",
        "label1": "fig:dqnAggregateComparison",
        "caption1": "Aggregate comparison of SAC \\rebuttal{and PPO} on the CALE with DER \\rebuttal{and SAC-D} on the ALE \\citep{vanHasselt2019der} (left), and DQN on the the ALE \\citep{mnih2015humanlevel} (right).} %\\vspace{-0.2cm",
        "text": "We compare the performance of our SAC baseline against DQN in the 200 million training regime, given that both are off-policy methods which have similar value estimation methods; for the 100k training regime we compare against Data-Efficient Rainbow~\\citep[DER;][]{vanHasselt2019der}, a popular off-policy method for this regime that is based on DQN. As \\cref{fig:dqnAggregateComparison} shows, SAC dramatically under-performs, relative to both these methods. While there may be a number of reasons for this, the most likely one is the fact that SAC was not tuned for CALE, whereas both DER and DQN were tuned specifically for the ALE.\n\\centering     {\\includegraphics[width=0.8\\textwidth]{figures/dqnAggregateComparison.pdf}}     %\\vspace{-0.1cm}     \\caption{Aggregate comparison of SAC \\rebuttal{and PPO} on the CALE with DER \\rebuttal{and SAC-D} on the ALE \\citep{vanHasselt2019der} (left), and DQN on the the ALE \\citep{mnih2015humanlevel} (right).}     %\\vspace{-0.2cm}     \\label{fig:dqnAggregateComparison}",
        "summarize_figure": "figure1",
        "summarization": "The figure compares the performance of different algorithms under different training regimes. The left panel shows the Human Normalized IQM of SAC, SAC-D, PPO, and DER algorithms under 10K agent steps. The DER algorithm shows a significant advantage, with its Human Normalized IQM rapidly increasing from 0 to about 0.12, while the performance of SAC, SAC-D, and PPO is relatively low, remaining below 0.02. The right panel compares the performance of SAC and DQN algorithms under 200 million env frames. DQN significantly outperforms SAC, with DQN's Human Normalized IQM rapidly increasing from 0.6 to about 1.5, while SAC's growth is relatively slow, eventually stabilizing at about 0.8."
    },
    "133": {
        "figure1": "2410.23810v1_sacD100kComparison",
        "label1": "fig:sacD100kComparison",
        "caption1": "\\bf Left:} Comparison of encoders on SAC-D with $\\epsilon$-greedy exploration; {\\bf Right:} Comparison of exploration strategies with the $\\psi_{DQN}$ encoder. Reporting IQM averaged over the 26 Atari 100K games 5 runs with 95\\% stratified bootstrap intervals \\citep{agarwal21deep}.} %\\vspace{-0.2cm",
        "text": "\\centering     %\\raisebox{0.2\\height}     {\\includegraphics[width=0.9\\textwidth]{figures/sacD100kComparison.pdf}}     %\\vspace{-0.1cm}     \\caption{{\\bf Left:} Comparison of encoders on SAC-D with $\\epsilon$-greedy exploration; {\\bf Right:} Comparison of exploration strategies with the $\\psi_{DQN}$ encoder. Reporting IQM averaged over the 26 Atari 100K games 5 runs with 95\\% stratified bootstrap intervals \\citep{agarwal21deep}.}     %\\vspace{-0.2cm}     \\label{fig:sacD100kComparison}",
        "summarize_figure": "figure1",
        "summarization": "The left side of the picture shows a comparison of the performance of the phi_SAC and phi_DQN encoders on SAC-D using epsilon-greedy exploration, measured by Human Normalized IQM over agent steps (in units of 10K). In the early stages of training, the IQM of phi_SAC is slightly higher than that of phi_DQN. However, as training progresses, phi_DQN shows a stronger improvement trend, recovering quickly after an initial dip and continuously increasing, surpassing phi_SAC's IQM after approximately 50K steps. In contrast, the performance of phi_SAC fluctuates more significantly. By the time agent steps reach 100K, the final Human Normalized IQM of phi_DQN is higher than that of phi_SAC. The shaded regions indicate the 95% stratified bootstrap confidence intervals over multiple runs. The right side of the picture compares the performance difference between standard and epsilon-greedy exploration strategies using the psi_DQN encoder, showing that the epsilon-greedy strategy performs significantly better than the standard strategy."
    },
    "134": {
        "figure1": "2410.23828v1_fig7",
        "label1": "tab:ablation_numbers",
        "caption1": "Effectiveness of the number of V-L Decoder's layers.}  \\vspace{-5mm",
        "text": "The impact of the number of layers in visual-language decoder is shown in \\cref{tab:ablation_numbers}. When the visual representations are sequentially processed by more layers, the model consistently achieves higher OA and oIoU. However, the setting of n $\\ge$ 4 introduces more parameters, which could increase the risk of over-fitting. Considering the performance and efficiency, we set n = 3 as the default in our framework.\n\\centerline{\\includegraphics[width=\\linewidth]{fig/fig7.pdf}}   \\vspace{-2mm}   \\caption{Effectiveness of the number of V-L Decoder's layers.} \t\\label{tab:ablation_numbers}   \\vspace{-5mm}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the impact of the number of layers in the visual-language decoder on OA (Overall Accuracy) and oIoU (overlap-based IoU). When the number of layers is 1, the OA value is approximately 70.4, and the oIoU value is approximately 41. As the number of layers increases to 3, both OA and oIoU significantly improve, with OA reaching approximately 71.6 and oIoU reaching approximately 43.5. When the number of layers exceeds 3, the values of OA and oIoU start to decline, with OA around 70.9 and oIoU around 42 when the number of layers is 5. The figure suggests that appropriately increasing the number of layers in the visual-language decoder can improve model performance, but too many layers may lead to performance degradation."
    },
    "135": {
        "figure1": "2410.23844v1_fig2",
        "label1": "fig3",
        "caption1": " Storing Factual and Commonsense Knowledge in LLMs.",
        "text": "\\centering     \\includegraphics[width=7.7cm]{figure/fig2.pdf}     \\caption{ Storing Factual and Commonsense Knowledge in LLMs.}     \\label{fig3}\nWe compared the differences between factual and commonsense knowledge in storage locations by KLFT method. As show in the Figure \\ref{fig3}, the fact prompt is \"Beats Music is owned by\", the target answer is \"Apple\", the commonsense knowledge is sample 3 in Table \\ref{table1}. The horizontal axis represents the layers in LLMs, and the vertical axis represents the tokens $x_{i}$ of different knowledge. The depth of color is determined by $P_{l'}$, and the larger $P_{l'}$, the darker the color, indicating a higher probability of storing knowledge in that layer.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the storage location differences between factual and commonsense knowledge in LLMs. For factual knowledge, the prompt is \"Beats Music is owned by,\" and the target answer is \"Apple.\" In the figure, the horizontal axis represents the layers in LLMs, and the vertical axis represents the tokens (xi) of different knowledge. The color depth is determined by Pl', with darker colors indicating a higher probability of storing knowledge in that layer. In the MLP layers, the probability of tokens related to \"Apple\" (such as \"Be*\", \"ats*\", \"is\", \"owned\", \"by\") is low, and the color is lighter. In the Attention layers, the color of the token \"by\" near layers 15 to 20 is significantly darker, indicating that the Attention layers store more factual knowledge in these layers."
    },
    "136": {
        "figure1": "2410.23856v1_GPT3.5_NoRa",
        "label1": "fig:gpt3.5_nora_performance",
        "caption1": " \\textbf{GPT-3.5-Turbo} Full Performance Evaluation on the NoRa Dataset. } % clean/noise/baseline/our method  \\vspace{-5pt",
        "figure2": "2410.23856v1_Gemini_NoRa",
        "label2": "fig:gemini_nora_performance",
        "caption2": " \\textbf{Gemini} Full Performance Evaluation on the NoRa Dataset. } % clean/noise/baseline/our method  \\vspace{-5pt",
        "text": "Fig.~\\ref{fig:gpt3.5_nora_performance} displays the result of the GPT-3.5-Turbo model's evaluation on the NoRa Dataset.  It corresponds to base model results in Tab.~\\ref{tab:vanilia_LLM}. We have also conducted comprehensive experiments on the Gemini model to evaluate various types of noise. Fig.~\\ref{fig:gemini_nora_performance} shows the full performance evaluation of Gemini on the NoRa dataset.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the full performance evaluation of the GPT-3.5-Turbo model on the NoRa dataset. The horizontal axis represents different datasets: Math-Base9, Sysbolic-Equal, and Commonsense, while the vertical axis represents accuracy (percentage). Different colored bars in the legend represent different noise types, including Zero-shot (gray), Clean (beige), Irrelevant (easy, medium, and hard, represented by light pink, pink, and red, respectively), and Inaccurate (easy, medium, and hard, represented by light blue, blue, and dark blue, respectively). As can be seen from the figure, the Clean data has the highest accuracy on the Math-Base9 dataset, at 46.4 percent, followed by Irrelevant(Medium) at 30.3 percent, and Zero-shot has the lowest accuracy, at 7.2 percent. On the Sysbolic-Equal dataset, the Clean data has the highest accuracy, at 32.7 percent, and Zero-shot has the lowest accuracy, at 8.8 percent. On the Commonsense dataset, the Clean data has the highest accuracy, at 46.0 percent, and Zero-shot has the lowest accuracy, at 40.1 percent."
    },
    "137": {
        "figure1": "2410.23856v1_Gemini_NoRa",
        "label1": "fig:gemini_nora_performance",
        "caption1": " \\textbf{Gemini} Full Performance Evaluation on the NoRa Dataset. } % clean/noise/baseline/our method  \\vspace{-5pt",
        "figure2": "2410.23856v1_GPT3.5_NoRa",
        "label2": "fig:gpt3.5_nora_performance",
        "caption2": " \\textbf{GPT-3.5-Turbo} Full Performance Evaluation on the NoRa Dataset. } % clean/noise/baseline/our method  \\vspace{-5pt",
        "text": "Fig.~\\ref{fig:gpt3.5_nora_performance} displays the result of the GPT-3.5-Turbo model's evaluation on the NoRa Dataset.  It corresponds to base model results in Tab.~\\ref{tab:vanilia_LLM}. We have also conducted comprehensive experiments on the Gemini model to evaluate various types of noise. Fig.~\\ref{fig:gemini_nora_performance} shows the full performance evaluation of Gemini on the NoRa dataset.",
        "summarize_figure": "figure1",
        "summarization": "The picture illustrates the full performance evaluation of the GPT-3.5-Turbo model on the NoRa dataset. For the Math-Base9 task, the accuracy is 7.2% for Zero-shot, 46.4% for Clean data, 39.3% for Irrelevant (Easy), 30.3% for Irrelevant (Medium), 26.6% for Irrelevant (Hard), 23.2% for Inaccurate (Easy), 10.1% for Inaccurate (Medium), and 6.0% for Inaccurate (Hard). For the Sysbolic-Equal task, the accuracy is 8.8% for Zero-shot, 32.7% for Clean data, 28.1% for Irrelevant (Easy), 25.1% for Irrelevant (Medium), 23.0% for Irrelevant (Hard), 29.1% for Inaccurate (Easy), 26.1% for Inaccurate (Medium), and 22.7% for Inaccurate (Hard). For the Commonsense task, the accuracy is 40.1% for Zero-shot, 46.0% for Clean data, 44.3% for Irrelevant (Easy), 42.3% for Irrelevant (Medium), 41.4% for Irrelevant (Hard), 36.7% for Inaccurate (Easy), 33.4% for Inaccurate (Medium), and 28.3% for Inaccurate (Hard)."
    },
    "138": {
        "figure1": "2410.23875v1_casestudy-reverse",
        "label1": "Fig:prop_reverse",
        "caption1": "skip}{-2mm} \\setlength{\\belowcaptionskip}{-2mm} \\caption{The proportion of correct answers obtained by PoG after self-correction.} % \\vspace{-3mm",
        "text": "\\centering  \\vspace{-1.3cm} \t\\includegraphics[width=\\linewidth]{reverse-ratio.pdf}  % \\vspace{-4cm}         \\setlength{\\abovecaptionskip}{-2mm}         \\setlength{\\belowcaptionskip}{2mm} \t\\caption{The proportion of cases with reverse occurrences among all data.}  % \\vspace{-3mm} \t\\label{Fig:reverse_r} In PoG, we design a reflection mechanism to provide the opportunity for self-correction for exploring reasoning paths. In Figure~\\ref{Fig:reverse_r}, we calculate the proportion of cases with reverse occurrences among all questions in CWQ, and the results show that 24\\% of cases involve reversing during the exploration process to achieve self-correction. This demonstrates that LLMs are indeed not always capable of making correct judgments in KG exploration and that self-correction is necessary for KG-augmented LLMs. Figure~\\ref{Fig:prop_reverse} presents the proportion of correct answers obtained by PoG after self-correction on three datasets. Overall, the self-correction in PoG appears to have positively impacted the accuracy of KGQA, particularly for the WebQSP and CWQ datasets, where the proportion of correct answers reached 64\\% and 48\\% after the self-correction process. This analysis suggests that the reflection mechanism in PoG has the potential to enhance the reasoning capabilities of KG-augmented LLM and improve the performance across various datasets by allowing for self-correction and exploration of alternative reasoning paths. \t\\centering         % \\vspace{-2.8mm} \t\\includegraphics[width=\\linewidth]{casestudy-reverse.pdf}  % \\vspace{-4mm}         \\setlength{\\abovecaptionskip}{-2mm}         \\setlength{\\belowcaptionskip}{-2mm} \t\\caption{The proportion of correct answers obtained by PoG after self-correction.}         % \\vspace{-3mm} \t\\label{Fig:prop_reverse}",
        "summarize_figure": "figure1",
        "summarization": "The image displays the proportion of cases with reverse occurrences among all data on the CWQ dataset. Specifically, 24% of cases involved reverse occurrences. This indicates that self-correction is necessary during knowledge graph exploration."
    },
    "139": {
        "figure1": "2410.23889v2_in_domain_numb_env",
        "label1": "fig2:erm_me",
        "caption1": "Comparison of ERM approaches ({\\color{blue}shades of blue}) and Poseidon foundation model ({\\color{ForestGreen}green}) with our framework GEPS ({\\color{red}red}) when increasing the number of training environments.",
        "text": "As shown in Figure \\ref{fig2:erm_me}, Transolver, FNO, MP-PDE, and CNN fail to capture the diversity of behaviors and their performance stagnate when increasing the number of training environments. Non-conditioned methods are not able to capture the diversity of behaviors for several environments, regardless of the backbone used, when using only an initial state as input. Poseidon on its side behaves much better on Gray-Scott and is able to capture this diversity of dynamics. Our adaptive conditioning approach (GEPS on the figures) performs significantly better than all the baselines, outperforming also the large Poseidon foundation model. We can also observe that GEPS benefits from being trained on a large amounts of environments, as its generalization performance improves with the number of training environments.     \\centering     \\includegraphics[width=1.\\textwidth]{figures/in_domain_numb_env.png}     \\caption{Comparison of ERM approaches ({\\color{blue}shades of blue}) and Poseidon foundation model ({\\color{ForestGreen}green}) with our framework GEPS ({\\color{red}red}) when increasing the number of training environments.}     \\label{fig2:erm_me}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the relative loss (log scale) of various models for the Gray-Scott equation under different numbers of training environments. The relative losses of models such as CNN, MPPDE, FNO, and Transolver do not decrease significantly with the increase in the number of training environments, showing limited performance improvement. The Poseidon model exhibits a significantly lower relative loss than the aforementioned models, with the loss decreasing as the number of training environments increases, indicating better performance. The GEPS model has the lowest relative loss, and the loss continues to decrease with the increase in the number of training environments, suggesting that the GEPS model's generalization performance is significantly improved when increasing the number of training environments."
    },
    "140": {
        "figure1": "2410.23889v2_in_domain_numb_traj",
        "label1": "fig3:erm_me",
        "caption1": "Comparison of ERM approaches ({\\color{blue}shades of blue}) and Poseidon ({\\color{ForestGreen}green}) with our framework GEPS ({\\color{red}red}) when increasing the number of trajectories per environment.}  \\vspace{-5mm",
        "text": "For this second series of experiments, we fix the number of environments at 4 and vary the number of training trajectories per environment from 4 up to 1024. Figure \\ref{fig3:erm_me} shows the same behavior as for the previous experiments: ERM approaches rapidly reach a plateau and do not capture the variety of behaviors, even with a large number of trajectories, while GEPS scales well and improves with the number of training samples per environment. We additionally make a comparison with a CNN model trained and evaluated separately for each environment. We plot the average of the models' scores (indicated as \"average\" on the figure). This is an upper-bound of the performance that could be obtained with ERM models trained from scratch (no pretraining as for Poseidon). Note that this requires as many models as environments and is not scalable. While this performs significantly better than training over all the environments, GEPS matches or surpasses this approach.     \\centering     \\includegraphics[width=1.\\textwidth]{figures/in_domain_numb_traj.png}     \\caption{Comparison of ERM approaches ({\\color{blue}shades of blue}) and Poseidon ({\\color{ForestGreen}green}) with our framework GEPS ({\\color{red}red}) when increasing the number of trajectories per environment.}     \\label{fig3:erm_me}",
        "summarize_figure": "figure1",
        "summarization": "The image illustrates the relative loss of different algorithms as the number of training trajectories increases in the Gray-Scott equation. It compares several Empirical Risk Minimization (ERM) methods (including CNN, MPPDE, FNO, and Transolver), Poseidon, and the GEPS framework. As the number of training trajectories increases, the relative loss of ERM methods rapidly reaches a plateau, while the relative loss of the GEPS framework continues to decrease, indicating that GEPS can better utilize the increased training samples to improve performance. In addition, the image also shows the average score of a CNN model trained and evaluated separately for each environment (labeled as \"average\"). Although this method is better than ERM models trained over all environments, GEPS performs as well as or even better than this approach."
    },
    "141": {
        "figure1": "2410.23889v2_adapt_erm_avg",
        "label1": "fig:adapt_erm",
        "caption1": "Out-distribution generalization on 4 new environments using one trajectory per environment for fine-tuning or adaptation. Models have either been pretrained on 4 environments (left column) or 1024 environments (right columns). Metric is Relative L2 loss.",
        "text": "Let us now consider the out-of-distribution behavior of the two approaches. The models are trained on a sample of the environments from $\\mathcal{E}_{\\text{tr}}$ and their associated trajectories, in the same condition as for section \\ref{sec:in-dist}. They are then evaluated on the trajectories of new environments. We report in figure \\ref{fig:adapt_erm} the out-of-distribution generalization performance of ERM methods and GEPS, for the Gray-Scott and Burgers equations, when pretrained on 4 (left) and 1024 (right) environments, with 4 trajectories per environment. For the test, one considers 4 new environments and evaluate on 32 trajectories per environment. Adaptation (GEPS) or fine tuning (baselines) is performed on one trajectory of a new environment.  As for the baselines, we consider CNN and Transolver, plus the Poseidon foundation model for the 2-D Gray-Scott equation only. As above, one may observe a large performance gap between the non adaptive approaches and the adaptive GEPS. This supports our claim on the limitations of pure ERM based approaches to generalize to unseen dynamics configurations and new environments.\n\\centering     \\includegraphics[width=0.9\\linewidth]{figures/adapt_erm_avg.png}     \\caption{Out-distribution generalization on 4 new environments using one trajectory per environment for fine-tuning or adaptation. Models have either been pretrained on 4 environments (left column) or 1024 environments (right columns). Metric is Relative L2 loss.}     \\label{fig:adapt_erm}",
        "summarize_figure": "figure1",
        "summarization": "The picture shows the out-of-distribution generalization performance of models measured by relative L2 loss. The chart is divided into four panels: the top row for the Gray-Scott equation and the bottom row for the Burgers equation. The left column shows models pretrained on 4 environments, and the right column shows models pretrained on 1024 environments. For the Gray-Scott equation, the GEPS model consistently exhibits the lowest relative L2 loss, significantly lower than CNN and T-SOLVER for both pretraining sizes, and lower than POSEIDON when pretrained on 1024 environments. For the Burgers equation, the GEPS model also shows substantially lower relative L2 loss compared to CNN and T-SOLVER under both pretraining conditions. GEPS demonstrates superior out-of-distribution generalization performance, and increasing the number of pretraining environments generally leads to lower relative L2 loss for all models."
    },
    "142": {
        "figure1": "2410.23889v2_envs_pendulum",
        "label1": "fig:pendulum",
        "caption1": "Visualization of different behaviors for the damped and driven pendulum equation",
        "text": "We present the equations and the data generation settings used for all dynamical systems considered in this work. In table \\ref{tab:env_description}, we report the different PDE parameters changed to generate training and adaptation environments.     \\sisetup{table-format=1.1e+1}     \\scriptsize     \\setlength{\\tabcolsep}{1.5pt}     \\caption{A description of the pivotal factors used to generate environments}     \\label{tab:env_description}     \\centering     \\begin{tabular}{ccc}         \\toprule         PDE & training ODE/PDE parameters & adaptation ODE/PDE parameters \\\\         \\midrule         \\multirow{4}{*}{Damped driven pendulum} & $w_0 \\in \\{0.5, 0.7\\}$ & $w_0 \\in \\{0.5, 0.75, 1.0\\}$ \\\\         & $w_f \\in \\{0, 0.75, 1.0\\}$ & $w_f \\in \\{0.3, 0.5, 0.7, 1.0\\}$ \\\\         & $F \\in \\{0.0, 0.1, 0.2 \\} $ & $F \\in \\{0.05, 0.1, 0.15, 0.2\\} $\\\\         & $ \\alpha \\in \\{0., 0.2, 0.5\\}$ & $ \\alpha \\in \\{0.1, 0.5 \\}$ \\\\         \\midrule         \\multirow{3}{*}{1D Burgers} & $\\nu \\in \\{ 5\\mathrm{e}{-1}, 5\\mathrm{e}{-2}, 5\\mathrm{e}{-4}\\}$ & $ \\nu \\in \\{1.0,  5\\mathrm{e}{-5}\\}$ \\\\         & $f(x,t) = \\{F(\\sin(w_f x) + \\cos(w_f t)), 0\\}$ & $f(x,t) = F\\exp(-w_f x^2)$ \\\\         & $ w_f = 1.5$ & $w_f = \\{1.5, 3.0\\}$ \\\\         \\midrule         \\multirow{2}{*}{2D Gray-Scott} & $F \\in \\{0.03, 0.039\\}$ & $F \\in \\{0.025, 0.042\\}$ \\\\         & $k \\in \\{0.058, 0.062\\}$ & $k \\in \\{0.050, 0.065\\}$ \\\\         \\midrule         \\multirow{3}{*}{2D Kolmogorov Flow} & $\\nu \\in \\{1\\mathrm{e}-3, 1\\mathrm{e}-4\\}$ &  $\\nu \\in \\{5\\mathrm{e}-4\\}$ \\\\         & $f(x,y,t) = \\{F \\sin(w_f y), 0\\}$ & $\\{F \\exp(w_f y^2), F(\\cos(w_f x) + \\sin(w_f t)\\}$ \\\\         & Domain $= \\{[0, 0.75 \\pi], [0, \\pi] \\}$ & Domain $= \\{[0, 1.25 \\pi]\\}$ \\\\         \\bottomrule     \\end{tabular}     \\centering     \\includegraphics[width=1\\textwidth]{figures/envs_pendulum.png}     \\caption{Visualization of different behaviors for the damped and driven pendulum equation}     \\label{fig:pendulum}",
        "summarize_figure": "figure1",
        "summarization": "Here is a summary based on the provided chart information:The image shows the behavior of the damped and driven pendulum equation in six different environments. In Environment 1, the value of theta(t) exhibits significant damped oscillation, with the amplitude gradually decreasing and stabilizing over time. In Environment 2, theta(t) decays rapidly to a stable state near zero. Environment 3 also shows a rapid decay to zero, but with a lower initial peak. In Environment 4, theta(t) exhibits periodic oscillation behavior with amplitude fluctuating between positive and negative 0.5. In Environment 5, the change in theta(t) is more complex, gradually increasing from a negative value starting at time step approximately 250, and reaching a peak at time step 700. Environment 6 exhibits small oscillations before time step 700, after which the value of theta(t) increases rapidly."
    },
    "143": {
        "figure1": "2410.23889v2_codavsfuels",
        "label1": "fig:enter-label",
        "caption1": "Convergence speed of the model $\\mathcal{G}_{\\theta}$ to adapt to new environments for the Burgers dataset",
        "text": "In CoDa, the authors advocate to choose relatively small code size, corresponding to the degree of variations defining each environment. In our case, we proposed datasets with more degree of variations than the datasets used by \\citep{Yin2022}, presenting much more differences, necessitating larger code dimension size. In table \\ref{tab:code_dim}, we show the performance of our model on both Gray-Scott and Burgers equation when varying the code dimension for CoDA and our method and the increase in number parameters.     \\centering     \\begin{adjustbox}{valign=t}     \\begin{minipage}{0.7\\textwidth}         \\centering         \\caption{Model size with respect to code dimension}         \\includegraphics[width=\\textwidth]{figures/code_dim.png}         \\label{fig:code_dim}     \\end{minipage}     \\end{adjustbox}     \\vfill     \\begin{adjustbox}{valign=t}      \\begin{minipage}{0.7\\textwidth}         \\centering         \\captionof{table}{Relative MSE on the full trajectory with varying code dimension}         \\label{tab:code_dim}         \\sisetup{table-format=1.1e+1}         \\small         \\setlength{\\tabcolsep}{5pt}         \\begin{tabular}{ccccc}             \\toprule             \\multirow{2}{*}{code size $\\downarrow$} & \\multicolumn{2}{c}{\\textit{CoDA}} & \\multicolumn{2}{c}{\\textit{GEPS}} \\\\             \\cmidrule(l){2-3} \\cmidrule(l){4-5}             & {\\textit{Burgers}} & {\\textit{Gray-Scott}} & {\\textit{Burgers}} & {\\textit{Gray-Scott}} \\\\             \\midrule             1 & 2.04e-2 & 9.72e-2 & 1.49e-2 & 1.10e-1 \\\\             2 & 2.03e-2 & 8.73e-2 & 1.35e-2 & 7.12e-2 \\\\             4 & 1.96e-2 & 9.29e-2 & 1.46e-2 & \\textbf{5.65e-2} \\\\             8 & 1.78e-2 & 9.38e-2 & 1.43e-2 & 7.27e-2 \\\\             16 & 1.84e-2 & 1.01e-1 & \\textbf{1.30e-2} & 5.82e-2 \\\\              \\bottomrule          \\end{tabular}      \\end{minipage}      \\end{adjustbox} One important property of meta-learning framework is their ability to adapt to unseen environments in few adaptation steps. Our framework allows fast adaptation to new environments, compared to a state-of the art method like CoDA. We report in Figure the convergence of the loss during adaption for both our method and CoDA.     \\centering     \\includegraphics[width=\\textwidth]{figures/codavsfuels.png}     \\caption{Convergence speed of the model $\\mathcal{G}_{\\theta}$ to adapt to new environments for the Burgers dataset}     \\label{fig:enter-label} With our framework, we are able to adapt to new environments in less than 100 steps, compared to CoDA which needs 500 steps. For both runs, we used the same learning rate $\\text{lr} = 0.01$. We evaluate the ability of the different adaptation mechanisms for out-range temporal horizon extrapolation. All models have been trained on a temporal horizon $[0, T]$. We thus evaluate the performance of our models when extrapolating outside the trained temporal horizon. For out-range extrapolation, models are evaluated on the horizon $[T, 2T]$. We report in-distribution and out-distribution performance in table \\ref{tab:out_t}. Error accumulates over time and leads to lower performance outside the training horizon, but GEPS still outperforms existing baselines.  & Model $\\downarrow$ & {\\textit{In-d}} & {\\textit{Out-d}} & {\\textit{In-d}} & {\\textit{Out-d}} & {\\textit{In-d}} & {\\textit{Out-d}} & {\\textit{In-d}} & {\\textit{Out-d}} \\\\  \\midrule  & LEADS & \\uline{6.30e-1} & \\textbf{7.28e-1} & \\uline{1.45e-1} & 1.27e-1 & 4.55e-2 & 3.22e-1 & 9.80e-1 & \\uline{1.01} \\\\ & FOCA & 14.0 & 1.08 & 3.56e-1 & 3.34e-1 & 3.12e-1 & 4.60e-1 & 1.0 & 1.05 \\\\ & CoDA & 9.54e-1 & 9.75e-1 & 1.52e-1 & \\uline{1.19e-1} & \\uline{3.41e-2} & 3.35e-1 & \\uline{8.29e-1} & \\uline{9.99e-1} \\\\ & \\textit{GEPS} & \\textbf{4.92e-1} & \\uline{8.45e-1} & \\textbf{8.37e-2} & \\textbf{6.91e-2} & \\textbf{2.23e-2} & \\textbf{2.68e-1} & \\textbf{7.70e-1} & \\textbf{9.74e-1} \\\\ & \\textit{GEPS-Phy} & \\textbf{2.88e-1} & \\textbf{7.12e-1} & \\uline{2.38e-2} & \\uline{1.92e-2} & 4.72e-2 & 3.59e-1 & \\textbf{7.33e-1} & \\textbf{9.32e-1} \\\\",
        "summarize_figure": "figure1",
        "summarization": "This is a graph about the convergence speed of model Gθ adapting to new environments for the Burgers dataset. The graph shows the change in relative mean square error (MSE) with training epoch for two methods. The red curve represents the \"Ours\" method, and the blue curve represents the CoDA method. It can be seen that the \"Ours\" method converges rapidly in less than 100 training epochs, reaching a low MSE value, while the CoDA method converges more slowly, requiring about 500 training epochs to reach a similar MSE level."
    },
    "144": {
        "figure1": "2410.23891v1_exp_ScalingLaw_d100p_at_3ep_v2",
        "label1": "fig:exp_scaling_law_stratified_evaluation",
        "caption1": " Land cover stratified evaluation of models trained with different fractions of the AllClear dataset: $1\\%$, $3.4\\%$, $10\\%$, and $100\\%$.}  \\vspace{-1mm",
        "text": "We use the available land-cover type labels in \\textit{AllClear} to conduct a stratified evaluation across land-cover types. As shown in Figure~\\ref{fig:exp_scaling_law_stratified_evaluation},  we find that both PSNR and SSIM metrics are generally much worse for both water bodies and snow cover. Water bodies have transient wave patterns, and snow cover is also often transient, which may explain the difficulty of predicting these classes. Snow may also be confused with cloud.\n\\centering     \\includegraphics[width=1.\\textwidth]{images/exp_ScalingLaw_d100p_at_3ep_v2.pdf}     \\vspace{-4mm}     \\caption{     Land cover stratified evaluation of models trained with different fractions of the AllClear dataset: $1\\%$, $3.4\\%$, $10\\%$, and $100\\%$.}     \\label{fig:exp_scaling_law_stratified_evaluation}     \\vspace{-1mm}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates a stratified evaluation of models trained with varying fractions (1%, 3%, 10%, 100%) of the AllClear dataset across different land cover types, using PSNR (Peak Signal-to-Noise Ratio) and SSIM (Structural Similarity Index) as evaluation metrics. According to the PSNR metric, water bodies and snow/ice cover exhibit significantly lower PSNR values compared to other land cover types, while grassland, trees, and similar land cover types show relatively higher PSNR values. As the proportion of training data increases, the PSNR values for all land cover types generally trend upward. A similar pattern is observed in the SSIM metric, where water bodies and snow/ice cover have lower SSIM values, other land cover types are relatively higher, and the SSIM values increase correspondingly with the increase in the proportion of training data."
    },
    "145": {
        "figure1": "2410.23891v1_uncrtaints_cldshdw_stratification_comparison_psnr_ssim_d100ps1s2_pretrain",
        "label1": "fig:uncrtaints_cldshdw_stratification_comparison_psnr_ssim_pretrain_d100ps1s2",
        "caption1": " % Cloud removal quality (in PSNR and SSIM) at different cloud and shadow coverage levels for UnCRtainTS models trained on the full AllClear (row 1) and SEN12MS-CR-TS (row 2) datasets. Cloud removal quality measured by PSNR (left column) and SSIM (right column) at different cloud and shadow coverage levels. The top row represents models trained on the full AllClear dataset, and the bottom row represents models trained on the SEN12MS-CR-TS dataset. ",
        "text": "Following past work~\\citep{ebel2022sen12ms}, we also perform a stratified evaluation of accuracy relative to the extent of cloud cover and shadows (Figure~\\ref{fig:uncrtaints_cldshdw_stratification_comparison_psnr_ssim_pretrain_d100ps1s2}). For cloud cover, generally performance decreases with cloud percentage, which is expected.  Training on a larger dataset (AllClear) substantially improves accuracy for low and medium cloud cover, but not for fully clouded regions. Note that the striped pattern is because of fully cloudy images as explained in the Appendix. Shadows are generally less of a problem, and shadow percentage seems to be uncorrelated with performance.\n\\centering   \\includegraphics[width=\\textwidth]{images/uncrtaints_cldshdw_stratification_comparison_psnr_ssim_d100ps1s2_pretrain.pdf}   \\caption{   % Cloud removal quality (in PSNR and SSIM) at different cloud and shadow coverage levels for UnCRtainTS models trained on the full AllClear (row 1) and SEN12MS-CR-TS (row 2) datasets.   Cloud removal quality measured by PSNR (left column) and SSIM (right column) at different cloud and shadow coverage levels. The top row represents models trained on the full AllClear dataset, and the bottom row represents models trained on the SEN12MS-CR-TS dataset.    }   \\label{fig:uncrtaints_cldshdw_stratification_comparison_psnr_ssim_pretrain_d100ps1s2}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the Peak Signal-to-Noise Ratio (PSNR) of cloud removal quality at different cloud and shadow coverage levels for UnCRtainTS models trained on the full AllClear dataset. The horizontal axis represents the average cloud percentage, and the vertical axis represents the PSNR value. As the average cloud percentage increases, the PSNR value generally shows a decreasing trend, indicating that the higher the cloud coverage, the worse the cloud removal effect. In the low cloud coverage area, the PSNR value is relatively high, concentrated between 30 and 45, indicating a better cloud removal effect. As cloud coverage increases, the PSNR value gradually decreases, and the distribution of data points becomes more dispersed, indicating an increase in the uncertainty of the cloud removal effect. Striped patterns can be observed in fully cloudy regions."
    },
    "146": {
        "figure1": "2410.23891v1_uncrtaints_sen12mscrts-pretrain_cldshdw_stratification",
        "label1": "fig:uncrtaints_allclear-d100p_cldshdw_stratification",
        "caption1": "Correlation between cloud removal quality and cloud and shadow coverage of UnCRtainTS trained on full AllClear train set, evaluated on the AllClear test set. From left to right, the columns indicate average cloud coverage, average shadow mask coverage, consistent cloud coverage, and consistent shadow coverage. From top to bottom, the rows indicate the metrics MAE, RMSE, PSNR, SAM, and SSIM. The subplots show a consistent trend that a higher cloud coverage rate correlates with lower image reconstruction quality. ",
        "text": "We illustrate the relationship between qualitative performance and cloud and shadow coverage in Figure~\\ref{fig:uncrtaints_allclear-d100p_cldshdw_stratification}. From the left to the right columns, we quantify the cloud and shadow mask using (1) average cloud coverage, (2) average shadow mask coverage, (3) consistent cloud coverage, and (4) consistent shadow coverage. Specifically, consistent cloud (shadow) coverage refers to the percentage of pixels in the input images that are always covered by clouds (shadows). This shows a consistent trend where higher cloud coverage correlates with decreased quality of the target images, consistent with previous observations. The strips in the subplots, especially in the left column at x-axis values of 0.33, 0.67, and 1.0, are due to the fact that some images are fully clouded, resulting in more data points in particular positions in those subplots. During shadow mask synthesis, we discard regions of shadow masks that overlap with cloud masks. Thus images with low shadow percentage may have extremely high or extremely low cloud coverage. This explains the high variance of model performance in the low shadow percentage region.\n\\centering     \\includegraphics[width=\\textwidth]{images/uncrtaints_sen12mscrts-pretrain_cldshdw_stratification.pdf}     \\caption{Correlation between cloud removal quality and cloud and shadow coverage of UnCRtainTS trained on full AllClear train set, evaluated on the AllClear test set.     From left to right, the columns indicate average cloud coverage, average shadow mask coverage, consistent cloud coverage, and consistent shadow coverage. From top to bottom, the rows indicate the metrics MAE, RMSE, PSNR, SAM, and SSIM.     The subplots show a consistent trend that a higher cloud coverage rate correlates with lower image reconstruction quality.      }     \\label{fig:uncrtaints_allclear-d100p_cldshdw_stratification}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the correlation between cloud removal quality and cloud and shadow coverage of the UnCRtainTS model, evaluated on the AllClear test set. The model was trained on the full AllClear train set. From left to right, the columns represent average cloud coverage, average shadow mask coverage, consistent cloud coverage, and consistent shadow coverage. From top to bottom, the rows represent the metrics MAE, RMSE, PSNR, SAM, and SSIM. The image shows a consistent trend where higher cloud coverage correlates with lower image reconstruction quality."
    },
    "147": {
        "figure1": "2410.23933v1_exsiting_results",
        "label1": "fig:exsiting-results",
        "caption1": "\\textbf{Existing LLMs are struggling to generate long, aligned outputs.} (Left) The actual output lengths when prompting LLMs for generation tasks with specific length constraints under default parameters. All four tested LLMs struggle to exceed 2,000 words. (Right) We attempted to get longer outputs by adjusting the decoding strategies (sampling and beam-search) and parameters (min\\_tokens, length\\_penalty, etc.); see~\\Cref{appx:pilot-study} for details. However, the quality evaluated by humans significantly deteriorated after reaching 2,000 words. By employing our proposed \\methodname{} method, we significantly enhanced the output length of the Qwen2-7B-Instruct backbone model while preserving the quality of the generated content.}  \\vspace{-0.4cm",
        "text": "Recently we have witnessed significant breakthroughs in Large Language Models (LLMs), accompanied by extensive research aimed at improving their alignment with human demands~\\citep{cao2024towards, gao2024towards, quan2024automatically, quan2024dmoerm}.  One key research area is enhancing LLMs' abilities to manage increasingly long contexts~\\citep{han2023lm, pawar2024and, bai2023longbench}.  While much of the existing research on long context LLM alignment emphasizes processing lengthy inputs, and several open-source models like Qwen2~\\citep{yang2024qwen2} and LLaMA3~\\citep{dubey2024llama} herds of models already demonstrate impressive capabilities in long context understanding tasks, such as Needle in a Haystack~\\citep{needle}, a notable gap remains in the ability of LLMs to generate long aligned outputs effectively, as illustrated in~\\Cref{fig:exsiting-results}, and this issue cannot be easily addressed by manipulating decoding strategies.  We analyze that this is due to a training gap: during the pre-training stage, despite having access to a vast array of long text sources, there is a lack of effective instructions to cultivate this capability. Conversely, in the post-training stage, the majority of human-conducted or AI-augmented query-response pairs are short, which leads to the trained LLMs facing challenges in generating long, aligned outputs.\n\\centering     \\vspace{-0.1cm}     \\includegraphics[width=\\linewidth]{figures/exsiting_results.pdf} % Adjust the width as needed and change 'example-image' to your file name     \\caption{\\textbf{Existing LLMs are struggling to generate long, aligned outputs.} (Left) The actual output lengths when prompting LLMs for generation tasks with specific length constraints under default parameters. All four tested LLMs struggle to exceed 2,000 words. (Right) We attempted to get longer outputs by adjusting the decoding strategies (sampling and beam-search) and parameters (min\\_tokens, length\\_penalty, etc.); see~\\Cref{appx:pilot-study} for details. However, the quality evaluated by humans significantly deteriorated after reaching 2,000 words. By employing our proposed \\methodname{} method, we significantly enhanced the output length of the Qwen2-7B-Instruct backbone model while preserving the quality of the generated content.}     \\label{fig:exsiting-results}     \\vspace{-0.4cm}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the length control capability of existing Large Language Models (LLMs) in generating long texts. The x-axis represents the required length, and the y-axis represents the actual generated length. The graph compares the performance of Qwen2-72B-Instruct, Llama3.1-70B-Instruct, Mistral-Large-Instruct, and Qwen2-7B-Instruct under default parameters. The results show that the actual output length of all models is difficult to exceed 2000 words without using the self-lengthen method, failing to meet the longer length requirements. The Qwen2-7B-Instruct model shows some improvement in generation length after adopting the self-lengthen method."
    },
    "148": {
        "figure1": "2410.24019v1_regression_lang_All_xcomet2_strong5",
        "label1": "fig:regression_lang_strong",
        "caption1": "Regression Analysis of language pairs.}  \\vspace{-0.1cm",
        "figure2": "2410.24019v1_regression_many_All_xcomet2_strong5",
        "label2": "fig:regression_strong",
        "caption2": "Regression Analysis of model types and model sizes per language pair.",
        "text": "\\textbf{Is the level of prosody-awareness language-dependent?} In Figure~\\ref{fig:regression_lang_strong} we carry out a similar regression analysis as in Figure~\\ref{fig:regression_strong}, but with the language pair as an independent categorical variable. Interestingly, we observe that there are differences between the three language pairs, and also significant for Spanish vs.\\ German, which indicates that prosody-awareness in \\stt{} could be language-dependent. We hypothesize that the expressivity of the target language might be a relevant factor, since more expressive languages might be able to easier encode the prosody of the source speech into text.          \\begin{figure}[h]         \\centering         \\includegraphics[width=0.99\\columnwidth]{figures/regression_lang_All_xcomet2_strong5.png}         \\caption{Regression Analysis of language pairs.}         \\label{fig:regression_lang_strong}         \\vspace{-0.1cm}     \\end{figure}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the regression analysis of language pairs. The x-axis represents model size, and the y-axis represents Contra-Q (Global). Blue circles, green squares, and red diamonds represent English-German (En-De), English-Spanish (En-Es), and English-Japanese (En-Ja) language pairs, respectively. The figure also includes the regression line for each language pair. The subfigure below shows the regression coefficients and their significance for log(Model Size), Spanish (Es), Japanese (Ja), and the intercept. The regression coefficients of the intercept and Spanish are statistically significant, while those of Japanese and the logarithm of model size are not significant."
    },
    "149": {
        "figure1": "2410.24022v1_scaling_law",
        "label1": "fig:scaling_law",
        "caption1": "Loss \\textit{vs.} Compute during Pre-training",
        "text": "\\centering     \\includegraphics[width=0.55\\textwidth]{figures/scaling_law.png}     \\caption{Loss \\textit{vs.} Compute during Pre-training}     \\label{fig:scaling_law}\nDuring the pre-training phase, we observe a consistent improvement in the model's performance that correlates with increased computational resources, specifically floating-point operations per second (FLOPS), as depicted in Figure~\\ref{fig:scaling_law}. In downstream tasks, we note a general enhancement in performance when the number of parameters is augmented from 650 million to 3 billion. This trend holds true for both the ESM2 and our proposed models, indicating that larger models are more adept at capturing intricate patterns and relationships within biological datasets.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the relationship between loss and compute during pre-training. The x-axis represents compute (FLOPS) ranging from 10 to the power of 16 to 10 to the power of 21, and the y-axis represents loss. There are multiple curves in the figure, each representing a model with a different parameter size (Parameter N), ranging from 5M to 5B, with the color gradient from dark blue to yellow indicating an increase in parameter size. Overall, as the amount of computation increases, the loss of all models decreases. The larger the parameter size of the model, the larger the initial computation amount for its loss to decrease, and the faster the loss decreases. For example, the model with a parameter size of 5B only starts to decrease significantly when the amount of computation reaches 10 to the power of 20, but the rate of decrease is rapid, while the model with a parameter size of 5M starts to decrease when the amount of computation is small, but the rate of decrease is relatively slow."
    },
    "150": {
        "figure1": "2410.24031v1_spoof_examples",
        "label1": "fig:spoof_examples",
        "caption1": "Examples for spoof samples in the collected data.",
        "text": "\\centering   \\includegraphics[width=0.8\\columnwidth]{figures/spoof_examples.png}   \\caption{Examples for spoof samples in the collected data.}   \\label{fig:spoof_examples}\nMoreover, variety of spoof attacks were collected. The 2D attacks involved printed papers of different sizes (A3, A4), television screens, and mobile phone screens as well as curved screens. In each 2D spoof attack, a face that fulfilled the real footage criteria was displayed. The 3D attacks incorporated cut papers placed over a live instance’s face that revealed its nose, papers rolled into a cylinder, and latex masks worn by live individuals. In total, 2M various spoof data were collected (\\autoref{fig:spoof_examples}).",
        "summarize_figure": "figure1",
        "summarization": "The image shows various spoof samples in the collected dataset. These spoof attacks are diverse, including the use of 2D media such as printed papers of different sizes (A3, A4), television screens, mobile phone screens, and curved screens. In each 2D spoof attack, a face image that meets the real video criteria is displayed on the screen. In addition, it also includes 3D attacks, such as covering the face of a real person with cut paper to expose the nose, rolling paper into a cylinder, and wearing latex masks by real people."
    },
    "151": {
        "figure1": "2410.24031v1_spoof_disp_example",
        "label1": "fig:spoof_examples_disp",
        "caption1": "Examples for Disparity maps produced from 2D spoof attacks samples.",
        "text": "\\centering   \\includegraphics[width=0.9\\columnwidth]{figures/spoof_disp_example.png}   \\caption{Examples for Disparity maps produced from 2D spoof attacks samples.}   \\label{fig:spoof_examples_disp}",
        "summarize_figure": "figure1",
        "summarization": "The figure shows examples of disparity maps generated from 2D spoof attack samples. Each row demonstrates a spoofing attack example, arranged from left to right as follows: the original image, the first disparity map, the second disparity map, and the result of overlaying the disparity map on the original image. The disparity maps are primarily focused on the facial region, with colors ranging from green to red, potentially representing varying depth information. Through these disparity maps, the performance of 2D spoofing attacks in terms of depth information can be observed, as well as how these attacks can be detected and identified using disparity maps."
    },
    "152": {
        "figure1": "2410.24031v1_weights",
        "label1": "fig:weights",
        "caption1": "First Layer's weights distribution",
        "text": "\\centering   \\includegraphics[width=0.5\\columnwidth]{figures/weights.png}   \\caption{First Layer's weights distribution}   \\label{fig:weights}\nMoreover, we analyzed the weights from the first convolutional layer of the Disparity model to assess the impact of each input type on the model’s output. The disparity maps reached a maximum of value of 480, while the raw sensor data values could attain up to 1024. To ensure a fair comparison, the kernel weights of the disparity maps were scaled by a factor of ($\\frac{480}{1024}$). The normalized histogram of the first layer’s weights, as shown in \\autoref{fig:weights}, indicates that the weight values corresponding to the disparity maps are distributed similarly to those of the raw sensor data. This suggests that both input types contribute equally to the model’s decision making.",
        "summarize_figure": "figure1",
        "summarization": "The figure shows the weights distribution of the first convolutional layer, corresponding to raw sensor data and disparity maps. The horizontal axis represents the first layer weights, and the vertical axis represents the normalized histogram. The kernel weights of the disparity maps were scaled by a factor of 480/1024 to ensure a fair comparison with the raw sensor data. As can be seen, the weight values corresponding to the disparity maps are distributed similarly to those of the raw sensor data, indicating that both input types have a similar impact on the model's decision-making process."
    },
    "153": {
        "figure1": "2410.24049v2_BiasFigureAnalysis",
        "label1": "fig:biasWinLoseChart",
        "caption1": "Distribution of Bias Across Categories: Displaying eight plots, this chart shows the ASR for six target models in Section \\ref{sec:models}, highlighting the vulnerability of these models in categorizing Arab and Western groups as losers across various categories. The red bars indicate the percentage of successful biases against Arab groups, showcasing the differential treatment based on geographic and cultural markers.",
        "text": "Figure \\ref{fig:biasWinLoseChart} represents the win-lose in which the win the ASR that the model has chosen the Arabs as the loser group and its in the red colored bar and the lose is the percentage that the model has selected the western as loser group     \\centering     \\includegraphics[width=\\linewidth]{images/BiasFigureAnalysis.png}     \\caption{Distribution of Bias Across Categories: Displaying eight plots, this chart shows the ASR for six target models in Section \\ref{sec:models}, highlighting the vulnerability of these models in categorizing Arab and Western groups as losers across various categories. The red bars indicate the percentage of successful biases against Arab groups, showcasing the differential treatment based on geographic and cultural markers.}     \\label{fig:biasWinLoseChart}",
        "summarize_figure": "figure1",
        "summarization": "The picture displays the distribution of bias across eight different categories for six language models. Red bars represent the percentage of times the models categorized the Arab group as the \"Loser\", while blue bars represent the percentage for the Western group. The image reveals that in most categories and across the models, the percentage shown by the red bars is generally higher than the blue bars, indicating a tendency for these models to categorize the Arab group as the \"Loser\". This bias is particularly pronounced in categories such as Backward, Religion, Terrorism, and Hostile Values, where some models exhibit high bias percentages against the Arab group."
    },
    "154": {
        "figure1": "2410.24049v2_Model_Per_categories_jailbreak",
        "label1": "fig:ModelPerCategoryJailbreak",
        "caption1": "Target Models vulnerability in Jailbreak Scenarios: This figure compares the vulnerability of various models to different categories in jailbreak scenarios, highlighting those with the highest and lowest vulnerabilities. It illustrates a trend towards greater vulnerability in 'Entertainment' and 'Hostile Values', while 'Terrorism' tends to be handled with more caution, reflecting a widespread approach to dealing with sensitive topics.",
        "text": "Analysis in Figure \\ref{fig:ModelPerCategoryJailbreak} of several language models reveals distinct patterns in vulnerability to bias across categories. GPT-4 shows its highest vulnerability in 'Hostile Values' with a success rate of 99\\%, while it is least vulnerable in 'Terrorism' at 26\\%. GPT-4 O mirrors this, with its highest bias in 'Hostile Values' at 100\\% and lowest in 'Terrorism' at 40.59\\%. Llama3.1-405B finds 'Entertainment' the most vulernable, with a 99\\% success rate, contrasting with a lower rate of 22.77\\% in 'Terrorism'. Similarly, Llama-3.1-8B and Mistral 7B Instruct-v0.2 both identify 'Entertainment' and 'Hostile Values' as the most vulnerable categories at 100\\%, but show more resilience in 'Terrorism' with success rates around 30\\%. Claude 3.5 Sonnet, however, stands out with a different pattern; it is most vulnerable in 'Scientific Scholar' at 47\\%, and shows complete resilience in 'Women's Rights', 'Religion', and 'Terrorism', all at 0\\%. These insights suggest that while categories like 'Entertainment' and 'Hostile Values' consistently emerge as highly susceptible across multiple models, 'Terrorism' is generally handled with more neutrality, indicating a broader modeling trend towards cautious engagement with this sensitive topic.\n\\centering     \\includegraphics[width=0.8\\linewidth]{images/Model_Per_categories_jailbreak.png}     \\caption{Target Models vulnerability in Jailbreak Scenarios: This figure compares the vulnerability of various models to different categories in jailbreak scenarios, highlighting those with the highest and lowest vulnerabilities. It illustrates a trend towards greater vulnerability in 'Entertainment' and 'Hostile Values', while 'Terrorism' tends to be handled with more caution, reflecting a widespread approach to dealing with sensitive topics.}     \\label{fig:ModelPerCategoryJailbreak}",
        "summarize_figure": "figure1",
        "summarization": "The image presents the success rates of jailbreak attacks on various language models across different categories. For models such as GPT-4, GPT-4 O, the Llama series, and Mistral 7B, attack success rates are notably high, often reaching 99 percent or 100 percent, in categories like 'Entertainment' and 'Hostile Values'. Conversely, these models exhibit significantly lower success rates in the 'Terrorism' category, generally ranging from around 20 percent to 40 percent. Claude 3.5 Sonnet shows a different pattern, with much lower overall success rates; its highest vulnerability is in the 'Scientific Scholar' category at 47 percent, while success rates are near 0 percent in several other categories, including 'Terrorism'. This highlights the varying vulnerabilities across different models and categories, indicating that categories like 'Entertainment' and 'Hostile Values' are generally more susceptible than sensitive topics like 'Terrorism' for most models tested."
    },
    "155": {
        "figure1": "2410.24059v2_interven_questions_supp_4",
        "label1": "fig:psy_index4",
        "caption1": "Intervention on the fourth component of the noise vector and subsequent re-mixing generate a new observed space — a new score distribution. Notably, only \\textit{Extraversion} exhibits significant changes after intervention, leading us to label the fourth component of the noise vector (after sorting) as \\textit{Extraversion}.",
        "figure2": "2410.24059v2_interven_questions_supp_5",
        "label2": "fig:psy_index5",
        "caption2": "Intervention on the fifth component of the noise vector and subsequent re-mixing generate a new observed space — a new score distribution. Notably, only \\textit{Neuroticism} exhibits significant changes after intervention, leading us to label the fifth component of the noise vector (after sorting) as \\textit{Neuroticism}.",
        "figure3": "2410.24059v2_interven_questions_supp_1",
        "label3": "fig:psy_index1",
        "caption3": "Intervention on the first component of the noise vector and subsequent re-mixing generate a new observed space — a new score distribution. Notably, only \\textit{Agreeableness} exhibits significant changes after intervention, leading us to label the first component of the noise vector (after sorting) as \\textit{Agreeableness}.",
        "figure4": "2410.24059v2_interven_questions_supp_3",
        "label4": "fig:psy_index3",
        "caption4": "Intervention on the third component of the noise vector and subsequent re-mixing generate a new observed space — a new score distribution. Notably, only \\textit{Conscientiousness} exhibits significant changes after intervention, leading us to label the third component of the noise vector (after sorting) as \\textit{Conscientiousness}.",
        "figure5": "2410.24059v2_interven_questions_supp_2",
        "label5": "fig:psy_index2",
        "caption5": "Intervention on the second component of the noise vector and subsequent re-mixing generate a new observed space — a new score distribution. Notably, only \\textit{Openness} exhibits significant changes after intervention, leading us to label the second component of the noise vector (after sorting) as \\textit{Openness}.",
        "text": "To identify the semantic label for the first component of \\(\\widetilde{\\epsilon}\\), we set its corresponding noise vector component to \\(0\\), effectively nullifying the first component of \\(\\widetilde{\\epsilon}^{male}\\). This intervention yields an estimated noise matrix samples from \\(\\widetilde{\\epsilon}^{male}_{inv}\\), denoted as \\(\\widetilde{\\bm\\epsilon}^{male}_{inv}\\). The intervened reconstruction, \\(\\bm{X}^{male}_{inv} = {G}(\\widetilde{\\bm{\\epsilon}}^{male}_{intv})^T\\), and the original score distribution, \\(\\bm{X}^{male} = {G}(\\widetilde{\\bm{\\epsilon}}^{male})^T\\), allow us to compare question scores pre- and post-intervention. Figure \\ref{fig:psy_index1} plots these distributions, revealing significant shifts for questions pertaining to the \\textit{Agreeableness} dimension, with minimal impact on other scores, thereby identifying the first noise component as \\textit{Agreeableness}. This process is replicated for the second through fifth columns of \\(\\bm{\\epsilon}^{male}\\), with results illustrated in Figures \\ref{fig:psy_index2}, \\ref{fig:psy_index3}, \\ref{fig:psy_index4}, and \\ref{fig:psy_index5}. Each plot demonstrates that interventions result in significant distribution changes for questions related to a single personality dimension, with negligible effects on others. Consequently, we label these noise components as \\textit{Openness}, \\textit{Conscientiousness}, \\textit{Extraversion}, and \\textit{Neuroticism}, respectively. These labels will be used for all the following analysis.",
        "summarize_figure": "figure1",
        "summarization": "The picture shows the new observed space generated by intervening on the first component of the noise vector and subsequent re-mixing, which is a new score distribution. Each subplot represents a question and displays the change in score distribution before and after the intervention. Notably, questions related to the \"Agreeableness\" dimension exhibit significant changes after the intervention, while scores for other dimensions are minimally affected. This indicates that the first component of the noise vector is closely related to the \"Agreeableness\" trait and is therefore labeled as \"Agreeableness\"."
    },
    "156": {
        "figure1": "2410.24059v2_interven_questions_supp_5",
        "label1": "fig:psy_index5",
        "caption1": "Intervention on the fifth component of the noise vector and subsequent re-mixing generate a new observed space — a new score distribution. Notably, only \\textit{Neuroticism} exhibits significant changes after intervention, leading us to label the fifth component of the noise vector (after sorting) as \\textit{Neuroticism}.",
        "figure2": "2410.24059v2_interven_questions_supp_4",
        "label2": "fig:psy_index4",
        "caption2": "Intervention on the fourth component of the noise vector and subsequent re-mixing generate a new observed space — a new score distribution. Notably, only \\textit{Extraversion} exhibits significant changes after intervention, leading us to label the fourth component of the noise vector (after sorting) as \\textit{Extraversion}.",
        "figure3": "2410.24059v2_interven_questions_supp_1",
        "label3": "fig:psy_index1",
        "caption3": "Intervention on the first component of the noise vector and subsequent re-mixing generate a new observed space — a new score distribution. Notably, only \\textit{Agreeableness} exhibits significant changes after intervention, leading us to label the first component of the noise vector (after sorting) as \\textit{Agreeableness}.",
        "figure4": "2410.24059v2_interven_questions_supp_3",
        "label4": "fig:psy_index3",
        "caption4": "Intervention on the third component of the noise vector and subsequent re-mixing generate a new observed space — a new score distribution. Notably, only \\textit{Conscientiousness} exhibits significant changes after intervention, leading us to label the third component of the noise vector (after sorting) as \\textit{Conscientiousness}.",
        "figure5": "2410.24059v2_interven_questions_supp_2",
        "label5": "fig:psy_index2",
        "caption5": "Intervention on the second component of the noise vector and subsequent re-mixing generate a new observed space — a new score distribution. Notably, only \\textit{Openness} exhibits significant changes after intervention, leading us to label the second component of the noise vector (after sorting) as \\textit{Openness}.",
        "text": "To identify the semantic label for the first component of \\(\\widetilde{\\epsilon}\\), we set its corresponding noise vector component to \\(0\\), effectively nullifying the first component of \\(\\widetilde{\\epsilon}^{male}\\). This intervention yields an estimated noise matrix samples from \\(\\widetilde{\\epsilon}^{male}_{inv}\\), denoted as \\(\\widetilde{\\bm\\epsilon}^{male}_{inv}\\). The intervened reconstruction, \\(\\bm{X}^{male}_{inv} = {G}(\\widetilde{\\bm{\\epsilon}}^{male}_{intv})^T\\), and the original score distribution, \\(\\bm{X}^{male} = {G}(\\widetilde{\\bm{\\epsilon}}^{male})^T\\), allow us to compare question scores pre- and post-intervention. Figure \\ref{fig:psy_index1} plots these distributions, revealing significant shifts for questions pertaining to the \\textit{Agreeableness} dimension, with minimal impact on other scores, thereby identifying the first noise component as \\textit{Agreeableness}. This process is replicated for the second through fifth columns of \\(\\bm{\\epsilon}^{male}\\), with results illustrated in Figures \\ref{fig:psy_index2}, \\ref{fig:psy_index3}, \\ref{fig:psy_index4}, and \\ref{fig:psy_index5}. Each plot demonstrates that interventions result in significant distribution changes for questions related to a single personality dimension, with negligible effects on others. Consequently, we label these noise components as \\textit{Openness}, \\textit{Conscientiousness}, \\textit{Extraversion}, and \\textit{Neuroticism}, respectively. These labels will be used for all the following analysis.",
        "summarize_figure": "figure1",
        "summarization": "The image shows the new observed space generated by intervening on the first component of the noise vector and subsequent re-mixing, which is a new score distribution. Notably, only questions in the \"Agreeableness\" dimension exhibit significant changes after the intervention, leading us to label the first component of the noise vector (after sorting) as \"Agreeableness\". Each subplot in the image displays distribution changes for questions related to a specific personality dimension, where blue represents the original distribution and red represents the intervened distribution. It can be seen that the intervention mainly affects the scores of \"Agreeableness\"-related questions, with minimal impact on other dimensions."
    },
    "157": {
        "figure1": "2410.24059v2_interven_questions_supp_1",
        "label1": "fig:psy_index1",
        "caption1": "Intervention on the first component of the noise vector and subsequent re-mixing generate a new observed space — a new score distribution. Notably, only \\textit{Agreeableness} exhibits significant changes after intervention, leading us to label the first component of the noise vector (after sorting) as \\textit{Agreeableness}.",
        "figure2": "2410.24059v2_interven_questions_supp_4",
        "label2": "fig:psy_index4",
        "caption2": "Intervention on the fourth component of the noise vector and subsequent re-mixing generate a new observed space — a new score distribution. Notably, only \\textit{Extraversion} exhibits significant changes after intervention, leading us to label the fourth component of the noise vector (after sorting) as \\textit{Extraversion}.",
        "figure3": "2410.24059v2_interven_questions_supp_5",
        "label3": "fig:psy_index5",
        "caption3": "Intervention on the fifth component of the noise vector and subsequent re-mixing generate a new observed space — a new score distribution. Notably, only \\textit{Neuroticism} exhibits significant changes after intervention, leading us to label the fifth component of the noise vector (after sorting) as \\textit{Neuroticism}.",
        "figure4": "2410.24059v2_interven_questions_supp_3",
        "label4": "fig:psy_index3",
        "caption4": "Intervention on the third component of the noise vector and subsequent re-mixing generate a new observed space — a new score distribution. Notably, only \\textit{Conscientiousness} exhibits significant changes after intervention, leading us to label the third component of the noise vector (after sorting) as \\textit{Conscientiousness}.",
        "figure5": "2410.24059v2_interven_questions_supp_2",
        "label5": "fig:psy_index2",
        "caption5": "Intervention on the second component of the noise vector and subsequent re-mixing generate a new observed space — a new score distribution. Notably, only \\textit{Openness} exhibits significant changes after intervention, leading us to label the second component of the noise vector (after sorting) as \\textit{Openness}.",
        "text": "To identify the semantic label for the first component of \\(\\widetilde{\\epsilon}\\), we set its corresponding noise vector component to \\(0\\), effectively nullifying the first component of \\(\\widetilde{\\epsilon}^{male}\\). This intervention yields an estimated noise matrix samples from \\(\\widetilde{\\epsilon}^{male}_{inv}\\), denoted as \\(\\widetilde{\\bm\\epsilon}^{male}_{inv}\\). The intervened reconstruction, \\(\\bm{X}^{male}_{inv} = {G}(\\widetilde{\\bm{\\epsilon}}^{male}_{intv})^T\\), and the original score distribution, \\(\\bm{X}^{male} = {G}(\\widetilde{\\bm{\\epsilon}}^{male})^T\\), allow us to compare question scores pre- and post-intervention. Figure \\ref{fig:psy_index1} plots these distributions, revealing significant shifts for questions pertaining to the \\textit{Agreeableness} dimension, with minimal impact on other scores, thereby identifying the first noise component as \\textit{Agreeableness}. This process is replicated for the second through fifth columns of \\(\\bm{\\epsilon}^{male}\\), with results illustrated in Figures \\ref{fig:psy_index2}, \\ref{fig:psy_index3}, \\ref{fig:psy_index4}, and \\ref{fig:psy_index5}. Each plot demonstrates that interventions result in significant distribution changes for questions related to a single personality dimension, with negligible effects on others. Consequently, we label these noise components as \\textit{Openness}, \\textit{Conscientiousness}, \\textit{Extraversion}, and \\textit{Neuroticism}, respectively. These labels will be used for all the following analysis.",
        "summarize_figure": "figure1",
        "summarization": "The image illustrates the new observed space generated by intervening on the first component of the noise vector and subsequent remixing, resulting in a new score distribution. Notably, only questions pertaining to the \"Agreeableness\" dimension exhibit significant changes after the intervention, with minimal impact on other scores. Thus, the first component of the noise vector is labeled as \"Agreeableness\". In the image, blue represents the original score distribution, and red represents the score distribution after intervention. After the intervention, the score distribution of \"Agreeableness\"-related questions shifted significantly, while the items in other dimensions remained largely unchanged."
    },
    "158": {
        "figure1": "2410.24059v2_interven_questions_supp_3",
        "label1": "fig:psy_index3",
        "caption1": "Intervention on the third component of the noise vector and subsequent re-mixing generate a new observed space — a new score distribution. Notably, only \\textit{Conscientiousness} exhibits significant changes after intervention, leading us to label the third component of the noise vector (after sorting) as \\textit{Conscientiousness}.",
        "figure2": "2410.24059v2_interven_questions_supp_4",
        "label2": "fig:psy_index4",
        "caption2": "Intervention on the fourth component of the noise vector and subsequent re-mixing generate a new observed space — a new score distribution. Notably, only \\textit{Extraversion} exhibits significant changes after intervention, leading us to label the fourth component of the noise vector (after sorting) as \\textit{Extraversion}.",
        "figure3": "2410.24059v2_interven_questions_supp_5",
        "label3": "fig:psy_index5",
        "caption3": "Intervention on the fifth component of the noise vector and subsequent re-mixing generate a new observed space — a new score distribution. Notably, only \\textit{Neuroticism} exhibits significant changes after intervention, leading us to label the fifth component of the noise vector (after sorting) as \\textit{Neuroticism}.",
        "figure4": "2410.24059v2_interven_questions_supp_1",
        "label4": "fig:psy_index1",
        "caption4": "Intervention on the first component of the noise vector and subsequent re-mixing generate a new observed space — a new score distribution. Notably, only \\textit{Agreeableness} exhibits significant changes after intervention, leading us to label the first component of the noise vector (after sorting) as \\textit{Agreeableness}.",
        "figure5": "2410.24059v2_interven_questions_supp_2",
        "label5": "fig:psy_index2",
        "caption5": "Intervention on the second component of the noise vector and subsequent re-mixing generate a new observed space — a new score distribution. Notably, only \\textit{Openness} exhibits significant changes after intervention, leading us to label the second component of the noise vector (after sorting) as \\textit{Openness}.",
        "text": "To identify the semantic label for the first component of \\(\\widetilde{\\epsilon}\\), we set its corresponding noise vector component to \\(0\\), effectively nullifying the first component of \\(\\widetilde{\\epsilon}^{male}\\). This intervention yields an estimated noise matrix samples from \\(\\widetilde{\\epsilon}^{male}_{inv}\\), denoted as \\(\\widetilde{\\bm\\epsilon}^{male}_{inv}\\). The intervened reconstruction, \\(\\bm{X}^{male}_{inv} = {G}(\\widetilde{\\bm{\\epsilon}}^{male}_{intv})^T\\), and the original score distribution, \\(\\bm{X}^{male} = {G}(\\widetilde{\\bm{\\epsilon}}^{male})^T\\), allow us to compare question scores pre- and post-intervention. Figure \\ref{fig:psy_index1} plots these distributions, revealing significant shifts for questions pertaining to the \\textit{Agreeableness} dimension, with minimal impact on other scores, thereby identifying the first noise component as \\textit{Agreeableness}. This process is replicated for the second through fifth columns of \\(\\bm{\\epsilon}^{male}\\), with results illustrated in Figures \\ref{fig:psy_index2}, \\ref{fig:psy_index3}, \\ref{fig:psy_index4}, and \\ref{fig:psy_index5}. Each plot demonstrates that interventions result in significant distribution changes for questions related to a single personality dimension, with negligible effects on others. Consequently, we label these noise components as \\textit{Openness}, \\textit{Conscientiousness}, \\textit{Extraversion}, and \\textit{Neuroticism}, respectively. These labels will be used for all the following analysis.",
        "summarize_figure": "figure1",
        "summarization": "The picture shows the new observed space generated by intervening on the first component of the noise vector and subsequent re-mixing—a new score distribution. Each subplot represents a question, and each question corresponds to a personality dimension (Extraversion, Neuroticism, Agreeableness, Conscientiousness, Openness). Blue and red represent the score distributions before and after the intervention, respectively. It is observed that only questions related to \"Agreeableness\" show significant changes after the intervention, while the scores of other personality dimensions are almost unaffected. Therefore, the first component of the noise vector is labeled as \"Agreeableness\"."
    },
    "159": {
        "figure1": "2410.24059v2_interven_questions_supp_2",
        "label1": "fig:psy_index2",
        "caption1": "Intervention on the second component of the noise vector and subsequent re-mixing generate a new observed space — a new score distribution. Notably, only \\textit{Openness} exhibits significant changes after intervention, leading us to label the second component of the noise vector (after sorting) as \\textit{Openness}.",
        "figure2": "2410.24059v2_interven_questions_supp_4",
        "label2": "fig:psy_index4",
        "caption2": "Intervention on the fourth component of the noise vector and subsequent re-mixing generate a new observed space — a new score distribution. Notably, only \\textit{Extraversion} exhibits significant changes after intervention, leading us to label the fourth component of the noise vector (after sorting) as \\textit{Extraversion}.",
        "figure3": "2410.24059v2_interven_questions_supp_5",
        "label3": "fig:psy_index5",
        "caption3": "Intervention on the fifth component of the noise vector and subsequent re-mixing generate a new observed space — a new score distribution. Notably, only \\textit{Neuroticism} exhibits significant changes after intervention, leading us to label the fifth component of the noise vector (after sorting) as \\textit{Neuroticism}.",
        "figure4": "2410.24059v2_interven_questions_supp_1",
        "label4": "fig:psy_index1",
        "caption4": "Intervention on the first component of the noise vector and subsequent re-mixing generate a new observed space — a new score distribution. Notably, only \\textit{Agreeableness} exhibits significant changes after intervention, leading us to label the first component of the noise vector (after sorting) as \\textit{Agreeableness}.",
        "figure5": "2410.24059v2_interven_questions_supp_3",
        "label5": "fig:psy_index3",
        "caption5": "Intervention on the third component of the noise vector and subsequent re-mixing generate a new observed space — a new score distribution. Notably, only \\textit{Conscientiousness} exhibits significant changes after intervention, leading us to label the third component of the noise vector (after sorting) as \\textit{Conscientiousness}.",
        "text": "To identify the semantic label for the first component of \\(\\widetilde{\\epsilon}\\), we set its corresponding noise vector component to \\(0\\), effectively nullifying the first component of \\(\\widetilde{\\epsilon}^{male}\\). This intervention yields an estimated noise matrix samples from \\(\\widetilde{\\epsilon}^{male}_{inv}\\), denoted as \\(\\widetilde{\\bm\\epsilon}^{male}_{inv}\\). The intervened reconstruction, \\(\\bm{X}^{male}_{inv} = {G}(\\widetilde{\\bm{\\epsilon}}^{male}_{intv})^T\\), and the original score distribution, \\(\\bm{X}^{male} = {G}(\\widetilde{\\bm{\\epsilon}}^{male})^T\\), allow us to compare question scores pre- and post-intervention. Figure \\ref{fig:psy_index1} plots these distributions, revealing significant shifts for questions pertaining to the \\textit{Agreeableness} dimension, with minimal impact on other scores, thereby identifying the first noise component as \\textit{Agreeableness}. This process is replicated for the second through fifth columns of \\(\\bm{\\epsilon}^{male}\\), with results illustrated in Figures \\ref{fig:psy_index2}, \\ref{fig:psy_index3}, \\ref{fig:psy_index4}, and \\ref{fig:psy_index5}. Each plot demonstrates that interventions result in significant distribution changes for questions related to a single personality dimension, with negligible effects on others. Consequently, we label these noise components as \\textit{Openness}, \\textit{Conscientiousness}, \\textit{Extraversion}, and \\textit{Neuroticism}, respectively. These labels will be used for all the following analysis.",
        "summarize_figure": "figure1",
        "summarization": "The figure shows the new observed space generated after intervening on the first component of the noise vector, which is a new score distribution. Notably, questions in the \"Agreeableness\" dimension exhibit significant changes after the intervention, while scores in other dimensions are minimally affected. Therefore, the first component of the noise vector is labeled as \"Agreeableness\". The figure reveals the correspondence between the noise vector components and specific personality dimensions by comparing the score distributions of questions before and after the intervention."
    },
    "160": {
        "figure1": "2410.24059v2_toy_example",
        "label1": "fig:toy_example",
        "caption1": " We have 5 \\textit{latent} variables $Z$ which in this case relate to personality concepts, and the observations $X$ represent the scores of 50 questions from a psychometric personality test. The latent variables $Z$ follow a linear SCM, while the \\textit{unknown} shared linear mixing is a full-rank matrix $G \\in \\sR^{50\\times 5}$. Then, for environment $k = \\{\\mathrm{US, UK, AU}\\}$, the observables are generated through $\\Xs{k} = G \\Zs{k}$. Here, $\\sP^{(\\mathrm{US})}$ is taken as the ``observational'' (reference) distribution, and the distribution shifts in $\\sP^{(\\mathrm{UK})}$ and $\\sP^{(\\mathrm{AU})}$ are due to changes in the causal mechanisms of $\\{Z_1\\}$ and $\\{Z_2, Z_3, Z_5\\}$, respectively. Finally, the types of interventions are general; for UK, the edge $Z_4 \\to Z_1$ is removed and the dashed red lines indicate changes in the edge weights to $Z_1$; for AU, $Z_2$ was intervened by removing $Z_5\\to Z_2$ and \\textit{adding} $Z_3\\to Z_2$, while the edge $Z_5\\to Z_3$ was \\textit{reversed}, thus changing the mechanisms of $Z_3$ and $Z_5$. Thus, we aim to identify $\\{Z_1\\}$ and $\\{Z_2, Z_3, Z_5\\}$. ",
        "text": "Notably, we allow generalized interventions that allow for $\\As{k}$ to be arbitrary, which includes \\textit{soft} and \\textit{hard} interventions, interventions targeting \\textit{single} or \\textit{multiple} nodes, as well as interventions capable of \\textit{adding} or \\textit{removing} parent nodes and \\textit{reversing} edges.  This contrasts with the existing literature on CRL, where single-node soft/hard interventions are the standard assumption \\citep{von2023nonparametric,seigal2022linear,buchholz2023learning,ahuja2023interventional}. See Figure \\ref{fig:toy_example}, for a toy example of what we aim to estimate.\n\\centering     \\includegraphics[width=\\textwidth]{toy_example.pdf}     \\caption{     We have 5 \\textit{latent} variables $Z$ which in this case relate to personality concepts, and the observations $X$ represent the scores of 50 questions from a psychometric personality test.     The latent variables $Z$ follow a linear SCM, while the \\textit{unknown} shared linear mixing is a full-rank matrix $G \\in \\sR^{50\\times 5}$.     Then, for environment $k = \\{\\mathrm{US, UK, AU}\\}$, the observables are generated through $\\Xs{k} = G \\Zs{k}$.      Here, $\\sP^{(\\mathrm{US})}$ is taken as the ``observational'' (reference) distribution, and the distribution shifts in $\\sP^{(\\mathrm{UK})}$ and $\\sP^{(\\mathrm{AU})}$ are due to changes in the causal mechanisms of $\\{Z_1\\}$ and $\\{Z_2, Z_3, Z_5\\}$, respectively.     Finally, the types of interventions are general; for UK, the edge $Z_4 \\to Z_1$ is removed and the dashed red lines indicate changes in the edge weights to $Z_1$; for AU, $Z_2$ was intervened by removing $Z_5\\to Z_2$ and \\textit{adding} $Z_3\\to Z_2$, while the edge $Z_5\\to Z_3$ was \\textit{reversed}, thus changing the mechanisms of $Z_3$ and $Z_5$.     Thus, we aim to identify $\\{Z_1\\}$ and $\\{Z_2, Z_3, Z_5\\}$.     }     \\label{fig:toy_example} To develop our identifiability result and algorithm, we will make additional assumptions on the noise distributions of the linear SEMs.      For any environment $k \\in [K]$, let $\\epss{k} = (\\epss{k}_1,\\ldots, \\epss{k}_d)$ be the vector of $d$ independent noises with $\\Cov(\\epss{k}) = I_d$. We have:     \\begin{enumerate}         \\item Identically distributed across environments: $\\sP(\\epss{k}) = \\sP(\\epss{k'})$, for all $k' \\neq k$. \\label{item1}         \\item Non-Gaussianity: At most one noise component $\\epss{k}_i$ is Gaussian distributed. \\label{item2}         \\item Pairwise differences: For any $i\\neq j$, we have $\\mathbb{P}(\\epss{k}_i)\\neq \\mathbb{P}(\\epss{k}_j)$ and $\\mathbb{P}(\\epss{k}_i)\\neq \\mathbb{P}(-\\epss{k}_j)$. \\label{item3}     \\end{enumerate}",
        "summarize_figure": "figure1",
        "summarization": "Here is the summary of the figure:The picture shows a toy example for causal relationship learning, where five latent variables Z (Z1 to Z5, representing personality concepts: Extraversion, Agreeableness, Openness, Neuroticism, and Conscientiousness) influence 50 observed variables X through an unknown shared linear mixing matrix G. The figure shows causal diagrams under three environments: the United States (US), the United Kingdom (UK), and Australia (AU). The US environment is considered the observational (reference) distribution. The change of causal mechanism in the UK environment is reflected in Z1, specifically, the edge from Z4 to Z1 is removed, and the weights of the edges affecting Z1 are changed. The change of causal mechanism in the Australian environment is reflected in Z2, Z3, and Z5. Z2 is intervened by removing the edge from Z5 to Z2 and adding the edge from Z3 to Z2. Meanwhile, the edge from Z5 to Z3 is reversed, thus changing the mechanisms of Z3 and Z5. The aim is to identify the set of intervened variables {Z1} and {Z2, Z3, Z5}."
    },
    "161": {
        "figure1": "2410.24059v2_consistency_all",
        "label1": "fig:consistency",
        "caption1": "Illustration of the efficacy of our method in accurately identifying latent shifted nodes as the sample size increases, for ER2 graphs. In the first subplot, for a latent graph with $d = 5$ nodes, we examine scenarios with observed dimensions $p = 10, 20, 40$ and plot their corresponding F1 scores against the number of samples $n$. It is observed that the F1 score approaches 1 with a sufficiently large sample size. Detailed experimental procedures and results are discussed in Section \\ref{sec:experiment}. ",
        "text": "Algorithm \\ref{alg} is consistent with the ground truth set of shifted nodes as $n$ approaches infinity. Empirical evidence supporting this claim is presented in Figure \\ref{fig:consistency}, which shows that with a sufficiently large sample size, all shifted nodes are correctly identified, and the F1 score reaches 1. %\\textcolor{blue}{ Further theoretical discussion on the sample complexity of our method can be found in Appendix \\ref{app:sample_complex}.\n\\centering     \\includegraphics[width=0.85\\textwidth]{fig/consistency_all.pdf}     \\caption{Illustration of the efficacy of our method in accurately identifying latent shifted nodes as the sample size increases, for ER2 graphs. In the first subplot, for a latent graph with $d = 5$ nodes, we examine scenarios with observed dimensions $p = 10, 20, 40$ and plot their corresponding F1 scores against the number of samples $n$. It is observed that the F1 score approaches 1 with a sufficiently large sample size.      Detailed experimental procedures and results are discussed in Section \\ref{sec:experiment}.     }     \\label{fig:consistency}\nEmpirically, we have observed that the following formulation of \\(L_i^{k,k'}\\) leads to improved results:     L_i^{k,k'} = \\frac{\\| |\\widetilde{M}^{(k)}_i| - |\\widetilde{M}^{(k')}_i| \\|_1}{\\| \\widetilde{M}^{(k)}_i \\|_1 + \\| \\widetilde{M}^{(k')}_i \\|_1}, where \\( |\\widetilde{M}^{(k)}_i| \\) denotes the element-wise absolute value of the vector \\( \\widetilde{M}^{(k)}_i \\). We will utilized the new formula of $L_{i}^{k,k'}$ to detect shifts in the following experiment. Then we explore sample sizes \\(n\\) from \\(500\\) to \\(10^6\\), using the observed samples \\({X}^{(k)}\\) as input. The parameter \\(\\alpha\\) is set to \\(0.2\\) for \\(d \\leq 10\\) and \\(0.5\\) for higher dimensions, reflecting the increased complexity in estimating larger dimensional latent graphs and thus necessitating a higher tolerance for $L_1$ norm differences in detecting shifted nodes. For each setting, we independently generate $10$ datasets and take the average of the metrics. The results for \\(n = 10^6\\) are shown in Table \\ref{table:graph_result}, and the asymptotic consistency results for specific \\(p\\) values are illustrated in Figure \\ref{fig:consistency}. %\\textcolor{blue}{ In addition to the causal representative setting, our method can also directly identify mechanism shifts in a fully observed setting, where $G=I$. We further compare our method's results in this fully observed setting against the baseline DCI \\cite{wang2018direct}, which addresses direct mechanism shifts in linear settings. The results of this comparison are provided in Appendix \\ref{app:dci}, demonstrating that our method outperforms DCI in most settings.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the efficacy of the method in accurately identifying latent shifted nodes as the sample size increases, specifically for ER2 graphs. For a latent graph with d = 5 nodes, scenarios with observed dimensions p = 10, 20, and 40 are examined, and their corresponding F1 scores are plotted against the number of samples n. It can be observed that the F1 score approaches 1 with a sufficiently large sample size."
    },
    "162": {
        "figure1": "2410.24075v1_fig_02",
        "label1": "fig:2",
        "caption1": "Qualitative results on the synthetic CERRA reanalysis from the test set at time step 2160. \\legendbox{teal} is the prediction, \\legendbox{NavyBlue} is the ground truth, and \\legendbox{red} is the false positive. Albedo and relative humidity are not correlated with extremes, meaning that they do not contain drivers, but only random anomalies.",
        "text": "\\centering   \\includegraphics[draft=\\draft, width=.95\\textwidth]{figures/fig_02.png}   \\caption{Qualitative results on the synthetic CERRA reanalysis from the test set at time step 2160. \\legendbox{teal} is the prediction, \\legendbox{NavyBlue} is the ground truth, and \\legendbox{red} is the false positive. Albedo and relative humidity are not correlated with extremes, meaning that they do not contain drivers, but only random anomalies.}\\label{fig:2}\nThe quantitative results are shown in Table \\ref{table:1}. The naive baseline is impacted by two main issues; first by the time delay between drivers and extreme events, and second not all variables are correlated with the extremes. The second issue affects the one-class and reconstruction-based baselines where they suffer mostly from false positives.  In fact, both integrated gradients models achieve high F1-scores for detecting extremes (93.32 for Integrated Gradients I and 93.80 for Integrated Gradients II), but they have worse performance on identifying the drivers. SimpleNet is trained with our model as a feature extractor which explains its good performance.  However, SimpleNet showed a drop of performance when it is tested on other datasets (see Appendix Sec.~\\ref{sec:9.2} for results on two more synthetic datasets). Among the reconstruction-based approach, STEALNet outperforms UniAD.  This is probability because STEALNet exploits more weakly supervision information during training by maximizing the reconstruction loss for locations with extreme flags.  MIL-based baselines are more suitable for the task. Finally, our model consistently outperforms the baselines on all metrics.  \\begin{wrapfigure}{r}{0.5\\textwidth} Qualitative samples in comparison with baselines are presented in Fig.~\\ref{fig:2}. The qualitative examples indicate that our model and the MIL-based baselines except RTFM are capable of learning which variables are correlated with the extremes. The main weakness of RTFM is the reliance on feature magnitudes and the cross attention module (see Appendix Sec.~\\ref{sec:9.5} and Table \\ref{table:2}), which make it more prone to produce false positives. Other baselines predict incorrect relations between the variables and extremes. Regarding the explainable AI methods, when we add more interactions between the variables (Integrated Gradients II), the gradients tend to omit some variables (soil moisture). Both integrated gradients models have also difficulties with the synthetic t2m, which includes red noise by design. These results demonstrate that networks that predict the extremes directly from the input variables utilize much more information even when it is not correlated with an extreme. It is thus beneficial to introduce a bottleneck into the network that enforces the network to explicitly identify drivers of extremes.",
        "summarize_figure": "figure1",
        "summarization": "The image shows qualitative results on the synthetic CERRA reanalysis test set at time step 2160. It compares the performance of different models on six synthetic variables (albedo, 2m temperature, total cloud cover, total precipitation, relative humidity, and volumetric soil moisture). The \"Ground Truth\" column represents the ground truth, the \"Ours\" column represents the results of the proposed model, and the other columns represent the results of other models. Teal boxes represent model predictions, navy blue boxes represent ground truth, and red boxes represent false positives. It can be seen that albedo and relative humidity are different from other variables, they are not correlated with extremes and only contain random anomalies. For 2m temperature, both Integrated Gradients I and Integrated Gradients II models have a large number of false positives."
    },
    "163": {
        "figure1": "2410.24075v1_fig_06",
        "label1": "fig:6",
        "caption1": "Examples of the synthetic CERRA reanalysis data described in Table \\ref{table:3}. The drivers/anomalies \\legendbox{Gold} are visualized under each variable directly. Here, albedo and relative humidity are not correlated with the extremes.",
        "figure2": "2410.24075v1_fig_11",
        "label2": "fig:11",
        "caption2": "Visualization of the generated signals $\\mathbf{\\Phi}$ for $6$ different variables from the synthetic artificial data described in Table \\ref{table:5}. The time series are shown for the location ($\\text{lat}=50, \\text{lon}=50$). \\legendbox{red} are the drivers/anomalies which are correlated with extremes, and \\legendbox{teal} are random anomalies.",
        "text": "The third dataset (Artificial) does not depend on real-world data but rather consists of basis signals with trigonometric functions where we also add a linear latitudinal gradient (latGrad). Finally, we mask out some regions to exhibit no anomalies i.e., pixels over water surface. Each generated dataset consists of $52\\times46$ time steps corresponding to $46$ years of simulated data in $7$-day intervals. Examples of the synthesized dataset can be found in Figs.~\\ref{fig:6}-\\ref{fig:11}. The results on the synthetic CERRA reanalysis are reported in Table \\ref{table:1} and the results on synthetic NOAA and the artificial data are shown Sec.~\\ref{sec:9.2} in Tables \\ref{table:6} and \\ref{table:7}.\n\\centering   \\includegraphics[draft=\\draft,width=.87\\textwidth]{figures/fig_06.pdf}   \\caption{Examples of the synthetic CERRA reanalysis data described in Table \\ref{table:3}. The drivers/anomalies \\legendbox{Gold} are visualized under each variable directly. Here, albedo and relative humidity are not correlated with the extremes.}\\label{fig:6}",
        "summarize_figure": "figure1",
        "summarization": "Here is the summary in English:The figure shows examples of synthetic CERRA reanalysis data, including variables such as albedo, 2m temperature, total cloud cover, total precipitation, relative humidity, and volumetric soil moisture over time. Each row shows the state of a variable over eight consecutive time steps (Δt0 to Δt7). Gold regions indicate drivers or anomalies. It can be seen that albedo and relative humidity are not correlated with extremes. Other variables, such as 2m temperature, total cloud cover, total precipitation, and volumetric soil moisture, show patterns and anomalous regions that change over time."
    },
    "164": {
        "figure1": "2410.24075v1_fig_08",
        "label1": "fig:8",
        "caption1": "Examples of the synthetic artificial data described in Table \\ref{table:5}. The drivers/anomalies \\legendbox{Gold} are visualized under each variable directly. Here, variables 01 and 05 are not correlated with the extremes.",
        "text": "\\centering   \\includegraphics[draft=\\draft,width=.87\\textwidth]{figures/fig_08}   \\caption{Examples of the synthetic artificial data described in Table \\ref{table:5}. The drivers/anomalies \\legendbox{Gold} are visualized under each variable directly. Here, variables 01 and 05 are not correlated with the extremes.}\\label{fig:8}",
        "summarize_figure": "figure1",
        "summarization": "This picture presents examples of synthetic artificial data for six variables (variable 01 to variable 06) across eight time steps. For each variable and time step, the upper panel displays the variable's data representation, while the lower panel visualizes the corresponding drivers or anomalies in yellow. Variables 01 and 05 show complex data patterns but consistently exhibit no yellow anomalies across all time steps, confirming their lack of correlation with extremes as noted. In contrast, variables 03, 04, and 06 display various yellow anomaly shapes and patterns in their lower panels throughout these time steps, indicating the presence of drivers or anomalies associated with these variables. Variable 02 shows changing data patterns but, similar to variables 01 and 05, has no associated yellow anomalies. This visualization highlights the distinct relationships between the data of each variable and the presence or absence of these defined anomalies over time."
    },
    "165": {
        "figure1": "2410.24075v1_fig_09",
        "label1": "fig:9",
        "caption1": "Visualization of the generated signals $\\mathbf{\\Phi}$ for $6$ different variables from the synthetic CERRA reanalysis described in Table \\ref{table:3}. The time series are shown for the location ($\\text{lat}=50, \\text{lon}=50$). \\legendbox{red} are the drivers/anomalies which are correlated with extremes, and \\legendbox{teal} are random anomalies.",
        "text": "\\centering   \\includegraphics[draft=\\draft,width=.7\\textwidth]{figures/fig_09.png}   \\caption{Visualization of the generated signals $\\mathbf{\\Phi}$ for $6$ different variables from the synthetic CERRA reanalysis described in Table \\ref{table:3}. The time series are shown for the location ($\\text{lat}=50, \\text{lon}=50$). \\legendbox{red} are the drivers/anomalies which are correlated with extremes, and \\legendbox{teal} are random anomalies.}\\label{fig:9}",
        "summarize_figure": "figure1",
        "summarization": "Here is the summary of the figure:The figure visualizes the generated signals PHI for 6 different variables from the synthetic CERRA reanalysis, with time series shown for the location at latitude equals 50, longitude equals 50. The first subplot illustrates the albedo variation over time steps. The black curve represents the simulated signal, red vertical lines indicate anomalies correlated with extreme events, and teal vertical lines represent random anomalous events. The albedo's simulated signal fluctuates between 0.15 and 0.25, with random anomalous events at certain time steps."
    },
    "166": {
        "figure1": "2410.24075v1_fig_10",
        "label1": "fig:10",
        "caption1": "Visualization of the generated signals $\\mathbf{\\Phi}$ for $5$ different variables from the synthetic NOAA data described in Table \\ref{table:4}. The time series are shown for the location ($\\text{lat}=50, \\text{lon}=50$). \\legendbox{red} are the drivers/anomalies which are correlated with extremes, and \\legendbox{teal} are random anomalies.",
        "text": "\\centering   \\includegraphics[draft=\\draft,width=.7\\textwidth]{figures/fig_10.png}   \\caption{Visualization of the generated signals $\\mathbf{\\Phi}$ for $5$ different variables from the synthetic NOAA data described in Table \\ref{table:4}. The time series are shown for the location ($\\text{lat}=50, \\text{lon}=50$). \\legendbox{red} are the drivers/anomalies which are correlated with extremes, and \\legendbox{teal} are random anomalies.}\\label{fig:10}",
        "summarize_figure": "figure1",
        "summarization": "The image visualizes the generated signals Phi for five different variables from synthetic NOAA data at latitude=50 and longitude=50. The figure illustrates the NDVI (Normalized Difference Vegetation Index) changes over time steps. The curve shows periodic fluctuations, and there are some red and teal vertical lines on the timeline, representing anomalous events correlated with extreme events and random anomalous events, respectively."
    },
    "167": {
        "figure1": "2410.24075v1_fig_11",
        "label1": "fig:11",
        "caption1": "Visualization of the generated signals $\\mathbf{\\Phi}$ for $6$ different variables from the synthetic artificial data described in Table \\ref{table:5}. The time series are shown for the location ($\\text{lat}=50, \\text{lon}=50$). \\legendbox{red} are the drivers/anomalies which are correlated with extremes, and \\legendbox{teal} are random anomalies.",
        "figure2": "2410.24075v1_fig_06",
        "label2": "fig:6",
        "caption2": "Examples of the synthetic CERRA reanalysis data described in Table \\ref{table:3}. The drivers/anomalies \\legendbox{Gold} are visualized under each variable directly. Here, albedo and relative humidity are not correlated with the extremes.",
        "text": "The third dataset (Artificial) does not depend on real-world data but rather consists of basis signals with trigonometric functions where we also add a linear latitudinal gradient (latGrad). Finally, we mask out some regions to exhibit no anomalies i.e., pixels over water surface. Each generated dataset consists of $52\\times46$ time steps corresponding to $46$ years of simulated data in $7$-day intervals. Examples of the synthesized dataset can be found in Figs.~\\ref{fig:6}-\\ref{fig:11}. The results on the synthetic CERRA reanalysis are reported in Table \\ref{table:1} and the results on synthetic NOAA and the artificial data are shown Sec.~\\ref{sec:9.2} in Tables \\ref{table:6} and \\ref{table:7}.\n\\centering   \\includegraphics[draft=\\draft,width=.7\\textwidth]{figures/fig_11.png}   \\caption{Visualization of the generated signals $\\mathbf{\\Phi}$ for $6$ different variables from the synthetic artificial data described in Table \\ref{table:5}. The time series are shown for the location ($\\text{lat}=50, \\text{lon}=50$). \\legendbox{red} are the drivers/anomalies which are correlated with extremes, and \\legendbox{teal} are random anomalies.}\\label{fig:11}",
        "summarize_figure": "figure1",
        "summarization": "The figure visualizes the generated signals Φ for 6 different variables from the synthetic artificial data described in Table 5. The time series are shown for the location at latitude 50 and longitude 50. The red areas indicate drivers or anomalies correlated with extremes, and the teal areas represent random anomalies. The signals for each variable exhibit different patterns and anomalies. Variable 1 and Variable 5 show relatively regular sinusoidal patterns with sporadic red and teal anomalies. Variable 2, Variable 4, and Variable 6 also show periodic patterns but with different types of anomaly distributions. The signal for Variable 3 shows a more irregular pattern with more frequent and irregular anomalies."
    },
    "168": {
        "figure1": "2410.24075v1_fig_14",
        "label1": "fig:14",
        "caption1": "The averaged spatial distribution of drivers and anomalies related to Portugal in Europe. For this experiment, we use prediction on EUR-11 from ERA5-Land and select frames (times) within the period 2018-2024 where there were extreme drought of at least 25\\% of the pixels in the Portugal. Then we normalize the identified drivers and anomalies by the total number of frames to obtain the final map. As can be seen drivers are spatially centered around where extremes were reported.",
        "text": "\\centering   \\includegraphics[width=.75\\textwidth]{figures/fig_14.png}   \\caption{The averaged spatial distribution of drivers and anomalies related to Portugal in Europe. For this experiment, we use prediction on EUR-11 from ERA5-Land and select frames (times) within the period 2018-2024 where there were extreme drought of at least 25\\% of the pixels in the Portugal. Then we normalize the identified drivers and anomalies by the total number of frames to obtain the final map. As can be seen drivers are spatially centered around where extremes were reported.}\\label{fig:14}",
        "summarize_figure": "figure1",
        "summarization": "The picture illustrates the averaged spatial distribution of drivers and anomalies related to Portugal in Europe. The study uses EUR-11 prediction from ERA5-Land, selecting frames (times) from 2018-2024 where extreme drought affected at least 25% of Portugal's pixels. Identified drivers and anomalies are then normalized by the total frame number to create the final map. As shown in the picture, drivers are spatially centered around the reported extreme events. The distribution maps of total evaporation and volumetric soil water show that Portugal and surrounding areas, such as Spain, exhibit apparent drought characteristics."
    },
    "169": {
        "figure1": "2410.24075v1_fig_15",
        "label1": "fig:15",
        "caption1": "The averaged spatial distribution of drivers and anomalies related to a specific place in Europe (North Rhine-Westphalia). For this experiment, we use prediction on EUR-11 from ERA5-Land and select frames (times) within the period 2018-2024 where there were extreme drought of at least 25\\% of the pixels in the North Rhine-Westphalia. Then we normalize the identified drivers and anomalies by the total number of frames to obtain the final map. As can be seen drivers are spatially centered around where extremes were reported.",
        "text": "\\centering   \\includegraphics[width=.75\\textwidth]{figures/fig_15.png}   \\caption{The averaged spatial distribution of drivers and anomalies related to a specific place in Europe (North Rhine-Westphalia). For this experiment, we use prediction on EUR-11 from ERA5-Land and select frames (times) within the period 2018-2024 where there were extreme drought of at least 25\\% of the pixels in the North Rhine-Westphalia. Then we normalize the identified drivers and anomalies by the total number of frames to obtain the final map. As can be seen drivers are spatially centered around where extremes were reported.}\\label{fig:15}",
        "summarize_figure": "figure1",
        "summarization": "The image displays the averaged spatial distribution of anomalies for six different climatic variables, including total evaporation, forecast albedo, soil temperature, volumetric soil water, 2m temperature, and total precipitation, during periods of extreme drought in North Rhine-Westphalia. The color bar indicates the percentage of weeks with anomalous events. The subplot for total evaporation in the image clearly shows that high percentages of anomalous events are concentrated primarily over Germany, where North Rhine-Westphalia is located, and surrounding areas, extending into France, Belgium, and the Netherlands. The spatial pattern for volumetric soil water is similar, also showing strong anomalous distribution concentrated in central Europe. In contrast, the anomalous distributions for forecast albedo, soil temperature, 2m temperature, and total precipitation during these extreme drought events show different patterns, not exhibiting significant concentration in the central European area. This indicates that total evaporation and volumetric soil water are spatially strongly associated with the drought area during extreme drought in North Rhine-Westphalia."
    },
    "170": {
        "figure1": "2410.24075v1_fig_17",
        "label1": "fig:17",
        "caption1": "Temporal evolution of drivers and anomalies related to the extremes in ERA5-Land for Europe (EUR-11). For this experiments, we select pixels with extreme events during summer (weeks $25$-$38$) for the years 2018-2023 and compute the average distribution of drivers and anomalies with time. The red line at $\\delta t_0$ indicates the beginning of the extreme droughts. $Z_{score}$ in the underneath curve represents the deviation from the mean computed from the ERA5-Land climatology.",
        "text": "\\centering   \\includegraphics[width=.99\\textwidth]{figures/fig_17.pdf}   \\caption{Temporal evolution of drivers and anomalies related to the extremes in ERA5-Land for Europe (EUR-11). For this experiments, we select pixels with extreme events during summer (weeks $25$-$38$) for the years 2018-2023 and compute the average distribution of drivers and anomalies with time. The red line at $\\delta t_0$ indicates the beginning of the extreme droughts. $Z_{score}$ in the underneath curve represents the deviation from the mean computed from the ERA5-Land climatology.}\\label{fig:17}",
        "summarize_figure": "figure1",
        "summarization": "The picture illustrates the temporal evolution of various drivers and anomalies around the onset of extreme droughts in Europe. Before the beginning of extreme drought, marked by delta\\_t0, the bottom curves show a significant decrease in volumetric soil water and total precipitation, with their Z-scores deviating negatively from the normal condition (Z-score equal to 0). Concurrently, the Z-scores for total evaporation and 2m temperature begin to rise and become positive. Soil temperature Z-score shows a smaller change, increasing slightly after the drought onset. Forecast albedo Z-score changes are relatively less pronounced around the drought onset, remaining close to normal levels. The bar charts at the top of the panels collectively indicate that the percentage of pixels in severe-to-exceptional agricultural drought increases rapidly at the beginning of the extreme drought and remains high thereafter. These patterns reveal key hydrological and meteorological changes characterizing the period around extreme drought events."
    },
    "171": {
        "figure1": "2410.24075v1_fig_19",
        "label1": "fig:19",
        "caption1": "Temporal evolution of drivers and anomalies related to the extremes in ERA5-Land for Africa (AFR-11). For this experiments, we select pixels with extreme events during summer (weeks $25$-$38$) for the years 2019-2023 and compute the average distribution of drivers and anomalies with time. The red line at $\\delta t_0$ indicates the beginning of the extreme droughts. $Z_{score}$ in the underneath curve represents the deviation from the mean computed from the ERA5-Land climatology.",
        "text": "\\centering   \\includegraphics[width=.99\\textwidth]{figures/fig_19.pdf}   \\caption{Temporal evolution of drivers and anomalies related to the extremes in ERA5-Land for Africa (AFR-11). For this experiments, we select pixels with extreme events during summer (weeks $25$-$38$) for the years 2019-2023 and compute the average distribution of drivers and anomalies with time. The red line at $\\delta t_0$ indicates the beginning of the extreme droughts. $Z_{score}$ in the underneath curve represents the deviation from the mean computed from the ERA5-Land climatology.}\\label{fig:19}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the temporal evolution of drivers and anomalies related to extreme events in ERA5-Land for Africa (AFR-11). Pixels with extreme events during summer (weeks 25-38) for the years 2019-2023 were selected, and the average distribution of drivers and anomalies was computed over time. The red line at time equals 0 indicates the beginning of extreme droughts. The underneath curve shows the Z-score, representing the deviation from the mean computed from the ERA5-Land climatology. The figure shows the variation of total evaporation over time. The total evaporation increases slightly before the beginning of extreme drought, and then decreases significantly. The Z-score remains near 0 throughout the period."
    },
    "172": {
        "figure1": "2410.24075v1_fig_21",
        "label1": "fig:21",
        "caption1": "Temporal evolution of drivers and anomalies related to the extremes in ERA5-Land for South America (SAM-11). For this experiments, we select pixels with extreme events during summer (weeks $25$-$38$) for the years 2018-2023 and compute the average distribution of drivers and anomalies with time. The red line at $\\delta t_0$ indicates the beginning of the extreme droughts. $Z_{score}$ in the underneath curve represents the deviation from the mean computed from the ERA5-Land climatology.",
        "text": "\\centering   \\includegraphics[width=.99\\textwidth]{figures/fig_21.pdf}   \\caption{Temporal evolution of drivers and anomalies related to the extremes in ERA5-Land for South America (SAM-11). For this experiments, we select pixels with extreme events during summer (weeks $25$-$38$) for the years 2018-2023 and compute the average distribution of drivers and anomalies with time. The red line at $\\delta t_0$ indicates the beginning of the extreme droughts. $Z_{score}$ in the underneath curve represents the deviation from the mean computed from the ERA5-Land climatology.}\\label{fig:21}",
        "summarize_figure": "figure1",
        "summarization": "This figure shows the temporal evolution of drivers and anomalies related to extreme events in ERA5-Land for South America (SAM-11). It presents the average distribution of drivers and anomalies over time for pixels with extreme events during summer (weeks 25-38) from 2018-2023. The red line at delta t0 indicates the beginning of extreme droughts. The total evaporation (e) increases over time, reaching a peak before the onset of extreme drought. The Z score in the bottom curve represents the deviation from the mean computed from the ERA5-Land climatology."
    },
    "173": {
        "figure1": "2410.24075v1_fig_23",
        "label1": "fig:23",
        "caption1": "Temporal evolution of drivers and anomalies related to the extremes in ERA5-Land for North America (NAM-11). For this experiments, we select pixels with extreme events during summer (weeks $25$-$38$) for the years 2018-2023 and compute the average distribution of drivers and anomalies with time. The red line at $\\delta t_0$ indicates the beginning of the extreme droughts. $Z_{score}$ in the underneath curve represents the deviation from the mean computed from the ERA5-Land climatology.",
        "text": "\\centering   \\includegraphics[width=.99\\textwidth]{figures/fig_23.pdf}   \\caption{Temporal evolution of drivers and anomalies related to the extremes in ERA5-Land for North America (NAM-11). For this experiments, we select pixels with extreme events during summer (weeks $25$-$38$) for the years 2018-2023 and compute the average distribution of drivers and anomalies with time. The red line at $\\delta t_0$ indicates the beginning of the extreme droughts. $Z_{score}$ in the underneath curve represents the deviation from the mean computed from the ERA5-Land climatology.}\\label{fig:23}",
        "summarize_figure": "figure1",
        "summarization": "Here is a summary of the figure:The figure illustrates the temporal evolution of drivers and anomalies related to extreme events in ERA5-Land for North America (NAM-11) during summer (weeks 25-38) from 2018-2023. For total evaporation (e), the figure displays the percentage of anomalous pixels over time, along with the z-score variation. The red line indicates the start time of extreme droughts. In the temporal evolution, the z-score represents the deviation from the mean computed from the ERA5-Land climatology."
    },
    "174": {
        "figure1": "2410.24075v1_fig_25",
        "label1": "fig:25",
        "caption1": "Temporal evolution of drivers and anomalies related to the extremes in ERA5-Land for East Asia (EAS-11). For this experiments, we select pixels with extreme events during summer (weeks $25$-$38$) for the years 2018-2023 and compute the average distribution of drivers and anomalies with time. The red line at $\\delta t_0$ indicates the beginning of the extreme droughts. $Z_{score}$ in the underneath curve represents the deviation from the mean computed from the ERA5-Land climatology.",
        "text": "\\centering   \\includegraphics[width=.99\\textwidth]{figures/fig_25.pdf}   \\caption{Temporal evolution of drivers and anomalies related to the extremes in ERA5-Land for East Asia (EAS-11). For this experiments, we select pixels with extreme events during summer (weeks $25$-$38$) for the years 2018-2023 and compute the average distribution of drivers and anomalies with time. The red line at $\\delta t_0$ indicates the beginning of the extreme droughts. $Z_{score}$ in the underneath curve represents the deviation from the mean computed from the ERA5-Land climatology.}\\label{fig:25}",
        "summarize_figure": "figure1",
        "summarization": "The figure shows the temporal evolution of drivers and anomalies related to extreme events in ERA5-Land for East Asia (EAS-11) during summer (weeks 25-37) from 2018-2023. The figure presents the changes in total evaporation, forecast albedo, soil temperature, volumetric soil water, 2m temperature and total precipitation over time. The red line at delta t0 indicates the beginning of extreme droughts. In the total evaporation chart, it can be seen that the percentage of anomalous pixels of total evaporation gradually increases before the start of extreme drought, and continues to increase after the drought begins."
    },
    "175": {
        "figure1": "2410.24075v1_fig_27",
        "label1": "fig:27",
        "caption1": "Temporal evolution of drivers and anomalies related to the extremes in ERA5-Land for Central Asia (CAS-11). For this experiments, we select pixels with extreme events during summer (weeks $25$-$38$) for the years 2018-2023 nd compute the average distribution of drivers and anomalies with time. The red line at $\\delta t_0$ indicates the beginning of the extreme droughts. $Z_{score}$ in the underneath curve represents the deviation from the mean computed from the ERA5-Land climatology.",
        "text": "\\centering   \\includegraphics[width=.99\\textwidth]{figures/fig_27.pdf}   \\caption{Temporal evolution of drivers and anomalies related to the extremes in ERA5-Land for Central Asia (CAS-11). For this experiments, we select pixels with extreme events during summer (weeks $25$-$38$) for the years 2018-2023 nd compute the average distribution of drivers and anomalies with time. The red line at $\\delta t_0$ indicates the beginning of the extreme droughts. $Z_{score}$ in the underneath curve represents the deviation from the mean computed from the ERA5-Land climatology.}\\label{fig:27}",
        "summarize_figure": "figure1",
        "summarization": "The picture shows the temporal evolution of drivers and anomalies related to extreme events in ERA5-Land for Central Asia (CAS-11) during summer (weeks 25-38) from 2018-2023. It selects pixels with extreme events and computes the average distribution of drivers and anomalies over time. The red vertical line δt0 indicates the beginning of extreme droughts. Total evaporation (e) shows a gradually increasing trend before the onset of drought, with a significant decrease in the percentage of anomalous pixels after the drought begins."
    },
    "176": {
        "figure1": "2410.24075v1_fig_29",
        "label1": "fig:29",
        "caption1": "Temporal evolution of drivers and anomalies related to the extremes in CERRA reanalysis for Europe. For this experiments, we select pixels with extreme events during summer (weeks $25$-$38$) for the years 2016-2020 and compute the average distribution of drivers and anomalies with time. The red line at $\\delta t_0$ indicates the beginning of the extreme droughts. $Z_{score}$ in the underneath curve represents the deviation from the mean computed from the CERRA climatology.",
        "text": "\\centering   \\includegraphics[width=.99\\textwidth]{figures/fig_29.pdf}   \\caption{Temporal evolution of drivers and anomalies related to the extremes in CERRA reanalysis for Europe. For this experiments, we select pixels with extreme events during summer (weeks $25$-$38$) for the years 2016-2020 and compute the average distribution of drivers and anomalies with time. The red line at $\\delta t_0$ indicates the beginning of the extreme droughts. $Z_{score}$ in the underneath curve represents the deviation from the mean computed from the CERRA climatology.}\\label{fig:29}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the temporal evolution of drivers and anomalies related to extreme events in the CERRA reanalysis for Europe. It shows the percentage of anomalous pixels for variables such as albedo (al), 2m temperature (t2m), relative humidity (r2), total cloud cover (tcc), total precipitation (tp), and volumetric soil moisture (vsw) around the onset of extreme droughts during summer (weeks 25-38) from 2016-2020. The red line (δt0) indicates the beginning of the extreme droughts. The Z-score curve below each variable represents the deviation from the CERRA climatological mean. After the onset of extreme drought, albedo and relative humidity exhibit a decreasing trend, while volumetric soil moisture shows a significant increase."
    },
    "177": {
        "figure1": "2410.24091v1_physical",
        "label1": "fig:physical exp",
        "caption1": "\\small{\\textbf{Results of Physical Characteristics Experiments.} \\textbf{Part (a)} shows the results of individual sensors' performance according to the force applied to their surface. \\textbf{Part (b)} demonstrates the tactile sensor pad's consistency under different normal forces. Each heatmap displays the tactile sensor pad's readings in an 8 $\\times$ 8 grid, where each number represents the sum of four sensor units. \\textbf{Part (c)} presents the results from part (b) in a single figure, illustrating the mean and standard deviation.}} \\vspace{-15pt",
        "text": "\\centering     \\includegraphics[width=1\\linewidth]{figures/physical.pdf}     \\vspace{-16pt}     \\caption{\\small{\\textbf{Results of Physical Characteristics Experiments.} \\textbf{Part (a)} shows the results of individual sensors' performance according to the force applied to their surface. \\textbf{Part (b)} demonstrates the tactile sensor pad's consistency under different normal forces. Each heatmap displays the tactile sensor pad's readings in an 8 $\\times$ 8 grid, where each number represents the sum of four sensor units. \\textbf{Part (c)} presents the results from part (b) in a single figure, illustrating the mean and standard deviation.}}          \\vspace{-15pt}     \\label{fig:physical exp}",
        "summarize_figure": "figure1",
        "summarization": "English:The image shows the relationship between tactile reading signal and applied force for 10 individual sensor units. As the applied force increases from 0 Newton to approximately 12 Newton, the tactile reading signal exhibits an increasing trend. The sensor response is non-linear, with the signal tending to saturate at higher force values. While the curves for the 10 individual sensor units display some variation, they all follow a similar pattern of increase and saturation, and the average response (thick black line) clearly depicts this overall trend."
    },
    "178": {
        "figure1": "2410.24116v1_ablation_confmat",
        "label1": "fig:ablation_confmat",
        "caption1": "Confusion matrices for the real and augmented (with AIDOVECL) datasets using mixup and mosaic augmentations.",
        "text": "In an ablation study examining AIDOVECL's capability to augment real datasets, we train YOLOv8 models on real images augmented by AIDOVECL, generated solely for the training split and doubling its size (one outpainted image per real image). The validation and test splits consist only of real images. This method assesses how augmentation with outpainted vehicles influences model performance on unseen real images, relative to using only the real dataset. In addition, we evaluate AIDOVECL affect on metrics in the presence of two other augmentation techniques, i.e. mixup and mosaic, as shown in Table \\ref{tab:metrics}. The confusion matrices in Figure \\ref{fig:ablation_confmat} complements the results by demonstrating improvements over different classes.\nFigure \\ref{fig:ablation_confmat} provides detailed predictions for real and augmented datasets across different vehicle classes using confusion matrices. The numbers in parentheses indicate the count of vehicles, and the fractions in each cell represent the proportion of predictions for that class, with the sum of percentages in each column equaling one. Augmenting with AIDOVECL significantly enhances the representation of underrepresented classes (2-4, 6, and 7), demonstrating the effectiveness of outpainting as a method to address class imbalance. However, outpainting vehicles can lead to some confusion; for example, COUPES (0) and PICKUPS (7) are more likely to be confused with SEDANS (1) and SUVs (2), respectively. This occurs because outpainting uses a faded mask to maintain scene integrity, which may obscure features like the two doors on a COUPE or artificially extend the trunk of a PICKUP to resemble an SUV roof. Despite these few instances of increased confusion, the general trend shows improved predictions when using AIDOVECL, which also significantly reduces the occurrence of false positives.\n\\centering     \\includegraphics[width=0.83\\linewidth]{ablation_confmat.pdf}     \\caption{Confusion matrices for the real and augmented (with AIDOVECL) datasets using mixup and mosaic augmentations.}     \\label{fig:ablation_confmat}",
        "summarize_figure": "figure1",
        "summarization": "Here is the summary of the figure:The picture presents confusion matrices for the real and augmented datasets using the mixup augmentation technique. For the real dataset, sedans (1) have the highest prediction accuracy at 0.83, followed by SUVs (2) with an accuracy of 0.76. For the augmented dataset using AIDOVECL, the prediction accuracy for sedans (1) increases to 0.91, and the accuracy for SUVs (2) also rises to 0.86. Overall, data augmentation with AIDOVECL improves the model's prediction accuracy across various categories, especially for vehicle types 1 and 2."
    },
    "179": {
        "figure1": "2410.24151v1_statics",
        "label1": "fig:statics",
        "caption1": "Analysis of the trend of concept removal. We erase target concepts from given images and audio clips using the proposed inversion and sampling process. We report the number of samples with target concepts before and after concept removal.",
        "text": "\\centering     \\includegraphics[width=0.8\\textwidth]{figures/statics.pdf}     \\caption{Analysis of the trend of concept removal. We erase target concepts from given images and audio clips using the proposed inversion and sampling process. We report the number of samples with target concepts before and after concept removal.}     \\label{fig:statics}\nTo investigate this, we conduct a similar experiment with audio, another common modality. Using the AVE dataset~\\citep{tian2018audio}, an audio event classification dataset containing clips from 28 sound classes, we randomly sample 5 audio clips from each class. We employ AudioLDM 2~\\citep{audioldm2} to replicate the process used in the image-based experiment. To determine whether the concept is removed from the original audio clip, we use EnCLAP~\\citep{kim2024enclap}, an audio captioning framework, to generate captions for both $\\boldsymbol{x_0}$ and $\\hat{\\boldsymbol{x}}_0$. We then check whether the word ``[class]'' appears in the caption. As shown in \\Cref{fig:statics}, the same trend of concept removal is observed in audio, despite its fundamentally different nature compared to images.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the analysis of the trend of concept removal. The study removes target concepts from given images using the proposed inversion and sampling process, and reports the number of samples with target concepts before and after concept removal. For original images x0, there are 95 samples with the target concept. After concept removal, the number of samples detected with the target concept on the reconstructed image x̂0 is reduced to 19. Meanwhile, the detector detected 79 samples with the target concept on x0. This indicates that the concept removal process effectively reduces the presence of the target concept in the images."
    },
    "180": {
        "figure1": "2410.24151v1_method",
        "label1": "fig:method",
        "caption1": "Overview of the ScalingConcept framework. Our method consists of two steps: 1) extracting the latent variable from $\\boldsymbol{x}_0$, and 2) constructing different sampling branches and modeling the difference between them. } \\vspace{-3mm",
        "text": "\\centering \t\\includegraphics[width=0.8\\textwidth]{figures/method.pdf} \t\\caption{Overview of the ScalingConcept framework. Our method consists of two steps: 1) extracting the latent variable from $\\boldsymbol{x}_0$, and 2) constructing different sampling branches and modeling the difference between them. }         \\vspace{-3mm} \t\\label{fig:method}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates an overview of the ScalingConcept framework, which consists of two steps. Step 1 involves inverting x0 with the text prompt c to obtain a series of intermediate images x1, x2, ..., xT-2, xT-1, up to xT. These intermediate images are stored in a memory bank. Simultaneously, an average image is obtained by averaging these intermediate images for subsequent steps. Step 2 involves constructing different sampling branches and modeling the differences between them. Specifically, xt is input into two diffusion models, one using the text prompt c and the other not. Then, through noise regularization, combining the results of the two models and the average image, the final estimate is obtained."
    },
    "181": {
        "figure1": "2410.24159v1_hybrid_illustration",
        "label1": "fig:hybrid",
        "caption1": "\\textbf{Two modes of a single model}\\hspace{1.5em}Causal and masked language modeling can be easily unified by shifting both outputs by one token to the right. Then we can train one language model on both paradigms at the same time just by modifying the input tokens, output tokens and attention masks.",
        "text": "\\centering         \\includegraphics[width=\\linewidth]{figures/hybrid_illustration.pdf}         \\caption{\\textbf{Two modes of a single model}\\hspace{1.5em}Causal and masked language modeling can be easily unified by shifting both outputs by one token to the right. Then we can train one language model on both paradigms at the same time just by modifying the input tokens, output tokens and attention masks.}         \\label{fig:hybrid}     \\end{figure}\nIn order to align both objectives we use a slightly modified version of masked language modeling called \\textbf{masked next-token prediction} \\citep[MNTP;][]{behnamghader2024llmvec}. The only difference from traditional MLM is that when the token at position $k+1$ is masked, its prediction should be outputed at position $k$. In this way both MLM and CLM are unified as the output at position $k$ always represents the token at position $k+1$. These two modes are illustrated in \\Cref{fig:hybrid}.          \\paragraph{Dataset handling} To ensure that our model sees all the data for both objectives, we duplicate our dataset. One is used for the causal objective, and the other for the masked objective. We can then decide a ratio of causal-to-masked in which to divide the data seen by the model at each batch.%For example, if we want our model to see as much data in a CLM fashion as in a MNTP fashion, we can have a ratio of 1:1, however, if we want to emphasize MTNP, we could have a 3:1 ratio where for every three MTNP examples, we have one CLM example (resulting in 75\\% of the batch having the MNTP objective). Therefore the number of steps per MNTP and CLM epochs can be different since both datasets have the same number of elements.           \\paragraph{Loss and transformer architecture} No additional changes are needed. Both training objectives minimize the cross-entropy loss, they share all learnable parameters, and use the same transformer encoder/decoder module.",
        "summarize_figure": "figure1",
        "summarization": "The picture illustrates two modes of a single model: Causal Language Modeling (CLM) and Masked Language Modeling (MLM). The upper part shows the CLM mode, where the model receives the input sequence \"<s>\", \"Baby\", \"language\", \"models\", \"rule\" and processes it through a Transformer with a causal attention mask, predicting the outputs \"Baby\", \"language\", \"models\", \"rule\", each corresponding to the prediction of the next token in the input sequence. The slider is on the GPT side, indicating that the model mainly operates in a causal language modeling manner. The lower part shows the MNTP mode, where the two tokens after \"<s>\", \"Baby\" are replaced by masks, and the model receives \"language\", \"models\" information. The slider is on the BERT side, indicating that the model mainly operates in a masked language modeling manner. Overall, by appropriately modifying the input tokens, output tokens, and attention masks, a single model can be trained to simultaneously adapt to both paradigms."
    },
    "182": {
        "figure1": "2410.24159v1_stacked_plot",
        "label1": "fig:lambada",
        "caption1": "\\textbf{The effect of the causal-to-mask ratio}\\hspace{1.5em}Comparison of performance of different tasks when varying the ratio of MNTP used during pre-training. We also look at the performance of the model using prefix language modeling with a partially-bidirectional attention mask. MNLI scores are reported with standard deviation error bars estimated by averaging the variations across three finetuning random seeds.",
        "text": "\\centering     \\includegraphics[width=\\textwidth]{figures/stacked_plot.pdf}     \\caption{\\textbf{The effect of the causal-to-mask ratio}\\hspace{1.5em}Comparison of performance of different tasks when varying the ratio of MNTP used during pre-training. We also look at the performance of the model using prefix language modeling with a partially-bidirectional attention mask. MNLI scores are reported with standard deviation error bars estimated by averaging the variations across three finetuning random seeds.}     \\label{fig:lambada}\nThe main results are presented in \\Cref{fig:lambada}. We evaluate the models on four tasks that cover distinct uses: \\circled{1} BLiMP is a zero-shot linguistic-preference task that is typically better suited for masked language models \\citep{salazar-etal-2020-masked}; \\circled{2} MNLI is a popular dataset for evaluating the finetunability of a language model, which also benefits masked language models; \\circled{3} LAMBADA, on the other hand, is a language modeling dataset mostly used to evaluate causal language models; and \\circled{4} we also directly compute the validation loss of each model. Furthermore, when applicable, each task is tested with three settings: fully-bidirectional processing (without any attention mask), unidirectional processing (with a causal mask), and partially-bidirectional processing (with a prefix mask).",
        "summarize_figure": "figure1",
        "summarization": "The image illustrates the accuracy of the model on the BLiMP task under different causal-to-mask ratios. The horizontal axis represents different causal-to-mask ratios, including settings from \"causal\" to \"15:1,\" \"7:1,\" \"3:1,\" \"1:1,\" \"1:3,\" \"1:7,\" \"1:15,\" and \"masked-only.\" The vertical axis represents the accuracy (in percentage) of the model on the BLiMP task. The image includes four curves corresponding to different attention mask types: \"none masked LM,\" \"causal,\" and \"prefix.\" The results indicate that for \"none masked LM,\" the accuracy tends to increase first and then decrease as the causal-to-mask ratio varies, reaching its highest point at a ratio of \"1:7\" and then decreasing significantly as the ratio approaches \"masked-only.\" For \"causal,\" the accuracy remains relatively stable across different ratios and is slightly higher than \"none masked LM.\" For \"prefix,\" the accuracy is generally lower than the other two, but peaks near \"1:7.\""
    },
    "183": {
        "figure1": "2410.24159v1_icl",
        "label1": "tab:icl_sst2_varying",
        "caption1": "\\textbf{SST-2 in-context learning}\\hspace{1.5em}20-shots ICL results on the SST-2 validation set for models trained on the 100M BabyLM datasets with varying degrees of each objective. The demonstrations (shots) were chosen at random from the training dataset. We do 20-runs and report mean as well as standard deviation. Note that the accuracy of the majority baseline on this dataset is 51.8\\%.",
        "text": "\\caption{\\textbf{SST-2 in-context learning}\\hspace{1.5em}20-shots ICL results on the SST-2 validation set for models trained on the 100M BabyLM datasets with varying degrees of each objective. The demonstrations (shots) were chosen at random from the training dataset. We do 20-runs and report mean as well as standard deviation. Note that the accuracy of the majority baseline on this dataset is 51.8\\%.}     \\label{tab:icl_sst2_varying}\nDespite the number of parameters and the size of the training corpus, our models show some signs of in-context learning, as can be seen in \\cref{tab:icl_sst2_varying}. When using the causal attention mask, we see that while the models trained with a single objective underperform the baseline, the hybrid models all perform above the majority baseline (from +0.5\\% to +5.7\\%); with the best results being achieved by the 3:1 model (with the 1:3 and 7:1 close second and third respectively). This indicates that our models are capable of doing in-context learning when trained with both objectives. When run fully bidirectionally, the trend is similar but with lower absolute performance. %One thing to note is that even though the models underperform the baseline, they have a positive Mathews Correlation Coefficient (indicating that they are not just random guessing). This is not the case for the causal-only model with either masking and masked-only model with causal masking.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the 20-shot in-context learning results on the SST-2 dataset. The models were trained on the 100M BabyLM dataset with varying ratios of causal attention mask and masked LM. The horizontal axis represents different training ratios (Causal:Masked), including Causal-only, 15:1, 7:1, 3:1, 1:1, 1:3, 1:7, 1:15, and Masked-only. The vertical axis represents the accuracy (%). There are two curves representing the model performance with causal attention mask (red) and without (blue, equivalent to masked LM). A horizontal line indicates the majority baseline accuracy of 51.8%.It can be seen that when using a causal attention mask, the model performs best at a 3:1 ratio, with an accuracy close to 57.5%. All hybrid models (i.e., non-Causal-only and Masked-only) outperform the majority baseline. When not using a causal attention mask (i.e., using bidirectional attention), the trend is similar, but the overall performance is lower."
    },
    "184": {
        "figure1": "2410.24159v1_blimp_supp_accuracy_temp_1",
        "label1": "fig:blimpsupp1",
        "caption1": "\\textbf{BLiMP-Supplement Accuracy}\\hspace{1.5em}Comparison of BLiMP-Supplement accuracy when varying the ratio of MNTP used during pre-training. We set the temperature to apply on the logits to 1 for fair comparison between the evaluation strategies. Fused is the sum of the logits from the causal and masked evaluation.",
        "text": "\\centering     \\includegraphics[width=0.7\\linewidth]{figures/blimp_supp_accuracy_temp_1.pdf}     \\caption{\\textbf{BLiMP-Supplement Accuracy}\\hspace{1.5em}Comparison of BLiMP-Supplement accuracy when varying the ratio of MNTP used during pre-training. We set the temperature to apply on the logits to 1 for fair comparison between the evaluation strategies. Fused is the sum of the logits from the causal and masked evaluation.}     \\label{fig:blimpsupp1}",
        "summarize_figure": "figure1",
        "summarization": "This is a chart comparing the accuracy of BLiMP-Supplement pre-training at different MNTP ratios. In the figure, the horizontal axis represents the MNTP ratio, ranging from 0.0 to 1.0, and the vertical axis represents the accuracy. The chart shows the accuracy of three evaluation strategies: Masked, Causal, and Fused. The accuracy of the Masked strategy remains relatively stable between 77% and 79% as the MNTP ratio changes. The accuracy of the Causal strategy is approximately 73% when the MNTP ratio is low, remains stable as the ratio increases, and decreases significantly when approaching 1.0. The accuracy of the Fused strategy is also relatively stable but slightly lower than the Masked strategy, with a slight decrease when the MNTP ratio approaches 1.0."
    },
    "185": {
        "figure1": "2410.24159v1_ewok_accuracy_temp_1",
        "label1": "fig:ewok1",
        "caption1": "\\textbf{EWoK Accuracy}\\hspace{1.5em}Comparison of EWoK accuracy when varying the ratio of MNTP used during pre-training. We set the temperature to apply on the logits to 1 for fair comparison between the evaluation strategies. Fused is the sum of the logits from the causal and masked evaluation. We also look at the performance of the model using a prefix masking strategy where the whole context is visible to the model.",
        "text": "\\centering     \\includegraphics[width=0.7\\linewidth]{figures/ewok_accuracy_temp_1.pdf}     \\caption{\\textbf{EWoK Accuracy}\\hspace{1.5em}Comparison of EWoK accuracy when varying the ratio of MNTP used during pre-training. We set the temperature to apply on the logits to 1 for fair comparison between the evaluation strategies. Fused is the sum of the logits from the causal and masked evaluation. We also look at the performance of the model using a prefix masking strategy where the whole context is visible to the model.}     \\label{fig:ewok1}",
        "summarize_figure": "figure1",
        "summarization": "The image illustrates a comparison of EWOK model accuracy during pre-training with varying Masked Next Token Prediction (MNTP) ratios, with the temperature set to 1. The plot includes four curves representing different evaluation strategies: Masked (blue), Causal (orange), Fused (green), and Prefix (red). The accuracy of the Masked strategy initially decreases as the MNTP ratio increases from 0, then rises, reaching its peak when the ratio approaches 1. The accuracy of the Causal strategy fluctuates significantly and is generally lower than that of the Masked strategy. The accuracy of the Fused strategy is higher when the MNTP ratio is low, but it tends to fluctuate and decrease as the ratio increases. The accuracy of the Prefix strategy remains relatively stable, maintained between 53% and 54%."
    },
    "186": {
        "figure1": "2410.24160v1_CreTok_fig7",
        "label1": "fig:loss",
        "caption1": "Loss curve of continual redefinition process.",
        "text": "\\centering   \\includegraphics[width=\\linewidth]{image/CreTok_fig7.pdf}   \\caption{Loss curve of continual redefinition process.}   \\label{fig:loss}\nCreTok's continual refinement of ``creative'' is shown in the training loss curve in Figure~\\ref{fig:loss}, which captures the convergence process of $\\texttt{<CreTok>}$.  Additionally, visualizations of randomly selected text pairs, recorded every 2,000 training steps, depict this refining representation.  In the initial stages, \\texttt{<CreTok>} primarily absorbs semantic information from the pairs—evident at step 2000, where creativity is narrowly aligned with the concept of ``bear''.  As training progresses, however, \\texttt{<CreTok>} transitions to embody a more generalized mode of creativity generation, independent of specific concepts. This shift is marked by increasingly aligned text-image relationships and improved image quality, leading to final convergence.",
        "summarize_figure": "figure1",
        "summarization": "This is a loss curve of the continual redefinition process. The graph shows the change in loss value of the CreTok model during training, with the horizontal axis representing the number of training steps (from 0 to 10000) and the vertical axis representing the loss value. In the initial stages of training, the loss value decreases rapidly, indicating that the model is quickly learning and adapting to the data. As training progresses, the rate of decrease in the loss value slows down, and it gradually stabilizes in the later stages, indicating that the model is gradually converging. In addition, the graph also shows the visualization results of randomly selected text pairs every 2000 steps during the training process, and these visualization results reflect the continuous improvement and redefinition of the concept of \"creativity\" by the CreTok model."
    },
    "187": {
        "figure1": "2410.24177v1_ued",
        "label1": "fig:streaming-ued",
        "caption1": " UED between tokenizing offline and chunk-wise streaming. All models are 500-unit tokenizers. Smaller $T_{\\text{shift}}/T_{\\text{chunk}}$ indicate higher overlap between chunks. Solid, dashed, and dotted lines depict $T_{\\text{chunk}} =$ 1, 2, and 5 seconds, respectively. }  % \\vspace{-10pt",
        "text": "\\centering     \\includegraphics[width=0.52\\linewidth]{figures/ued.pdf}     % \\vspace{-10pt}     \\caption{         UED between tokenizing offline and chunk-wise streaming.         All models are 500-unit tokenizers.         Smaller $T_{\\text{shift}}/T_{\\text{chunk}}$ indicate higher overlap between chunks.         Solid, dashed, and dotted lines depict $T_{\\text{chunk}} =$ 1, 2, and 5 seconds, respectively.     }     \\label{fig:streaming-ued}     % \\vspace{-10pt}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the Unit Error Distance (UED) of different models under varying chunk overlap ratios (Tshift/Tchunk). The horizontal axis represents the chunk overlap ratio, ranging from 0.4 to 1.0, while the vertical axis represents the UED in percentage. Four models are included: HuBERT + K-Means, HuBERT + DC-Spin, SpinHuBERT + DC-Spin, and SpinHuBERT-PR3k + DC-Spin. Solid, dashed, and dotted lines indicate chunk lengths (Tchunk) of 1, 2, and 5 seconds, respectively. As shown in the figure, the UED of all models generally increases with the increase of the chunk overlap ratio. For the same chunk length, the HuBERT + K-Means model usually has the highest UED value, while the SpinHuBERT-PR3k + DC-Spin model usually has the lowest. In addition, the chunk length also affects the UED, with smaller chunk lengths generally corresponding to lower UED."
    },
    "188": {
        "figure1": "2410.24177v1_pearson_corr",
        "label1": "fig:exp-task-corr",
        "caption1": " Pearson correlation coefficients between evaluation metrics computed with tokenizers operate at a 50Hz framerate with 500 units. The upper right corner is the same as Figure~\\ref{fig:exp-task-corr-part}. }  % \\vspace{-8pt",
        "text": "\\centering     % \\vspace{-10pt}     \\includegraphics[width=0.65\\linewidth]{figures/pearson_corr.pdf}     % \\vspace{-20pt}     \\caption{         Pearson correlation coefficients between evaluation metrics computed with tokenizers operate at a 50Hz framerate with 500 units.         The upper right corner is the same as Figure~\\ref{fig:exp-task-corr-part}.     }     \\label{fig:exp-task-corr}     % \\vspace{-8pt}",
        "summarize_figure": "figure1",
        "summarization": "The figure shows the Pearson correlation coefficients between different evaluation metrics computed at a 50Hz framerate with 500 units. Specifically, there is a high positive correlation between TSC and sBLIMP, TSC and ASR, and sWUGGY and ASR, with correlation coefficients of 0.85, 0.74, and 0.77, respectively. A strong negative correlation is observed between Bitrate and ABX, with a correlation coefficient of -0.63. Additionally, PNMI and CNMI show a high correlation of 0.75."
    },
    "189": {
        "figure1": "2410.24181v1_combined_data_distr",
        "label1": "distr",
        "caption1": "Pixel Density distribution of (L) the CAMVID Dataset and (R) the ISIC Dataset. Since majority of ISIC pixels are either 0 or 255 for all centers, these have been omitted for better visualization. Since each of the clients has a different distribution, data from one client can be considered as Out-of-Distribution (OOD) for other clients.",
        "text": "\\centering   {\\includegraphics[width=\\linewidth]{images/combined_data_distr.png}}   \\vskip-8pt \\caption{Pixel Density distribution of (L) the CAMVID Dataset and (R) the ISIC Dataset. Since majority of ISIC pixels are either 0 or 255 for all centers, these have been omitted for better visualization. Since each of the clients has a different distribution, data from one client can be considered as Out-of-Distribution (OOD) for other clients.}  \\label{distr}",
        "summarize_figure": "figure1",
        "summarization": "The image displays the pixel density distribution for the CAMVID dataset, where different colored lines represent various centers (Center 1, Center 2, Center 3, and Center 4). The left plot in the image shows that these centers exhibit distinct distribution peaks and densities within a pixel value range of approximately 0 to 100. Center 1 shows the highest density in the low pixel value region, while Center 2 and Center 3 have their peaks in the middle range. Center 4 displays a flatter distribution extending to higher pixel values. This notable variation indicates significant differences in data distribution across centers, potentially rendering data from one center out-of-distribution for others, as mentioned in the text description."
    },
    "190": {
        "figure1": "2410.24190v3_model_refusal_rate",
        "label1": "fig:model_refusal",
        "caption1": "\\textbf{Refusal rate for each neutral/positive/negative question for each tested LLM.} The error bars represent the 95\\% confidence interval.",
        "text": "\\includegraphics[width=\\linewidth]{figures/model_refusal_rate.pdf}   \\caption{\\textbf{Refusal rate for each neutral/positive/negative question for each tested LLM.} The error bars represent the 95\\% confidence interval.}   \\label{fig:model_refusal}",
        "summarize_figure": "figure1",
        "summarization": "The image displays the refusal rates for various large language models in response to neutral, positive, and negative questions about Biden and Trump. The chart consists of multiple subplots, each representing a tested model. For most models, the refusal rate for neutral questions is consistently very low. However, there are notable differences in refusal rates for positive and negative questions across the models. Some models, such as GPT-4, GPT-3.5, and Llama-3, exhibit very low refusal rates for all question types and subjects. Other models, including Claude-2, Claude-1, Qwen, Solar, and Vicuna, show significantly higher refusal rates for positive and negative questions. Among models with higher refusal rates, the relative rates for questions concerning Biden versus Trump vary depending on the specific model and question type, with some models demonstrating considerably higher refusal for one subject over the other in positive or negative contexts."
    },
    "191": {
        "figure1": "2410.24190v3_model_response_length",
        "label1": "fig:model_length",
        "caption1": "\\textbf{Response length for each neutral/positive/negative question for each LLM.} The letter-value plot starts with the median (50\\%) as the centerline, with each successive level outward containing half of the remaining data.",
        "text": "\\includegraphics[width=\\linewidth]{figures/model_response_length.pdf}   \\caption{\\textbf{Response length for each neutral/positive/negative question for each LLM.} The letter-value plot starts with the median (50\\%) as the centerline, with each successive level outward containing half of the remaining data.}   \\label{fig:model_length}",
        "summarize_figure": "figure1",
        "summarization": "The image presents the response length distributions of various large language models for neutral, positive, and negative questions concerning Biden and Trump. Overall, response lengths vary significantly across different models, with some models tending to generate longer responses and others producing shorter ones. For most models, the response length distributions for questions about Biden and Trump exhibit similar patterns within the neutral, positive, and negative sentiment categories. However, some models show notable differences in response length or distribution shapes when handling specific sentiment orientations or topics, such as the Claude series models' performance on positive questions."
    },
    "192": {
        "figure1": "2410.24190v3_model_sentiment",
        "label1": "fig:model_sentiment",
        "caption1": "\\textbf{Sentiment score for each neutral/positive/negative question for each LLM.",
        "text": "\\includegraphics[width=\\linewidth]{figures/model_sentiment.pdf}   \\caption{\\textbf{Sentiment score for each neutral/positive/negative question for each LLM.}}   \\label{fig:model_sentiment}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the sentiment scores of various Large Language Models (LLMs) for neutral, positive, and negative questions. Each LLM has a set of subplots corresponding to neutral, positive, and negative questions. In each subplot, the blue area represents the tendency towards Biden, while the red area represents the tendency towards Trump. For example, for GPT-4, when answering neutral questions, its sentiment score tends towards Biden; when answering positive questions, the sentiment score also tends more towards Biden; and when answering negative questions, the sentiment score tends more towards Trump."
    },
    "193": {
        "figure1": "2410.24190v3_base_vs_instructed",
        "label1": "fig:base",
        "caption1": "\\textbf{Comparison of sentiment scores between instruction-tuned and base models.} Instruction-tuned models include Llama-3-70B-Chat, Mixtral-8x7B-Instruct, and Qwen1.5-72B-Chat; the corresponding base models are Llama-3-70B, Mixtral-8x7B, and Qwen1.5-72B.",
        "text": "\\includegraphics[width=0.7\\linewidth]{figures/base_vs_instructed.pdf}   \\caption{\\textbf{Comparison of sentiment scores between instruction-tuned and base models.} Instruction-tuned models include Llama-3-70B-Chat, Mixtral-8x7B-Instruct, and Qwen1.5-72B-Chat; the corresponding base models are Llama-3-70B, Mixtral-8x7B, and Qwen1.5-72B.}   \\label{fig:base}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates a comparison of sentiment scores between instruction-tuned models and base models. The horizontal axis represents different sentiment polarities (neutral, positive, negative), and the vertical axis represents sentiment scores. For instruction-tuned models, under neutral sentiment, Biden's sentiment score is slightly higher than Trump's; under positive sentiment, Biden's sentiment score is significantly higher than Trump's; and under negative sentiment, Biden's sentiment score is lower than Trump's."
    },
    "194": {
        "figure1": "2410.24190v3_topic_modeling2",
        "label1": "fig:topic",
        "caption1": "\\textbf{Top 8 topics and their frequencies mentioned by LLMs during conversations with humans.} We trained a BERTopic model using the default setting~\\cite{grootendorst2022bertopic} on the conversational text collected from our experiment. Based on the representative keywords for each topic provided by the topic model, we manually labeled the eight topics as follows: (1) \\textit{climate}, (2) \\textit{pandemic}, (3) \\textit{healthcare}, (4) \\textit{immigration}, (5) \\textit{media}, (6) \\textit{education}, (7) \\textit{Israel-Palestinian} and (8) \\textit{Afghanistan}. Overall, the topics of \\textit{climate}, \\textit{pandemic}, \\textit{healthcare}, and \\textit{education} might be generally advantageous for Biden, whereas \\textit{immigration}, \\textit{media}, \\textit{Israel-Palestinian}, and \\textit{Afghanistan} might be more favorable for Trump. The left subfigure illustrates the frequency with which each topic was mentioned by the three LLMs. The distribution of topics varies across models. Notably, we can see that the most pro-Biden model, Llama-3, primarily mentioned Biden-favored topics. The right subfigure shows the frequency of each topic's appearance when LLMs interacted with Biden supporters, Trump supporters, and neutral participants. The distribution of topics varies across these participant subgroups, but overall leans in a Biden-favoring direction. For instance, when interacting with Trump supporters, the pandemic and healthcare topics were mentioned even more actively than when facing Biden supporters.} %These results imply that LLMs may strategically direct human attention towards specific information for purposes of persuasion.",
        "text": "\\includegraphics[width=\\linewidth]{figures/topic_modeling2.pdf}   \\caption{\\textbf{Top 8 topics and their frequencies mentioned by LLMs during conversations with humans.} We trained a BERTopic model using the default setting~\\cite{grootendorst2022bertopic} on the conversational text collected from our experiment. Based on the representative keywords for each topic provided by the topic model, we manually labeled the eight topics as follows: (1) \\textit{climate}, (2) \\textit{pandemic}, (3) \\textit{healthcare}, (4) \\textit{immigration}, (5) \\textit{media}, (6) \\textit{education}, (7) \\textit{Israel-Palestinian} and (8) \\textit{Afghanistan}. Overall, the topics of \\textit{climate}, \\textit{pandemic}, \\textit{healthcare}, and \\textit{education} might be generally advantageous for Biden, whereas \\textit{immigration}, \\textit{media}, \\textit{Israel-Palestinian}, and \\textit{Afghanistan} might be more favorable for Trump. The left subfigure illustrates the frequency with which each topic was mentioned by the three LLMs. The distribution of topics varies across models. Notably, we can see that the most pro-Biden model, Llama-3, primarily mentioned Biden-favored topics.   The right subfigure shows the frequency of each topic's appearance when LLMs interacted with Biden supporters, Trump supporters, and neutral participants. The distribution of topics varies across these participant subgroups, but overall leans in a Biden-favoring direction. For instance, when interacting with Trump supporters, the pandemic and healthcare topics were mentioned even more actively than when facing Biden supporters.} %These results imply that LLMs may strategically direct human attention towards specific information for purposes of persuasion.}   \\label{fig:topic}",
        "summarize_figure": "figure1",
        "summarization": "The picture shows the frequency of eight key topics mentioned by large language models during conversations. The left chart specifically illustrates the frequency distribution of these topics as mentioned by Claude-3, Llama-3, and GPT-4 models. Llama-3 exhibits the highest overall frequency of topic mentions, with particularly frequent mentions of climate, pandemic, and healthcare topics. Claude-3 shows higher frequencies for healthcare, immigration, and climate. GPT-4, in contrast, demonstrates higher frequencies for media and immigration topics. Based on the provided context, climate, pandemic, healthcare, and education topics might be advantageous for Biden, while immigration, media, Israel-Palestinian, and Afghanistan topics might favor Trump. Notably, Llama-3, described as the most pro-Biden model, indeed shows a frequency distribution leaning towards topics considered advantageous for Biden."
    },
    "195": {
        "figure1": "2410.24190v3_satisfaction",
        "label1": "fig:satisfaction",
        "caption1": "\\textbf{Conversation satisfaction by LLM.} Participants who interacted with Claude-3 reported the highest level of satisfaction.",
        "text": "\\includegraphics[width=0.6\\linewidth]{figures/satisfaction.pdf}   \\caption{\\textbf{Conversation satisfaction by LLM.} Participants who interacted with Claude-3 reported the highest level of satisfaction.}   \\label{fig:satisfaction}",
        "summarize_figure": "figure1",
        "summarization": "The figure presents a survey of conversation satisfaction with large language models (LLMs). It shows a bar chart of the conversation satisfaction for three models: Claude-3, Llama-3, and GPT-4. Claude-3 has the highest satisfaction score, approximately 4.15, while Llama-3 has the lowest, around 3.75, and GPT-4's satisfaction is about 3.95. The figure suggests that, in the participants' experience, conversations with Claude-3 were more satisfying than those with Llama-3 and GPT-4."
    },
    "196": {
        "figure1": "2410.24190v3_conv_by_diff",
        "label1": "fig:conv_diff",
        "caption1": "\\textbf{Correlation between a perceived conversation quality and the change in Biden-leaning percentage.} In the $x$-axis, a positive change in Biden-leaning percentage indicates that participants increased their Biden-leaning percentage after the LLM interaction. Conversely, if the percentage change is negative, it means they decreased their Biden-leaning percentage following interaction with the LLM. The $y$-axis represents whether participants rated that the LLM conversation was better than their regular political talks. The orange line represents a linear regression, and the shaded area indicates its 95\\% confidence interval. This figure shows a significantly positive correlation between the two variables. That is, participants who increased their Biden-leaning percentage tended to feel higher satisfaction with the conversation with the LLM.",
        "text": "\\includegraphics[width=0.7\\linewidth]{figures/conv_by_diff.pdf}   \\caption{\\textbf{Correlation between a perceived conversation quality and the change in Biden-leaning percentage.} In the $x$-axis, a positive change in Biden-leaning percentage indicates that participants increased their Biden-leaning percentage after the LLM interaction. Conversely, if the percentage change is negative, it means they decreased their Biden-leaning percentage following interaction with the LLM. The $y$-axis represents whether participants rated that the LLM conversation was better than their regular political talks. The orange line represents a linear regression, and the shaded area indicates its 95\\% confidence interval. This figure shows a significantly positive correlation between the two variables. That is, participants who increased their Biden-leaning percentage tended to feel higher satisfaction with the conversation with the LLM.}   \\label{fig:conv_diff}",
        "summarize_figure": "figure1",
        "summarization": "The image illustrates the correlation between perceived conversation quality and the change in Biden-leaning percentage. The x-axis represents the change in Biden-leaning percentage, where positive values indicate an increase in Biden-leaning percentage after interaction with the LLM, and negative values indicate a decrease. The y-axis represents how much participants rated the LLM conversation better than their usual political talks. The image shows a significantly positive correlation between the two variables, indicating that participants who increased their Biden-leaning percentage tended to feel more satisfied with the conversation with the LLM. The orange line represents a linear regression with a 95% confidence interval represented by the shaded area."
    },
    "197": {
        "figure1": "2410.24190v3_attitude_change_by_diff",
        "label1": "fig:change_diff",
        "caption1": "\\textbf{Correlation between the change in attitude about AI and the change in Biden-leaning percentage.} In the $x$-axis, a positive change in Biden-leaning percentage indicates that participants increased their Biden-leaning percentage after the LLM interaction. Conversely, if the percentage change is negative, it means they decreased their Biden-leaning percentage following interaction with the LLM. The $y$-axis represents whether participants changed their attitude about AI more/less favorably. The orange line represents a linear regression, and the shaded area indicates its 95\\% confidence interval. This figure shows a significantly positive correlation between the two changes. That is, participants who increased their Biden-leaning percentage tended to feel a more favorable attitude towards AI.",
        "text": "\\includegraphics[width=0.7\\linewidth]{figures/attitude_change_by_diff.pdf}   \\caption{\\textbf{Correlation between the change in attitude about AI and the change in Biden-leaning percentage.} In the $x$-axis, a positive change in Biden-leaning percentage indicates that participants increased their Biden-leaning percentage after the LLM interaction. Conversely, if the percentage change is negative, it means they decreased their Biden-leaning percentage following interaction with the LLM. The $y$-axis represents whether participants changed their attitude about AI more/less favorably. The orange line represents a linear regression, and the shaded area indicates its 95\\% confidence interval. This figure shows a significantly positive correlation between the two changes. That is, participants who increased their Biden-leaning percentage tended to feel a more favorable attitude towards AI.}   \\label{fig:change_diff}",
        "summarize_figure": "figure1",
        "summarization": "The picture illustrates the correlation between the change in attitude about artificial intelligence (AI) and the change in Biden-leaning percentage. The x-axis represents the change in Biden-leaning percentage, where positive values indicate an increase in Biden-leaning after the LLM interaction, and negative values indicate a decrease. The y-axis represents the change in participants' attitudes toward AI, with higher values indicating a more positive attitude. The orange line represents a linear regression, and the shaded area indicates the 95% confidence interval. The picture demonstrates a significantly positive correlation between these two changes, meaning that participants who increased their Biden-leaning percentage tended to have a more favorable attitude toward AI."
    },
    "198": {
        "figure1": "2410.24199v1_rte_l_1",
        "label1": "fig:aug-rte-l",
        "caption1": "Attribute distributions for effective vs. ineffective augmentation on the RTE (Limited) dataset. Effective augmentation has a greater percentage of shorter sentences.}  \\vspace{-10pt",
        "text": "\\centering     \\includegraphics[width=\\linewidth ]{figures/rte_l_1.pdf}     \\vspace{-15pt}     \\caption{Attribute distributions for effective vs. ineffective augmentation on the RTE (Limited) dataset. Effective augmentation has a greater percentage of shorter sentences.}     \\label{fig:aug-rte-l}     \\vspace{-10pt}\nFigure~\\ref{fig:aug-rte-l} visualizes attribute distributions that lead to effective and ineffective augmentation for the RTE (Limited) dataset. For effective augmentation, target attributes should have a significantly higher prevalence of shorter sentences, while ineffective augmentation produces more medium-length sentences. The Mann–Whitney U test confirms significant differences with p-value $< 0.05$ in the attribute distributions between effective and ineffective sets across all our six datasets. Details are provided in Appendix~\\ref{app:aug-data}.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the attribute distributions for effective and ineffective data augmentation on the RTE (Limited) dataset. The x-axis represents the number of words per sentence, categorized into four intervals: 7.0, 31.7, 56.3, and 81.0. The y-axis represents the percentage. For shorter sentences (7.0 words), the percentage of ineffective augmentation is significantly higher than no augmentation and effective augmentation, specifically, the ineffective augmentation close to 70%, the no augmentation close to 100%, and the effective augmentation close to 80%, ineffective augmentation is 34.8% lower than no augmentation, and 21.9% lower than effective augmentation. For medium-length sentences (31.7 words), the percentage of ineffective augmentation is higher than no augmentation and effective augmentation, and ineffective augmentation is 33.6% higher than no augmentation and 19.9% higher than effective augmentation. For longer sentences (56.3 and 81.0 words), the percentage of effective augmentation is slightly higher than ineffective augmentation. The overall trend indicates that effective data augmentation tends to favor short sentences, while ineffective data augmentation tends to favor medium-length sentences."
    },
    "199": {
        "figure1": "2410.24201v1_delta",
        "label1": "fig:delta",
        "caption1": "Model Performance by number of attributes controlled. The graph shows the MSE for different models as the number of controlled attributes increases. LingGen (P-Masking) consistently achieves the lowest MSE, indicating effective multi-attribute control.",
        "text": "\\label{fig:delta}\nFigure~\\ref{fig:delta} illustrates the MSE performance of various models as the number of controlled attributes increases. LingGen with P-Masking consistently achieves the lowest MSE across all attribute counts, demonstrating its superior ability to manage multiple attributes effectively. As the number of attributes increases, models like No Masking and Dropout show a significant rise in MSE, indicating challenges in handling complex attribute configurations. In contrast, LingGen with Fixed-rate Masking and BOLT maintain relatively stable MSEs, though not as low as LingGen with P-Masking. This highlights the robustness of the P-Masking strategy in multi-attribute control tasks.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the MSE performance of different models as the number of controlled attributes increases. LingGen (P-Masking) consistently achieves the lowest MSE across all attribute counts, demonstrating its superior ability to manage multiple attributes effectively. As the number of attributes increases, models like No Masking show a significant rise in MSE, indicating challenges in handling complex attribute configurations. In contrast, LingGen (Fixed-rate Masking) and BOLT maintain relatively stable MSEs, though not as low as LingGen (P-Masking). These results highlight the robustness of the P-Masking strategy in multi-attribute control tasks."
    },
    "200": {
        "figure1": "2410.24201v1_heatmap",
        "label1": "fig:pairwise",
        "caption1": "Pairwise attribute relations. Each cell represents the normalized MSE for a primary controlled attribute (rows) when paired with an accompanying attribute (columns). Lighter colors indicate higher errors.",
        "text": "\\centering     \\includegraphics[width=\\linewidth]{figures/heatmap.pdf}     \\caption{Pairwise attribute relations. Each cell represents the normalized MSE for a primary controlled attribute (rows) when paired with an accompanying attribute (columns). Lighter colors indicate higher errors.}     \\label{fig:pairwise}",
        "summarize_figure": "figure1",
        "summarization": "The figure displays a heatmap of pairwise attribute relations. Each cell in the image represents the normalized mean squared error (MSE) for a primary controlled attribute (rows) when paired with an accompanying attribute (columns). Lighter colors indicate higher errors. Specifically, when \"Reading Time\" is the primary controlled attribute, combining it with \"# Characters\" results in a relatively high error. Similarly, combining \"Age of Acquisition\" and \"# Characters\" also leads to a higher error. Conversely, darker areas indicate lower errors, implying that these attribute combinations have less impact on the primary controlled attribute."
    },
    "201": {
        "figure1": "2410.24206v1_stationary",
        "label1": "fig:rmsprop-stationarity",
        "caption1": "\\textbf{The EMA $\\boldsymbol{\\nu}$ reaches stationarity during training.} While running the RMSProp central flow, we monitor the cosine similarity between $\\nu(t)$, the real EMA, and $\\bar{\\nu}(w(t))$, the stationary EMA.  This cosine similarity rises to high values during training, implying that $\\nu(t)$ reaches stationarity.  Thus, we can reason about the stationary $\\nu$ (and in particular, the corresponding stationary preconditioner \\cref{eq:rmsprop_nu_convex_program}) in order to reason about RMSProp's preconditioning strategy.  Note that we compute $\\bar{\\nu}(w)$ using the Burer-Monteiro factorization, as described in \\cref{sec:experimental_details_appendix}.",
        "figure2": "2410.24206v1_stationary-flow",
        "label2": "fig:rmsprop-stationary-predict-loss",
        "caption2": "\\textbf{Stationary flow predicts the rate of loss decrease}.  The stationary flow \\cref{eq:rmsprop-stationary-flow}, which incorporates implicit curvature regularization, predicts (green) the rate of loss decrease (blue) more accurately than a naive estimate (orange) which uses the stationary preconditioner but does not incorporate curvature regularization.  Note that all the estimates are off early in training, as the preconditioner has not yet reached stationarity (see \\Cref{fig:rmsprop-stationarity}).",
        "text": "\\includegraphics[width=0.9\\linewidth]{images/stationary.pdf}     \\caption{\\textbf{The EMA $\\boldsymbol{\\nu}$ reaches stationarity during training.} While running the RMSProp central flow, we monitor the cosine similarity between $\\nu(t)$, the real EMA, and $\\bar{\\nu}(w(t))$, the stationary EMA.  This cosine similarity rises to high values during training, implying that $\\nu(t)$ reaches stationarity.  Thus, we can reason about the stationary $\\nu$ (and in particular, the corresponding stationary preconditioner \\cref{eq:rmsprop_nu_convex_program}) in order to reason about RMSProp's preconditioning strategy.  Note that we compute $\\bar{\\nu}(w)$ using the Burer-Monteiro factorization, as described in \\cref{sec:experimental_details_appendix}.}     \\label{fig:rmsprop-stationarity}\n\\includegraphics[width=0.9\\linewidth]{images/stationary-flow.pdf}     \\caption{\\textbf{Stationary flow predicts the rate of loss decrease}.  The stationary flow \\cref{eq:rmsprop-stationary-flow}, which incorporates implicit curvature regularization, predicts (green) the rate of loss decrease (blue) more accurately than a naive estimate (orange) which uses the stationary preconditioner but does not incorporate curvature regularization.  Note that all the estimates are off early in training, as the preconditioner has not yet reached stationarity (see \\Cref{fig:rmsprop-stationarity}).}     \\label{fig:rmsprop-stationary-predict-loss}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the stationarity of EMA during training. The upper plots show the cosine similarity between the real EMA, denoted as nu(t), and the stationary EMA, denoted as nu_bar(w(t)), for CNN, ResNet, and ViT models on the CIFAR-4 dataset with MSE loss. The lower plots show the same models and dataset, but with CE loss. The x-axis represents the training step, and the y-axis represents the cosine similarity. As training progresses, the cosine similarity increases and approaches 1, indicating that nu(t) reaches stationarity. This suggests that RMSProp's preconditioning strategy can be reasoned about by analyzing the stationary nu and its corresponding preconditioner."
    },
    "202": {
        "figure1": "2410.24206v1_suboptimal",
        "label1": "fig:rmsprop-suboptimal",
        "caption1": "\\textbf{RMSProp's implicit preconditioner is suboptimal}.  We compare RMSProp's stationary preconditioner, defined as the solution to the optimization problem \\cref{eq:rmsprop_nu_convex_program}, to an alternative preconditioner defined as the solution to an analogous optimization problem without the second term in the objective.  We assess the efficacy of each preconditioner $P$ by computing $\\| \\nabla L(w(t)) \\|^2_{P^{-1}}$, the instantaneous rate of loss decrease under the preconditioned gradient flow with preconditioner $P$. We observe that the rate of loss decrease for the alternative preconditioner (in green) is higher than the rate of loss decrease for the RMSProp stationary preconditioner (in orange), indicating that the alternative preconditioner would be better.  Both preconditioners are computed using the Burer-Monteiro factorization, as described in \\Cref{sec:experimental_details_appendix}.",
        "text": "\\includegraphics[width=0.9\\linewidth]{images/suboptimal.pdf}     \\caption{\\textbf{RMSProp's implicit preconditioner is suboptimal}.  We compare RMSProp's stationary preconditioner, defined as the solution to the optimization problem \\cref{eq:rmsprop_nu_convex_program}, to an alternative preconditioner defined as the solution to an analogous optimization problem without the second term in the objective.  We assess the efficacy of each preconditioner $P$ by computing $\\| \\nabla L(w(t)) \\|^2_{P^{-1}}$, the instantaneous rate of loss decrease under the preconditioned gradient flow with preconditioner $P$. We observe that the rate of loss decrease for the alternative preconditioner (in green) is higher than the rate of loss decrease for the RMSProp stationary preconditioner (in orange), indicating that the alternative preconditioner would be better.  Both preconditioners are computed using the Burer-Monteiro factorization, as described in \\Cref{sec:experimental_details_appendix}.}     \\label{fig:rmsprop-suboptimal}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates that RMSProp's implicit preconditioner is suboptimal. Specifically, the first subplot (CNN/CIFAR-4/MSE) compares RMSProp's stationary preconditioner (orange line) to an alternative preconditioner (green line), which is the solution obtained through a similar optimization problem without the second term in the objective function. The efficacy of each preconditioner P is assessed by computing the norm of the gradient L(w(t)) squared under the inverse of the preconditioner P, representing the instantaneous rate of loss decrease under the preconditioned gradient flow with preconditioner P. The rate of loss decrease for the alternative preconditioner is observed to be higher than that of the RMSProp stationary preconditioner, indicating that the alternative preconditioner may be better."
    },
    "203": {
        "figure1": "2410.24206v1_stationary-flow",
        "label1": "fig:rmsprop-stationary-predict-loss",
        "caption1": "\\textbf{Stationary flow predicts the rate of loss decrease}.  The stationary flow \\cref{eq:rmsprop-stationary-flow}, which incorporates implicit curvature regularization, predicts (green) the rate of loss decrease (blue) more accurately than a naive estimate (orange) which uses the stationary preconditioner but does not incorporate curvature regularization.  Note that all the estimates are off early in training, as the preconditioner has not yet reached stationarity (see \\Cref{fig:rmsprop-stationarity}).",
        "figure2": "2410.24206v1_stationary",
        "label2": "fig:rmsprop-stationarity",
        "caption2": "\\textbf{The EMA $\\boldsymbol{\\nu}$ reaches stationarity during training.} While running the RMSProp central flow, we monitor the cosine similarity between $\\nu(t)$, the real EMA, and $\\bar{\\nu}(w(t))$, the stationary EMA.  This cosine similarity rises to high values during training, implying that $\\nu(t)$ reaches stationarity.  Thus, we can reason about the stationary $\\nu$ (and in particular, the corresponding stationary preconditioner \\cref{eq:rmsprop_nu_convex_program}) in order to reason about RMSProp's preconditioning strategy.  Note that we compute $\\bar{\\nu}(w)$ using the Burer-Monteiro factorization, as described in \\cref{sec:experimental_details_appendix}.",
        "text": "\\includegraphics[width=0.9\\linewidth]{images/stationary-flow.pdf}     \\caption{\\textbf{Stationary flow predicts the rate of loss decrease}.  The stationary flow \\cref{eq:rmsprop-stationary-flow}, which incorporates implicit curvature regularization, predicts (green) the rate of loss decrease (blue) more accurately than a naive estimate (orange) which uses the stationary preconditioner but does not incorporate curvature regularization.  Note that all the estimates are off early in training, as the preconditioner has not yet reached stationarity (see \\Cref{fig:rmsprop-stationarity}).}     \\label{fig:rmsprop-stationary-predict-loss}",
        "summarize_figure": "figure1",
        "summarization": "The figure compares the actual rate of loss decrease with two estimated rates under different models (CNN, ResNet, ViT) and loss functions (MSE, CE). The blue curve represents the actual rate of loss decrease, the orange curve represents the estimate without curvature regularization, and the green curve represents the prediction of the stationary flow incorporating implicit curvature regularization. It is observed that the green curve (estimate with curvature regularization) predicts the rate of loss decrease more accurately than the orange curve (estimate without curvature regularization). All estimates deviate early in training because the preconditioner has not yet reached a stationary state."
    },
    "204": {
        "figure1": "2410.24206v1_flow-accuracy-lr",
        "label1": "fig:flow-accuracy-lr",
        "caption1": "\\textbf{Central flow approximation is less accurate at larger learning rates}.  We run both gradient descent and its central flow at three learning rates (colors).  The larger the learning rate, the faster the growth in the accumulated approximation error (right).\\protect\\footnotemark Indeed, at larger learning rates, the network's output on an arbitrary test example can be visually seen to be slightly different between the central flow and gradient descent (middle).  Nevertheless, the central flow approximation is still accurate enough here to accurately capture the train loss curves (left).",
        "text": "Our full set of experiments, which can be found in \\Cref{sec:experiments_appendix}, demonstrate that the central flow accurately approximates the true optimization trajectory in a variety of deep learning settings. In many experiments, the weight-space distance between the central flow and the real optimizer trajectory stays at essentially zero throughout training (e.g. \\Cref{fig:gradient-descent}). Even in cases where this distance grows non-negligibly during training, the central flow's predictions for derived quantities such as the train loss, or individual network outputs, often still closely match those of the real optimizer trajectory (e.g. \\Cref{fig:flow-accuracy-lr}).\nWe now discuss some factors which affect the accuracy of the central flow approximation.  First, the quality of the central flow approximation often degrades as the learning rate grows increasingly large (see \\Cref{fig:flow-accuracy-lr}).\\footnote{It is unclear whether there exists a correction to the central flow that could improve its accuracy in these settings, or whether the underlying trajectory is simply too chaotic to be captured by any flow.} Second, the central flow approximation is usually more accurate for MSE loss than for cross-entropy loss.  The issue may be that higher-order terms cause the sharpness equilibrium point to differ slightly from $2/\\eta$ for cross-entropy loss, as previously noted by \\citet{damian2023selfstabilization} in their Appendix F. An interesting direction for future work is to design corrections to the central flow that improve its predictions in this situation. Third, the regularity of the loss landscape also seems to play a role \\citep{wang2023good}; in \\Cref{fig:beta-gelu-distances}, we consider a family of activations which interpolate between GeLU (smooth) and ReLU (not smooth). We observe that the weight-space discrepancy between the flow and the optimizer trajectory grows larger as the activation function becomes less smooth.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the training process of gradient descent and its central flow at different learning rates. The left panel shows that the central flow approximation accurately captures the training loss curves across different learning rates. The middle panel indicates that as the learning rate increases, visual differences emerge between the central flow and gradient descent in terms of network output on a test sample. The right panel demonstrates that the larger the learning rate, the faster the growth in the accumulated approximation error. Specifically, the weight space distance remains low when the learning rate is 0.01. When the learning rates are 0.02 and 0.04, the weight space distance increases significantly, with the growth being most rapid at 0.04. Thus, the accuracy of the central flow approximation decreases at larger learning rates."
    },
    "205": {
        "figure1": "2410.24206v1_gd_all",
        "label1": "fig:gd_all",
        "caption1": "\\textbf{The central flow accurately predicts the loss curve of \\gd.} We train three architectures at three different learning rates on a subset of CIFAR-10. For each gradient descent trajectory, we also run the central flow starting from the same initialization. The central flow (black) accurately predicts the time-averaged, i.e. smoothed, loss curves of gradient descent (colors, solid).  Raw loss curves are plotted in semi-transparent colors. ",
        "text": "\\item In \\Cref{sec:gd:dynamics}, we describe the dynamics of gradient descent in deep learning.  Gradient descent typically operates in a regime termed the \\emph{edge of stability} (EOS) characterized by complex oscillatory dynamics \\citep{cohen2021gradient}.     The oscillations dominate the trajectory over short timescales, causing the loss to behave non-monotonically, as shown in \\Cref{fig:gd_all}. More importantly, they also affect the \\emph{long-term} trajectory by implicitly steering gradient descent away from high-curvature regions of the loss landscape. As a result, the raw update rule \\cref{eq:gd} is largely uninformative as to actual behavior of the algorithm.          \\item In \\Cref{sec:gd:deriving}, generalizing a result by \\citet{damian2023selfstabilization}, we show that while the gradient descent trajectory is highly oscillatory, its \\emph{time-averaged} (i.e. smoothed) trajectory can be captured by a differential equation called a \\emph{central flow}.      We derive this central flow using informal mathematical reasoning, and empirically demonstrate that it can successfully predict long-term optimization trajectories of neural networks with a high degree of numerical accuracy. For example, \\Cref{fig:gd_all} shows that the central flow can predict training loss curves on a variety of neural network architectures.  Because the central flow makes \\emph{explicit} the path that is \\emph{implicitly} taken by gradient descent, it makes reasoning about the optimizer's behavior easier.          \\item In \\Cref{sec:gd:interpreting} we use this central flow to understand the behavior of gradient descent. For example, we show that gradient descent's non-monotonic loss curve can be viewed as the superposition of the loss along the central flow plus a contribution from the oscillations. The loss along the central flow is a smoothly varying quantity that monotonically decreases, and therefore constitutes a hidden progress metric for gradient descent.\n\\centering     \\includegraphics[width=\\textwidth]{images/gd_all.pdf}     \\caption{\\textbf{The central flow accurately predicts the loss curve of \\gd.} We train three architectures at three different learning rates on a subset of CIFAR-10. For each gradient descent trajectory, we also run the central flow starting from the same initialization.   The central flow (black) accurately predicts the time-averaged, i.e. smoothed, loss curves of gradient descent (colors, solid).  Raw loss curves are plotted in semi-transparent colors.     }     \\label{fig:gd_all}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the loss curves of a CNN model trained on the CIFAR-10 dataset with different learning rates (η = 0.01, 0.02, 0.04). The horizontal axis represents the training step, and the vertical axis represents the loss value. Solid colored lines represent the loss curves of gradient descent, semi-transparent colored lines represent the raw loss curves, and black dashed lines represent the loss curves predicted by the central flow. As the learning rate increases, the loss decreases faster. The central flow accurately predicts the smoothed loss curves of gradient descent."
    },
    "206": {
        "figure1": "2410.24206v1_main_gd",
        "label1": "fig:gradient-descent",
        "caption1": "\\textbf{Central flow for \\protect\\gd.} A ViT is trained on CIFAR-10 using \\protect\\gd with $\\eta = 2/100$ (blue). Gradient descent enters EOS at step 600 and after step 1450 multiple eigenvalues are unstable (dotted red). The central flow (black) accurately models \\protect\\gd even at EOS, whereas gradient flow (gray) follows a different path. The distance between the central flow and \\protect\\gd (black) even remains smaller than the distance between consecutive iterates of \\protect\\gd (blue) throughout the trajectory.",
        "text": "In this section, we will derive a differential equation which models the trajectory of gradient descent. The standard continuous-time approximation to gradient descent is the \\gflow:\\footnote{We fold $\\eta$ into the definition of gradient flow so that there is a correspondence between step $t$ of \\gd and time $t$ of \\gflow.  This will especially be useful when analyzing adaptive optimizers where the effective step size is a dynamic quantity.}     \\frac{dw}{dt} = - \\eta \\nabla L(w). \\label{eq:gflow} by that of \\gflow so long as training is \\emph{stable}, i.e. so long as the sharpness $S(w)$ remains below $2/\\eta$. However, once the sharpness reaches $2/\\eta$ and the optimizer enters the EOS regime, gradient descent departs from the gradient flow trajectory and takes a different path. For example, the gray line in \\Cref{fig:gradient-descent}(d) plots the weight-space distance between the gradient descent iterate at step $t$ and the gradient flow solution at time $t$. This distance remains small so long as training is stable, but starts to grow large once training enters EOS, indicating that these trajectories diverge.\nWe will derive the central flow using informal mathematical reasoning, and we will empirically demonstrate that it can accurately predict long-term optimization trajectories of neural networks with a high degree of numerical accuracy. For example, the black line in \\Cref{fig:gradient-descent}(d) plots the weight-space distance between gradient descent at step $t$ and the central flow at time $t$. Observe that this distance stays small throughout training, indicating that while the \\gd and gradient flow trajectories diverge, the \\gd and \\emph{central} flow trajectories remain close.",
        "summarize_figure": "figure1",
        "summarization": "The image displays the loss progression of different optimization methods during training. The first subplot shows the change in loss over training steps for Gradient Descent (GD), Averaged GD, Central Flow Prediction, and Gradient Flow. The loss for GD decreases with oscillations, while the Averaged GD shows a smooth decreasing trend. The Central Flow prediction curve closely matches the loss curve of Averaged GD, maintaining a consistent downward path throughout training. In contrast, the Gradient Flow loss curve is initially close to Gradient Descent but begins to deviate significantly after approximately step 1000, exhibiting different convergence behavior. This suggests that the Central Flow provides a more accurate prediction or simulation of the Gradient Descent trajectory in the loss space compared to the Gradient Flow."
    },
    "207": {
        "figure1": "2410.24206v1_main_rmsprop",
        "label1": "fig:rmsprop",
        "caption1": "\\textbf{Central Flow for RMSProp.} A ViT is trained on a subset of CIFAR-10 using \\protect\\rmsprop with $\\eta = 4{\\times}10^{-5}$ and $\\beta_2 = 0.995$ (blue). It enters EOS at step 230 and multiple eigenvalues become unstable at step 360  (dotted red). The central flow (black) accurately models the time-averaged trajectory of \\protect\\rmsprop even at EOS, whereas the naive stable flow (gray) follows a different path.",
        "text": "In \\Cref{appendix:derivations:rmsprop} we derive a central flow $(w(t),\\nu(t))$, in the same way as above, which models the time-averaged trajectory of \\rmsprop. We verify its accuracy in \\Cref{fig:rmsprop} and \\Cref{sec:experiments_appendix}.",
        "summarize_figure": "figure1",
        "summarization": "Here is a summary of the chart regarding the central flow for RMSProp.In the figure, the loss function decreases with the increase of steps. The loss decreases rapidly in the initial stage, and then flattens out, approaching 0.2 at 1000 steps. The loss value of the RMSProp algorithm is represented in blue, the loss value predicted by the central flow is represented by a black dashed line, and the loss value of the stable flow is represented by a gray dashed line. There are two vertical red dashed lines at step 230 and step 360."
    },
    "208": {
        "figure1": "2410.24206v1_main_rmsprop_norm",
        "label1": "fig:rmsprop-norm",
        "caption1": "\\textbf{Central flow for \\protect\\rmsnorm.} A ViT is trained on a subset of CIFAR-10 using \\protect\\rmsnorm with $\\eta = 2/100$ and $\\beta_2 = 0.995$ (blue).  It enters EOS around step 300, and around step 1300 multiple eigenvalues go unstable (dotted red). The central flow (black) accurately models the time-averaged trajectory of \\protect\\rmsnorm even at the edge of stability, whereas the naive stable flow (gray) follows a different path. The effective step size (ESS) plot shows that after reaching EOS, the ESS equilibrates around the \\emph{maximal locally stable step size} $2/S$.",
        "text": "The dynamics of \\rmsnorm revolve around the \\emph{effective sharpness}, defined as $\\Seff := S(w)/\\sqrt{\\nu}$. First, the effective sharpness controls the oscillations: when $\\Seff > 2/\\eta$, \\rmsnorm oscillates with growing magnitude along high curvature direction(s). Second, such oscillations in turn trigger a reduction of effective sharpness. This occurs via a combination of two distinct mechanisms. One mechanism, shared with \\gd, is that oscillations implicitly reduce sharpness due to \\Cref{eq:gd:cubic_taylor}, thereby decreasing the effective sharpness via its \\emph{numerator}. The other mechanism, new to \\rmsnorm, is that oscillations increase the gradient norm and hence $\\nu$, thereby decreasing effective sharpness via its \\emph{denominator}. These dynamics give rise to a negative feedback loop that keeps the effective sharpness automatically regulated around $2/\\eta$, as depicted in the bottom left plot in \\Cref{fig:rmsprop-norm}. The fine-grained dynamics are complex and challenging to analyze, even in the case of a single oscillatory direction. Fortunately, we will see in the next section that analyzing the \\emph{time-averaged} dynamics is much simpler.\nIn \\cref{sec:gd:single}, we derived an approximation for the time-averaged gradient, $\\E[\\nabla L(w)]$. Using the first two terms of \\cref{eq:gd:cubic_taylor}, we can also derive a time-averaged approximation for the squared gradient norm $\\E[\\norm{\\nabla L(w)}^2]$:     \\E[\\norm{\\nabla L(w)}^2] \\approx \\norm{\\nabla L(\\overline{w})}^2 + \\cancel{2\\ev{\\nabla L(\\overline{w}), u} S(\\overline{w}) \\E[x]} +  S(\\overline{w})^2 \\E[x^2] where we again used $\\E[x] = 0$ to ignore the middle term. Based on these time averages, we make the ansatz that the joint dynamics of $(w_t,\\nu_{t})$ follow a central flow $(w(t),\\nu(t))$ of the form:     \\frac{dw}{dt} = - \\frac{\\eta}{\\sqrt{\\nu}} \\qty\\Big[\\underbrace{\\nabla L(w) + \\tfrac{1}{2} \\sigma^2(t) \\nabla S(w)}_{\\E[\\nabla L(w_t)]}] ,\\quad\\quad     \\frac{d\\nu}{dt} = \\frac{1-\\beta_2}{\\beta_2}\\qty\\Big[\\underbrace{\\norm{\\nabla L(w)}^2 + S(w)^2 \\sigma^2(t)}_{\\E[\\norm{\\nabla L(w_t)}^2]} - \\nu] where $\\sigma^2(t)$ is a still-unknown quantity intended to model $\\E[x_t^2]$, the instantaneous variance of the oscillations. As in our analysis of \\gd, there is a unique value of $\\sigma^2(t)$ that maintains $\\Seff(w,\\nu) = 2/\\eta$. To compute it, we expand $\\frac{d\\Seff}{dt}$ using the chain rule: $     \\frac{d\\Seff}{dt} = \\langle \\frac{\\partial \\Seff}{\\partial w}, \\frac{dw}{dt}\\rangle + \\frac{\\partial \\Seff}{\\partial \\nu} \\cdot \\frac{d\\nu}{dt} $. Plugging in $\\frac{dw}{dt}, \\frac{d\\nu}{dt}$ from \\cref{eq:rmsprop_norm_flow} shows that $\\frac{d\\Seff}{dt}$ is linear in $\\sigma^2$. Thus, there is a unique value of $\\sigma^2$ that will ensure  $\\frac{d\\Seff}{dt} = 0$, which is given by:     \\sigma^2(w;\\eta,\\beta_2) = \\frac{\\beta_2 \\overbrace{\\ev{-\\nabla L(w), \\nabla S(w)}}^{\\mathclap{\\text{progressive sharpening}}} + (1{-}\\beta_2) \\overbrace{\\qty[S(w)^2/4 - \\|\\nabla L(w)\\|^2/\\eta^2]}^{\\mathclap{\\text{effect of mean reversion on $\\nu$}}}}{\\beta_2\\underbrace{\\tfrac{1}{2} \\norm{\\nabla S(w)}^2}_{\\mathclap{\\text{sharpness reduction}}} + (1{-}\\beta_2) \\underbrace{S(w)^2/\\eta^2}_{\\mathclap{\\text{effect of oscillation on $\\nu$}}}}.\\label{eq:rmsprop_norm_x} The central flow for \\rmsnorm with a single unstable eigenvalue is given by \\cref{eq:rmsprop_norm_flow} with this value of $\\sigma^2$.\\footnote{The \\rmsnorm central flow can be interpreted as a projected flow in the augmented space $(w,\\nu)$ under a certain non-Euclidean norm. However, because this flow is not a gradient flow, it does not immediately suggest a decreasing potential function for \\rmsnorm.} The full \\rmsnorm central flow, derived in \\Cref{appendix:derivations:rmsprop_norm}, is given in \\Cref{def:rmsnorm:flow:ccp}. In \\Cref{fig:rmsprop-norm} and \\Cref{sec:experiments_appendix} we empirically verify that this flow accurately predicts long-term optimization trajectories of neural networks.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the central flow of scalar RMSNorm training a ViT model on a subset of CIFAR-10. The first row, from left to right, shows the curves of the loss function value, nu, the square of the gradient norm, and the square of the oscillation norm versus the number of training steps. Scalar RMSProp (blue) shows a rapid decrease in loss early in training, followed by a plateau. The value of nu first decreases and then increases, and the square of the gradient norm and the square of the oscillation norm also show a similar trend. The second row, from left to right, shows the curves of effective eigenvalues, Hessian matrix eigenvalues, ESS (effective step size) versus 2/S, and distance versus the number of training steps. The effective eigenvalue oscillates early on and then stabilizes around 2/eta. The central flow (black) accurately models the time-averaged trajectory of scalar RMSNorm, even at the edge of stability."
    },
    "209": {
        "figure1": "2410.24213v1_results_vit_all",
        "label1": "fig:results_vit_all",
        "caption1": "\\textbf{Action recognition accuracy on UCF101.} We present the UCF101 classification accuracy of the progression of models $\\{M_i\\}$, after fine-tuning each of them on UCF101. The accuracy increases along the progression.",
        "text": "\\centering     \\includegraphics[width=1.0\\textwidth]{figures/results_vit_all.pdf}     % \\vspace{-1.7em}     \\caption{\\textbf{Action recognition accuracy on UCF101.} We present the UCF101 classification accuracy of the progression of models $\\{M_i\\}$, after fine-tuning each of them on UCF101. The accuracy increases along the progression.}     \\label{fig:results_vit_all}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the action recognition accuracy of different models fine-tuned on the UCF101 dataset. The figure is divided into two parts, corresponding to ViT-B and ViT-L models, respectively. For ViT-B, the accuracy shows an upward trend from \"Random\" to \"UCF101 (official)\", where \"Random\" has the lowest accuracy of 51.4% and \"UCF101 (official)\" achieves the highest accuracy of 91.3%. Similarly, the ViT-L model also shows a similar trend, with an accuracy of 49.0% for \"Random\" and 91.6% for \"UCF101 (official)\". Both models approach their highest accuracy when using \"Accelerating and transforming image crops\"."
    },
    "210": {
        "figure1": "2410.24213v1_results_distribution_shift",
        "label1": "fig:ood",
        "caption1": "\\textbf{Distribution Shift results on UCF101-P~\\citep{robustness2022large} (ViT-B)} The last model in our progression outperforms pre-training on natural videos for 11 out of 14 corruption datasets.",
        "text": "We fine-tune the pre-trained models $\\{M_i\\}$ on UCF-101, and evaluate on corrupted datasets from UCF101-P~\\citep{robustness2022large}. The results for the last two models in the progression are presented in \\Cref{fig:ood}. As shown, the last model in the progression outperforms the UCF101 pre-trained model on 11 out of 14 tasks and performs comparably on the rest. This suggests that while the current pre-train recipe fails to generalize to out-of-distribution datasets. We note that the second to last model in our progression, which does not use real images, performs better only on 6 out of the 14 datasets. This suggests that differently from StyleGAN textures, the natural image crops unlock generalization capabilities to out-of-distribution \\textit{video} corruptions.\n\\centering     \\includegraphics[width=1.0\\textwidth]{figures/results_distribution_shift.pdf}     \\vspace{-1em}     \\caption{\\textbf{Distribution Shift results on UCF101-P~\\citep{robustness2022large} (ViT-B)} The last model in our progression outperforms pre-training on natural videos for 11 out of 14 corruption datasets.}     \\label{fig:ood}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the performance (accuracy) of three pre-training methods on the UCF101-P dataset for the ViT-B model under various data corruption conditions. The three methods are Accelerating and transforming image crops, Accelerating and transforming StyleGAN crops, and UCF pre-training. The horizontal axis represents different types of data corruption, including original, translate, rotate, img rotate, shot, speckle, gauss, impulse, defocus, zoom, Jpeg, freeze, box jumble, and reverse. The vertical axis represents the accuracy, expressed as a percentage.As can be seen from the figure, all three pre-training methods show high accuracy in the case of original data and translation. For corruption types such as rotate, img rotate, shot, speckle, gauss, impulse, defocus, zoom, Jpeg, and freeze, the accuracy of different pre-training methods varies. In the case of box jumble and reverse, all three pre-training methods show high accuracy and little difference."
    },
    "211": {
        "figure1": "2410.24213v1_all_property",
        "label1": "fig:all_property",
        "caption1": "\\textbf{Dataset properties compared to downstream performance.} We compare the downstream classification accuracy on UCF101 after fine-tuning to frame and video properties of all the dataset variants we used in our analysis (see datasets list in~\\Cref{appendix:data}). % \\xueyang{$r=-0.72, -0.27, 0.53, -0.35, -0.42$ from left to right} }\\vspace{-1.em",
        "text": "As shown in~\\Cref{fig:all_property}.a there is a strong negative correlation between the frame similarity to the accuracy ($r=-0.72$). This suggests that improving frame similarity can lead to better performance. Nevertheless, the FID scores are considered to be high, and our datasets are significantly different from the original UCF101 data.\n\\centering     \\includegraphics[width=1.0\\textwidth]{figures/all_property.pdf}          \\caption{\\textbf{Dataset properties compared to downstream performance.} We compare the downstream classification accuracy on UCF101 after fine-tuning to frame and video properties of all the dataset variants we used in our analysis (see datasets list in~\\Cref{appendix:data}).     % \\xueyang{$r=-0.72, -0.27, 0.53, -0.35, -0.42$ from left to right}     \\label{fig:all_property}}\\vspace{-1.em}      We visualize the learned representation produced by the models $M_i$. Following \\citealt{amir2021deep}, we compute PCA on the attention keys extracted from the last VideoMAE encoder layer across 32 frames from 70 videos from the same class of UCF101. We plot the first three principal components as red, green, and blue channels and present features for 2-frame inputs  (see temporal PCA for full videos in the \\href{https://unicorn53547.github.io/video_syn_rep/}{project page}).",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the relationship between dataset properties and downstream performance. Specifically, it depicts the downstream classification accuracy on UCF101 after fine-tuning, plotted against the frame similarity of the dataset, as measured by FID. The figure indicates that as the FID value increases (indicating lower frame similarity), the accuracy tends to decrease. Most data points are clustered in the region of lower FID values and higher accuracy, but some points are distributed in the region of higher FID values and lower accuracy, suggesting a negative correlation between frame similarity and accuracy."
    },
    "212": {
        "figure1": "2410.24218v1_explain_messenger",
        "label1": "explain_messenger",
        "caption1": "In the Messenger environment, when trained with more diverse foresight and hindsight languages, the agents can perform better than those trained without languages. Furthermore, agents trained with more informative languages demonstrate stronger performance.",
        "text": "In the Messenger environment, models trained with only template foresight or hindsight languages struggle to generalize to diverse languages during testing. Without exposure to diverse languages during training, these models fail to extract the learned hindsight or foresight information from mixed and diverse languages. However, Figure \\ref{explain_messenger} demonstrates that models trained with more diverse hindsight or foresight languages can overcome the generalization problem, and outperform those trained without language feedback, showcasing the importance of diversity in the training languages. Furthermore, the agents trained with both hindsight and foresight information still perform the best, aligning with results in other environments.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the impact of different language training methods on model rewards in the Messenger environment. Models trained with GPT augmented hindsight and foresight languages perform better than those without language information. Specifically, the model trained with only GPT augmented hindsight language has a higher reward value than the model with no language information. The model trained with GPT augmented foresight language has a slightly higher reward value than the one trained with only hindsight language. The model trained with both GPT augmented hindsight and foresight languages achieves the highest reward value, significantly outperforming the other three scenarios. This suggests that leveraging both hindsight and foresight information can significantly improve model performance in the Messenger environment."
    },
    "213": {
        "figure1": "2410.24218v1_appendixD",
        "label1": "fig:appendixD",
        "caption1": "Examples for language feedback generated by online GPT in evaluation.",
        "text": "As discussed in section \\ref{sec:eval_lang_feedback}, we feed template hindsight ($l^{hind}$) and template foresight ($l^{fore}$) into an online GPT to generate language feedback as a proxy for real-world human feedback, which can be further extended into multi-turn human-machine dialogue systems in task-oriented settings \\cite{he2022space, he2022unified, he2022galaxy}. In Figure \\ref{fig:appendixD}, we demonstrate three examples of the GPT outcome. In example 1, we find GPT can concatenate both hindsight and foresight and integrate them into a new fluent sentence. In the second example, we observe that GPT decides to discard the hindsight part and provides only foresight as the outcome. In example 3, GPT chooses not to respond when it thinks the current agent doesn't need help.\n\\centering     \\includegraphics[width=0.5\\textwidth]{figures/appendixD.pdf}     \\caption{Examples for language feedback generated by online GPT in evaluation.}     \\label{fig:appendixD}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates three examples of language feedback generated by an online GPT in evaluation. In the first example, the GPT concatenates both hindsight and foresight into a fluent sentence: \"Good effort, but the fruit is in the kitchen area.\" In the second example, the GPT chooses to discard the hindsight part and provides only foresight as the outcome: \"You should turn around and face the opposite way.\" In the third example, the GPT chooses not to respond when it thinks the current agent doesn't need help."
    },
    "214": {
        "figure1": "2410.24218v1_hypo_2_all",
        "label1": "fig:hypo2",
        "caption1": "Comparison of agent performance on \\textbf{unseen tasks} in four environments (averaged across 100 seeds in each environment) under varying language informativeness in agent pre-training. Agent trained with more informative language adapts to new tasks faster and better.} \\vspace{-18pt",
        "text": "The results in Figure \\ref{fig:hypo2} reveal that agents pre-trained with more informative language can adapt to unseen tasks \\emph{faster} and \\emph{better}.  ``Adapting faster'' is evident by the fact that agents pre-trained with GPT-augmented Hindsight + Foresight language in 5 or 10 shots can already achieve a similar performance 20-shot performance of agents trained with less informative language. ``Adapting better'' is evident by the fact that, at a given number of shots available for adaptation, the agent trained with the most informative language performs the best compared to its less informatively-pretrained counterparts. These results indicate that agents pre-trained with more informative language can adapt and generalize to new tasks faster and better.",
        "summarize_figure": "figure1",
        "summarization": "Here is a summary of the first subplot in the figure.The picture shows the performance of different language informativeness agent pre-training methods on unseen tasks in the HomeGrid environment. The horizontal axis represents the number of shots (5 shot, 10 shot, 20 shot), and the vertical axis represents the reward value. The four pre-training methods are: No Language Pretrained, GPT-augmented Hindsight Pretrained, GPT-augmented Foresight Pretrained, and GPT-augmented Hindsight plus Foresight Pretrained. As the number of shots increases, the reward values under all pre-training methods increase. GPT-augmented Hindsight plus Foresight Pretrained performs the best under all shot numbers, while No Language Pretrained performs the worst."
    },
    "215": {
        "figure1": "2410.24218v1_efficiency_gain",
        "label1": "fig:ablation1",
        "caption1": "Efficiency gain vs. task difficulty. We fit the scatter plots with a second-degree polynomial to visualize the overall trend. As task difficulty increases, the general trend of the efficiency gain is to rise initially and then decline, suggesting: (1) for tasks that are too easy or too hard, language feedback does not improve efficiency; (2) language feedback is most helpful in increasing efficiency for moderate tasks.}  \\vspace{-18pt",
        "text": "We define \\textbf{task difficulty} for each configuration by calculating the average success rates of agents trained without language feedback, ranking these from lowest to highest. Configurations with lower success rates are considered more difficult, indicating greater challenges for agents learning from these configurations without language assistance. As shown in Figure \\ref{fig:ablation1}, the efficiency gain generally rises with increasing learning difficulty, then declines. This suggests that: (1) for tasks that are too easy or too hard, language feedback does not improve efficiency; (2) language feedback is most helpful in increasing efficiency for moderate tasks.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the relationship between efficiency gain and task difficulty in the HomeGrid environment. The gray dots represent efficiency gains at different task difficulties, and the red curve is the trend line after fitting these data points with a second-degree polynomial. It can be seen that as task difficulty increases, the overall trend of efficiency gain is to increase initially and then decline. When the task difficulty is low, the efficiency gain is negative. As the task difficulty increases, the efficiency gain gradually becomes positive and reaches its peak at medium difficulty. After that, as the task difficulty continues to increase, the efficiency gain gradually decreases again. This indicates that for tasks that are too easy or too difficult, language feedback does not significantly improve efficiency, while for tasks of medium difficulty, language feedback is most helpful in improving efficiency."
    },
    "216": {
        "figure1": "2410.24218v1_lang_prob",
        "label1": "fig:ablation2",
        "caption1": "Performance vs. language frequency. Agents perform better with more frequent language feedback across four environments.} \\vspace{-14pt",
        "text": "In the main experiments, we utilize an online GPT model to determine whether to provide language feedback at each time step. However, it is important to explore how varying the frequency of language feedback influences agent performance. To investigate this, we control the feedback frequency by sampling according to pre-defined probabilities (e.g., 20\\%, 40\\%). The language feedback is extracted from the GPT-augmented language pool; if no language is sampled, an empty string is provided to the agent. The evaluation is conducted on agents trained with both hindsight and foresight feedback derived from the GPT-augmented language pool. As illustrated in Figure \\ref{fig:ablation2}, agents' performance improves steadily across all environments with more frequent language feedback during evaluation. This finding suggests that agents trained with informative and diverse language feedback can continually absorb and leverage new information when additional feedback is provided, leading to enhanced performance.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the reward performance of agents in four environments (Messenger, Metaworld, HomeGrid, ALFWorld) at different language feedback frequencies. The horizontal axis represents language feedback frequency, ranging from 0% to 100%, and the vertical axis represents reward value. It can be seen that as the language feedback frequency increases, the reward of agents in all environments generally shows an upward trend, indicating that more frequent language feedback usually improves the performance of the agent. The reward value in the Messenger environment is relatively high, and the reward value in the HomeGrid environment is relatively low."
    },
    "217": {
        "figure1": "2411.00054v1_figure_1",
        "label1": "fig-1",
        "caption1": "eDOC architecture and illustration of evidential learning to find OOD cells and detect marker genes in OOD cells. If Gene 1 causes Uncertainty drop significantly, so Gene 1 is one of marker genes for IND cell types. Adding Gene 3 causes the increase of uncertainty so Gene 3 is one of marker genes for OOD cells. Whether a cell is OOD or IND cell depending the last position in the transformer when all genes are visible to the model.",
        "text": "In this studies, we treat the unknown cell type detection as an out-of-domain (OOD) sample detection task similar to OOD detection for languages \\cite{dai2007co} or images \\cite{li2021semantic}. Although various types of OOD detection methods exist, quantifying the uncertainty of cell type annotation for a new cell and finding which features make a sample different from known cell types as an OOD case remain challenging. For In-Domain samples,  interpretability techniques, like SHAP, can assess feature importance of a case given a trained model. However, these methods are not designed for unseen OOD samples that are significantly different from training dataset. For example, in a training set with $K$ classification labels for all sentences, and we can use current interpretability methods to show which words contribute to $K$ label classifications. However, when we meet the sentence belongs to an unknown label (OOD case) in the test set, we cannot tell which words contribute it be a OOD sample. For the unknown cell type annotation, task are summarized as: recognize if a cell is a unknown cell type, and identify which genes characterize a cell to be a new cell type. The left part of Fig. \\ref{fig-1} illustrates the task for scRNA-seq data analysis.\n\\centering   \\includegraphics[width=\\textwidth]{figure_1}   \\caption{eDOC architecture and illustration of evidential learning to find OOD cells and detect marker genes in OOD cells. If Gene 1 causes Uncertainty drop significantly, so Gene 1 is one of marker genes for IND cell types. Adding Gene 3 causes the increase of uncertainty so Gene 3 is one of marker genes for OOD cells. Whether a cell is OOD or IND cell depending the last position in the transformer when all genes are visible to the model.}   \\label{fig-1}\nFor explaining a cell with $\\hat{N}$ expressed genes, we use decoder to output $\\hat{N}+1$ uncertainty score $\\{u\\}_{n=1}^{\\hat{N}+1}$ for each tokens. Explaining $n$th gene, except the first one, can be simple done by computing $u_n-u_{n-1}$. The overall architecture is summarized in Fig. \\ref{fig-1}.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the eDOC architecture and the process of identifying out-of-domain (OOD) cells and detecting marker genes in OOD cells through evidential learning. In the gene expression matrix, different cell types (such as NK cells, B cells, Treg cells, and unknown cells) exhibit different gene expression patterns. Genes like CD34 may be characteristic genes for OOD cells, while genes like CD4 may be characteristic genes for in-domain (IND) cell types. Using a transformer model, the impact of each gene on uncertainty can be calculated. If Gene 1 causes a significant drop in uncertainty, then Gene 1 is a marker gene for IND cell types; if Gene 3 causes an increase in uncertainty, then Gene 3 is a marker gene for OOD cells. The model determines whether a cell is an OOD cell or an IND cell based on the impact of all genes on uncertainty. Finally, through model learning, cells can be distinguished into IND cells (circles) and OOD cells (triangles), and differentiated by the degree of uncertainty in high-dimensional space, with regions of high uncertainty (deep blue) tending towards OOD cells."
    },
    "218": {
        "figure1": "2411.00054v1_figure_2",
        "label1": "fig-2",
        "caption1": "Two examples of uncertainty score changes. In each plot, we randomly select 5 OOD cells and 5 IND cells to draw uncertainty score $u_n$ changes with $n$ genes. Each line represent a cell. ",
        "text": "Our method provides an intuitive insight into how gene expression contributes to cell type annotations. By removing masks of attention, the decoder can use more genes to decide cell types. For IND cells, more genes make the model more ``confident\" in its cell types. Conversely, some genes make the model feel ``confused\" for OOD cells. When we use $u_n$ to describe this difference, we can observe $u_n-u_{n-1} < 0$ or $u_n-u_{n-1} > 0$ if the $n$th gene makes model more ``confident\"  or more ``confused\", respectively. We use the last position $\\hat{N}+1$ as the cell representation because, in this position, the model can view all gene expressions, making it the most informative cell type annotator. We use eDOC to produce $u_n$ for each token position to compare OOD cells and IND cells in Fig. \\ref{fig-2}.\n\\centering   \\includegraphics[width=\\textwidth]{figure_2}   \\caption{Two examples of uncertainty score changes. In each plot, we randomly select 5 OOD cells and 5 IND cells to draw uncertainty score $u_n$ changes with $n$ genes. Each line represent a cell. }   \\label{fig-2}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates two examples of uncertainty score changes with the number of genes (Token Position). Each plot randomly selects 5 in-distribution (IND) cells and 5 out-of-distribution (OOD) cells, plotting their uncertainty scores (u_n) as the number of genes n varies, where each line represents a cell. It is observed that for in-distribution (IND) cells, the uncertainty score is generally lower and changes smoothly, while for out-of-distribution (OOD) cells, the uncertainty score is higher and fluctuates more significantly."
    },
    "219": {
        "figure1": "2411.00054v1_figure_4",
        "label1": "fig-4",
        "caption1": "Examples of identified marker genes by eDOC (top) and attention weights (bottom) for three different IND cells. Each heatmap panel represents a cell with five marker genes for the cell. Heatmaps in the top row show eDOC's $u_n^{IND}$ with 40 runs, and the number in each heatmap panel represents the $n$th position. Heatmaps in the bottom row show attention weights for all heads (8) of the Transformer model. ",
        "text": "eDOC allows us to investigate how different genes contribute to IND cell type annotation and OOD cell detection robustly by simply changing the input order of genes. Because cell types are decided by multiple genes, and some genes may hide other genes' effects. eDOC can improve the gene interpretability for cell type annotation by placing genes in early $n$ positions, and genes located in late positions (like $n+1$ and $n+2$) make no contributions to cell type annotations in the $n$th position because the attention mask blocks the effect of genes in the late position. This allows us to reveal how genes affect cell types without being affected by non-marker genes. Many deep learning-based methods use attention weights for models' interpretability.  Here, we  compare the explanation from eDOC with that by attention weights. For IND cell type annotation, we select three cell types, and each cell type has five expressed important marker genes for this cell type, which are supported by literature\\footnote{References in the Appendix.}. Fig. \\ref{fig-4} shows that difference between eDOC and attention weights.\n\\centering   \\includegraphics[width=\\textwidth]{figure_4}   \\caption{Examples of identified marker genes by eDOC (top) and attention weights (bottom) for three different IND cells. Each heatmap panel represents a cell with five marker genes for the cell. Heatmaps in the top row show eDOC's $u_n^{IND}$ with 40 runs, and the number in each heatmap panel represents the $n$th position. Heatmaps in the bottom row show attention weights for all heads (8) of the Transformer model. }   \\label{fig-4}",
        "summarize_figure": "figure1",
        "summarization": "The image presents a comparison between the eDOC method and attention weights for identifying known marker genes in three different IND cell types (CD19+ B, CD56+ NK, and Dendritic cells). The top three heatmaps in the image show the position (n) at which eDOC identified five marker genes for each cell type across 40 runs. The numerical values indicate the gene's position in the input sequence, and the color intensity reflects this position, with darker colors representing earlier positions (lower n values), signifying the gene's contribution to cell type annotation without interference from subsequent genes. The identified important position n for each gene varies across different runs, indicating eDOC's result is influenced by the gene input order. The bottom three heatmaps show the attention weights from all 8 attention heads of a Transformer model for the same set of marker genes, with color intensity reflecting the magnitude of the attention weight, brighter colors indicating higher weights. Unlike eDOC, which shows a gene's contribution at a specific position, attention weights provide a measure of a gene's relative importance within the given context (all input genes), and their distribution varies across different attention heads. The comparison shows that eDOC offers a more direct measurement approach by controlling gene position to reveal its individual contribution to cell type annotation, while attention weights provide a holistic, context-dependent explanation."
    },
    "220": {
        "figure1": "2411.00054v1_figure_5",
        "label1": "fig-5",
        "caption1": "Examples of identified marker genes with eDOC for three different OOD cells. Each heatmap panel represents a cell with five marker genes for the cell. The heapmap shows eDOC's $u_n^{OOD}$ with 20 runs, and the number in each heatmap's cell represents the $n$th position.",
        "text": "\\centering   \\includegraphics[width=\\textwidth]{figure_5}   \\caption{Examples of identified marker genes with eDOC for three different OOD cells. Each heatmap panel represents a cell with five marker genes for the cell. The heapmap shows eDOC's $u_n^{OOD}$ with 20 runs, and the number in each heatmap's cell represents the $n$th position.}   \\label{fig-5}\nFrom heatmaps, attention weights only focus on a portion of gene sets but miss a number of important genes. It may be caused by some genes with high attention weights hiding other genes' contributions. Instead eDOC performs a robust explanation for marker genes by running multiple times with different orders for gene inputs. For the explanation of the NK cell, both attention weights and eDOC show the importance of CD8A and FGFBP, but CST7, NKG7, and CCL5 are overlooked by attention weights. The heatmap that is produced by eDOC suggests when CD8A and FGFBP are in front of other genes, they can mask the importance of other genes. eDOC eventually highlights the importance of CST7, NKG7, and CCL5 by putting them in front of CD8A and FGFBP, but attention weight misses these genes. Thus, eDOC can use $u_n^{OOD}$ to identify marker genes for OOD cells, which would be missed by attention weights. In addition, eDOC can reliably identify marker genes for IND cell types.  Fig \\ref{fig-5} shows that five marker genes identified for three OOD cell types by eDOC's $u_n^{OOD}$.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the marker genes identified by the eDOC method for CD4+CD45RA+CD25-Naive T cells. The heatmap displays eDOC's u_n to the power of OOD values over 20 runs, with each cell's number indicating the gene's position (nth position) in each run. The color intensity in the heatmap represents the gene's weight at the corresponding position, with darker colors indicating higher weights. It is observed that the CD4 gene consistently exhibits high weights across multiple runs, particularly in runs 17 and 20, while HLA-DMA also shows relatively high weight in run 17. The remaining genes, such as PLAC8, RHOA, and NDUFB10, have a more dispersed weight distribution across different runs and overall lower weights."
    },
    "221": {
        "figure1": "2411.00064v1_33823caa-805d-4f72-be47-6c4d1fa58d4a",
        "label1": "fig:CER_of_each_team",
        "caption1": "Average CER for each team across different text sets. The redder the color, the higher the CER.",
        "text": "\\centering   \\includegraphics[width=\\linewidth]{./33823caa-805d-4f72-be47-6c4d1fa58d4a.pdf}   \\caption{Average CER for each team across different text sets. The redder the color, the higher the CER.}   \\label{fig:CER_of_each_team}\nWe calculated the average CER of each team on different types of test texts. The results are shown in Figure~\\ref{fig:CER_of_each_team}. All participating teams performed well on the tone sandhi test set, while all teams showed an increased CER on the rhotic accent test set. This indicates that current systems learned well for the tone sandhi patterns with a large amount of paired training data, but their performance is relatively limited on the less frequent rhotic accent data.",
        "summarize_figure": "figure1",
        "summarization": "This is a heatmap of the average Character Error Rate (CER) for different teams across different text sets. The redder the color, the higher the CER. In the conversational text set, the average CER is 6.50 for short texts, 6.37 for middle texts, and 8.34 for long texts. The average CER for the colloquial text set is 8.98, the stammer text set is 11.09, the rhotic accent text set is 13.47, the tone sandhi text set is 4.27, the polyphonic character text set is 7.33, and the longform audiobook text set is 24.78. The C5 team shows the highest CER across all text sets, especially on the longform audiobook text set, with a CER of 95.37."
    },
    "222": {
        "figure1": "2411.00064v1_team_scores",
        "label1": "fig:CERvstxt_len",
        "caption1": "The CER results between different text lengths.",
        "text": "\\centering   \\includegraphics[width=1.1\\linewidth]{team_scores.pdf}   \\caption{The CER results between different text lengths.}   \\label{fig:CERvstxt_len}\nWe plotted the curve of the CER with text length across some typical AR and NAR systems as shown in Figure~\\ref{fig:CERvstxt_len}. It can be seen that three NAR teams (U1-MASTER-TTS, U3-Orion, U7-hySoundClone) in the unconstrained track obtained low CER and were less affected by text length.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the character error rate (CER) results for several typical autoregressive (AR) and non-autoregressive (NAR) systems across different text lengths. The horizontal axis represents text length, categorized as short, medium, and long, while the vertical axis indicates the CER value. As observed, the NAR systems U1, U3, and U7 exhibit lower CER values and are less affected by text length, maintaining a consistently low CER across different lengths. In contrast, the other AR systems show an increasing trend in CER with longer text lengths, indicating a performance decline when processing longer texts. Notably, the C1 system experiences a significant increase in CER with long texts."
    },
    "223": {
        "figure1": "2411.00064v1_team_sim",
        "label1": "fig:team_sim_tend",
        "caption1": "The similarity results between different duration lengths of the audio prompt.",
        "text": "In terms of the objective metric of speaker similarity, we found that as the duration of the target speaker's audio increases, the speaker similarity (SIM) tends to rise, as shown in Figure~\\ref{fig:team_sim_tend}. This trend is particularly noticeable when the target speaker's audio duration increases from the range of 0 to 5 seconds to the range of 5 to 10 seconds.\n\\centering   \\includegraphics[width=1.1\\linewidth]{team_sim.pdf}   \\caption{The similarity results between different duration lengths of the audio prompt.}   \\label{fig:team_sim_tend}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the relationship between different audio prompt durations and speaker similarity (SIM). As the audio prompt duration increases, the speaker similarity generally shows an upward trend. Specifically, the increasing trend is particularly noticeable when the audio prompt duration increases from 0-5 seconds to 5-10 seconds. Different curves represent different experimental objects (U2, U5, U4, U6, C1, C4, C5, U3, U0, C2, U1, U7, C3), and their similarity increases with the increase of audio duration."
    },
    "224": {
        "figure1": "2411.00064v1_team_scores_bar_chart_sorted",
        "label1": "fig:sss_in_2",
        "caption1": "MOS score of SSS between two types of audio prompt.",
        "text": "\\centering   \\includegraphics[width=1\\linewidth]{team_scores_bar_chart_sorted.pdf}   \\caption{MOS score of SSS between two types of audio prompt.}   \\label{fig:sss_in_2}\nComparing the speech spontaneous style (SSS) scores between two types of target speakers, i.e., the speakers with ordinary audio prompts and dialog prompts, we found that the scores for conversational spontanous style target speakers were consistently higher than those for ordinary reading style target speakers, as shown in Figure \\ref{fig:sss_in_2}.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the Mean Opinion Score (MOS) of Speech Spontaneous Style (SSS) under two types of audio prompts: ordinary reading prompts (blue bars) and conversational spontaneous prompts (red bars). As shown in the figure, for most speakers (U0, U4, U2, U3, C1, U1, U6, U5, C2, C3, U7, C4), the SSS scores for conversational spontaneous prompts are generally higher than those for ordinary reading prompts. In particular, the difference in scores between the two prompt types is significant for speaker C5, with the score for conversational spontaneous prompts being much higher than that for ordinary reading prompts."
    },
    "225": {
        "figure1": "2411.00074v1_evolution_acc_per_batch_all_params_Books",
        "label1": "fig:evolution_acc_per_batch_all_params_Books",
        "caption1": "\\small Evolution of the accuracy per batch with different parameters on \\Books. Learning timestamps are in red. \\reservoirSize: reservoir size, $N$: batch size, $ld$: learning duration, $pd$: predict duration}}  ",
        "text": "We first present the behavior of \\myalgo-based online classifiers with different parameter changes, such as the sample size, the reservoir size, the learning duration, and the predict duration, by considering the \\Books database. Figure \\ref{fig:evolution_acc_per_batch_all_params_Books} shows that many \\myalgo-based models improve their performance after each learning interval. Additionally, the maintained reservoir patterns are representative enough to provide good accuracies for long-term predictions, thereby avoiding accuracy drift.\n\\centering     {\\small \t\\includegraphics[scale=.355]{fig/evolution_acc_per_batch_all_params_Books.eps}     \\caption{{\\small Evolution of the accuracy per batch with different parameters on \\Books. Learning timestamps are in red. \\reservoirSize: reservoir size, $N$: batch size, $ld$: learning duration, $pd$: predict duration}} \t\\label{fig:evolution_acc_per_batch_all_params_Books}  }",
        "summarize_figure": "figure1",
        "summarization": "Here is the summary generated based on the chart, title, and description you provided:The figure shows the evolution of accuracy per batch with different parameters on the Books dataset for myalgo-based online classifiers. The red vertical lines indicate learning timestamps. The four subplots show the accuracy changes under different reservoir sizes (k), batch sizes (N), learning durations (ld), and predict durations (pd). From the figure, it can be seen that many myalgo-based models improve their performance after each learning interval. At the same time, the maintained reservoir patterns are representative enough to provide good accuracy for long-term predictions, thus avoiding accuracy drift."
    },
    "226": {
        "figure1": "2411.00074v1_accuracy_comparison",
        "label1": "fig:accuracy_comparison",
        "caption1": "Comparison between \\myalgo-based classifiers (with reservoir size \\reservoirSize=10,000; batch size=1,000; learning duration=2 time-units, predict duration=52 time-units) vs cheater classifiers (with 50\\% train and 50\\% test)",
        "text": "This section evaluates our approach by comparing it with cheater classifiers such as {\\it Dumb classifier (strategy=`most\\_frequent')}, {\\it LogisticRegression (max\\_iter=1,000)}, {\\it KNN (k=10)}, {\\it Centroid (Normalized Sum)}, {\\it Naive Bayes (MultinomialNB)}, and {\\it SVM (Linear Kernel)}. These classifiers are our references since they have access to all the training data (50\\%), while \\myalgo uses batches that appear during the learning intervals. This means that our goal is not to outperform them but to come closer to their performance as seen in Figure \\ref{fig:accuracy_comparison}. However, \\myalgo should be better than {\\it Dumb classifier} which is a naturally lower baseline.",
        "summarize_figure": "figure1",
        "summarization": "The image is a bar graph showing the average accuracy of different classifiers on multiple datasets. The horizontal axis represents the datasets, including Reuters8, Reuters52, cade, webkb, and Books. The vertical axis represents the average accuracy, ranging from 0.0 to 1.0. Each dataset has a set of bar graphs representing the performance of different classifiers, including Dumb classifier, LogisticRegression, KNN, Centroid, Naive Bayes, SVM, and several RPS-based classifiers (RPS@MLP, RPS@MNB, RPS@PAC, RPS@PER, RPS@SOD) with different epsilon parameters (0, 0.1, 0.5). The accuracy of Dumb classifier is significantly lower than other classifiers on all datasets, with performances of 0.49, 0.42, 0.21, 0.16, and 0.56 respectively. SVM performs best on the Reuters8 and Reuters52 datasets, with accuracies of 0.94 and 0.94 respectively. On the cade dataset, the accuracy of each classifier is generally low, with the highest SVM only reaching 0.57. On the webkb and Books datasets, the performance of various classifiers is relatively close, but still better than Dumb classifier."
    },
    "227": {
        "figure1": "2411.00109v1_time_embed_mats",
        "label1": "fig:time-embed-mats",
        "caption1": "The time-embeddings computed using (1) frequencies from~\\citet{vaswani2017attention} \\textit{(left)}, and (2) the frequencies from proposed in our work \\textit{(right)}. ",
        "text": "In order to incorporate time into the hypothesis class, we consider an embedding function $\\varphi : \\mathbb{R} \\rightarrow \\mathbb{R}^d$ that takes raw time as an input and returns a $d$-dimensional vector denoted as the time-embedding. In our experiments, we define $\\varphi:\\mathbb{R} \\rightarrow \\mathbb{R}^d$ as a function that maps     t \\mapsto  (\\sin(\\w_1 t), \\dots, \\sin(\\w_{\\scriptscriptstyle d/2} t), \\cos(\\w_1 t), \\dots,  \\cos(\\w_{\\scriptscriptstyle d/2} t)), where, $\\omega_i = \\pi / i, \\ i = 1, \\dots, d/2$ to the be the collection of angular frequencies. We briefly discuss the rationale for this choice in~\\Cref{fig:time-embed-mats}. In our experiments, we use $d=50$.\nIn the original Transformer architecture,~\\citet{vaswani2017attention} use a position-embedding using the frequencies $\\omega_i = 1/10000^{2i/d}$ $ \\ i = 1, \\dots, d/2$. There are two key differences: (1) We use the absolute time $t$ instead of the relative position, (2) We use the angular frequencies $2\\pi / i$. In~\\Cref{fig:time-embed-mats} \\textit{(right)}, we illustrate the time-embeddings when we use the two different choices for angular frequencies. For $d=128$, we find that the frequencies from~\\citet{vaswani2017attention} result in slowly changing features which makes it less suitable for our task, i.e., many of the dimensions are constant over time which makes many of the dimensions uniformative for the task. In our experiments, we found out that MLPs and CNNs that use the frequencies from~\\citet{vaswani2017attention} perform poorly on the MNIST task for Scenario 2 and 3.\n\\centering     \\includegraphics[width=\\textwidth]{figures/time_embed_mats.pdf}     \\caption{The time-embeddings computed using (1) frequencies from~\\citet{vaswani2017attention} \\textit{(left)}, and (2) the frequencies from proposed in our work \\textit{(right)}. }     \\label{fig:time-embed-mats}",
        "summarize_figure": "figure1",
        "summarization": "The picture shows a comparison of two time-embedding matrices, with the horizontal axis representing time ranging from 0 to 350, and the vertical axis representing dimension ranging from 0 to 120. The left picture shows the time embeddings computed using the frequencies used in the original Transformer. It is observed that at lower dimensions, the embeddings change more densely with time, but as the dimension increases, the changes gradually become slower, and the color tends to be uniform in most regions, indicating that the features at higher dimensions do not change significantly with time."
    },
    "228": {
        "figure1": "2411.00109v1_proMAP",
        "label1": "fig:pro-map",
        "caption1": "Prospective risk of MLE \\textit{(blue)}, MAP \\textit{(purple)}, and prospective MAP \\textit{(red)} based learners with respect to time. Both MAP and prospective MAP estimators assume a prior distribution of $\\text{Beta}(12, 16)$ over $p$.",
        "text": "\\centering     \\includegraphics[width=0.6\\textwidth]{figures/proMAP.pdf}     \\caption{Prospective risk of MLE \\textit{(blue)}, MAP \\textit{(purple)}, and prospective MAP \\textit{(red)} based learners with respect to time. Both MAP and prospective MAP estimators assume a prior distribution of $\\text{Beta}(12, 16)$ over $p$.}     \\label{fig:pro-map}\nWe refer to this as the prospective MAP estimate. Based on it, we set the hypothesis to be $h_{t'} = \\text{threshold}(\\hat p_{t'}>0.5)$ for all future times beyond $t$. In~\\Cref{fig:pro-map}, we plot the prospective risk the MLE, MAP, and prospective MAP-based learners. Due to an unfavorable prior, the MAP-based learner converges slowly. However, prospective MAP-based learner manages to leverage its forecasting to achieve a faster convergence rate despite having the same prior as the MAP. This shows that we can indeed benefit from prospection even in the IID case.",
        "summarize_figure": "figure1",
        "summarization": "Here is the English version:The figure illustrates the prospective risk over time for Maximum Likelihood Estimation (MLE), Maximum a Posteriori (MAP), and prospective MAP (proMAP) based learners under the independent and identically distributed (IID) data scenario. Both MAP and proMAP estimators assume a Beta(12, 16) prior distribution. The figure indicates that the MAP learner converges slowly due to an unfavorable prior. However, the proMAP learner achieves a faster convergence rate by leveraging its forecasting ability, despite having the same prior as MAP."
    },
    "229": {
        "figure1": "2411.00151v1_nimba_ablation",
        "label1": "fig:robustness",
        "caption1": "Results of applying noise to the training set, test set, or both. \\alg demonstrates greater robustness compared to PointMamba, particularly when the noise does not alter the spatial distances between points, such as in the case of rotation.}  \\vspace{-2mm",
        "text": "\\centering     \\includegraphics[width=0.9\\textwidth]{figures/nimba_ablation.pdf}     \\caption{Results of applying noise to the training set, test set, or both. \\alg demonstrates greater robustness compared to PointMamba, particularly when the noise does not alter the spatial distances between points, such as in the case of rotation.}     \\label{fig:robustness}     \\vspace{-2mm} To further explore the differences between PointMamba and \\alg, we tested both models by applying the following noise injections to the input point clouds:\nEach type of noise was applied to the training set, the test set, or both. As shown in Figure \\ref{fig:robustness}, our method \\alg generally exhibits greater robustness to noise, particularly in the case of rotation, where we even observe an improvement in performance. This confirms that the reordering strategy employed by \\alg is resilient to noise that preserves pairwise distances between points, such as after a rotation.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the accuracy comparison between PointMamba and NIMBA on the ScanObjectNN dataset when different types of noise (Rotation, RHF, Jitter, RID, All) are applied to the training set, test set, or both. NIMBA shows a slightly higher accuracy than PointMamba under baseline conditions. When Rotation noise is applied, NIMBA exhibits higher accuracy on both the training and test sets compared to PointMamba. Under RHF noise, NIMBA generally outperforms PointMamba on both sets as well. With Jitter noise, NIMBA maintains high accuracy, while PointMamba's accuracy significantly decreases. Under RID noise, NIMBA's accuracy on both sets is notably higher than PointMamba's. When all noises are applied, NIMBA's accuracy is significantly higher than PointMamba's. Overall, NIMBA demonstrates stronger robustness to various noise conditions compared to PointMamba."
    },
    "230": {
        "figure1": "2411.00154v1_haritz-mia-teaser-v3",
        "label1": "fig:intro",
        "caption1": "\\textbf{Focusing on the Right Scale.} MIA has traditionally been considered ineffective for LLMs. However, we argue that MIA remains effective for LLMs when applied at a much larger scale, considering significantly longer token sequences. This \\textit{large-scale MIA} is also practically and legally relevant, as copyright is often determined at the document level.",
        "text": "\\centering     \\includegraphics[width=\\linewidth]{figures/haritz-mia-teaser-v3.pdf}     \\vspace{-1em}     \\caption{\\textbf{Focusing on the Right Scale.} MIA has traditionally been considered ineffective for LLMs. However, we argue that MIA remains effective for LLMs when applied at a much larger scale, considering significantly longer token sequences. This \\textit{large-scale MIA} is also practically and legally relevant, as copyright is often determined at the document level.}     \\label{fig:intro} The application of MIAs on LLMs has faced discouraging results so far.   Initial claims of success \\citep{shi2024detecting, meeus2024did, carlini2021extracting, mattern_membership_2023} were later disproven by \\citet{duan2024membership,das_blind_2024,maini_llm_2024, meeus_inherent_2024}. Before their work, it was common practice to select true non-members from documents created after the LLM’s \\textit{cut-off date}. They showed that this approach allowed MIA methods to exploit temporal cues, rather than identifying membership through the model's inherent response characteristics. \\citet{duan2024membership,das_blind_2024,maini_llm_2024} introduced a new evaluation method based on an independent, identically distributed (IID) split between true members and non-members. Since adopting this method, no further MIA studies on LLMs have shown performance significantly better than random. Reported membership detection has remained below 60\\% AUROC, close to the random-chance level of 50\\% AUROC \\citep{duan2024membership,das_blind_2024,maini_llm_2024, xie2024recall, zhang_min-k_2024}.\nIn this work, we demonstrate that MIA approaches begin to show meaningful performance only when applied to much longer token sequences, such as 10K tokens. To show this, we introduce four scales of token sequences: sentences, paragraphs, documents, and datasets, as shown in \\Cref{fig:intro}. We propose MIA evaluation protocols and benchmarks for the binary detection of training data samples given at four different scales. As a baseline, we adapt the Dataset Inference \\citep{maini_llm_2024} method for the MIA task at multiple scales. As a result, our aggregation-based MIA demonstrates significant performance improvements for document sets, achieving AUROC scores of 80\\% or higher.",
        "summarize_figure": "figure1",
        "summarization": "Here is a summary of the figure:The figure illustrates the performance of Membership Inference Attacks (MIA) on Large Language Models (LLMs), with the Area Under the Receiver Operating Characteristic curve (AUROC) as the evaluation metric. The horizontal axis represents the length of token sequences, ranging from sentences (approximately 10 to the power of 2 tokens) to datasets (approximately 10 to the power of 6 tokens). In the figure, when the length of token sequences is short, the performance of MIA is close to the random chance level (50% AUROC), indicating that MIA is ineffective for LLMs at this scale. However, when the length of token sequences reaches the document level (approximately 10 to the power of 4 tokens) and above, the performance of MIA significantly improves, with AUROC values exceeding 80%, indicating that large-scale MIA is effective on LLMs. The figure also highlights the previously focused area (sentence and paragraph level) and the focus area of this study (document and dataset level) with rectangular boxes, emphasizing the effectiveness and legal relevance of applying MIA on a large scale."
    },
    "231": {
        "figure1": "2411.00154v1_compounding-v2",
        "label1": "fig:compounding",
        "caption1": "\\textbf{Impact of paragraph-MIA performance} after aggregation to dataset- and document-level MIA.",
        "text": "Our experiments reveal a powerful compounding effect in the aggregation of MIA scores from smaller to larger textual units. This relationship follows an approximately square root function, where small improvements in paragraph-level MIA performance lead to substantial gains at the collection or document level. \\Cref{fig:compounding} illustrates this relationship across all our experiments, encompassing various sources and hyperparameters.\n\\centering     \\includegraphics[width=\\linewidth]{figures/compounding-v2.pdf}     \\caption{\\textbf{Impact of paragraph-MIA performance} after aggregation to dataset- and document-level MIA.}     \\label{fig:compounding}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the relationship between paragraph-level MIA AUROC scores and dataset-level MIA AUROC scores. The x-axis represents paragraph MIA AUROC, ranging from approximately 0.50 to 0.56, and the y-axis represents dataset MIA AUROC, ranging from approximately 0.4 to 1.0. The data points are dense and show a positive correlation, meaning higher paragraph MIA AUROC scores generally correspond to higher dataset MIA AUROC scores. Especially when the paragraph MIA AUROC score is high, the dispersion of the dataset MIA AUROC score is also large. The overall trend indicates that small performance improvements at the paragraph level can significantly enhance MIA performance at the dataset level."
    },
    "232": {
        "figure1": "2411.00154v1_continual_indomain_v3",
        "label1": "fig:continual_indomain",
        "caption1": "\\textbf{Continual learning results} for Pythia 2.8B on Wikipedia. Collection-level MIA shows near-perfect AUROC with sufficient documents, while document-level MIA shows a significant improvement.",
        "text": "In this section, we investigate the performance of MIA on LLMs that have undergone a continual learning process to adapt to specific domains. \\Cref{fig:continual_indomain} and \\Cref{table:ID} show the MIA performance on Pythia 2.8B after it was further trained on the validation sets of Wikipedia and GitHub (independently) from the Pile dataset.\n\\centering     \\includegraphics[width=.8\\linewidth]{figures/continual_indomain_v3.pdf}     \\vspace{-.5em}     \\caption{\\textbf{Continual learning results} for Pythia 2.8B on Wikipedia. Collection-level MIA shows near-perfect AUROC with sufficient documents, while document-level MIA shows a significant improvement.}     \\label{fig:continual_indomain}",
        "summarize_figure": "figure1",
        "summarization": "This is an analysis of the continual learning results for Pythia 2.8B on Wikipedia. The upper part of the figure shows the collection-level membership inference attack (MIA) results, where the x-axis is the number of documents per dataset, and the y-axis is the AUROC (Area Under the Receiver Operating Characteristic curve). The solid line represents the MIA for the 2.8B dataset, and the dashed line represents the MIA for the 2.8B paragraph. It can be seen that as the number of documents increases, the AUROC of the MIA for the 2.8B dataset approaches 1.0, indicating that collection-level MIA performs near-perfect when there are sufficient documents."
    },
    "233": {
        "figure1": "2411.00169v1_confusion_matrix_normalized",
        "label1": "cm detection",
        "caption1": "Confusion Matrix obtained from YOLOv8 implementation",
        "text": "Firstly, since we are observing the performance of detection, it is more applicable to detect specific objects within a frame, like humans and domiciles, as flood zones are hard to include within the bounding boxes. So, we take the images of the `flood with humans' and `flood with domicile' categories from our dataset and divide them into training, validation, and testing. The model performs detection on the testing images through the use of bounding boxes to identify our desired labels, which are the two categories we are aiming to detect. Hence, during training, the bounding boxes that we label are `Domicile' and `Human'.  Figure \\ref{fig: all detections} below shows the prediction results obtained from three testing images. For each prediction, a confidence score is also obtained and displayed. Based on the confidence scores of each prediction of both categories, we can see that it can detect domiciles within these images better than humans, as we can see in Figure \\ref{fig: all detections} that not all humans are detected on the boat. The confidence score also fluctuates largely between 1.00 to 0.50, which indicates that the model requires further training data and much higher parameters to carry out a decent level of prediction. Figure \\ref{cm detection} shows the confusion matrix obtained from our implementation of YOLOv8, which shows the normalized representation of how many true predictions the model makes for humans and domiciles, and also for the background region where the model doesn't detect any of the objects. As we can see in the confusion matrix, about 83\\% of the instances where domiciles are present are correctly detected, which represents the highest number of detections for any of the categories.",
        "summarize_figure": "figure1",
        "summarization": "The image shows a confusion matrix obtained from YOLOv8 implementation to evaluate the model's detection performance on humans, domiciles, and background regions. The values in the matrix represent the normalized prediction accuracy. The results indicate that the model performs best in detecting domiciles, with approximately 83% of domicile instances being correctly detected. The accuracy for human detection is 43%, and the background region is 1%. This suggests that the model performs well in domicile detection, but there is still room for improvement in human detection and background differentiation."
    },
    "234": {
        "figure1": "2411.00177v2_model_performance_comparison",
        "label1": "fig:model_comparison",
        "caption1": "The performance comparison across models for each material representation is presented. The left y-axis shows the log-normalized performance of each LLM-based model relative to the baseline (CGCNN), while the right y-axis (bar plots) displays the average subword tokens per sample for each dataset. Datasets on the x-axis are ordered by increasing average subword tokens. Results for Llama 2-7b-chat:0S and Llama 2-7b-chat:5S are missing in plots (a) and (b), respectively, due to invalid outputs. Higher values in the line plots indicate better performance.",
        "figure2": "2411.00177v2_inputs_performance_comparison",
        "label2": "fig:input_comparison",
        "caption2": "The performance comparison across material representations for each LLM-based model is shown. The y-axis represents the log-normalized Weighted Average (MAD/MAE) score for each representation, while the x-axis displays randomly ordered datasets. In the (a) and (b) plots, some Composition and Structure performance results are missing due to invalid outputs. A higher y-axis value indicates better performance.",
        "text": "\\centering     \\includegraphics[width=\\linewidth]{figures/model_performance_comparison.pdf}     \\caption{The performance comparison across models for each material representation is presented. The left y-axis shows the log-normalized performance of each LLM-based model relative to the baseline (CGCNN), while the right y-axis (bar plots) displays the average subword tokens per sample for each dataset. Datasets on the x-axis are ordered by increasing average subword tokens. Results for Llama 2-7b-chat:0S and Llama 2-7b-chat:5S are missing in plots (a) and (b), respectively, due to invalid outputs. Higher values in the line plots indicate better performance.}     \\label{fig:model_comparison}\nTable \\ref{tab:results_regression} and \\ref{tab:results_classification}, and Figure \\ref{fig:model_comparison} and \\ref{fig:input_comparison} show the main results. The detailed results on each dataset can be found in Appendix \\ref{app:results_per_each_dataset}. The main observations are as follows:",
        "summarize_figure": "figure1",
        "summarization": "The image presents a performance comparison of different models across various material representations. The x-axis represents different datasets, ordered by increasing average subword tokens. The left y-axis shows the log-normalized performance of each LLM-based model relative to the baseline (CGCNN), with higher values indicating better performance. The right y-axis (bar plots) displays the average number of subword tokens per sample for each dataset. For the Composition input type, data points are missing for Llama 2-7b-chat:0S on some datasets due to invalid outputs. The figure compares the performance of MatBERT, LLM-Prop, and Llama 2-7b-chat across different datasets."
    },
    "235": {
        "figure1": "2411.00177v2_inputs_performance_comparison",
        "label1": "fig:input_comparison",
        "caption1": "The performance comparison across material representations for each LLM-based model is shown. The y-axis represents the log-normalized Weighted Average (MAD/MAE) score for each representation, while the x-axis displays randomly ordered datasets. In the (a) and (b) plots, some Composition and Structure performance results are missing due to invalid outputs. A higher y-axis value indicates better performance.",
        "figure2": "2411.00177v2_model_performance_comparison",
        "label2": "fig:model_comparison",
        "caption2": "The performance comparison across models for each material representation is presented. The left y-axis shows the log-normalized performance of each LLM-based model relative to the baseline (CGCNN), while the right y-axis (bar plots) displays the average subword tokens per sample for each dataset. Datasets on the x-axis are ordered by increasing average subword tokens. Results for Llama 2-7b-chat:0S and Llama 2-7b-chat:5S are missing in plots (a) and (b), respectively, due to invalid outputs. Higher values in the line plots indicate better performance.",
        "text": "\\centering     \\includegraphics[width=\\linewidth]{figures/inputs_performance_comparison.pdf}     \\caption{The performance comparison across material representations for each LLM-based model is shown. The y-axis represents the log-normalized Weighted Average (MAD/MAE) score for each representation, while the x-axis displays randomly ordered datasets. In the (a) and (b) plots, some Composition and Structure performance results are missing due to invalid outputs. A higher y-axis value indicates better performance.}     \\label{fig:input_comparison}\nTable \\ref{tab:results_regression} and \\ref{tab:results_classification}, and Figure \\ref{fig:model_comparison} and \\ref{fig:input_comparison} show the main results. The detailed results on each dataset can be found in Appendix \\ref{app:results_per_each_dataset}. The main observations are as follows:",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the performance comparison of the Llama 2-7b-chat:0S model across different material datasets using three material representations (Composition, CIF, Description). The y-axis represents the log-normalized Weighted Average MAD/MAE score, with higher scores indicating better performance. The x-axis displays randomly ordered datasets. It can be observed that the performance of the three material representations varies across different datasets for the Llama 2-7b-chat:0S model, but on some datasets, such as hMOF, Description performs significantly better than Composition and CIF. The missing results for Composition and CIF on some datasets indicate invalid outputs."
    },
    "236": {
        "figure1": "2411.00177v2_chat_models_comparison",
        "label1": "fig:chat-llms-comparison",
        "caption1": "The performance comparison of different chat-based LLM versions is presented with results based on 5-shot prompts, averaged over three inference runs. Panels (a)–(c) and (d)–(f) show each model's accuracy in predicting band gaps and stability in the MP dataset, respectively, while panels (g)–(i) and (j)–(l) depict the percentage of valid predictions for band gap and stability on the test set.",
        "text": "\\centering     \\includegraphics[width=\\linewidth]{figures/chat_models_comparison.pdf}     \\caption{The performance comparison of different chat-based LLM versions is presented with results based on 5-shot prompts, averaged over three inference runs. Panels (a)–(c) and (d)–(f) show each model's accuracy in predicting band gaps and stability in the MP dataset, respectively, while panels (g)–(i) and (j)–(l) depict the percentage of valid predictions for band gap and stability on the test set.}     \\label{fig:chat-llms-comparison}",
        "summarize_figure": "figure1",
        "summarization": "The image presents a performance comparison of different chat-based large language models on materials science tasks. For band gap prediction, panels a-c show that the Gemma family achieves the lowest MAD:MAE ratios with CIF input, indicating highest accuracy, while the Mistral family also shows lower MAD:MAE ratios and higher accuracy with Description input. For stability prediction, panels d-f indicate that all model families achieve similarly high ROC AUC scores across different input types. Regarding prediction validity, panels g-i demonstrate that the Mistral family exhibits significantly higher percentages of valid band gap predictions, particularly with Description input. Panels j-l show that the Llama and Mistral families generally have high validity for stability predictions, whereas the Gemma family shows lower validity."
    },
    "237": {
        "figure1": "2411.00186v1_0806_experiment_results_plot",
        "label1": "fig:corr_app",
        "caption1": "Effects of self-healing for five datasets as we vary the number of corrupted columns and the corruption value. Self-healing consistently identifies corrupted columns at test time. This typically becomes more important as the corruption level increases (either by corruption value or number of corrupted columns). Baseline is not implementing a self-healing mechanism upon drift detection.",
        "text": "We systematically analyze how self-healing effectiveness varies with corruption levels across our five datasets (Airlines, Poker, Weather, Electricity, and Forest Type). For each dataset, we vary both the corruption value $\\tau$ and the number of corrupted columns $k$, measuring accuracy with and without the self-healing mechanism. Figure \\ref{fig:corr_app} shows that self-healing's impact grows with corruption severity. Specifically, as either $\\tau$ or $k$ increases, the gap between baseline and self-healed performance widens. This pattern holds consistently across all datasets, though with varying magnitudes. These results demonstrate that self-healing becomes more crucial as data degradation becomes more severe, providing a safety mechanism for maintaining model performance under challenging conditions.\n\\centering     \\includegraphics[width=\\linewidth]{figures/0806_experiment_results_plot.pdf}     \\vspace{-7mm}     \\caption{Effects of self-healing for five datasets as we vary the number of corrupted columns and the corruption value. Self-healing consistently identifies corrupted columns at test time. This typically becomes more important as the corruption level increases (either by corruption value or number of corrupted columns). Baseline is not implementing a self-healing mechanism upon drift detection.}     \\label{fig:corr_app}",
        "summarize_figure": "figure1",
        "summarization": "The image displays the accuracy of models with and without a self-healing mechanism across five different datasets (Airlines, Covtype, Elec, Poker, and Weather) under varying levels of data corruption, measured by the number of corrupted columns and the corruption value. For all datasets shown, the accuracy achieved with self-healing is consistently greater than the accuracy without self-healing. The plots reveal that as either the number of corrupted columns increases or the corruption value increases, the performance gap between self-healing and no self-healing widens. This indicates that the self-healing approach is more effective at mitigating the impact of data corruption, and its benefit becomes more pronounced as the severity of the corruption increases."
    },
    "238": {
        "figure1": "2411.00201v1_Precision_Recall_Time_Signs",
        "label1": "fig: prec signs",
        "caption1": "Precision vs. Recall based on size results on traffic signs dataset. The size of each circle represents the size of the model, with larger circles indicating larger models.",
        "text": "\\centering     \\includegraphics[width=16cm, trim={0 0 0 2cm}, clip]{Precision_Recall_Time_Signs.png}     \\caption{Precision vs. Recall based on size results on traffic signs dataset. The size of each circle represents the size of the model, with larger circles indicating larger models.}     \\label{fig: prec signs} Figure \\ref{fig: prec signs} elucidates the trade-off between precision and recall taking the size of the models into consideration. Models such as YOLO11m, YOLO10l, YOLOv9m, YOLOv5ux, and YOLO11l exhibit high precision and recall, specifically with YOLO11m achieving a precision of 0.898 and a recall of 0.826 while having a size of 67.9Mb, and YOLOv10l achieving a precision of 0.873 and a recall of 0.807 with a significantly bigger size (126.8 Mb). In contrast, smaller models such as YOLOv10n (precision 0.722, recall 0.602), YOLOv8n (precision 0.749, recall 0.688), and YOLO11n (precision 0.768, recall 0.695) underperform in both metrics. This underscores the superior performance of larger models on the Traffic Signs dataset. Moreover, the high precision (0.849) and low recall (0.701) of YOLOv5um indicate a propensity for false negatives, while YOLOv3u's high recall (0.849) and low precision (0.75) suggest a tendency for false positives.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the comparison of precision and recall based on model size on the traffic signs dataset. The size of each circle represents the model size, with larger circles indicating larger models. Models like YOLO11m, YOLO10l, YOLOv9m, YOLOv5ux, and YOLO11l exhibit high precision and recall. Specifically, YOLO11m achieves a precision of 0.898 and a recall of 0.826 with a model size of 67.9 Mb, while YOLO10l achieves a precision of 0.873 and a recall of 0.807 with a model size of 126.8 Mb. In contrast, smaller models such as YOLOv10n (precision 0.722, recall 0.602), YOLOv8n (precision 0.749, recall 0.688), and YOLO11n (precision 0.768, recall 0.695) underperform in both metrics. This indicates the superior performance of larger models on the traffic signs dataset. Additionally, the high precision (0.849) and low recall (0.701) of YOLOv5um suggest a tendency for false negatives, while YOLOv3u's high recall (0.849) and low precision (0.75) suggest a tendency for false positives."
    },
    "239": {
        "figure1": "2411.00201v1_mAP50_Animals",
        "label1": "fig: map africa",
        "caption1": "mAP50 and mAP50-95 YOLO results on Africa wildlife dataset. Each model is represented by two bars: the left bar shows the mAP50 score, while the right bar represents the mAP50-95 score.",
        "text": "\\centering     \\includegraphics[width=16cm]{mAP50_Animals.png}     \\caption{mAP50 and mAP50-95 YOLO results on Africa wildlife dataset. Each model is represented by two bars: the left bar shows the mAP50 score, while the right bar represents the mAP50-95 score.}     \\label{fig: map africa}\nAs illustrated in Figure \\ref{fig: map africa}, YOLOv9s demonstrates exceptional performance with a high mAP50-95 of 0.832 and a mAP50 of 0.956, showcasing its robust accuracy across various IoU thresholds. YOLOv9c and YOLOv9t follow closely, with mAP50 scores of 0.96 and 0.948 and mAP50-95 scores of 0.83 and 0.825, respectively. These results highlight the YOLOv9 family’s ability to effectively learn patterns from a small sample of images, making it particularly suited for smaller datasets. In contrast, YOLOv5un, YOLOv10n, and YOLOv3u-tiny show lower mAP50-95 scores of 0.791, 0.786, and 0.725, indicating their limitations in accuracy. The underperformance of larger models like YOLO11x, YOLOv5ux, YOLOv5ul, and YOLOv10l can be attributed to overfitting, especially given the small dataset size.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the mAP50 and mAP50-95 results of different YOLO models on the Africa wildlife dataset. YOLOv9s demonstrates outstanding performance, with a mAP50-95 of 0.832 and a mAP50 of 0.956, indicating its strong accuracy across various IoU thresholds. YOLOv9c and YOLOv9t closely follow, with mAP50 scores of 0.96 and 0.948, and mAP50-95 scores of 0.83 and 0.825, respectively. These results highlight the ability of the YOLOv9 family to effectively learn patterns from small sample images, making it particularly suitable for smaller datasets. In contrast, YOLOv5un, YOLOv10n, and YOLOv3u-tiny show lower mAP50-95 scores of 0.791, 0.786, and 0.725, indicating limitations in their accuracy. The underperformance of larger models like YOLO11x, YOLOv5ux, YOLOv5ul, and YOLOv10l may be attributed to overfitting, especially given the small dataset size."
    },
    "240": {
        "figure1": "2411.00201v1_Precision_Recall_Size_Animals",
        "label1": "fig: prec africa",
        "caption1": "Precision vs. Recall based on size results on Africa wildlife dataset. The size of each circle represents the size of the model, with larger circles indicating larger models.",
        "figure2": "2411.00201v1_Time_Animals",
        "label2": "fig: proc time animals",
        "caption2": "Total processing time results on Africa wildlife dataset. Each bar represents the total processing time, divided into three sections: Preprocessing Time (bottom), Inference Time (middle), and Postprocessing Time (top). ",
        "figure3": "2411.00201v1_time_and_gflop_africa",
        "label3": "fig: time and gflop africa",
        "caption3": "Total processing time and GFLOPs count results on Africa wildlife dataset.",
        "text": "\\centering     \\includegraphics[width=16cm, trim={0 0 0 2cm}, clip]{Precision_Recall_Size_Animals.png}     \\caption{Precision vs. Recall based on size results on Africa wildlife dataset. The size of each circle represents the size of the model, with larger circles indicating larger models.}     \\label{fig: prec africa}\nFigure \\ref{fig: prec africa} reveals that YOLO8l and YOLO11l achieve the highest precision and recall, with values of 0.942 and 0.937 for precision, and 0.898 and 0.896 for recall, respectively. Notably, YOLOv8n achieves similar results (0.932 for precision, 0.908 for recall) with a compact size of 6.55Mb, demonstrating its efficiency. In contrast, YOLOv3u and YOLOv5ul exhibit lower precision and recall scores (0.91 and 0.88 for YOLOv3u, 0.916 and 0.881 for YOLOv5ul), despite their larger sizes (204.86Mb for YOLOv3u, 106.85Mb for YOLOv5ul), which may be attributed to overfitting issues.\nYOLOv9t and YOLOv9s consistently excel across all metrics, delivering high accuracy while maintaining small model sizes, low GFLOPs, and short inference times, as shown in Table \\ref{tab:evaluation African animals}, and Figures \\ref{fig: prec africa}, \\ref{fig: proc time animals}, and \\ref{fig: time and gflop africa}. This demonstrates the robustness of YOLOv9’s smaller models and their effectiveness on small datasets. In contrast, YOLO5ux and YOLO11x show suboptimal accuracy despite their larger sizes and longer inference times, likely due to overfitting. Most large models underperformed on this dataset, with the exception of YOLOv10x, which benefited from a modern architecture that prevents overfitting.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the precision versus recall based on size for different YOLO models on the Africa wildlife dataset. The size of each circle represents the model size. YOLO8l and YOLO11l exhibit the highest precision and recall. YOLOv8n achieves similar results (precision of 0.932, recall of 0.908) with a smaller size (6.55Mb), demonstrating its efficiency. In contrast, YOLOv3u and YOLOv5ul, despite their larger sizes (204.86Mb and 106.85Mb, respectively), show lower precision and recall (0.91 and 0.88 for YOLOv3u, 0.916 and 0.881 for YOLOv5ul), potentially due to overfitting."
    },
    "241": {
        "figure1": "2411.00201v1_map_Ships",
        "label1": "fig: map ships",
        "caption1": "mAP50 and mAP50-95 YOLO results on ships and vessel dataset. Each model is represented by two bars: the left bar shows the mAP50 score, while the right bar represents the mAP50-95 score.",
        "figure2": "2411.00201v1_Precision_Recall_Size_Ships",
        "label2": "fig: prec ships",
        "caption2": "Precision vs. Recall based on size results on ships and vessels dataset. The size of each circle represents the size of the model, with larger circles indicating larger models.",
        "figure3": "2411.00201v1_Time_Ships",
        "label3": "fig: proc time ships",
        "caption3": "Total processing time results on ships and vessels dataset. Each bar represents the total processing time, divided into three sections: Preprocessing Time (bottom), Inference Time (middle), and Postprocessing Time (top).",
        "text": "The disparity between mAP50-95 and mAP50, illustrated in Figure \\ref{fig: map ships}, underscores the challenges YOLO models face with higher IoU thresholds when detecting small objects. Additionally, YOLO models struggle with detecting objects of varying rotations. Among the models, YOLO11x achieved the highest accuracy, with a mAP50 of 0.529 and a mAP50-95 of 0.327, closely followed by YOLO11l, YOLO11m, and YOLO11s, which recorded mAP50 values of 0.529, 0.528, and 0.53, and mAP50-95 values of 0.327, 0.325, and 0.325, respectively. These results highlight the robustness of the YOLO11 family in detecting small and tiny objects. In contrast, YOLOv3u-tiny, YOLOv8n, YOLOv3u, and YOLOv5n exhibited the lowest accuracy, with mAP50 scores of 0.489, 0.515, 0.519, and 0.514, and mAP50-95 scores of 0.273, 0.297, 0.298, and 0.298, respectively. This suggests the outdated architecture of YOLOv3u and the potential underfitting of smaller models due to the large dataset size.\nThe results in Table \\ref{tab:evaluation ships} and Figures \\ref{fig: map ships}, \\ref{fig: prec ships}, and \\ref{fig: proc time ships} demonstrate that YOLO11s and YOLOv10s excelled in accuracy while maintaining compact sizes, low GFLOPs, and quick processing times. In contrast, YOLOv3u, YOLOv8x, and YOLOv8l fell short of expectations despite their larger sizes and longer processing times. These findings highlight the robustness and reliability of the YOLO11 family, particularly in improving the YOLO family's performance in detecting small and tiny objects while ensuring efficient processing. Additionally, the results reveal the underperformance of YOLOv9 models when faced with large datasets and small objects, despite their modern architecture.\n\\centering     \\includegraphics[width=16cm]{map_Ships.png}     \\caption{mAP50 and mAP50-95 YOLO results on ships and vessel dataset. Each model is represented by two bars: the left bar shows the mAP50 score, while the right bar represents the mAP50-95 score.}     \\label{fig: map ships}",
        "summarize_figure": "figure1",
        "summarization": "The image illustrates a comparison of mAP50 and mAP50-95 for YOLO algorithms on a ships and vessel dataset. Each YOLO model is represented by two bars, with the left bar indicating the mAP50 score and the right bar indicating the mAP50-95 score. The YOLO11x model demonstrates the highest accuracy, with an mAP50 of 0.529 and an mAP50-95 of 0.327, closely followed by YOLO11l, YOLO11m, and YOLO11s. In contrast, YOLOv3u-tiny, YOLOv8n, YOLOv3u, and YOLOv5n exhibit the lowest accuracy."
    },
    "242": {
        "figure1": "2411.00201v1_Precision_Recall_Size_Ships",
        "label1": "fig: prec ships",
        "caption1": "Precision vs. Recall based on size results on ships and vessels dataset. The size of each circle represents the size of the model, with larger circles indicating larger models.",
        "figure2": "2411.00201v1_map_Ships",
        "label2": "fig: map ships",
        "caption2": "mAP50 and mAP50-95 YOLO results on ships and vessel dataset. Each model is represented by two bars: the left bar shows the mAP50 score, while the right bar represents the mAP50-95 score.",
        "figure3": "2411.00201v1_Time_Ships",
        "label3": "fig: proc time ships",
        "caption3": "Total processing time results on ships and vessels dataset. Each bar represents the total processing time, divided into three sections: Preprocessing Time (bottom), Inference Time (middle), and Postprocessing Time (top).",
        "text": "Figure \\ref{fig: prec ships} indicates that YOLOv5ux outperformed other models, achieving a precision of 0.668 and a recall of 0.555. It was closely followed by YOLOv9m (precision of 0.668, recall of 0.551) and YOLOv8m (precision of 0.669, recall of 0.525), both of which are significantly smaller in size (40.98 Mb for YOLOv9m and 52.12 Mb for YOLOv8m). In contrast, YOLO11n and YOLOv10s exhibited lower performance, with precisions of 0.574 and 0.586 and recalls of 0.51 and 0.511, respectively, likely due to underfitting issues. Generally, YOLO11 models tended to produce false positives, reflected in their low precision and high recall. Meanwhile, YOLOv10 underperformed in both precision and recall, despite being one of the newest models in the YOLO family.\nThe results in Table \\ref{tab:evaluation ships} and Figures \\ref{fig: map ships}, \\ref{fig: prec ships}, and \\ref{fig: proc time ships} demonstrate that YOLO11s and YOLOv10s excelled in accuracy while maintaining compact sizes, low GFLOPs, and quick processing times. In contrast, YOLOv3u, YOLOv8x, and YOLOv8l fell short of expectations despite their larger sizes and longer processing times. These findings highlight the robustness and reliability of the YOLO11 family, particularly in improving the YOLO family's performance in detecting small and tiny objects while ensuring efficient processing. Additionally, the results reveal the underperformance of YOLOv9 models when faced with large datasets and small objects, despite their modern architecture.\n\\centering     \\includegraphics[width=16cm, trim={0 0 0 2cm}, clip]{Precision_Recall_Size_Ships.png}     \\caption{Precision vs. Recall based on size results on ships and vessels dataset. The size of each circle represents the size of the model, with larger circles indicating larger models.}     \\label{fig: prec ships}",
        "summarize_figure": "figure1",
        "summarization": "Here is a summary of the figure:The picture illustrates the precision versus recall of different YOLO versions on a ships dataset, where the size of each circle represents the size of the model. YOLOv5ux demonstrates the best performance, achieving a precision of 0.668 and a recall of 0.555. YOLOv9m (precision of 0.668, recall of 0.551) and YOLOv8m (precision of 0.669, recall of 0.525) closely follow, but they have significantly smaller model sizes. In contrast, YOLO11n and YOLOv10s show lower performance. YOLO11 models tend to produce false positives, while YOLOv10 performs poorly in both precision and recall."
    },
    "243": {
        "figure1": "2411.00201v1_time_and_gflop_ships",
        "label1": "fig: time and gflop ships",
        "caption1": "Total processing time and GFLOPs count results on ships and vessels dataset.",
        "figure2": "2411.00201v1_Time_Ships",
        "label2": "fig: proc time ships",
        "caption2": "Total processing time results on ships and vessels dataset. Each bar represents the total processing time, divided into three sections: Preprocessing Time (bottom), Inference Time (middle), and Postprocessing Time (top).",
        "text": "As illustrated in Figures \\ref{fig: proc time ships} and \\ref{fig: time and gflop ships}, YOLOv3u-tiny achieved the fastest processing time at 2 ms, closely followed by YOLOv8n and YOLOv5un, both recording 2.3 ms. YOLOv10 and YOLO11 models also excelled in speed, with YOLOv10n and YOLO11n achieving rapid inference times of 2.4 ms and 2.5 ms, along with GFLOPs counts of 8.2 and 6.3, respectively. In contrast, YOLOv9e exhibited the slowest speed, with an inference time of 7.6 ms and a GFLOPs count of 189.3, highlighting the trade-off between accuracy and efficiency within the YOLOv9 family.\n\\centering     \\includegraphics[width=16cm, trim={0 0 0 2.5cm}, clip]{time_and_gflop_ships.png}     \\caption{Total processing time and GFLOPs count results on ships and vessels dataset.}     \\label{fig: time and gflop ships}",
        "summarize_figure": "figure1",
        "summarization": "The image shows a comparison of total processing time for YOLO algorithms on a ships dataset, with each bar representing the total processing time divided into three sections: Preprocessing Time (bottom), Inference Time (middle), and Postprocessing Time (top). The processing times vary significantly across different YOLO models. The inference time of YOLOv10b is relatively short, but its post-processing time is relatively long. The preprocessing time of YOLOv8x is relatively long."
    },
    "244": {
        "figure1": "2411.00222v1_pc_fc_adv_avg_energy",
        "label1": "fig:advPCenergy",
        "caption1": "The network's energy drops as PCnet perturbs AE. The network's energy increases and stabilizes when the input is clamped to AE, and it decreases as AE changes through the network's dynamics in PCnet, in unclamp mode.",
        "text": "\\includegraphics[width=0.80\\columnwidth]{figs/pc_fc_adv_avg_energy.pdf}     \\caption{The network's energy drops as PCnet perturbs AE. The network's energy increases and stabilizes when the input is clamped to AE, and it decreases as AE changes through the network's dynamics in PCnet, in unclamp mode.}     \\label{fig:advPCenergy} }",
        "summarize_figure": "figure1",
        "summarization": "Here is a summary of the figure:The figure shows the network's energy variation over time in both clamped and unclamped modes. In clamped mode, the network's energy increases with time and stabilizes at approximately 0.21 around 1 second. Conversely, in unclamped mode, the network's energy rapidly decreases starting at 1 second and stabilizes after 1.25 seconds, with the energy value approaching 0."
    },
    "245": {
        "figure1": "2411.00222v1_pc_ff_targeted_non_targeted_mnist_BarChart_c",
        "label1": "fig:advPredictionCorrectMNIST",
        "caption1": "Classification accuracy of AEs on MNIST dataset. For 18k attempted targeted attacks on $X$, using the C\\&W method, (1) only AEs that were successful against FFnet were included in these results. (2) PCnet classifies $70\\%$ and $68\\%$ of AEs for FCnet and CNN, respectively. (3) FFnet classifies $81\\%$ and $84\\%$ of $P$ (i.e., adjusted AEs by PCnet) for FCnet and CNN, respectively.",
        "text": "\\centering     \\includegraphics[width=.95\\columnwidth]{figs/pc_ff_targeted_non_targeted_mnist_BarChart_c.pdf}     \\caption{Classification accuracy of AEs on MNIST dataset. For 18k attempted targeted attacks on $X$, using the C\\&W method, (1) only AEs that were successful against FFnet were included in these results. (2) PCnet classifies $70\\%$ and $68\\%$ of AEs for FCnet and CNN, respectively. (3) FFnet classifies $81\\%$ and $84\\%$ of $P$ (i.e., adjusted AEs by PCnet) for FCnet and CNN, respectively.}     \\label{fig:advPredictionCorrectMNIST} }",
        "summarize_figure": "figure1",
        "summarization": "The image shows the classification accuracy of AEs on the MNIST dataset. For 18k attempted targeted attacks on X, using the C and W method, the results included only AEs that were successful against FFnet. The image compares the classification performance of three models, FFnet(Z), PCnet(Z), and FFnet(P), on FCnet and CNN networks. For FCnet, the misclassification rate of FFnet(Z) is 100%, the correct classification rate of PCnet(Z) is 70%, and the correct classification rate of FFnet(P) is 81%. For CNN, the misclassification rate of FFnet(Z) is 100%, the correct classification rate of PCnet(Z) is 68%, and the correct classification rate of FFnet(P) is 84%."
    },
    "246": {
        "figure1": "2411.00222v1_pc_ff_targeted_non_targeted_mnist_BarChart_w",
        "label1": "fig:advPredictionWrongMNIST",
        "caption1": "Classification accuracy of AEs on MNIST dataset. For 2.7k attempted targeted attacks on $\\tilde{X}$ using the C\\&W attack, (1) only AEs that were successful against FFnet were included in these results. (2) PCnet classifies $9\\%$ and $4\\%$ of AEs for FCnet and CNN, respectively. (3) FFnet classifies $44\\%$ and $57\\%$ of adjusted $\\tilde{Z}$ for FCnet and CNN, respectively.",
        "text": "\\centering     \\includegraphics[width=.95\\columnwidth]{figs/pc_ff_targeted_non_targeted_mnist_BarChart_w.pdf}     \\caption{Classification accuracy of AEs on MNIST dataset. For 2.7k attempted targeted attacks on $\\tilde{X}$ using the C\\&W attack, (1) only AEs that were successful against FFnet were included in these results. (2) PCnet classifies $9\\%$ and $4\\%$ of AEs for FCnet and CNN, respectively. (3) FFnet classifies $44\\%$ and $57\\%$ of adjusted $\\tilde{Z}$ for FCnet and CNN, respectively.}     \\label{fig:advPredictionWrongMNIST} }",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the classification accuracy of AEs on the MNIST dataset after 2.7k attempted targeted attacks on X using the C&W attack against FFnet. The left chart shows the classification results for 2511 AEs per MNIST class for FCnet, where FFnet correctly classifies 44% of adjusted Z, PCnet correctly classifies 9% of adjusted Z, and FFnet correctly classifies 0% of P. The right chart shows the classification results for 2367 AEs per MNIST class for CNN, where FFnet correctly classifies 57% of adjusted Z, PCnet correctly classifies 4% of adjusted Z, and FFnet correctly classifies 0% of P."
    },
    "247": {
        "figure1": "2411.00222v1_pc_ff_targeted_non_targeted_cifar10_BarChart_c",
        "label1": "fig:advPredictionCorrectCIFAR",
        "caption1": "Classification accuracy of AEs on CIFAR10 dataset. For 900 attempted targeted attacks on $X$ using the C\\&W attack, (1) only AEs that were successful against FFnet were included in these results. (2) PCnet classifies $79\\%$ and $85\\%$ of AEs for FCnet and CNN, respectively. (3) FFnet classifies $48\\%$ and $83\\%$ of $P$ (i.e., adjusted AEs by PCnet) for FCnet and CNN, respectively.",
        "text": "\\centering     \\includegraphics[width=.95\\columnwidth]{figs/pc_ff_targeted_non_targeted_cifar10_BarChart_c.pdf}     \\caption{Classification accuracy of AEs on CIFAR10 dataset. For 900 attempted targeted attacks on $X$ using the C\\&W attack, (1) only AEs that were successful against FFnet were included in these results. (2) PCnet classifies $79\\%$ and $85\\%$ of AEs for FCnet and CNN, respectively. (3) FFnet classifies $48\\%$ and $83\\%$ of $P$ (i.e., adjusted AEs by PCnet) for FCnet and CNN, respectively.}     \\label{fig:advPredictionCorrectCIFAR} }",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the classification accuracy of AEs on the CIFAR10 dataset. The left part shows the classification results of 657 AEs for FCnet, where the accuracy of FFnet(Z) is 0%, PCnet(Z) is 79%, and FFnet(P) is 48%. The right part displays the classification results of 899 AEs for CNN, with FFnet(Z) at 0%, PCnet(Z) at 85%, and FFnet(P) at 83%. Overall, PCnet(Z) has higher classification accuracy than FFnet(P) for both FCnet and CNN."
    },
    "248": {
        "figure1": "2411.00222v1_fc_pc_fc_stacked_bar_plot",
        "label1": "fig:advPredictionPC&FFnet",
        "caption1": "FFnet classifies better on perturbed AEs. In the visual representation, red bars indicate success rates in adversarial attacks, while green bars indicate success rates in classification. For each class, the columns show the classification accuracy of FFnet($Z$), PCnet($Z$), and FFnet($P$), respectively. PCnet performs better than FCnet in classifying targeted and non-targeted adversarial examples (AEs). Additionally, FCnet improves its classification of AEs when PCnet perturbs them. Although the performance of PCnet and FCnet varies across different classes, on average, FCnet performs better on perturbed AEs. Similar behaviour is observed when using CNN. Therefore, FFnets, either FCnet or CCN, show improvement in the classification of AEs when perturbed by PCnet.",
        "text": "\\centering     \\caption{FFnet classifies better on perturbed AEs. In the visual representation, red bars indicate success rates in adversarial attacks, while green bars indicate success rates in classification. For each class, the columns show the classification accuracy of FFnet($Z$), PCnet($Z$), and FFnet($P$), respectively. PCnet performs better than FCnet in classifying targeted and non-targeted adversarial examples (AEs). Additionally, FCnet improves its classification of AEs when PCnet perturbs them. Although the performance of PCnet and FCnet varies across different classes, on average, FCnet performs better on perturbed AEs. Similar behaviour is observed when using CNN. Therefore, FFnets, either FCnet or CCN, show improvement in the classification of AEs when perturbed by PCnet.}     \\label{fig:advPredictionPC&FFnet} }",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the classification accuracy and success rates of adversarial attacks for three networks (FFnet(Z), PCnet(Z), and FFnet(P)) across different classes. Red and pink bars indicate the success rates of attacks on targeted and non-targeted adversarial examples, respectively, while green and light green bars represent the success rates of defense against targeted and non-targeted adversarial examples, respectively. For each class, the bar chart shows the classification accuracy of FFnet(Z), PCnet(Z), and FFnet(P), respectively. The defense success rates vary across different classes."
    },
    "249": {
        "figure1": "2411.00222v1_fc_pc_fc_mnist_StackedBarChart",
        "label1": "fig:advPredictionPC&FC",
        "caption1": "FCnet classifies better on perturbed AEs. In the visual representation, the red bars indicate success rates in adversarial attacks, while the green bars indicate success rates in classification. For each class, the columns show the classification accuracy of FCnet($Z$), PCnet($Z$), and FCnet($P$), respectively. PCnet classifies targeted and non-targeted adversarial examples (AEs) better than FCnet. Additionally, FCnet improves its classification of AEs when PCnet perturbs them. Although the performance of PCnet and FCnet varies across different classes, on average, FCnet performs better on perturbed AEs.",
        "text": "\\centering     \\caption{FCnet classifies better on perturbed AEs. In the visual representation, the red bars indicate success rates in adversarial attacks, while the green bars indicate success rates in classification. For each class, the columns show the classification accuracy of FCnet($Z$), PCnet($Z$), and FCnet($P$), respectively. PCnet classifies targeted and non-targeted adversarial examples (AEs) better than FCnet. Additionally, FCnet improves its classification of AEs when PCnet perturbs them. Although the performance of PCnet and FCnet varies across different classes, on average, FCnet performs better on perturbed AEs.}     \\label{fig:advPredictionPC&FC} }",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the classification accuracy of adversarial examples by FCnet and PCnet on the MNIST dataset. In the figure, red bars represent the success rates of adversarial attacks, while green bars represent the success rates of classification. The horizontal axis represents the MNIST classes, and the vertical axis represents the classification accuracy. For each class, the classification accuracy of FCnet(Z), PCnet(Z), and FCnet(P) are shown respectively. It can be seen that PCnet outperforms FCnet in classifying targeted and non-targeted adversarial examples. In addition, FCnet improves its classification of adversarial examples when they are perturbed by PCnet. Overall, FCnet performs better on perturbed adversarial examples."
    },
    "250": {
        "figure1": "2411.00222v1_cnn_pc_cnn_mnist_StackedBarChart",
        "label1": "fig:advPredictionPC&CNN",
        "caption1": "CNN classifies better on perturbed AEs. In the visual representation, the red bars indicate success rates in adversarial attacks, while the green bars indicate success rates in classification. For each class, the columns show the classification accuracy of CNN($Z$), PCnet($Z$), and CNN($P$), respectively. PCnet classifies targeted and non-targeted adversarial examples (AEs) better than CNN. Additionally, CNN improves its classification of AEs when PCnet perturbs them. Although the performance of PCnet and CNN varies across different classes, on average, CNN performs better on perturbed AEs.",
        "text": "\\centering     \\caption{CNN classifies better on perturbed AEs. In the visual representation, the red bars indicate success rates in adversarial attacks, while the green bars indicate success rates in classification. For each class, the columns show the classification accuracy of CNN($Z$), PCnet($Z$), and CNN($P$), respectively. PCnet classifies targeted and non-targeted adversarial examples (AEs) better than CNN. Additionally, CNN improves its classification of AEs when PCnet perturbs them. Although the performance of PCnet and CNN varies across different classes, on average, CNN performs better on perturbed AEs.}     \\label{fig:advPredictionPC&CNN} }",
        "summarize_figure": "figure1",
        "summarization": "The image illustrates the classification accuracy of CNN, PCnet, and perturbed CNN on adversarial examples within the MNIST dataset. The horizontal axis represents the MNIST classes (0-9) and the average, while the vertical axis indicates the classification accuracy. Red bars denote the success rates of adversarial attacks, and green bars represent classification success rates. For each class, the columns show the classification accuracy of CNN(Z), PCnet(Z), and CNN(P), respectively. PCnet demonstrates better classification of targeted and non-targeted adversarial examples compared to CNN. Furthermore, CNN improves its classification of adversarial examples when they are perturbed by PCnet. Although the performance of PCnet and CNN varies across different classes, on average, CNN performs better on perturbed adversarial examples."
    },
    "251": {
        "figure1": "2411.00222v1_pc_fc_adv_prob",
        "label1": "fig:FC_adjusted_adv",
        "caption1": "Perturbation and reversion process. The red arrow illustrates FFnet misclassification as perturbed images transition from $AE_0$ to $AE_n$, mistakenly predicting `3' instead of `0'. Conversely, the green arrow depicts PCnet's iterative adjustments, enabling FFnet to correctly classify AEs as `0' after some iterations.",
        "text": "\\centering     \\includegraphics[width=0.85\\columnwidth]{figs/pc_fc_adv_prob.pdf}     \\caption{Perturbation and reversion process. The red arrow illustrates FFnet misclassification as perturbed images transition from $AE_0$ to $AE_n$, mistakenly predicting `3' instead of `0'. Conversely, the green arrow depicts PCnet's iterative adjustments, enabling FFnet to correctly classify AEs as `0' after some iterations.}     \\label{fig:FC_adjusted_adv} }",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the perturbation and reversion process. The x-axis represents the probability of FFnet classifying the input as class \"3\", and the y-axis represents the probability of FFnet classifying the input as class \"0\". The red arrow indicates the adversarial attack process, where FFnet perturbs the adversarial sample from AE0 to AEn, leading to misclassification of \"0\" as \"3\". The green arrow shows the iterative adjustment process of PCnet, which enables FFnet to correctly classify adversarial samples as \"0\" after multiple iterations. In the figure, green triangles represent adversarial samples AEi labeled as \"0\", red triangles represent adversarial samples AEi labeled as \"3\", purple squares represent samples xi_hat labeled as \"3\", and blue squares represent samples xi_hat labeled as \"0\"."
    },
    "252": {
        "figure1": "2411.00222v1_perturbation_reversion_process",
        "label1": "fig:perturb_reverte",
        "caption1": "PCnet reverts adversarial attack disguise. The level curves of the network's energy, as a function of the activity of 2 neurons in the network, involve multiple local minima. C\\&W attack tries to perturb the original image $O_1$ such that FFnet classifies it as $T_1$. The red curve shows the perturbation path crossing the decision boundary of the FFnet. In the other direction, PCnet, using its dynamics, reverts the perturbation and brings the perturbed image to a state corresponding to lower energy, which is depicted by the green arrow from $T_1$ to $O_1$. When the perturbed image passes the PCnet decision boundary as well, the dynamics push it even further away. For example, instead of returning $T_2$ to $O_2$, it goes toward $O_3$.",
        "text": "\\centering     \\includegraphics[width=0.70\\columnwidth]{figs/perturbation_reversion_process.pdf}     \\caption{PCnet reverts adversarial attack disguise. The level curves of the network's energy, as a function of the activity of 2 neurons in the network, involve multiple local minima. C\\&W attack tries to perturb the original image $O_1$ such that FFnet classifies it as $T_1$. The red curve shows the perturbation path crossing the decision boundary of the FFnet. In the other direction, PCnet, using its dynamics, reverts the perturbation and brings the perturbed image to a state corresponding to lower energy, which is depicted by the green arrow from $T_1$ to $O_1$. When the perturbed image passes the PCnet decision boundary as well, the dynamics push it even further away. For example, instead of returning $T_2$ to $O_2$, it goes toward $O_3$.}     \\label{fig:perturb_reverte} }",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates how PCnet reverts adversarial attack disguise. It shows level curves of the network's energy as a function of the activity of 2 neurons, involving multiple local minima. The C&W attack perturbs the original image O1, causing FFnet to classify it as T1. The red dashed curve shows the perturbation path crossing the FFnet's decision boundary. Conversely, PCnet's dynamics revert this perturbation, bringing the perturbed image to a lower energy state, depicted by the green arrow from T1 to O1. When the perturbed image crosses the PCnet decision boundary, the dynamics push it further away, for example, moving T2 towards O3 instead of returning it to O2."
    },
    "253": {
        "figure1": "2411.00222v1_PCnet_filter_boxplot_MNIST_CIFAR10_FC_correct",
        "label1": "fig:PCnet_filter_boxplot_MNIST_CIFAR10_FC_correct",
        "caption1": "PCnet's perturbation outperforms filter effect on classifying AEs. When AEs, created using classified data by the PCnet, are processed with a Gaussian filter, the classification accuracy of the FCnet improves. However, in most cases, the perturbation caused by the PCnet has a greater impact on FCnet classification than the effects of the Gaussian filter. On average, FCnet performs better on AEs perturbed by PCnet than when a Gaussian filter is used. This trend is consistent for AEs from both the MNIST and CIFAR10 datasets. Note that: $Z\\leftarrow\\mathrm{AT_{FCnet}}(X)$,\\; $G\\leftarrow\\mathrm{GaussianBlur}(Z)$,\\; $\\left(P, \\textrm{and labels}\\right) \\leftarrow \\mathrm{PCnet}(Z)$,\\; $\\textrm{labels} \\leftarrow \\mathrm{FCnet}(\\cdot)$. ",
        "text": "\\centering     \\includegraphics[width=1.0\\columnwidth]{figs/PCnet_filter_boxplot_MNIST_CIFAR10_FC_correct.pdf}     \\caption{PCnet's perturbation outperforms filter effect on classifying AEs. When AEs, created using classified data by the PCnet, are processed with a Gaussian filter, the classification accuracy of the FCnet improves. However, in most cases, the perturbation caused by the PCnet has a greater impact on FCnet classification than the effects of the Gaussian filter. On average, FCnet performs better on AEs perturbed by PCnet than when a Gaussian filter is used. This trend is consistent for AEs from both the MNIST and CIFAR10 datasets.     Note that:     $Z\\leftarrow\\mathrm{AT_{FCnet}}(X)$,\\;     $G\\leftarrow\\mathrm{GaussianBlur}(Z)$,\\;     $\\left(P, \\textrm{and labels}\\right) \\leftarrow \\mathrm{PCnet}(Z)$,\\;     $\\textrm{labels} \\leftarrow \\mathrm{FCnet}(\\cdot)$.     }     \\label{fig:PCnet_filter_boxplot_MNIST_CIFAR10_FC_correct} }",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the classification accuracy of FCnet when processing adversarial examples (AEs) perturbed by PCnet and processed by Gaussian filter. The horizontal axis represents classes 0 to 9 of the MNIST dataset, as well as the average classification accuracy for MNIST and CIFAR10 datasets. The figure compares the classification accuracy in four cases: FCnet(Z), PCnet(Z), FCnet(P), and FCnet(G). For each class in the MNIST dataset, the accuracy of FCnet(Z) is generally lower, while the accuracy of PCnet(Z) is relatively higher. The accuracy of FCnet(P) is generally higher than that of FCnet(G). On the average accuracy of MNIST and CIFAR10, both PCnet(Z) and FCnet(P) are significantly higher than FCnet(Z) and FCnet(G)."
    },
    "254": {
        "figure1": "2411.00222v1_PCnet_filter_boxplot_MNIST_CIFAR10_CNN_correct",
        "label1": "fig:PCnet_filter_boxplot_MNIST_CIFAR10_CNN_correct",
        "caption1": "PCnet's perturbation outperforms filter effect on classifying AEs. When AEs, created using classified data by the PCnet, are processed with a Gaussian filter, the classification accuracy of the CNN improves. However, in most cases, the perturbation caused by the PCnet has a greater impact on CNN classification than the effects of the Gaussian filter. On average, CNN performs better on AEs perturbed by PCnet than when a Gaussian filter is used. This trend is consistent for AEs from both the MNIST and CIFAR10 datasets. Note that: $Z\\leftarrow\\mathrm{AT_{CNN}}(X)$,\\; $G\\leftarrow\\mathrm{GaussianBlur}(Z)$,\\; $\\left(P, \\textrm{and labels}\\right) \\leftarrow \\mathrm{PCnet}(Z)$,\\; $\\textrm{labels} \\leftarrow \\mathrm{CNN}(\\cdot)$. ",
        "text": "\\centering     \\includegraphics[width=1.0\\columnwidth]{figs/PCnet_filter_boxplot_MNIST_CIFAR10_CNN_correct.pdf}     \\caption{PCnet's perturbation outperforms filter effect on classifying AEs. When AEs, created using classified data by the PCnet, are processed with a Gaussian filter, the classification accuracy of the CNN improves. However, in most cases, the perturbation caused by the PCnet has a greater impact on CNN classification than the effects of the Gaussian filter. On average, CNN performs better on AEs perturbed by PCnet than when a Gaussian filter is used. This trend is consistent for AEs from both the MNIST and CIFAR10 datasets.     Note that:     $Z\\leftarrow\\mathrm{AT_{CNN}}(X)$,\\;     $G\\leftarrow\\mathrm{GaussianBlur}(Z)$,\\;     $\\left(P, \\textrm{and labels}\\right) \\leftarrow \\mathrm{PCnet}(Z)$,\\;     $\\textrm{labels} \\leftarrow \\mathrm{CNN}(\\cdot)$.     }     \\label{fig:PCnet_filter_boxplot_MNIST_CIFAR10_CNN_correct} }",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates that PCnet's perturbation outperforms the Gaussian filter effect on classifying adversarial examples (AEs). The horizontal axis represents the 0-9 classes of the MNIST dataset and the average classification accuracy of the MNIST and CIFAR10 datasets, and the vertical axis represents the classification accuracy percentage. The figure compares the classification accuracy in four cases: the classification result of the original CNN classifier on adversarial sample Z (CNN(Z)), the classification result after PCnet perturbation (PCnet(Z)), the classification result after PCnet perturbation classified by CNN (CNN(P)), and the classification result of CNN after the adversarial sample is processed by a Gaussian filter (CNN(G)). For each class of the MNIST dataset, the accuracy of CNN(Z) is generally low, while the accuracy of PCnet(Z) is relatively high, indicating that the perturbation of PCnet can effectively affect the classification of CNN. At the same time, the accuracy of CNN(P) and CNN(G) is between the two. Similar trends are also observed in the average classification accuracy of MNIST and CIFAR10 datasets, with the classification result after PCnet perturbation being better than that after Gaussian filtering."
    },
    "255": {
        "figure1": "2411.00222v1_PCnet_filter_boxplot_MNIST_CIFAR10_FC_wrong",
        "label1": "fig:PCnet_filter_boxplot_MNIST_CIFAR10_FC_wrong",
        "caption1": "PCnet's perturbation outperforms filter effect on classifying AEs. When AEs, created using misclassified data by the PCnet, are processed with a Gaussian filter, the classification accuracy of the FCnet improves. However, in most cases, the perturbation caused by the PCnet has a greater impact on FCnet classification than the effects of the Gaussian filter. On average, FCnet performs better on AEs perturbed by PCnet than when a Gaussian filter is used. This trend is consistent for AEs from both the MNIST and CIFAR10 datasets. Note that: $\\tilde{Z}\\leftarrow\\mathrm{AT_{FCnet}}(\\tilde{X})$,\\; $\\tilde{G}\\leftarrow\\mathrm{GaussianBlur}(\\tilde{Z})$,\\; $\\left(\\tilde{P}, \\textrm{and labels}\\right) \\leftarrow \\mathrm{PCnet}(\\tilde{Z})$,\\; $\\textrm{labels} \\leftarrow \\mathrm{FCnet}(\\cdot)$. ",
        "text": "\\centering     \\includegraphics[width=1.0\\columnwidth]{figs/PCnet_filter_boxplot_MNIST_CIFAR10_FC_wrong.pdf}     \\caption{PCnet's perturbation outperforms filter effect on classifying AEs. When AEs, created using misclassified data by the PCnet, are processed with a Gaussian filter, the classification accuracy of the FCnet improves. However, in most cases, the perturbation caused by the PCnet has a greater impact on FCnet classification than the effects of the Gaussian filter. On average, FCnet performs better on AEs perturbed by PCnet than when a Gaussian filter is used. This trend is consistent for AEs from both the MNIST and CIFAR10 datasets.     Note that:     $\\tilde{Z}\\leftarrow\\mathrm{AT_{FCnet}}(\\tilde{X})$,\\;     $\\tilde{G}\\leftarrow\\mathrm{GaussianBlur}(\\tilde{Z})$,\\;     $\\left(\\tilde{P}, \\textrm{and labels}\\right) \\leftarrow \\mathrm{PCnet}(\\tilde{Z})$,\\;     $\\textrm{labels} \\leftarrow \\mathrm{FCnet}(\\cdot)$.     }     \\label{fig:PCnet_filter_boxplot_MNIST_CIFAR10_FC_wrong} }",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the impact of PCnet's perturbation on classifying AEs, comparing it with the effect of a Gaussian filter. The x-axis represents MNIST classes from 0 to 9, along with the average classification accuracy for MNIST and CIFAR10 datasets. The y-axis indicates the classification accuracy percentage. The graph compares the classification accuracy of FCnet on original data Z, PCnet-perturbed data P, and Gaussian filtered data G. For MNIST classes 0 to 9, FCnet(P) generally shows higher classification accuracy than FCnet(Z) and FCnet(G), indicating that PCnet's perturbation has a greater impact on FCnet classification than the Gaussian filter. The average classification accuracy for MNIST and CIFAR10 also shows that FCnet performs better on PCnet-perturbed data than on Gaussian filtered data."
    },
    "256": {
        "figure1": "2411.00222v1_PCnet_filter_boxplot_MNIST_CIFAR10_CNN_wrong",
        "label1": "fig:PCnet_filter_boxplot_MNIST_CIFAR10_CNN_wrong",
        "caption1": "PCnet's perturbation outperforms filter effect on classifying AEs. When AEs, created using misclassified data by the PCnet, are processed with a Gaussian filter, the classification accuracy of the CNN improves. However, in most cases, the perturbation caused by the PCnet has a greater impact on CNN classification than the effects of the Gaussian filter. On average, CNN performs better on AEs perturbed by PCnet than when a Gaussian filter is used. This trend is consistent for AEs from both the MNIST and CIFAR10 datasets. Note that: $\\tilde{Z}\\leftarrow\\mathrm{AT_{CNN}}(\\tilde{X})$,\\; $\\tilde{G}\\leftarrow\\mathrm{GaussianBlur}(\\tilde{Z})$,\\; $\\left(\\tilde{P}, \\textrm{and labels}\\right) \\leftarrow \\mathrm{PCnet}(\\tilde{Z})$,\\; $\\textrm{labels} \\leftarrow \\mathrm{CNN}(\\cdot)$. ",
        "text": "\\centering     \\includegraphics[width=1.0\\columnwidth]{figs/PCnet_filter_boxplot_MNIST_CIFAR10_CNN_wrong.pdf}     \\caption{PCnet's perturbation outperforms filter effect on classifying AEs. When AEs, created using misclassified data by the PCnet, are processed with a Gaussian filter, the classification accuracy of the CNN improves. However, in most cases, the perturbation caused by the PCnet has a greater impact on CNN classification than the effects of the Gaussian filter. On average, CNN performs better on AEs perturbed by PCnet than when a Gaussian filter is used. This trend is consistent for AEs from both the MNIST and CIFAR10 datasets.      Note that:     $\\tilde{Z}\\leftarrow\\mathrm{AT_{CNN}}(\\tilde{X})$,\\;     $\\tilde{G}\\leftarrow\\mathrm{GaussianBlur}(\\tilde{Z})$,\\;     $\\left(\\tilde{P}, \\textrm{and labels}\\right) \\leftarrow \\mathrm{PCnet}(\\tilde{Z})$,\\;     $\\textrm{labels} \\leftarrow \\mathrm{CNN}(\\cdot)$.     }     \\label{fig:PCnet_filter_boxplot_MNIST_CIFAR10_CNN_wrong} }",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the impact of PCnet perturbation on classifying adversarial examples (AEs), compared with the effect of Gaussian filter. The horizontal axis represents MNIST classes 0-9, and the average classification accuracy for MNIST and CIFAR10 datasets(AvgMNIST and AvgCIFAR10), while the vertical axis represents the classification accuracy percentage. Four data sets are included in the figure: CNN(Z), PCnet(Z), CNN(P), and CNN(G). The classification accuracy of CNN(Z) and PCnet(Z) in classes 0-9 is relatively low, while CNN(P) and CNN(G) show higher classification accuracy. AvgMNIST and AvgCIFAR10 show the average classification accuracy in four cases, where CNN(P) and CNN(G) have significantly higher accuracy than CNN(Z) and PCnet(Z). In general, the perturbation of PCnet has a greater impact on CNN classification than the effect of the Gaussian filter, and this trend is consistent for both MNIST and CIFAR10 datasets."
    },
    "257": {
        "figure1": "2411.00222v1_sankey_adversaarial_classified_mnist_FCnet",
        "label1": "fig:AE_FCnet_classifiedMNIST",
        "caption1": "Classification improves over AEs. For the given classified images of the MNIST dataset, the classification improves through the following phases: (1) creating AEs using C\\&W Adversarial attacks on FCnet; (2) classifying AEs using PCnet; (3) classifying perturbed AEs using FCnet.",
        "text": "\\centering     \\includegraphics[width=0.99\\columnwidth]{figs/sankey_adversaarial_classified_mnist_FCnet.pdf}     \\caption{Classification improves over AEs. For the given classified images of the MNIST dataset, the classification improves through the following phases: (1) creating AEs using C\\&W Adversarial attacks on FCnet; (2) classifying AEs using PCnet; (3) classifying perturbed AEs using FCnet.}     \\label{fig:AE_FCnet_classifiedMNIST} }",
        "summarize_figure": "figure1",
        "summarization": "The image presents a Sankey diagram illustrating the classification improvement of adversarial examples (AEs) generated on the MNIST dataset through different processing phases. It tracks the flow of samples in four classification states: A (misclassified targeted AE), B (classified targeted AE), C (misclassified non-targeted AE), and D (classified non-targeted AE). Following the adversarial attack phase, FFnet(Z) (likely FCnet) classifies the AEs, resulting in a large proportion of misclassified targeted samples (A) and fewer correctly classified targeted samples (B), alongside some non-targeted samples (C and D). In Phase 2, PCnet(Z) processes the samples classified by FFnet(Z). A portion of the misclassified targeted samples (A) are corrected to classified (flow to B), while some correctly classified samples (B) become misclassified (flow to A). There are also transitions between non-targeted samples C and D. In Phase 3, FFnet(P) classifies the samples from PCnet(Z). The diagram clearly shows that most misclassified targeted samples (A) are successfully classified as targeted (flow to B), and most misclassified non-targeted samples (C) are classified as correct non-targeted samples (flow to D). This process visually demonstrates a significant improvement in the classification accuracy of adversarial examples, with initially misclassified samples being effectively corrected in later stages."
    },
    "258": {
        "figure1": "2411.00222v1_sankey_adversaarial_classified_mnist_CNN",
        "label1": "fig:AE_CNN_classifiedMNIST",
        "caption1": "Classification improves over AEs. For the given classified images of the MNIST dataset, the classification improves through the following phases: (1) creating AEs using C\\&W Adversarial attacks on CNN; (2) classifying AEs using PCnet; (3) classifying perturbed AEs using CNN.",
        "text": "\\centering     \\includegraphics[width=0.99\\columnwidth]{figs/sankey_adversaarial_classified_mnist_CNN.pdf}     \\caption{Classification improves over AEs. For the given classified images of the MNIST dataset, the classification improves through the following phases: (1) creating AEs using C\\&W Adversarial attacks on CNN; (2) classifying AEs using PCnet; (3) classifying perturbed AEs using CNN.}     \\label{fig:AE_CNN_classifiedMNIST} }",
        "summarize_figure": "figure1",
        "summarization": "This picture illustrates the process by which the classification effectiveness of Convolutional Neural Networks on adversarial examples improves across different stages. The image begins by showing adversarial examples generated via adversarial attack, categorized into four types: A (misclassified targeted AE), B (classified targeted AE), C (misclassified non-targeted AE), and D (classified non-targeted AE). These samples flow through FFnet(Z) (Phase 1), PCnet(Z) (Phase 2), and FFnet(P) (Phase 3). Throughout this process, the flow for categories B and D, representing classified examples, progressively increases, while the flow for categories A and C, representing misclassified examples, significantly decreases. Notably, after processing through PCnet(Z) and FFnet(P), most initially misclassified adversarial examples are ultimately correctly classified, indicating a significant improvement in classification performance during these phases."
    },
    "259": {
        "figure1": "2411.00222v1_sankey_adversaarial_misclassified_mnist_FCnet",
        "label1": "fig:AE_FCnet_misclassifiedMNIST",
        "caption1": "Classification improves over AEs. For the given misclassified images of the MNIST dataset, the classification improves through the following phases: (1) creating AEs using C\\&W Adversarial attacks on FCnet; (2) classifying AEs using PCnet; (3) classifying perturbed AEs using FCnet.",
        "text": "\\centering     \\includegraphics[width=0.99\\columnwidth]{figs/sankey_adversaarial_misclassified_mnist_FCnet.pdf}     \\caption{Classification improves over AEs. For the given misclassified images of the MNIST dataset, the classification improves through the following phases: (1) creating AEs using C\\&W Adversarial attacks on FCnet; (2) classifying AEs using PCnet; (3) classifying perturbed AEs using FCnet.}     \\label{fig:AE_FCnet_misclassifiedMNIST} }",
        "summarize_figure": "figure1",
        "summarization": "The image illustrates the improvement in FCnet's classification of adversarial examples (AEs) across different phases, derived from initially misclassified MNIST images. Presented as a Sankey diagram, it tracks the flow of four categories: A (misclassified targeted AEs), B (classified targeted AEs), C (misclassified non-targeted AEs), and D (classified non-targeted AEs) through AE generation, FFnet classification, PCnet classification, and re-classification by FFnet after perturbation. The diagram shows that the initial AEs (AE(X)) primarily consist of misclassified targeted AEs (A). After passing through intermediate phases of classification by FFnet (FFnet(Z)) and PCnet (PCnet(Z)), a substantial number of images from the misclassified categories A and C transition into the correctly classified categories B and D in the final classification by FFnet (FFnet(P)). Notably, from PCnet(Z) to FFnet(P), the width of categories A and C significantly decreases, while the width of categories B and D substantially increases, demonstrating that the overall process leads to a marked improvement in FFnet's classification accuracy on these adversarial examples."
    },
    "260": {
        "figure1": "2411.00222v1_sankey_adversaarial_misclassified_mnist_CNN",
        "label1": "fig:AE_CNN_misclassifiedMNIST",
        "caption1": "Classification improves over AEs. For the given misclassified images of the MNIST dataset, the classification improves through the following phases: (1) creating AEs using C\\&W Adversarial attacks on CNN; (2) classifying AEs using PCnet; (3) classifying perturbed AEs using CNN.",
        "text": "\\centering     \\includegraphics[width=0.99\\columnwidth]{figs/sankey_adversaarial_misclassified_mnist_CNN.pdf}     \\caption{Classification improves over AEs. For the given misclassified images of the MNIST dataset, the classification improves through the following phases: (1) creating AEs using C\\&W Adversarial attacks on CNN; (2) classifying AEs using PCnet; (3) classifying perturbed AEs using CNN.}     \\label{fig:AE_CNN_misclassifiedMNIST} }",
        "summarize_figure": "figure1",
        "summarization": "English:The image illustrates the classification improvement of adversarial examples (AEs) across three phases. Initially, after a C&W adversarial attack on misclassified MNIST images, the examples are primarily misclassified targeted AEs (A) with some classified targeted AEs (B), misclassified non-targeted AEs (C), and classified non-targeted AEs (D). In Phase 1, classifying these AEs using FFnet(Z) results in some state changes, but a large portion of targeted AEs remain misclassified (A), with a small flow to classified (B), and similar transitions for non-targeted samples. Phase 2, employing PCnet(Z) for classification, shows a notable shift where more misclassified targeted AEs (A) and non-targeted AEs (C) are correctly classified, flowing into the classified targeted (B) and classified non-targeted (D) categories, respectively. The final Phase 3, classifying perturbed AEs using FFnet(P), further improves accuracy, leading to a final distribution where correctly classified samples (B and D) are dominant, and the count of misclassified samples (A and C) is significantly reduced, demonstrating a clear improvement in the classification performance of adversarial examples through these sequential steps."
    },
    "261": {
        "figure1": "2411.00230v1_results",
        "label1": "fig:2_qubit_3_qubit_results_together",
        "caption1": "\\small\\textbf{Results for the 2-qubit (a, b, c, d) and 3-qubit (e, f, g, h) transverse field Ising model (TFIM)}. We compare the reinforcement learning-only approach (blue) with gadget reinforcement learning (GRL) with one (red) and two (green) extracted components, as shown in the legend. We show the energy of the constructed ground state in different initializations of the GRL agent for the TFIM with $h=5\\times10^{-2}$ (a, e) and $h=1$ (b, f). In (c, g), with a fixed compute budget, we compare the error scaling vs. a larger transverse field. The solid lines represent the average over multiple RL runs, the shaded area shows the range of solutions found by the agent (the smallest value is the most relevant one). GRL allows for finding the ground state with $h=1$ (i.e. harder regime) with good accuracy. (d, h) show the value of the RL reward threshold during training for $h=1$. A lower threshold implies that the agent found circuits with lower cost (see main text). For both the 2- and 3-qubit system, we see that, without gadget extraction, no reasonable approximation is found for $h=1$, but the maximum accuracy is $10^{-3}$ ($2$-qubits), or $10^{-1}$ ($3$-qubits). On the other hand, GRL finds a solution with as low as $\\approx 10^{-7}$ error for the 3-qubit system. } % % \\vspace{-5pt",
        "text": "\\caption{\\small\\textbf{Results for the 2-qubit (a, b, c, d) and 3-qubit (e, f, g, h) transverse field Ising model (TFIM)}.          We compare the reinforcement learning-only approach (blue) with gadget reinforcement learning (GRL) with one (red) and two (green) extracted components, as shown in the legend.         We show the energy of the constructed ground state in different initializations of the GRL agent for the TFIM with $h=5\\times10^{-2}$ (a, e) and $h=1$ (b, f).         In (c, g), with a fixed compute budget, we compare the error scaling vs. a larger transverse field. The solid lines represent the average over multiple RL runs, the shaded area shows the range of solutions found by the agent (the smallest value is the most relevant one).         GRL allows for finding the ground state with $h=1$ (i.e. harder regime) with good accuracy.         (d, h) show the value of the RL reward threshold during training for $h=1$. A lower threshold implies that the agent found circuits with lower cost (see main text). For both the 2- and 3-qubit system, we see that, without gadget extraction, no reasonable approximation is found for $h=1$, but the maximum accuracy is $10^{-3}$ ($2$-qubits), or $10^{-1}$ ($3$-qubits). On the other hand, GRL finds a solution with as low as $\\approx 10^{-7}$ error for the 3-qubit system.  }     \\label{fig:2_qubit_3_qubit_results_together}%     % \\vspace{-5pt}\nWe recall that GRL runs iteratively, with the agent and environment specifications as provided in Appendix~\\ref{app:hyperparams_grl_crl}. It  first considers a small $N=2$ qubit system and finds the ground state in the simple regime, e.g. weak transverse field ($h=10^{-3}$). The agent can find the ground state with the target accuracy within our compute budget, given by a fixed number of episodes. Subsequently, we try to solve an intermediate regime with a larger transverse field ($h=5\\times10^{-2}$). We can find a reasonable approximation to the ground state, but the size of the parameterized quantum circuits (PQCs) is large and the error compared is relatively high, especially given the circuit size. At this point, to improve the efficiency of the reinforcement learning algorithm, we analyze the top $k$ parameterized quantum circuits (PQCs) that we obtained in the previous cases and extract the most useful components as new primitive composite gates i.e. \\textit{gadgets}. In particular, we start by adding the gadget with the largest log-likelihood to the RL agent's action space. By running the reinforcement learning agent again with the extended action space, we find a much better approximation of the ground state. Results are summarized in Fig.~\\ref{fig:2_qubit_3_qubit_results_together}(a), where we compared our agent with a state-of-the-art curriculum-based RL approach, as presented in~\\cite{patel2024curriculum}. The figure shows the effectiveness of the gadget extraction features introduced in this work, especially for the hardest regimes.  It should be noted that the gadget extraction is conducted on an easier task.  Based on the outcome on the easy task, the action space is modified and the agent is then used to solve more difficult problems (e.g. $h=1$).\nFinally, we can also generalize to $N=3$ qubits. As Fig.~\\ref{fig:2_qubit_3_qubit_results_together}(b) shows, by also adding a second extracted gate, we can get a very good approximation not only in the easy regime but also in the hardest one also for a 3-qubit system. In contrast, the  RL-only agent was not able to catch any learning signal in that case, because of the necessary depth of the search.  Especially in the hard regime, $h=1$, we see that the pure reinforcement learning agent finds a state with a very large error, probably obtained by randomly assembling gates. In contrast, as we add more gadgets to the action space, we see that RL can find much better solutions, effectively solving the problem.  Please refer to the  Appendix~\\ref{app:numerical-results} for the numerical results of our ablation study.",
        "summarize_figure": "figure1",
        "summarization": "The image shows the number of successes in finding the ground state of the 2-qubit transverse field Ising model with transverse field h = 0.05, as a function of the absolute energy error |E - E_True|. Three approaches are compared: reinforcement learning only using elementary gates, Gadget Reinforcement Learning (GRL) with one extracted gadget, and GRL with two extracted gadgets. For larger energy errors, all three methods achieve a high number of successes. However, as the target error decreases, the advantage of the GRL methods becomes apparent. While the reinforcement learning only method shows a rapid decrease in success rate for errors below 10e-3 and virtually no successes below 10e-5, the GRL methods maintain higher success rates for lower errors. Specifically, GRL with one gadget achieves successes for errors down to about 10e-7, and GRL with two gadgets significantly extends the achievable accuracy, finding solutions with errors as low as 10e-13. This indicates that for the 2-qubit system with h = 0.05, incorporating extracted gadgets, especially two, greatly enhances the ability to find highly accurate ground states compared to using only elementary gates within a reinforcement learning framework."
    },
    "262": {
        "figure1": "2411.00230v1_delta_with_h_log",
        "label1": "fig:TFIM_hardness",
        "caption1": "Energy gap between first excited state and ground state of the TFIM model as a function of the transverse field strength. The separation $\\Delta E$ is negligible till $h=10^{-1}$. Hence, due to energy degeneracy, it is easy to find a good energy approximation for $h\\leq10^{-1}$. The problem becomes harder when we choose $h\\geq10^{-1}$ as $\\Delta E$ becomes non-negligible.}  \\end{center} \\vskip -0.2in",
        "text": "As shown in Fig.~\\ref{fig:TFIM_hardness}, by looking at the ground state energy gap, we can identify three different regimes in the Transverse Field Ising Model:     \\item in the low external field, the first excited state is almost degenerate with the ground state, therefore it is easy to find a low-energy state;     \\item the regime where $h\\simeq 0.1$, where the energy gap increases and the ground state starts to have a visibly different energy from the first excited state;     \\item $h\\gg 0.1$ where the energy gap is larger and the ground state energy is much smaller than that of the first excited state.\\todo{but we also said that the problem becomes easy again for very large $h$...}\n\\vskip 0.2in     \\begin{center}         \\centerline{\\includegraphics[width=0.5\\columnwidth]{figures/delta_with_h_log}}         \\caption{Energy gap between first excited state and ground state of the TFIM model as a function of the transverse field strength. The separation $\\Delta E$ is negligible till $h=10^{-1}$. Hence, due to energy degeneracy, it is easy to find a good energy approximation for $h\\leq10^{-1}$. The problem becomes harder when we choose $h\\geq10^{-1}$ as $\\Delta E$ becomes non-negligible.}         \\label{fig:TFIM_hardness}     \\end{center}     \\vskip -0.2in",
        "summarize_figure": "figure1",
        "summarization": "The image illustrates the relationship between the energy gap (delta E) between the first excited state and the ground state of the Transverse Field Ising Model (TFIM) as a function of the transverse field strength (h). When h is less than or equal to 10 to the power of minus 1, the energy gap delta E is close to zero, indicating energy degeneracy and making it easier to find a good energy approximation. However, when h is greater than or equal to 10 to the power of minus 1, delta E increases significantly, and the problem becomes more complex. The curve shows that as h increases from 10 to the power of minus 3 to 1, delta E exhibits a trend of initially being flat and then rising sharply."
    },
    "263": {
        "figure1": "2411.00238v1_figure2",
        "label1": "counting_results",
        "caption1": "\\textbf{Numerical estimation tasks and results.} Top left: Examples of images generated by text-to-image (T2I) models for different numbers and categories of objects. Top right: Performance of T2I models as a function of the number and category of objects. Results reflect an aggregate of four models (Stable Diffusion Ultra, DALL-E 3, Google Parti, and Google Muse). Bottom left: Examples of images (featuring either 2D or 3D objects) used to evaluate numerosity estimation. Feature entropy was varied in four conditions (low entropy, high entropy, and two medium entropy conditions). Bottom middle: Numerosity estimation results for four multimodal language models (GPT-4v, GPT-4o, Gemini Ultra 1.5, Claude Sonnet 3.5; see Supplementary Figure~\\ref{llava_supp_figure} for results with Llava-1.5). Bottom right: Numerosity estimation results plotted as a function of the number of objects in an image, aggregated across all four models (see Supplementary Figure~\\ref{numerosity_supp_figure} for individual model results). Error bars for all plots reflect 95\\% binomial confidence intervals.",
        "text": "We measured performance by calculating, for each condition, how accuracy varied with the number of objects present in the scene. The results indicated that, regardless of the type of stimuli used (2D vs. 3D shapes, or animals vs. food), and across two fundamentally different types of vision language model (multimodal language models and text-to-image models), VLMs displayed human-like capacity limits (Figure~\\ref{counting_results}). For both multimodal language models and T2I models, accuracy was very high for scenes involving a relatively small number of objects (1-5), but dropped sharply for scenes involving 6 or more objects. Moreover, the multimodal language models exhibited performance consistent with our hypothesis that capacity limits arise due to representational interference across objects (i.e., the binding problem), with overall performance highest in the high-entropy condition (lowest interference), lowest in the low-entropy condition (highest interference), and intermediate in between these two extremes in the medium-entropy conditions. Though there are slight differences between the capacity limits exhibited by these two classes of models, it is striking that they both fall within the subitizing limit of human vision, especially when considering the significant differences in both architecture and training procedures. Furthermore, the effect of feature entropy on these capacity limits strongly suggests that they are driven by representational interference, arising due an inability to manage the binding problem. To further investigate this hypothesis, we next turned to a novel scene description benchmark that allowed us to systematically investigate the likelihood of representational interference, thus enabling a direct test of the extent to which this factor is responsible for the shortcomings of VLMs.",
        "summarize_figure": "figure1",
        "summarization": "Here is a summary of the first image in the figure:The image illustrates the performance of text-to-image (T2I) models with varying numbers and categories of objects. The horizontal axis represents the number of objects, and the vertical axis represents the proportion of correct model judgments. The graph displays the average model performance for two categories of objects: foods and animals. As the number of objects increases, the proportion of correct model judgments decreases significantly, indicating that the accuracy of T2I models is limited when processing scenes containing a large number of objects. The performance for animals is slightly better than that for foods."
    },
    "264": {
        "figure1": "2411.00238v1_figure3",
        "label1": "scene_description_results",
        "caption1": "\\textbf{Scene description task and results.} A) Example image used in 2D scene description task, illustrating the concept of \\textit{feature triplets}: sets of three objects where one pair of objects shares a feature, and another pair shares a different feature. This example contains three feature triplets, demarcated by the dashed lines. 3D scenes were also investigated. B) Scene description results for text-to-image (T2I) models) as a function of the number of feature triplets. C) 2D scene description results for multimodal language models as a function of the number of feature triplets. Left panel illustrates the results aggregated across four models (GPT-4v, GPT-4o, Gemini Ultra 1.5, and Claude Sonnet 3.5). Right panel illustrates the results aggregated across scenes with different numbers of objects. D) 3D scene description results. Error bars represent the standard error of the mean.",
        "text": "We developed a novel scene description task to further investigate the extent to which VLM performance is driven by representational interference. The task is illustrated in Figure~\\ref{scene_description_results}a. For each image, the likelihood of representational interference was quantified as the number of \\textit{feature triplets} present in that image. A feature triplet is defined as any set of three objects for which one pair shares a feature, and another pair shares a different feature. For instance, \\{green X, green triangle, yellow triangle\\} is a feature triplet, because the feature `green' is shared by two objects (the green X and the green triangle), and the feature `triangle' is shared by two objects (the green triangle and the yellow triangle). Without the ability to accurately bind these features together at the level of objects, such feature triplets create opportunities for representational interference, and thus lead to illusory conjunctions. For instance, the feature triplet \\{green X, green triangle, yellow triangle\\} may lead to the erroneous identification of a yellow X. We studied the extent to which the presence of such feature triplets can account for scene description performance in VLMs.\nAs in the previous tasks, we generated datasets involving either 2D sprites or 3D objects. Each scene contained a variable number of objects (10-15 objects for the 2D dataset and 8-12 objects for the 3D dataset), and we systematically varied the number of feature triplets present in each scene. For example, the scene depicted in Figure~\\ref{scene_description_results}a contains three feature triplets (illustrated by the dashed lines). VLMs (GPT-4v, GPT-4o, Gemini Ultra 1.5, and Claude Sonnet 3.5) were prompted to provide a description of the objects in JSON format (see Appendix \\ref{app:prompts} for more details). We also generated prompts describing similar scenes (but involving real-world objects) and tested the ability of the T2I models (Stable Diffusion Ultra, DALL-E 3, Google Parti, and Google Muse) to accurately generate these scenes (as assessed by human evaluation; see Appendix \\ref{app:human} for more details). To obtain a representative sampling of scenes with different triplet counts, we systematically varied the diversity of colors and shapes across trials. This approach ensured adequate sampling of trials with different feature combinations and their associated triplet counts. To ensure reliable performance estimates, we excluded from analysis any triplet counts represented by fewer than 20 trials across all conditions. For the T2I experiments, we additionally excluded trials where models generated more than three extraneous objects not specified in the prompt, as these represented significant deviations from the intended scene structure.\nWe measured scene description performance by calculating how the number of errors (quantified as the edit distance between the true description of the scene and the model's description of the scene) varies as a function of the number of objects present in the scene, and the number of feature triplets. The results (Figure~\\ref{scene_description_results}) confirmed our prediction that performance should vary as a function of the number of triplets. Across multiple stimulus types (2D and 3D objects), and model types (both multimodal language models and text-to-image models), the largest number of errors occurred in the trials where the risk of binding errors was highest (i.e., the trials with the largest number of feature triplets), consistent with the hypothesis that errors would be driven primarily by the formation of illusory conjunctions. These results also showed a pattern of increasing errors as a function of the number of objects, consistent with a general capacity limit on the number of objects that can be accurately represented at the same time. Overall, these results suggest that the capacity limits displayed by VLMs are best explained by an inability to manage the binding problem. In the next section, we apply these insights to better understand the limited visual reasoning capabilities of VLMs.",
        "summarize_figure": "figure1",
        "summarization": "The figure shows an example image from a 2D scene description task, illustrating the concept of feature triplets. The image contains several geometric shapes, such as circles, triangles, x-shapes, and diamonds, each with different colors like red, green, blue, and yellow. Feature triplets are indicated by dashed lines, representing sets of three objects where one pair shares a feature (e.g., color or shape), and another pair shares a different feature. For example, a feature triplet might be \"green X, green triangle, yellow triangle,\" as \"green\" is shared by two objects (green X and green triangle), and \"triangle\" is shared by two objects (green triangle and yellow triangle). There are three such feature triplets in the image, labeled 1, 2, and 3."
    },
    "265": {
        "figure1": "2411.00238v1_supp-4",
        "label1": "llava_supp_figure",
        "caption1": "\\textbf{LLaVA 1.5 Performance.} Performance of LLaVA 1.5 on the visual search and numerosity estimation tasks. Performance was substantially weaker than all other models evaluated.",
        "text": "We tested the extent to which VLMs demonstrate similar capacity constraints to humans in visual search tasks. We evaluated four multimodal language models -- GPT-4v, GPT-4o, Gemini Ultra 1.5, and Claude Sonnet 3.5 -- on a task involving disjunctive and conjunctive search conditions.\\footnote{We also evaluated an open-source multimodal language model -- Llava 1.5 -- but performance was very low for these tasks. These results are presented in Supplementary Figure~\\ref{llava_supp_figure}.}We generated datasets involving either 2D sprites or 3D scenes created in Blender~\\cite{blendercite} (similar to those found in the CLEVR dataset~\\cite{johnson2017clevr}). The datasets were designed to evaluate the ability of the model to detect the presence of a target object among multiple distractors. In half of the images, a target was present, while in the other half, no target was present.",
        "summarize_figure": "figure1",
        "summarization": "The figure contains two subgraphs. The left subgraph shows the results of numerosity estimation. The horizontal axis represents the number of objects, ranging from 1 to 20, and the vertical axis represents the proportion correct. The graph includes three curves corresponding to high, medium, and low entropy conditions. Under high, medium, and low entropy, the proportion correct is higher when the number of objects is small, and as the number of objects increases, the proportion correct rapidly decreases. The right subgraph shows the results of visual search. The horizontal axis represents the number of objects, ranging from 5 to 50, and the vertical axis represents the proportion correct. The graph includes two curves corresponding to conjunctive search and disjunctive search. The accuracy of conjunctive search is between 0.5 and 0.6, and the accuracy of disjunctive search is between 0.6 and 0.7."
    },
    "266": {
        "figure1": "2411.00238v1_supp-2",
        "label1": "search_supp_figure2",
        "caption1": "\\textbf{Visual search results with additional control experiment.} Results for Claude Sonnet 3.5 on 2D visual search tasks, including disjunctive and conjunctive conditions, and an additional disjunctive search condition (`Disjunctive Search Control') in which target and distractor colors were varied between trials.",
        "figure2": "2411.00238v1_figure1",
        "label2": "search_figure",
        "caption2": "\\textbf{Visual search tasks and results.} Example trials for the 2D (top) and 3D (bottom) variants of the disjunctive (left/red column) and conjunctive (middle/blue column) search conditions. Performance for 2D and 3D task variants are plotted on the right. Results reflect aggregate performance for all four VLMs (GPT-4v, GPT-4o, Gemini Ultra 1.5, and Claude Sonnet 3.5; see Supplementary Figure~\\ref{search_supp_figure1} for separate model results). Error bars denote 95\\% binomial confidence intervals.",
        "text": "We measured the performance of each model by calculating, for each condition, how detection accuracy varied as a function of the number of distractors\\footnote{When studying visual search in human participants, a reaction time (RT) paradigm is typically employed, measuring the time required to locate the target object. Because RT measures cannot be straightforwardly obtained from VLMs, we instead measure accuracy, which is sometimes used in human behavioral studies that employ speeded responses. \\cite{mcelree1999temporal}}. The results indicate that performance in the disjunctive search (i.e., popout) condition was perfect, and invariant to the number of distractors. That is, regardless of the number of distractors, all models achieved perfect accuracy in this condition. In contrast, in the conjunctive search condition performance was inversely related to the number of objects: for 5 objects, all models displayed an accuracy of $\\sim$90\\%, but as the number of objects increased, performance dropped substantially. These results were consistently observed for both the 2D (top panel of Figure~\\ref{search_figure}) and 3D (bottom panel of Figure~\\ref{search_figure}) datasets. These results were also replicated in an alternative version of the disjunctive search task, in which target and distractor colors were varied between trials (Supplementary Figure~\\ref{search_supp_figure2}).",
        "summarize_figure": "figure1",
        "summarization": "The upper part of the figure shows the average performance of the 2D model in disjunctive and conjunctive search conditions. In the disjunctive search, the model's accuracy remains at 1.0, unaffected by the number of distractors. In the conjunctive search, when the number of objects is small (e.g., 5), the model's accuracy is around 0.9, but as the number of objects increases, the accuracy drops significantly. For example, when the number of objects reaches 50, the accuracy drops below 0.6. The shaded area represents a 95% binomial confidence interval."
    },
    "267": {
        "figure1": "2411.00246v1_id_columns_99_base",
        "label1": "fig:id_heads_base",
        "caption1": "Heads in early layers show low-dimensional, linear structures, as suggested by similar intrinsic dimension estimates from PCA (\\textbf{L}) and TwoNN (\\textbf{N}). Moving toward the output layer, the true dimensionality peaks and then decreases, while PCA’s linear estimate continues to rise, indicating increasing nonlinearity in head manifolds (\\textbf{Ratio} = $\\frac{L}{N}$). The first principal component (\\textbf{EVR$_1$}) explains around 50\\% of the variance in early layers, dropping to around 10\\% in later layers. ",
        "text": "We start by evaluating the intrinsic dimensionality of head representations across multiple transformer-based vision architectures pre-trained with different objectives (supervised, unsupervised, self-supervised). Namely, we employ OpenAI's CLIP \\citep{radford2021learning}, OpenCLIP \\citep{cherti2023reproducible}, BLIP \\citep{li2022blip}, ViT \\citep{dosovitskiy2021an} and DINOv2 \\citep{oquab2024dinov}, all in their version based on ViT-Large (results on ViT-Base models are in the Appendix in \\Cref{fig:id_heads_base}). We feed them a subset of the training set of ImageNet \\citep{russakovsky2015imagenet} containing 80000 images stratified on the class labels and we extract the representations for all attention heads. Then, we compute the intrinsic dimensionality of such representations using a linear estimator (PCA) and a nonlinear one (TwoNN). Linear $I_d$ is computed as the number of components needed by PCA to explain 99\\% of head variance.     \\includegraphics[width=\\textwidth]{figures/id_columns_99.pdf} } We observe that the \\textit{true} head dimensionality (the one computed with a nonlinear estimator, TwoNN) tends to increase in the first half of the model and to decrease towards the last few layers, following a characteristic hunchback shape, similar to previous findings in other vision architectures \\citep{ansuini2019intrinsic}. However, the number of dimensions returned by the linear estimator grows constantly through the model. This disparity, witnessed by the increasing ratio between the two estimates, indicates that the units of the first layers are close to linear, while in late layers they lie on more curved manifolds. The last column shows the average explained variance ratio (EVR) of the first PCA component and highlights that heads in the first layers are largely explained by this direction, while it still accounts for a nontrivial 10\\% of head variance in late layers.",
        "summarize_figure": "figure1",
        "summarization": "This figure evaluates the intrinsic dimensionality of attention head representations in different layers of OpenCLIP-B and CLIP-B models. It shows four metrics for each model across layers: linear dimension estimation (L), nonlinear dimension estimation (N), the ratio of L to N (Ratio), and the explained variance ratio of the first principal component (EVR1). In early layers, the values of L and N are relatively close, indicating near-linear representations. As the layer number increases, L continues to grow, while N initially increases and then decreases. The increasing Ratio indicates growing nonlinearity in later layers. Additionally, higher EVR1 in early layers suggests that the first principal component explains most of the variance, while decreasing EVR1 in later layers indicates a more complex information distribution."
    },
    "268": {
        "figure1": "2411.00246v1_omp_textspan_2",
        "label1": "sup:fig:ts_omp",
        "caption1": "Comparison between TextSpan and Orthogonal Matching Pursuit on the second principal component (OMP$_2$), applied to the heads of OpenCLIP-L. \\textit{Left}: agreement score between the descriptions returned by the two methods. \\textit{Right}: qualitative comparison of selected descriptions for 4 heads, one per layer, at different agreement levels.",
        "text": "\\centering     % Left subfigure (image)     \\begin{minipage}{0.23\\textwidth}         %\\centering         \\includegraphics[width=\\textwidth]{figures/omp_textspan_1.pdf} % Replace with your image path         \\label{fig:subfig-image}     \\end{minipage}%     % Right subfigure (table)     \\begin{minipage}{0.7\\textwidth}     \\resizebox{\\linewidth}{!}{         \\begin{tabular}{ll} Photo taken in Galápagos Islands & Unspoiled beauty \\\\ Image taken in Norway & Picture taken in Portugal \\\\ Evocative beauty & Crisp autumn leaves \\\\ Vibrant urban energy & Evocative candid gaze \\\\ A skirt & Picture taken in Cyprus \\\\ Picture taken in Cyprus & Picture taken in Cyprus \\\\ Picture taken in Ontario, Canada & Picture taken in the Canadian lakes \\\\ Photo taken in Rio de Janeiro, Brazil & Image taken in the Florida Everglades \\\\ Photo captured in the Arizona desert & Image taken in New England \\\\ Picture captured in the Scottish highlands & Warm and cozy indoor scene \\\\ A photo with the letter F & A photo with the letter F \\\\ A photo with the letter V & A photo with the letter P \\\\ A photo with the letter D & A photo with the letter T \\\\ A photo with the letter T & A photo with the letter X \\\\ A photo with the letter X & A photo with the letter B \\\\ Image showing prairie grouse & Picture of a feline \\\\ Image with a penguin & An image with dogs \\\\ A magnolia & Photo of a furry animal \\\\ An image with dogs & Photo taken in Grand Canyon \\\\ An image with cats & An image with cats \\\\ }     \\end{minipage}     \\caption{Comparison between TextSpan and Ortogonal Matching Pursuit on the first principal component (OMP$_1$), applied to the heads of OpenCLIP-L. \\textit{Left}: agreement score between the descriptions returned by the two methods. \\textit{Right}: qualitative comparison of selected descriptions for 4 heads, one per layer, at different agreement levels. A similar analysis for the second principal component is presented in the Appendix in \\Cref{sup:fig:ts_omp}.}     \\label{fig:ts_omp}\n\\centering     % Left subfigure (image)     \\begin{minipage}{0.18\\textwidth}         %\\centering         \\includegraphics[width=\\textwidth]{figures/omp_textspan_2.pdf} % Replace with your image path     \\end{minipage}%     % Right subfigure (table)     \\begin{minipage}{0.7\\textwidth}     \\resizebox{0.9\\linewidth}{!}{ Photo taken in Galápagos Islands & Picture taken in the Swiss chocolate factories \\\\ Image taken in Norway & Image with Mayan-inspired designs \\\\ Evocative beauty & Stark and minimalist urban scene \\\\ Vibrant urban energy & serene oceanside scene \\\\ A skirt & Vivid cultural ceremony \\\\ Picture taken in Cyprus & Picture taken in Hungary \\\\ Picture taken in Ontario, Canada & Photo taken in the Californian vineyards \\\\ Photo taken in Rio de Janeiro, Brazil & serene woodland refuge \\\\ Photo captured in the Arizona desert & Photo taken in the Australian rainforest \\\\ Picture captured in the Scottish highlands & Photo taken in Canadian Rockies \\\\ A photo with the letter F & A photo with the letter G \\\\ A photo with the letter V & A photo with the letter J \\\\ A photo with the letter D & Photo taken in Monument Valley \\\\ A photo with the letter T & Enchanting fantasy world \\\\ A photo with the letter X & A labyrinth \\\\ Image showing prairie grouse & A capacitor \\\\ Image with a penguin & A spiky texture \\\\ A magnolia & A wolf \\\\ An image with dogs & Image with an ant \\\\ An image with cats & A spirograph-like shape \\\\ }     \\end{minipage}     \\caption{Comparison between TextSpan and Orthogonal Matching Pursuit on the second principal component (OMP$_2$), applied to the heads of OpenCLIP-L. \\textit{Left}: agreement score between the descriptions returned by the two methods. \\textit{Right}: qualitative comparison of selected descriptions for 4 heads, one per layer, at different agreement levels.}     \\label{sup:fig:ts_omp}",
        "summarize_figure": "figure1",
        "summarization": "The figure displays a heatmap comparing the agreement scores between TextSpan and Orthogonal Matching Pursuit (OMP1) on the first principal component, applied to the heads of the OpenCLIP-L model. The vertical axis of the heatmap represents the Head, ranging from 0 to 15, and the horizontal axis represents the Layer, ranging from 20 to 23. The color intensity of each cell in the heatmap represents the magnitude of the agreement score, with darker colors indicating higher scores. It can be observed that there are differences in agreement scores between different heads and layers. For example, the 22nd layer of the 0th head has the highest agreement score, while some heads have low agreement scores across all layers."
    },
    "269": {
        "figure1": "2411.00246v1_pearson_models",
        "label1": "sup:fig:pearson_models",
        "caption1": "Pearson correlation between cross-dataset similarities on different models. Comparison is done between ImageNet and each of the other datasets and averaged over heads. ",
        "text": "The results of the spectral head-to-head comparison between ImageNet and all other datasets on OpenCLIP-L are reported in \\Cref{fig:head_comparison} (results on other models can be found in the Appendix in \\cref{sup:sec:head_comparison}). Rows are ordered according to the mean overall similarity between the corresponding dataset and ImageNet. Interestingly, dataset ordering is consistent across different encoders. The mean correlation coefficient between dataset similarities (averaged over all heads) across different models is 0.97. The full comparison between encoders is reported in the Appendix (\\Cref{sup:fig:pearson_models}). On OpenCLIP-L, we observe that datasets that maximally align with ImageNet share classes with it (e.g., SUN397 and Sketch) and/or contain generic images (e.g., DTD and Cars). Moreover, datasets that share the same input image structure and concepts (CIFAR-10 and CIFAR-100) have an almost identical similarity distribution across heads.  Overall, we observe a decreasing trend in head similarity scores as depth in the model increases. The simple, linear heads of the first layers are responsible for the extraction of low-level patterns \\citep{dosovitskiy2021an} and emerge as almost always identical across different data distributions. On the last layers, just a few heads per dataset stand out: this is where we are looking for specialization. Zooming in on a few of these heads, in \\Cref{tab:descriptions_exp2} we report their textual descriptions obtained with SOMP. Head 7 of layer 22 (specialized on seasons) stands out because it is highly activated in many datasets that contain pictures of scenery (such as SUN397, GTSRB, and more prominently EuroSAT). Head 11 of layer 22 (specialized on shades of gray) emerges as extremely different between ImageNet and Sketch, that contains grayscale drawings of ImageNet classes. Head 10 of layer 23 (specialized on numbers) is highly activated on both MNIST and SVHN. We note that this is not the only 'shared' head between the two, but others, like head 1 of the same layer, are also activated by the random dataset, signaling that they are not as specific.",
        "summarize_figure": "figure1",
        "summarization": "This figure displays the Pearson correlation coefficients of cross-dataset similarities between different models including OpenCLIP-L, CLIP-L, BLIP-L, DINOv2-L, and ViT-L. The similarities are based on comparisons between ImageNet and other datasets, averaged over heads. The heatmap clearly indicates the degree of correlation between these models using color intensity and numerical values. It is evident that all models exhibit very high correlation in these similarity measures, with values generally above 0.9 and many close to 1.00, reflecting a high degree of consistency among different encoders in evaluating inter-dataset similarity."
    },
    "270": {
        "figure1": "2411.00246v1_blip_l_flickr_allheads",
        "label1": "sup:fig:head_comparison_blipl",
        "caption1": "Attention head similarity across layers of BLIP-L, computed between ImageNet head representations and those obtained on other datasets.",
        "figure2": "2411.00246v1_openai_l_allheads",
        "label2": "sup:fig:head_comparison_clipl",
        "caption2": "Attention head similarity across layers of CLIP-L, computed between ImageNet head representations and those obtained on other datasets.",
        "text": "\\includegraphics[width=0.9\\textwidth]{figures/blip_l_flickr_allheads.pdf}     \\caption{Attention head similarity across layers of BLIP-L, computed between ImageNet head representations and those obtained on other datasets.}\\label{sup:fig:head_comparison_blipl}     \\includegraphics[width=0.9\\textwidth]{figures/openai_l_allheads.pdf}     \\caption{Attention head similarity across layers of CLIP-L, computed between ImageNet head representations and those obtained on other datasets.}\\label{sup:fig:head_comparison_clipl}",
        "summarize_figure": "figure1",
        "summarization": "The picture illustrates the attention head similarity across layers of the BLIP-L model, computed by calculating the similarity between ImageNet head representations and those obtained on other datasets. The x-axis represents the layers of the BLIP-L model, ranging from 0 to 23, and the y-axis represents different datasets, including SUN397, Cars, DTD, Sketch, CIFAR100, CIFAR10, RESISC45, PACS, SVHN, GTSRB, EuroSAT, MNIST, and Random. The color depth indicates the magnitude of the similarity, with darker colors indicating higher similarity and lighter colors indicating lower similarity. Observing the picture, it can be found that the similarity gradually decreases as the number of layers increases, especially after layer 20, where the attention heads on most datasets have low similarity with ImageNet. For different datasets, the trend of similarity decreasing is also slightly different."
    },
    "271": {
        "figure1": "2411.00246v1_openai_l_allheads",
        "label1": "sup:fig:head_comparison_clipl",
        "caption1": "Attention head similarity across layers of CLIP-L, computed between ImageNet head representations and those obtained on other datasets.",
        "figure2": "2411.00246v1_blip_l_flickr_allheads",
        "label2": "sup:fig:head_comparison_blipl",
        "caption2": "Attention head similarity across layers of BLIP-L, computed between ImageNet head representations and those obtained on other datasets.",
        "text": "\\includegraphics[width=0.9\\textwidth]{figures/blip_l_flickr_allheads.pdf}     \\caption{Attention head similarity across layers of BLIP-L, computed between ImageNet head representations and those obtained on other datasets.}\\label{sup:fig:head_comparison_blipl}     \\includegraphics[width=0.9\\textwidth]{figures/openai_l_allheads.pdf}     \\caption{Attention head similarity across layers of CLIP-L, computed between ImageNet head representations and those obtained on other datasets.}\\label{sup:fig:head_comparison_clipl}",
        "summarize_figure": "figure1",
        "summarization": "This is a study on the similarity of attention heads between different layers in the BLIP-L model. The picture shows the similarity between ImageNet head representations and head representations from other datasets. The horizontal axis represents the number of layers in the BLIP-L model, from 0 to 23, and the vertical axis represents different datasets, including SUN397, Cars, DTD, Sketch, CIFAR100, CIFAR10, RESISC45, PACS, SVHN, GTSRB, EuroSAT, MNIST, and Random. The color depth indicates the magnitude of the similarity, with darker colors indicating higher similarity. As can be seen from the picture, the similarity gradually decreases as the number of layers increases, especially in the higher layers, the similarity decreases significantly. The trend of similarity change also varies for different datasets."
    },
    "272": {
        "figure1": "2411.00246v1_openai_b_allheads",
        "label1": "sup:fig:head_comparison_clipb",
        "caption1": "Attention head similarity across layers of CLIP-B, computed between ImageNet head representations and those obtained on other datasets.",
        "figure2": "2411.00246v1_openclip_b_allheads",
        "label2": "sup:fig:head_comparison_openclipb",
        "caption2": "Attention head similarity across layers of OpenCLIP-B, computed between ImageNet head representations and those obtained on other datasets.",
        "text": "\\includegraphics[width=0.6\\textwidth]{figures/openai_b_allheads.pdf}     \\caption{Attention head similarity across layers of CLIP-B, computed between ImageNet head representations and those obtained on other datasets.}\\label{sup:fig:head_comparison_clipb}     \\includegraphics[width=0.6\\textwidth]{figures/openclip_b_allheads.pdf}     \\caption{Attention head similarity across layers of OpenCLIP-B, computed between ImageNet head representations and those obtained on other datasets.}\\label{sup:fig:head_comparison_openclipb}",
        "summarize_figure": "figure1",
        "summarization": "Summary:The figure illustrates the attention head similarity across layers in the CLIP-B model, computed by comparing ImageNet head representations with those from other datasets. The horizontal axis represents the layers of the CLIP-B model, ranging from 0 to 11. The vertical axis represents different datasets, including SUN397, DTD, Cars, Sketch, CIFAR100, CIFAR10, PACS, RESISC45, GTSRB, EuroSAT, SVHN, MNIST, and Random. The color intensity indicates the degree of similarity between attention heads, with darker colors representing higher similarity and lighter colors representing lower similarity. It can be observed that for most datasets, attention heads in earlier layers (e.g., layer 0) have higher similarity with those of ImageNet, while the similarity gradually decreases as the layer number increases. For the MNIST dataset, the similarity is low across all layers."
    },
    "273": {
        "figure1": "2411.00246v1_openclip_b_allheads",
        "label1": "sup:fig:head_comparison_openclipb",
        "caption1": "Attention head similarity across layers of OpenCLIP-B, computed between ImageNet head representations and those obtained on other datasets.",
        "figure2": "2411.00246v1_openai_b_allheads",
        "label2": "sup:fig:head_comparison_clipb",
        "caption2": "Attention head similarity across layers of CLIP-B, computed between ImageNet head representations and those obtained on other datasets.",
        "text": "\\includegraphics[width=0.6\\textwidth]{figures/openai_b_allheads.pdf}     \\caption{Attention head similarity across layers of CLIP-B, computed between ImageNet head representations and those obtained on other datasets.}\\label{sup:fig:head_comparison_clipb}     \\includegraphics[width=0.6\\textwidth]{figures/openclip_b_allheads.pdf}     \\caption{Attention head similarity across layers of OpenCLIP-B, computed between ImageNet head representations and those obtained on other datasets.}\\label{sup:fig:head_comparison_openclipb}",
        "summarize_figure": "figure1",
        "summarization": "The image illustrates the attention head similarity across layers of the CLIP-B model, computed between ImageNet head representations and those obtained on other datasets. As shown in the image, the attention head similarity between ImageNet and other datasets is generally high in the lower layers (layers 0-2), with darker colors indicating more consistent representations. However, as the number of layers increases, the similarity gradually decreases, with lighter colors indicating an increase in representation differences between different datasets. The similarity is lowest in the higher layers (layers 8-11), indicating that the attention heads in these layers are more sensitive to specific datasets."
    },
    "274": {
        "figure1": "2411.00246v1_dinov2_l_allheads",
        "label1": "sup:fig:head_comparison_dino",
        "caption1": "Attention head similarity across layers of DINOv2-L, computed between ImageNet head representations and those obtained on other datasets.",
        "text": "\\includegraphics[width=\\textwidth]{figures/dinov2_l_allheads.pdf}     \\caption{Attention head similarity across layers of DINOv2-L, computed between ImageNet head representations and those obtained on other datasets.}\\label{sup:fig:head_comparison_dino}",
        "summarize_figure": "figure1",
        "summarization": "The image illustrates the attention head similarity across different layers of the DINOv2-L model, computed by comparing head representations on the ImageNet dataset with those on other datasets. The horizontal axis represents the model layers, ranging from 0 to 23. The vertical axis represents various datasets, including SUN397, DTD, Cars, CIFAR100, Sketch, CIFAR10, RESISC45, PACS, GTSRB, SVHN, EuroSAT, MNIST, and Random. The color indicates the degree of similarity, with darker colors representing higher similarity and lighter colors representing lower similarity. The figure shows that the attention head similarity exhibits certain trends as the number of layers increases, and the changes in similarity vary across different datasets."
    },
    "275": {
        "figure1": "2411.00246v1_vit_l_allheads",
        "label1": "sup:fig:head_comparison_vitl",
        "caption1": "Attention head similarity across layers of ViT-L, computed between ImageNet head representations and those obtained on other datasets.",
        "text": "\\includegraphics[width=\\textwidth]{figures/vit_l_allheads.pdf}     \\caption{Attention head similarity across layers of ViT-L, computed between ImageNet head representations and those obtained on other datasets.}\\label{sup:fig:head_comparison_vitl}",
        "summarize_figure": "figure1",
        "summarization": "The picture illustrates the attention head similarity across layers of the ViT-L model, computed between the ImageNet dataset and other datasets. The horizontal axis represents the layers of the ViT-L model (from 0 to 23), and the vertical axis represents different datasets, including SUN397, CIFAR100, CIFAR10, Cars, DTD, Sketch, RESISC45, PACS, GTSRB, EuroSAT, SVHN, MNIST, and Random. The color intensity indicates the similarity of attention heads, with darker colors indicating higher similarity and vice versa. It can be observed that for most datasets, the attention head similarity in the initial layers is relatively high, and the similarity gradually decreases as the layers deepen."
    },
    "276": {
        "figure1": "2411.00246v1_all_diamond_base",
        "label1": "sup:fig:residual",
        "caption1": "Performance comparison between text-image alignment methods on zero-shot classification tasks. ",
        "text": "For this experiment, we work with 3 CLIP-like models, BLIP-L, CLIP-L and OpenCLIP-L (CLIP/OpenCLIP-B results can be found in the Appendix in \\Cref{sup:tab:residual} and \\Cref{sup:fig:residual}), and tune them on the 10 datasets employed in \\Cref{ssec:coarse_selection}. All training runs use the Schedule-Free Adam optimizer \\citep{defazio2024road} with the automatic learning rate finder by \\citet{smith2017cyclicallearningratestraining}, implemented in PyTorch Lightning \\citep{Falcon_PyTorch_Lightning_2019}. The maximum number of epochs is 30, with an early-stopping policy on the validation set accuracy with a patience of 5 epochs.",
        "summarize_figure": "figure1",
        "summarization": "The image displays a radar chart illustrating the performance of four text-image alignment methods on zero-shot classification across ten datasets using the CLIP-B model. The methods compared are Base, Linear Aligner, Full Finetuning, and ResiDual. The Base model shows the lowest performance across all datasets. Linear Aligner offers some improvement over the Base model. The ResiDual method consistently outperforms both Base and Linear Aligner on most datasets. Full Finetuning generally achieves the highest performance among the four methods, often exceeding ResiDual, particularly on datasets such as EuroSAT, DTD, CIFAR100, and GTSRB. On other datasets like CIFAR10, SVHN, SUN397, Cars, RESISC45, and MNIST, ResiDual's performance is competitive with or very close to that of Full Finetuning."
    },
    "277": {
        "figure1": "2411.00252v1_graph",
        "label1": "fig:your-label",
        "caption1": "Max accuracy over epochs trained",
        "text": "\\centering   \\includegraphics[width=0.8\\textwidth]{figures/graph.png}   \\caption{Max accuracy over epochs trained}   \\label{fig:your-label}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the maximum accuracy of different models over training epochs. The accuracy of all models increases with training and stabilizes after approximately 20 epochs. The SwinV2L_one_added_layer model performs relatively well, with slightly higher accuracy compared to other models. The IO_transformer_V8_with_cyclic_shifts model has the lowest accuracy among all models."
    },
    "278": {
        "figure1": "2411.00254v1_beforeaug_training",
        "label1": "fig9",
        "caption1": "Classification model performance before augmentation.",
        "figure2": "2411.00254v1_train_afteraug",
        "label2": "fig10",
        "caption2": "Classification model performance after proposed XAI-based augmentation.",
        "text": "Fig. \\ref{fig9} and \\ref{fig10} present classification model performance before and after augmentation respectively. Fig. \\ref{fig9} shows that before augmentation the model struggles with learning, showing instability in accuracy and only minimal decreases in loss, suggesting that augmentation plays a critical role in improving model performance. However, Fig. \\ref{fig10} shows significant improvements, with high and stable accuracy and rapidly decreasing loss, indicating effective learning and good generalization. Further, before augmentation, the model archives a training and validation accuracy of 46.39\\% and 39.58\\% whereas, after augmentation training and validation accuracy improved to 99.38\\%, and 92.49\\% respectively and the test dataset resulted in a test accuracy of 92.47\\%.",
        "summarize_figure": "figure1",
        "summarization": "This is a comparative graph of the classification model's performance before and after augmentation. The picture shows that before augmentation, the model's learning effect is not good, the accuracy is unstable, and the loss value decreases very little. Specifically, the training accuracy fluctuates between 0.475 and 0.525, and the validation accuracy fluctuates between 0.425 and 0.575, with a large gap between the two. This indicates that augmentation is critical to improving model performance."
    },
    "279": {
        "figure1": "2411.00254v1_train_afteraug",
        "label1": "fig10",
        "caption1": "Classification model performance after proposed XAI-based augmentation.",
        "figure2": "2411.00254v1_beforeaug_training",
        "label2": "fig9",
        "caption2": "Classification model performance before augmentation.",
        "text": "Fig. \\ref{fig9} and \\ref{fig10} present classification model performance before and after augmentation respectively. Fig. \\ref{fig9} shows that before augmentation the model struggles with learning, showing instability in accuracy and only minimal decreases in loss, suggesting that augmentation plays a critical role in improving model performance. However, Fig. \\ref{fig10} shows significant improvements, with high and stable accuracy and rapidly decreasing loss, indicating effective learning and good generalization. Further, before augmentation, the model archives a training and validation accuracy of 46.39\\% and 39.58\\% whereas, after augmentation training and validation accuracy improved to 99.38\\%, and 92.49\\% respectively and the test dataset resulted in a test accuracy of 92.47\\%.",
        "summarize_figure": "figure1",
        "summarization": "English Summary:The image depicts the model's performance before data augmentation. The left subplot shows that both training and validation accuracy fluctuate significantly over time, staying within a low range, primarily between 0.4 and 0.55, with validation accuracy often below training accuracy and exhibiting large drops, indicating unstable learning. The right subplot illustrates that both training and validation loss decrease slowly over time, with loss values remaining relatively high, ranging from 179.5 to 181.5, reflecting the model's inability to effectively reduce errors. At the end of training, the training accuracy reached only 46.39 percent and validation accuracy 39.58 percent. Collectively, these results suggest poor learning and generalization ability of the model before augmentation."
    },
    "280": {
        "figure1": "2411.00254v1_11",
        "label1": "fig11",
        "caption1": "(a) and (b) are the confusion matrix of the classification model before and after augmentation respectively.",
        "text": "We examine the confusion matrix of benign, and malignant for the ResNet50 classification models shown in Fig. \\ref{fig11}. Fig. \\ref{fig11} (a) shows that before augmentation 53 samples were correctly classified as benign and 68 samples were incorrectly classified as malignant rest of the part are 0. Whereas, Fig. \\ref{fig11} (b) shows a more balanced performance after augmentation, with a significant number of correct predictions for both benign and malignant cases.",
        "summarize_figure": "figure1",
        "summarization": "This image presents the confusion matrix for the classification model before data augmentation. Based on the matrix data, 53 true benign samples were correctly classified as benign by the model. However, all 68 true malignant samples were incorrectly predicted as benign, and no samples were predicted as malignant. This indicates that before data augmentation, the model performed extremely poorly in identifying malignant cases and showed a significant bias towards the benign class."
    },
    "281": {
        "figure1": "2411.00274v1_Softmax_cut_unknown_5_6_7_8_9_label_smoothing_04_2",
        "label1": "fig:softmax_cut",
        "caption1": "setup{justification=centering} \\caption{Scatter Plot of MSP with Untrained Labels 6$\\sim$10",
        "text": "\\centering     \\includegraphics[width=0.9\\columnwidth]{Softmax_cut_unknown_5_6_7_8_9_label_smoothing_04_2.png}     \\captionsetup{justification=centering}     \\caption{Scatter Plot of MSP with Untrained Labels 6$\\sim$10}     \\label{fig:softmax_cut}\n\\centering     \\includegraphics[width=0.9\\columnwidth]{Softmax_cut_unknown_0_1_2_3_4_label_smoothing_04_2.png}     \\captionsetup{justification=centering}     \\caption{Scatter Plot of MSP with Untrained Labels 1$\\sim$5}     \\label{fig:softmax_cut}",
        "summarize_figure": "figure1",
        "summarization": "The figure is a scatter plot with the index of test data on the x-axis and the maximum Softmax value on the y-axis. Blue dots represent known targets, and orange dots represent unknown targets. The plot also includes a horizontal dashed line, representing the optimal threshold, which is approximately at 0.43. The Softmax values for known targets are mainly concentrated between 0.3 and 0.8, while the Softmax values for unknown targets are distributed between 0.2 and 0.7. Some known targets have Softmax values below the optimal threshold, and some unknown targets have Softmax values above the optimal threshold."
    },
    "282": {
        "figure1": "2411.00274v1_Softmax_cut_unknown_0_1_2_3_4_label_smoothing_04_2",
        "label1": "fig:softmax_cut",
        "caption1": "setup{justification=centering} \\caption{Scatter Plot of MSP with Untrained Labels 1$\\sim$5",
        "text": "\\centering     \\includegraphics[width=0.9\\columnwidth]{Softmax_cut_unknown_5_6_7_8_9_label_smoothing_04_2.png}     \\captionsetup{justification=centering}     \\caption{Scatter Plot of MSP with Untrained Labels 6$\\sim$10}     \\label{fig:softmax_cut}\n\\centering     \\includegraphics[width=0.9\\columnwidth]{Softmax_cut_unknown_0_1_2_3_4_label_smoothing_04_2.png}     \\captionsetup{justification=centering}     \\caption{Scatter Plot of MSP with Untrained Labels 1$\\sim$5}     \\label{fig:softmax_cut}",
        "summarize_figure": "figure1",
        "summarization": "The figure displays a scatter plot with the index of test data on the x-axis and the maximum Softmax value on the y-axis. Blue points represent known targets, while orange points represent unknown targets. An optimal threshold is indicated by a black dashed line. The Softmax values for known targets are mainly above 0.4, whereas those for unknown targets are mainly below 0.4. The optimal threshold is around 0.37."
    },
    "283": {
        "figure1": "2411.00284v2_loss",
        "label1": "fig:loss",
        "caption1": "Loss curve of FSDP2 and \\sys{} on Llama 3.1 8B.",
        "text": "\\centering     \\includegraphics[width=0.6\\linewidth]{figs/loss.pdf}     \\vspace{-0.3cm}     \\caption{Loss curve of FSDP2 and \\sys{} on Llama 3.1 8B.}     \\label{fig:loss}",
        "summarize_figure": "figure1",
        "summarization": "The image illustrates the loss curves of FSDP2 and SimpleFSDP strategies on the Llama 3.1 8B model. The horizontal axis represents the training epoch, and the vertical axis represents the loss value. As can be seen from the image, the loss values of both strategies decrease rapidly with the increase of training epochs. In the initial stage, the loss value decreases rapidly, and then the decreasing speed slows down and gradually becomes flat. The two curves almost overlap, indicating that the performance of SimpleFSDP strategy is very close to that of FSDP2 strategy in terms of loss value."
    },
    "284": {
        "figure1": "2411.00300v1_figure3-edit",
        "label1": "figure:topk",
        "caption1": " The accuracy (y-axis) of Llama-3-8B-Instruct and Meerkat-7B with/without different RAG models when varying the number of top-k snippets (x-axis). ``RAG$^2$ w/o filter'' is our RAG framework only that does not the filtering model. The performance on the MMLU-Med dataset can be found in the appendix. ",
        "figure2": "2411.00300v1_figure-mmlumed",
        "label2": "figure:medmmlu",
        "caption2": " Accuracy (y-axis) of Llama-3-8B-Instruct and Meerkat-7B with/without different RAG models when varying the number of top-k documents (x-axis) on the MMLU-Med dataset. Since MMLU-Med only consists of the test set, we used the filtering model trained on MedMCQA during evaluation on MMLU-Med. The accuracy results indicate that the effective filter trained from a different dataset (MedMCQA) performs well across other dataset (MMLU-Med). ",
        "text": "Figure~\\ref{figure:topk} illustrates the performance of various RAG methods across different top-k values, using Llama-3-8B-Instruct and Meerkat-7B as backbone LLMs.  MedRAG demonstrates inconsistent performance, often underperforming compared to LLMs without retrieval capabilities.  In contrast, the use of rationale queries generally enhances the performance of the baseline LLMs, leading to improvements of up to 5.7\\% on MedQA and 4.3\\% on MedMCQA for Llama-3-8B-Instruct, and up to 3.4\\% on MedQA and 3.8\\% on MedMCQA for Meerkat-7B.  Further application of rationale-guided filtering results in additional performance gains, with Llama-3-8B achieving up to 6.9\\% on MedQA and 5.8\\% on MedMCQA, and Meerkat-7B reaching up to 4.4\\% on MedQA and 4.5\\% on MedMCQA.\nFigure~\\ref{figure:medmmlu} shows the performance of baseline models with and without RAG methods as the number of retrieved documents is varied. Note that our filtering model is trained using MedMCQA since the MMLU-Med training set does not exist. Similar to the results observed in MedQA and MedMCQA (Figure~\\ref{figure:topk}), our RAG$^2$ model achieves the best performance, demonstrating the effectiveness of our approach even in out-of-domain distributions.",
        "summarize_figure": "figure1",
        "summarization": "The image illustrates the accuracy of Llama-3-8B and Meerkat-7B models with different RAG methods (MedRAG, RAG^2 w/o filter, RAG^2) and a No RAG baseline on the MEDQA and MedMCQA datasets as the number of top-k snippets varies. The figure shows that MedRAG generally performs poorly, with accuracy often falling below the No RAG baseline. In contrast, the RAG^2 w/o filter method demonstrates better accuracy than the No RAG baseline across both datasets and models. The full RAG^2 method, which incorporates a filtering module, typically provides additional performance gains over RAG^2 w/o filter, achieving the highest accuracy and indicating the effectiveness of the filtering strategy. Accuracy tends to initially increase and then slightly decrease as the number of top-k snippets grows. For example, on the MEDQA dataset, the RAG^2 method reaches a peak accuracy of 64.6 for Llama-3-8B and 75.2 for Meerkat-7B. The proposed RAG^2 method demonstrates significant accuracy improvements for both models across the different datasets."
    },
    "285": {
        "figure1": "2411.00300v1_figure-mmlumed",
        "label1": "figure:medmmlu",
        "caption1": " Accuracy (y-axis) of Llama-3-8B-Instruct and Meerkat-7B with/without different RAG models when varying the number of top-k documents (x-axis) on the MMLU-Med dataset. Since MMLU-Med only consists of the test set, we used the filtering model trained on MedMCQA during evaluation on MMLU-Med. The accuracy results indicate that the effective filter trained from a different dataset (MedMCQA) performs well across other dataset (MMLU-Med). ",
        "figure2": "2411.00300v1_figure3-edit",
        "label2": "figure:topk",
        "caption2": " The accuracy (y-axis) of Llama-3-8B-Instruct and Meerkat-7B with/without different RAG models when varying the number of top-k snippets (x-axis). ``RAG$^2$ w/o filter'' is our RAG framework only that does not the filtering model. The performance on the MMLU-Med dataset can be found in the appendix. ",
        "text": "Figure~\\ref{figure:medmmlu} shows the performance of baseline models with and without RAG methods as the number of retrieved documents is varied. Note that our filtering model is trained using MedMCQA since the MMLU-Med training set does not exist. Similar to the results observed in MedQA and MedMCQA (Figure~\\ref{figure:topk}), our RAG$^2$ model achieves the best performance, demonstrating the effectiveness of our approach even in out-of-domain distributions.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the accuracy of the Llama-3-8B model on the MMLU-Med dataset with varying numbers of retrieved documents (k) for different RAG models, including MedRAG, No RAG, RAG squared, and RAG squared without filter. MedRAG performs worse than No RAG, while RAG squared outperforms the other models. As the number of retrieved documents increases, MedRAG's accuracy initially rises and then falls, peaking at k equals 8. RAG squared consistently outperforms RAG squared without the filter across different k values, indicating that the filter trained on the MedMCQA dataset performs well on the MMLU-Med dataset, effectively enhancing the performance of RAG squared."
    },
    "286": {
        "figure1": "2411.00300v1_figure-longform",
        "label1": "figure:longform",
        "caption1": " Performance of Llama-3-8B using different RAG pipelines on ClinicalQA25, which represents a real-world clinical scenario. ",
        "text": "Figure~\\ref{figure:longform} demonstrates that RAG, including both our proposed approach and MedRAG, outperforms the model-alone baseline. This suggests that RAG is effective in retrieving accurate and relevant information for open-ended questions. Additionally, the integration of our filtering mechanism seems to further improve performance, potentially enhancing retrieval accuracy. This experiment demonstrates that our filtering model, although trained on multiple-choice QA, still shows effectiveness in long-form and real-world clinical queries. In future work, we plan to further validate the broader applicability of our approach.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the performance of the Llama-3-8B model on the ClinicalQA25 dataset using different RAG pipelines. It compares five settings: No RAG, MedRAG, MedRAG w/ Filter, Ours w/o Filter, and Ours. According to the ROUGE-L metric, the \"Ours\" setting outperforms all other settings, significantly higher than \"No RAG\". Similarly, based on the BERTScore metric, the \"Ours\" setting shows the best performance, while \"No RAG\" has the worst performance. This indicates that using RAG, especially the \"Ours\" setting, can significantly improve the model's performance on this dataset."
    },
    "287": {
        "figure1": "2411.00304v1_section01_1",
        "label1": "fig:section01",
        "caption1": " (a) In WebQA~\\cite{WebQA21}, the accuracy roughly forms a “U” shape curve when the relevant image-text pair for a question appears at different positions. While our model also shows similar trends, it tends to be more stable overall. (b) The accuracy of various types of questions in MMVP-VLM~\\cite{tong2024eyes}, it can be observed that our model's performance improves on such tasks after introducing the discriminative training. Details can be seen in Appendix~\\ref{app_intro}}  \\vspace{-5mm",
        "text": "Some research~\\cite{li2023fine, wang2023makes} has found that the existing generative training framework cannot fully distinguish input semantics in certain contexts, causing MLLMs to struggle with tasks requiring fine-grained semantics~\\cite{li2022fine, yu2023visually}. As depicted in Figure~\\ref{fig:section01}(b), we noticed that MLLMs face challenges in choosing the right description for two similar images in the MMVP-VLM benchmark~\\cite{tong2024eyes}. This indicates that MLLMs struggle to effectively differentiate the detailed semantics of input samples, naturally leading to difficulties in forming effective queries for retrieval. \t\\centering   \t\\vspace{-4mm} \t\\includegraphics[width=0.99\\textwidth]{figures/section01_1.pdf} \t\\vspace{-1mm} \t\\caption{ \t\t(a) In WebQA~\\cite{WebQA21}, the accuracy roughly forms a “U” shape curve when the relevant image-text pair for a question appears at different positions. While our model also shows similar trends, it tends to be more stable overall. \t\t(b) The accuracy of various types of questions in MMVP-VLM~\\cite{tong2024eyes}, it can be observed that our model's performance improves on such tasks after introducing the discriminative training. Details can be seen in Appendix~\\ref{app_intro}} \t\\label{fig:section01}   \t\\vspace{-5mm}",
        "summarize_figure": "figure1",
        "summarization": "In the WebQA dataset, figure (a) illustrates the relationship between model accuracy and the position of relevant image-text pairs in the question. The accuracy of the VILA model roughly forms a \"U\" shape curve, indicating performance fluctuations when relevant information appears at different positions. In contrast, the proposed model also shows similar trends, but it exhibits more stable performance overall with a flatter accuracy curve. This implies that the proposed model has a lower dependence on the position of relevant information and is more robust."
    },
    "288": {
        "figure1": "2411.00308v1_Fig_General",
        "label1": "fig:trend",
        "caption1": "The General Trends of GPT for Games. By September, 76 new studies were published in 2024, up from 39 in 2023, showing a significant growth trend compared to previous years. There was also rising interest in the latest GPT models like GPT-3.5 and GPT-4.",
        "text": "As shown in Figure~\\ref{fig:trend}, from January to September 2024, there have been 76 relevant studies, reflecting a huge increase compared to 39 studies in 2023. Additionally, the interest in new models, such as GPT-3.5 and GPT-4, has also grown. This sharp rise aligns with the performance improvements of GPT models~\\cite{gpt2vsgpt3}, suggesting that studies will explore the application of the newest GPT models in games in the future.",
        "summarize_figure": "figure1",
        "summarization": "As shown in the image, the number of publications related to GPT models shows a significant growth trend from 2020 to 2024. Specifically, there were 6 relevant publications in 2020, 5 in 2021, and 5 in 2022. However, starting from 2023, the number of publications increased sharply to 39, and further climbed to 76 in 2024 (as of September), indicating strong growth momentum. Regarding model distribution, early research (2020-2022) primarily focused on GPT-2 and GPT-3. By 2023, newer models like ChatGPT, GPT-3.5, and GPT-4 began to contribute significantly to the publication count. In 2024, GPT-3.5 and GPT-4 became research hotspots, with 25 and 34 publications respectively, far exceeding other models and reflecting a shift in research interest towards the latest, more powerful GPT models."
    },
    "289": {
        "figure1": "2411.00308v1_Fig_General_Year",
        "label1": "fig:trendyear",
        "caption1": "The General Trends of Each Category. In 2024, most papers focused on PCG and MIG, with growing interest in MIGDD, PG, and GUR. Three papers were counted in both PCG and MIG. One paper was counted in both PCG and GUR. In all, 49 papers focused on PCG, 29 on MIGDD, 34 on MIG, 14 on GPT playing games, and nine on GUR.",
        "text": "As shown in Figure~\\ref{fig:trendyear}, 49 papers focused on using GPT for procedural content generation (PCG). 29 papers focused on using GPT for mixed-initiative game design and development (MIGDD). 34 papers used GPT for mixed-initiative gameplay (MIG). 14 papers used GPT to playing games (PG). Finally, nine papers used GPT for game user research (GUR). Three papers were counted in both PCG and MIG. One paper was counted in both PCG and GUR.",
        "summarize_figure": "figure1",
        "summarization": "This is a bar graph showing the number of publications in various categories from 2020 to 2024. The picture shows that PCG (procedural content generation) has the highest number of publications, with a total of 49, including 24 publications in 2024. MIGDD (mixed-initiative game design and development) has 29 publications, including 18 publications in 2024. MIG (mixed-initiative gameplay) has 34 publications, including 24 publications in 2024. PG (playing games) has 14 publications, including 8 publications in 2024. GUR (game user research) has the fewest publications, with a total of 9, including 6 publications in 2024."
    },
    "290": {
        "figure1": "2411.00311v1_increase_local_epochs",
        "label1": "analysis:num_epochs",
        "caption1": "Evaluation results of the test accuracy with different numbers of local epochs.",
        "text": "One of the crucial aspects in FL is communication efficiency. A simple way to achieve such efficiency is to reduce communication rounds while increasing local epochs. However, the increased local updates can result in greater susceptibility to client drifts \\cite{li2021model}. Thus we examine the trade-off between local epochs and communication rounds, as shown in Figure \\ref{analysis:num_epochs}. We compare C2A with three baselines under the same number of model updates (local epochs $\\times$ communication rounds). We observe that increasing the local epochs leads to worse performance due to the detrimental effect of client drift. Nevertheless, C2A clearly outperforms the other baselines in all settings. This further verifies the potency of C2A in mitigating the negative effects of the drift caused by excessive local updates, and shows that C2A can be efficiently trained with only a few rounds of communication.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the test accuracy of four methods, C2A (ours), Adapter, LoRA, and AdaMix, under different settings of local epochs and communication rounds. The horizontal axis represents the number of local epochs multiplied by the number of communication rounds, with three settings: 1 x 20, 5 x 4, and 10 x 2. The vertical axis represents the test accuracy in percentage. The figure shows that C2A outperforms the other three baseline methods in all settings. As the number of local epochs increases (while the number of communication rounds decreases), the test accuracy of all methods decreases, possibly due to the detrimental effect of client drift."
    },
    "291": {
        "figure1": "2411.00329v1_beta_8_4",
        "label1": "fig:learned_beta",
        "caption1": "\\footnotesize Comparison of client $\\beta$ and local dataset corruption on CIFAR10-S.",
        "text": "We additionally visualize the spread of learned $\\beta$ across clients as a function of their dataset corruption in \\prettyref{fig:learned_beta}. As expected, clients with clean datasets rely more on global knowledge (smaller $\\beta$ values) than corrupted clients. Moreover, corruptions with higher $\\beta$ values (e.g., contrast) often align with the more difficult corruptions encountered in \\prettyref{table:new_client}.\n\\centering     \\includegraphics[width=0.8\\textwidth]{figures/beta_8_4.pdf}     \\caption{\\footnotesize     Comparison of client $\\beta$ and local dataset corruption on CIFAR10-S.}     \\label{fig:learned_beta}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the comparison of client beta and local dataset corruption on the CIFAR10-S dataset. The horizontal axis represents different types of corruption introduced into the client datasets, including motion blur, defocus blur, Gaussian noise, shot noise, impulse noise, frost, fog, brightness, JPEG, and no corruption. The vertical axis represents the learned beta value. It can be observed that the beta value corresponding to contrast corruption is the highest, close to 0.9, while the beta value corresponding to no corruption is the lowest, slightly above 0.1. The beta values corresponding to other corruption types are between the two, in descending order: fog, impulse noise, shot noise, defocus blur, brightness, JPEG, and motion blur, Gaussian noise, frost. Error bars indicate the standard deviation of the data."
    },
    "292": {
        "figure1": "2411.00329v1_cifar100_epochs_final",
        "label1": "fig:local_epochs",
        "caption1": "Comparison of average test accuracy with varying local epochs on CIFAR100.",
        "text": "In many FL settings, we would like clients to perform more local training between rounds to reduce communication costs. However, too much local training can cause the model to diverge. In \\Cref{fig:local_epochs}, we compare the effect of the local amount of epochs for CIFAR100 and CIFAR100-S-25\\% sample datasets. We observe that (1) pFedFDA outperforms FedAvgFT at all equivalent budgets of $E$, (2) both methods follow exhibit a general plateau in accuracy after $E=5$, and (3) pFedFDA learns much faster than FedAvgFT, with significantly higher accuracy for $E=1$.",
        "summarize_figure": "figure1",
        "summarization": "English Summary:This image illustrates the average test accuracy of FedAvgFT and pFedFDA federated learning methods on CIFAR100 and CIFAR100-S Dir(0.5) 25% datasets across varying local training epochs. The figure shows that pFedFDA consistently achieves higher test accuracy than FedAvgFT for all tested local epochs. Specifically, on the CIFAR100 dataset (solid lines), pFedFDA (blue solid line) maintains a lead over FedAvgFT (green solid line), demonstrating significantly higher accuracy at just one local epoch. As local epochs increase from 1 to 5, both methods show considerable accuracy improvements on both datasets. Beyond five local epochs, the accuracy for both methods on both datasets tends to plateau or slightly decrease in some cases. On the CIFAR100-S Dir(0.5) 25% dataset (dashed lines), pFedFDA (blue dashed line) also outperforms FedAvgFT (green dashed line)."
    },
    "293": {
        "figure1": "2411.00347v1_figure_2.2.1",
        "label1": "fig:hulline-y",
        "caption1": "Hull lines of the dolphin model along the Y-axis. Cut in 10 slides in half of the body with equal spacing. End-cut excluded.",
        "figure2": "2411.00347v1_figure_2.3",
        "label2": "fig:2d_points",
        "caption2": "Discretized center cut dolphin body.",
        "text": "The fish-bone skeleton design is inspired by the segmented vertebrae of real dolphins and serves as the core structural framework of the robotic dolphin. This skeleton is derived from sequential cross-sectional cuts along the dolphin’s body length (Fig.~\\ref{fig:hulline-y}), with the dorsal fin removed from this design phase to streamline the profile. To achieve this, data points from 0.40 m to 0.61 m along the x-axis were excluded, creating a simplified dorsal profile for improved suitability in robotic applications. The resulting data points define the basic geometry of the skeleton, as shown in Fig.~\\ref{fig:2d_points}.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the hull lines of the dolphin model along the Y-axis. Ten equally spaced slices are made through one half of the dolphin's body (excluding the end cut). These slices show the cross-sectional contours at different locations along the dolphin's body, with the X-axis representing the chord (m) and the Y-axis representing the Z-coordinate (m). The dorsal fin of the dolphin was removed during the design phase to simplify the profile."
    },
    "294": {
        "figure1": "2411.00347v1_figure_2.3",
        "label1": "fig:2d_points",
        "caption1": "Discretized center cut dolphin body.",
        "figure2": "2411.00347v1_figure_2.2.1",
        "label2": "fig:hulline-y",
        "caption2": "Hull lines of the dolphin model along the Y-axis. Cut in 10 slides in half of the body with equal spacing. End-cut excluded.",
        "text": "The fish-bone skeleton design is inspired by the segmented vertebrae of real dolphins and serves as the core structural framework of the robotic dolphin. This skeleton is derived from sequential cross-sectional cuts along the dolphin’s body length (Fig.~\\ref{fig:hulline-y}), with the dorsal fin removed from this design phase to streamline the profile. To achieve this, data points from 0.40 m to 0.61 m along the x-axis were excluded, creating a simplified dorsal profile for improved suitability in robotic applications. The resulting data points define the basic geometry of the skeleton, as shown in Fig.~\\ref{fig:2d_points}.",
        "summarize_figure": "figure1",
        "summarization": "The figure shows the hull lines of the dolphin model along the Y-axis. The model is obtained by sequential cross-sectional cuts along the dolphin's body length, cut into 10 slides on half of the dolphin's body with equal spacing, excluding the end-cut. Data points between 0.40 m and 0.61 m on the x-axis are excluded, simplifying the dorsal fin profile. The figure shows the upper and lower curves of the dolphin model. The upper curve is represented by an orange polynomial, and the lower curve is represented by a blue polynomial, and points are used to represent points on the upper and lower curves."
    },
    "295": {
        "figure1": "2411.00347v1_force_with_silicone_poly",
        "label1": "fig:force-displace",
        "caption1": "Force-displacement analysis of the silicone used for the dolphin’s tail, conducted using an Instron machine.",
        "text": "Mechanical testing was conducted on an Instron machine to assess the silicone’s force-displacement properties, as illustrated in Fig.~\\ref{fig:force-displace}. The results provided insights into the tail’s stiffness.\n\\centering     \\includegraphics[width=1.1\\linewidth,trim={2cm 0 0 0},clip]{figures/force_with_silicone_poly.png}     \\caption{Force-displacement analysis of the silicone used for the dolphin’s tail, conducted using an Instron machine.}     \\label{fig:force-displace}",
        "summarize_figure": "figure1",
        "summarization": "The image shows the force-displacement analysis of the silicone used for the dolphin's tail. The blue dots represent the adjusted data. The red curve is the polynomial fit of the data, with the equation y = 1.085E-05x^3 - 1.279E-03x^2 + 1.031E-01x. This graph reflects the relationship between force and displacement of the silicone material, revealing the stiffness characteristics of the dolphin tail material."
    },
    "296": {
        "figure1": "2411.00348v1_head_vis",
        "label1": "fig:head_vis",
        "caption1": "\\textbf{Position of Important Heads: } Visualization of the $\\attninst{}$ for each head in different LLMs. The figure shows that the important head effect mostly occurs in the shallower or middle layers of the LLMs.",
        "text": "\\centering         \\includegraphics[width=\\textwidth]{figures/head_vis.pdf}     \\caption{\\textbf{Position of Important Heads: } Visualization of the $\\attninst{}$ for each head in different LLMs. The figure shows that the important head effect mostly occurs in the shallower or middle layers of the LLMs.}     \\label{fig:head_vis}\nIn addition to the number of heads that we should select for the detector, we are also interested in the positions of the attention heads that exhibit more pronounced distraction effect. As shown in Figure \\ref{fig:head_vis}, we visualize the $\\attninst{}$ of each attention heads. Interestingly, the visualization reveals a similar pattern across models: most important heads are located in the first few layers or the middle layers. This shows that attention heads in the first few layers or the middle layers may have a larger influence on the instruction-following behavior of LLMs.",
        "summarize_figure": "figure1",
        "summarization": "The picture visualizes the attention instance (attninst) for each head in different Large Language Models (LLMs). Specifically, the Qwen2 model has 27 layers. The attention instance values are unevenly distributed among different heads. Generally, the values of the heads in the shallower and middle layers are relatively higher, with colors leaning towards red or blue, while the values of the heads in the deeper layers are relatively lower, with colors leaning towards gray."
    },
    "297": {
        "figure1": "2411.00348v1_cross_dataset",
        "label1": "fig:dataset",
        "caption1": "\\textbf{Heads Generalization:} The figure illustrates the mean difference in $\\attninst{}$ scores between normal data and attack data from the deepset prompt injection dataset \\citep{huggingfaceDeepsetpromptinjectionsDatasets}, the Open-Prompt-Injection benchmark \\citep{liu2024formalizing}, and the set of LLM-generated data we used to find important heads.}  \\vspace{-1em",
        "text": "\\centering     \\vspace{-0.5em}     \\includegraphics[width=\\textwidth]{figures/cross_dataset.pdf}     \\caption{\\textbf{Heads Generalization:} The figure illustrates the mean difference in $\\attninst{}$ scores between normal data and attack data from the deepset prompt injection dataset \\citep{huggingfaceDeepsetpromptinjectionsDatasets}, the Open-Prompt-Injection benchmark \\citep{liu2024formalizing}, and the set of LLM-generated data we used to find important heads.}     \\label{fig:dataset}     \\vspace{-1em}\nTo demonstrate the generalization of important heads (i.e., specific heads consistently showing distraction effect across different prompt injection attacks and datasets), we visualized the mean difference in $\\attninst{}$ scores on Qwen-2 model \\citep{yang2024qwen2technicalreport} between normal and attack data from three datasets: the deepset prompt injection dataset \\citep{huggingfaceDeepsetpromptinjectionsDatasets}, the Open-Prompt-Injection benchmark \\citep{liu2024formalizing}, and a set of LLM-generated data used for head selection in Section \\ref{sec:select}. As shown in Figure \\ref{fig:dataset}, although the magnitude of differences in $\\attninst{}$ varies across datasets, the relative differences across attention heads remain consistent. In other words, the attention heads with the most distinct difference are consistent across different datasets, indicating that the distraction effect generalizes well across various data and attacks. For the LLM-generated data, we merely use a basic prompt injection attack (e.g., \\emph{ignore previous instruction and ...}), demonstrating that important heads remain consistent even with different attack methods. This further validates the effectiveness of identifying important heads using simple LLM-generated data, as discussed in Section \\ref{sec:select}.",
        "summarize_figure": "figure1",
        "summarization": "English Summary:The figure presents three heatmaps illustrating the mean difference in attention scores (attninst) between normal and attack data for the Qwen-2 model across three distinct datasets: deepset Prompt Injection, Open-Prompt-Injection, and LLM-Generated Data. The heatmaps' x-axis represents attention heads (Heads), and the y-axis represents model layers (Layers). Color intensity indicates the magnitude of the difference, with red denoting higher scores on attack data compared to normal data and blue indicating lower scores. A visual analysis reveals that despite potential variations in the magnitude of differences across the three datasets, similar patterns of prominent red areas (positive differences) appear in roughly the same layer and head locations. This consistency in patterns across datasets indicates that the specific attention heads significantly impacted by prompt injection attacks are generalizable, demonstrating a distraction effect that persists across different data types and attack methods."
    },
    "298": {
        "figure1": "2411.00348v1_length_vis",
        "label1": "fig:length",
        "caption1": "\\textbf{Impact of Data Length Proportion:} This figure illustrates the relationship between the $FS$ and varying data lengths using Llama3.\\citep{dubey2024llama3herdmodels}.}  \\vspace{-1em",
        "text": "\\centering     \\vspace{-0.5em}     \\includegraphics[width=\\linewidth]{figures/length_vis.pdf}     \\caption{\\textbf{Impact of Data Length Proportion:} This figure illustrates the relationship between the $FS$ and varying data lengths using Llama3.\\citep{dubey2024llama3herdmodels}.}     \\label{fig:length}     \\vspace{-1em}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the relationship between data length proportion and Focus Score. It shows two curves representing Focus Scores under normal and attack prompts. Under normal prompts, the Focus Score fluctuates between 0.3 and 0.45 without a clear trend as the data length proportion changes. In contrast, the Focus Score under attack prompts remains close to 0, with a red dashed line indicating the maximum score on attack data. This suggests that the data length proportion has a minimal impact on attack prompts, while the Focus Score for normal prompts is relatively high and stable."
    },
    "299": {
        "figure1": "2411.00348v1_attack_hist",
        "label1": "fig:attack_vis",
        "caption1": "\\textbf{Distraction Effect of Different Attack Strategies:} This figure shows the distribution of the aggregated $\\attninst{}$ across all layers and heads for different attacks on a subset of the Open-Prompt-Injection dataset \\citep{liu2024formalizing}. The legend indicates the color representing each attack strategy and the corresponding attack success rate (in round brackets).}  \\vspace{-1em",
        "text": "\\centering         \\includegraphics[width=0.95\\columnwidth]{figures/attack_hist.pdf}     \\caption{\\textbf{Distraction Effect of Different Attack Strategies:} This figure shows the distribution of the aggregated $\\attninst{}$ across all layers and heads for different attacks on a subset of the Open-Prompt-Injection dataset \\citep{liu2024formalizing}. The legend indicates the color representing each attack strategy and the corresponding attack success rate (in round brackets).}     \\label{fig:attack_vis}     \\vspace{-1em}\nTo further understand how various prompt injection attacks distract attentions, we also visualize their effect separately in Figure \\ref{fig:attack_vis}. In the figure, we plot the distribution of the aggregated $\\attninst{}$ across all attention heads (i.e. $\\sum_{l=1}^L\\sum_{h=1}^H\\attninst{}$). From this figure, we observe that as the strength of the attack increases (i.e., higher attack success rate), total attention score decreases, indicating a more pronounced distraction effect. This demonstrates a direct correlation between the success of prompt injection attacks and the distraction effect. We provide detailed introductions of these different attacks in Appendix \\ref{appendix:diff_attack}.",
        "summarize_figure": "figure1",
        "summarization": "Here is the summary of the figure:The figure shows the distribution of instruction attention under different attack strategies. The horizontal axis represents the attention score, and the vertical axis represents the density. The legend indicates that as the attack success rate increases, the total attention score shows a decreasing trend, indicating a more pronounced distraction effect. Specifically, the attention score of \"no attack\" is mainly concentrated between 0.6 and 0.7, while the attention score of \"combine\" is concentrated around 0.2. Other attack strategies, such as \"ignore\", \"fake comp\", and \"naive\", show different attention distributions."
    },
    "300": {
        "figure1": "2411.00393v3_activation_plot-eps-converted-to",
        "label1": "fig:activations",
        "caption1": "Activation values after the last linear layer were more extreme for the one-hot approach compared to the population code. Values are mean $\\pm$ SD (100 simulation runs).",
        "text": "So, we suspect that the one-hot method has larger linear-layer outputs compared to the population code. The larger values cause a problem for robustness because they make it more likely that a perturbation causes spurious activations that suppress the correct output value. To test this hypothesize, we evaluated the activations after the final linear layer, and compared one hot with population code. As a result, both the max and min values were indeed larger for one hot for the 3, 5, and 8-layer networks (Fig. \\ref{fig:activations}). For the 8-layer network, the values were about 5x larger.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the maximum and minimum activation values of the last layer in networks with different numbers of layers for One Hot encoding and Population Code encoding methods. The horizontal axis represents the number of network layers, ranging from 1 to 8, and the vertical axis represents the activation value, using a logarithmic scale. For the maximum activation value, both encoding methods increase with the number of network layers, and the maximum activation value of One Hot encoding is always higher than that of Population Code encoding. For the minimum activation value, both encoding methods decrease with the number of network layers, and the minimum activation value of One Hot encoding is always lower than that of Population Code encoding."
    },
    "301": {
        "figure1": "2411.00394v1_heatmap_full_paper_non_linear_color_map",
        "label1": "fig:confusion matrix baseline and finetune",
        "caption1": "The heatmaps of the model's prediction. (a1)-(a4) shows the baseline performance under zero-shot setting, and (b1)-(b4) shows the performances of fine-tuned models. `O' denotes the class \\texttt{leave it unchanged}, and `X' denotes the class \\texttt{none of the other options}.",
        "text": "\\centering     \\includegraphics[width=\\linewidth]{images/heatmap_full_paper_non_linear_color_map.pdf}%images/heatmap_full_paper_5.pdf     \\caption{The heatmaps of the model's prediction. (a1)-(a4) shows the baseline performance under zero-shot setting, and (b1)-(b4) shows the performances of fine-tuned models. `O' denotes the class \\texttt{leave it unchanged}, and `X' denotes the class \\texttt{none of the other options}.}     \\label{fig:confusion matrix baseline and finetune}\nAs mentioned in Section \\ref{sec:exp-setting}, we use different prompt settings for each group of models that suit their capabilities. For the 7b models, we use the two-round prompt because these models benefit from a more structured, step-by-step approach, which helps them handle the task more effectively. In contrast, we tested the LLaVA-1.5 13b and GPT-4o model with a single-round of prompting to see if they were capable of this task. The prediction results are presented in Fig. \\ref{fig:confusion matrix baseline and finetune}, from (a1) - (a4). We observe that three open-sourced models (LLaVA-1.5 7b/13b, and Instructblip 7b) show similar behaviors: these models tend to avoid predicting the reframing cases and mistakenly categorize them as either \\texttt{leave it unchanged} or \\texttt{none of the other options.} With the two-round prompt, LLaVA-1.5 7b and InstructBlip 7b make some correct predictions in the \\texttt{left} and \\texttt{right} categories. However, the correct and incorrect predictions are nearly balanced, with frequent misclassifications in the opposite direction. For example, the number of true \\texttt{left} predictions is equivalent to the number of erroneous \\texttt{left} predictions that were intended to be \\texttt{right}. For GPT-4o, there are fewer errors in categorizing reframing cases into the wrong categories, and more cases within the reframing category are correctly predicted. However, contradictory predictions also occur frequently within the reframing cases. The result demonstrates that all the models are generally incapable of accurately predicting the reframing cases under zero-shot prompt settings. However, every baseline model performs well in the \\texttt{none of the other options} category. We present the examples from each model in the Supplementary  material \\ref{appendix:examples_zero_shot} to visualize the model's pretrained capability on the Directional Guidance Task.\nWe present the heat maps of the fine-tuned model predictions in Fig. \\ref{fig:confusion matrix baseline and finetune}. By comparing the prediction results between the zero-shot baselines and the fine-tuned models, we observe significant and consistent improvements in prediction performance, demonstrating the effectiveness and generalizability of our proposed method. Although there is considerable potential to improve the overall accuracy, the fine-tuned models reduce confusion between \\texttt{reframing}, \\texttt{leaving it unchanged}(O), and \\texttt{none of the other options}(X). The fine-tuned models are more likely to provide directional guidance on the reframing cases. Moreover, the predictions within reframing cases show noticeable improvement, as indicated by the clear diagonal line in the heat map. Another interesting finding is the substantial reduction in wrong predictions with opposite directions (e.g., predicting an \\texttt{up} case as \\texttt{down}). This clarity is meaningful, as it lowers the chance of users receiving conflicting guidance, thereby enhancing safety and efficiency in real-world applications. Overall, the fine-tuned models reduce errors across all options, showing significant improvement in both cross-category and within reframing predictions.",
        "summarize_figure": "figure1",
        "summarization": "The image displays heatmaps of model prediction performance, contrasting zero-shot baseline results (top row) with fine-tuned model performances (bottom row). Under the zero-shot setting, models frequently misclassify reframing cases (up, down, left, right) as 'leave it unchanged' (O) or 'none of the other options' (X), and show significant confusion within reframing categories. Fine-tuned models demonstrate substantial and consistent performance improvements. They reduce confusion between reframing, O, and X categories, are more likely to provide directional guidance for reframing cases, and show marked improvement in prediction accuracy within the reframing categories, with higher diagonal values and significantly fewer erroneous opposite direction predictions, such as predicting up instead of down. This highlights the effectiveness of fine-tuning in enhancing prediction accuracy and reliability."
    },
    "302": {
        "figure1": "2411.00394v1_heatmap_appendix",
        "label1": "fig:enter-label",
        "caption1": "The heatmap of model's predictions under different settings. The $p$ denotes the perturbation ranges we applied when generating synthetic data.",
        "text": "In this section, we analyze the effect of different settings, including perturbation range and shuffling operations, on the generation of training data. A detailed heatmap of the model's predictions is presented in Supplementary Materials with Figure \\ref{fig:enter-label}. The perturbation range determines the crop ratios used to generate the training samples, ranging from 0.1 (minimal crop) to 0.9 (maximum crop). We observed that positive Guidance samples tend to cluster at high crop ratios, while lower ratios often correspond to negative samples (where the Guidance is \\texttt{leave it unchanged}). Therefore, a range of 0.3-0.7 leads to a more balanced selection of training data, while a range of 0.1-0.9 provides a more comprehensive and varied dataset. This setting can affect the model's performance due to the balance between the diversity and complexity of the generated training samples, and the trade-off works as follows: when the perturbation becomes more severe (e.g., at a ratio of 0.9), images are aggressively corrupted. This increases the chance of obtaining positive Guidance samples, as the model is more likely to fail in predicting these heavily perturbed samples, which it could have predicted accurately without perturbation. However, it also results in significant information loss and greater challenges in detecting objects, making it a harder sample to learn. Conversely, a moderate perturbation ratio results in less aggressive cropping, allowing the model to access more information and better respond to the original question. However, this can lead to fewer positive Guidance samples, as the perturbation does not sufficiently challenge the predictions. The differences with the shuffling settings could be attributed to the regularization effect, which prevents the model from memorizing fixed patterns and increases training difficulty, especially when training data is scarce. In scenarios with less training data, shuffling acts as a form of data augmentation, increasing the diversity of training examples and making the model more robust. However, shuffling might add unnecessary complexity to a larger training dataset derived from a wider perturbation range, making it harder for the model to learn effectively. In such cases, the unshuffled approach allows the model to quickly identify and leverage consistent patterns, facilitating faster and more efficient learning processes.\nHyper-parameters & Value \\\\ \\hline learning rate    & 5e-5  \\\\ LoRA r           & 128   \\\\ LoRA alpha       & 256   \\\\ batch size       & 4     \\\\ training epoch   & 5     \\\\ \\hline     \\centering        \\includegraphics[width=0.8\\linewidth]{images/heatmap_appendix.pdf}     \\caption{The heatmap of model's predictions under different settings. The $p$ denotes the perturbation ranges we applied when generating synthetic data.}     \\label{fig:enter-label}",
        "summarize_figure": "figure1",
        "summarization": "The image presents a heatmap showing model predictions under different settings. The heatmap uses Actual categories on the vertical axis and Predicted categories on the horizontal axis, including 'up', 'down', 'left', 'right', 'O' (Okay), and 'X' (Fail). It compares various configurations: different perturbation ranges (p values 0.3-0.7 and 0.1-0.9), data shuffling (Shuffled and Unshuffled), Baseline, and GPT-4 variants (GPT-4v and GPT-4o). Across most settings, the counts on the diagonal for 'O' and 'X' categories are significantly higher than for directional predictions. GPT-4 variants, particularly GPT-4o, show exceptionally high counts for predicting the 'X' category, surpassing other models. LLava-13b generally exhibits slightly higher diagonal values for 'O' and 'X' compared to LLava-7b and InstructBlip-7b. For InstructBlip and LLava models, the wider perturbation range (p value 0.1-0.9) typically results in higher counts on the 'O' and 'X' diagonals than the narrower range. The effect of shuffling varies: it slightly increases diagonal counts in the narrower range but unshuffled settings sometimes show higher diagonal counts in the wider range. Notable off-diagonal counts exist between 'O' and 'X', indicating confusion."
    },
    "303": {
        "figure1": "2411.00412v1_Evolution",
        "label1": "fig:evolution",
        "caption1": "Composition of Tool Usage Decisions in Climate Dataset Training: Evolution over growing momentum training terms.",
        "text": "Figure~\\ref{fig:evolution} illustrates the evolution of our model's performance in the form of different solution types (EN, ET, HN, HT) on the Climate dataset at different training epochs.\n\\centering     \\includegraphics[width=0.4\\columnwidth]{figs/Evolution.pdf}     \\caption{Composition of Tool Usage Decisions in Climate Dataset Training: Evolution over growing momentum training terms.}     \\label{fig:evolution}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the evolution of the model's tool usage decisions in climate dataset training across different training epochs. Initially (Base), \"Hard-Tool\" has the highest proportion, around 0.7, followed by \"Easy-Not Choosing Tool\" at approximately 0.3, with the other two decision types having small proportions. As training progresses from Epoch 1 to Epoch 5, the proportion of \"Easy-Not Choosing Tool\" gradually increases, becoming the dominant decision type, accounting for over 0.7 by Epoch 5. The proportion of \"Hard-Tool\" significantly decreases, dropping to a low level by Epoch 5. Meanwhile, the proportions of \"Easy-Tool\" and \"Hard-Not Choosing Tool\" also change during training, but their overall proportions are not high. The figure reveals that as training progresses, the model increasingly tends not to use tools in easy cases and reduces the use of tools in hard cases."
    },
    "304": {
        "figure1": "2411.00412v1_2Aba",
        "label1": "fig:subcomponent:function",
        "caption1": "Composition of Tool Usage Decisions in Climate Dataset Training: Impact of individual training components in ablation study.",
        "text": "\\centering   \\includegraphics[width=0.8\\columnwidth]{figs/2Aba.pdf}   \\vspace{-0.5cm}   \\caption{Composition of Tool Usage Decisions in Climate Dataset Training: Impact of individual training components in ablation study.}\\label{fig:subcomponent:function}\nFigure~\\ref{fig:subcomponent:function} presents an ablation study on the functionality of \\firstphaseshort~and \\secondphaseshort~by evaluating the proportion of the four tool usage decisions (EN, ET, HN, HT) on the Climate dataset. We observe that omitting either component leads to tool over-reliance. Moreover, without \\firstphaseshort, the model exhibits the lowest answer accuracy, as it is never trained on the distilled knowledge directly.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the composition of tool usage decisions in climate dataset training and the impact of individual training components in an ablation study. The chart compares the proportion of tool usage in four scenarios: Base (baseline model), w/o WKD (without knowledge distillation), w/o TUA (without tool usage augmentation), and Ours (complete model). The legend indicates that blue represents easy questions not using tools, light blue represents easy questions using tools, orange represents hard questions using tools, and light orange represents hard questions not using tools. It can be seen that removing either knowledge distillation or tool usage augmentation leads to over-reliance on tools by the model."
    },
    "305": {
        "figure1": "2411.00412v1_flowPDE",
        "label1": "fig:flow",
        "caption1": "Different models' performance on the PDE Dataset: comparing pre- and post-training, with and without tool usage.",
        "text": "\\centering   \\includegraphics[width=0.8\\columnwidth]{figs/flowPDE.pdf}   \\vspace{-0.3cm}   \\caption{Different models' performance on the PDE Dataset: comparing pre- and post-training, with and without tool usage.}   \\label{fig:flow} We present the model's performance on the PDE dataset in Figure~\\ref{fig:flow} before and after training, and with $P_n$ and $P_i$, respectively.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the performance comparison of different models on the Partial Differential Equation (PDE) dataset, focusing on pre- and post-training scenarios and the impact of tool usage. The horizontal axis represents different models and states: \"Base-Pn\" (untrained baseline model), \"Base-Pf\" (trained baseline model), \"Ours-Pn\" (untrained improved model), and \"Ours-Pi\" (trained improved model). The vertical axis indicates the correctness of predictions. The graph shows that \"Base-Pn\" performs poorly before training, with a large number of incorrect predictions and a small number of correct ones. After training, \"Base-Pf\" does not show a significant increase in accuracy. However, the improved model shows higher accuracy before training (\"Ours-Pn\") compared to \"Base-Pn\", and further improves after training (\"Ours-Pi\"), with a significant reduction in the error rate."
    },
    "306": {
        "figure1": "2411.00412v1_Noise",
        "label1": "fig:resist:noise",
        "caption1": "Model Performance vs. Noise Level: Comparison between our two-component method and SFT-only approaches on Climate dataset.",
        "text": "\\centering   \\includegraphics[width=0.8\\columnwidth]{figs/Noise.pdf}   \\vspace{-0.3cm}   \\caption{Model Performance vs. Noise Level: Comparison between our two-component method and SFT-only approaches on Climate dataset.}   \\label{fig:resist:noise}\nGenerating solutions via LLMs or human expert annotation inevitably introduces noise. We examine how model trained with our method performs with increasing noise on the Climate dataset. We compare the model trained with our two-component method, against a model that only undergoes SFT for comparison. The results are shown in Figure~\\ref{fig:resist:noise}.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the relationship between model performance and noise level on the Climate dataset, comparing our two-component method (Ours) with the SFT-only method (SFT). The horizontal axis represents the noise level, ranging from 0% to 100%, while the vertical axis represents the accuracy. At a noise level of 0%, the accuracy of SFT is 0.78, while our method achieves 0.93. As the noise level increases, the accuracy of SFT gradually decreases, reaching only 0.31 at a noise level of 100%. In contrast, our method demonstrates higher accuracy across different noise levels, still achieving 0.69 even at a noise level of 100%. The figure suggests that our method is more robust than the SFT-only method in the presence of noise."
    },
    "307": {
        "figure1": "2411.00412v1_Open_Heatmap",
        "label1": "fig:win_rate_heatmap",
        "caption1": "Win rate heatmap of the percentage that each model won in pairwise comparisons against other models. Each cell represents the win rate ($\\%$) of the model listed on the y-axis when compared with the model on the x-axis.",
        "text": "\\centering  \\includegraphics[width=0.8\\columnwidth]{figs/Open_Heatmap.pdf}   \\vspace{-0.5cm}   \\caption{Win rate heatmap of the percentage that each model won in pairwise comparisons against other models. Each cell represents the win rate ($\\%$) of the model listed on the y-axis when compared with the model on the x-axis.}   \\label{fig:win_rate_heatmap}\nWe evaluate these models alongside several baselines on climate-related open-ended questions. Table~\\ref{tab:open_answer} reports the percentage of qualified proposals, and Figure~\\ref{fig:win_rate_heatmap} shows win-rate comparisons among models. The results show improvements from incorporating both preference learning and the \\secondphaseshort. The contribution of these components can be attributed to: (1) preference learning implicitly learning the ranking among diverse proposals for each problem, and (2) the \\secondphaseshort~enabling intelligent tool switching for sampling and ranking proposals on harder questions.",
        "summarize_figure": "figure1",
        "summarization": "The figure is a heatmap comparing the win rates of different models in pairwise comparisons. The y-axis represents the winning model, and the x-axis represents the losing model. The value in each cell represents the win rate (percentage) of the y-axis model compared to the x-axis model. The Ours-RL-Pi model shows high win rates compared to other models, especially against Base, Base-Pf, GPT4o, and Claude3.5. The Claude3.5 model has relatively low win rates. The heatmap shows the performance differences between the models, with Ours-RL-Pi showing the most competitiveness and Claude3.5 showing relatively weak competitiveness."
    },
    "308": {
        "figure1": "2411.00412v1_MATH_Level",
        "label1": "fig:math",
        "caption1": "Tool usage decision of different models on MATH dataset of 5 difficulty levels. Investigated models are \\textbf{Claude}-3.5-Sonnet, Llama-3.1-8B-\\textbf{Base}, and Llama-3.1-8B-\\textbf{Ours}.",
        "text": "Besides the advantage shown in Table~\\ref{tab:toolaccuracy}, we further investigate the tool usage decisions on the MATH dataset, which provides a prior label of difficulty levels in Figure~\\ref{fig:math}. Our trained model exhibits a reasonable increase in tool usage with growing question difficulty, while the base model shows an over-reliance on tools regardless of difficulty. In contrast, Claude-3.5 demonstrates more confidence in answering directly for both easy and hard questions, possibly because MATH is a public dataset and the model has seen similar questions during training.\n\\centering   \\includegraphics[width=0.8\\textwidth]{figs/MATH_Level.pdf}   \\vspace{-0.5cm}   \\caption{Tool usage decision of different models on MATH dataset of 5 difficulty levels. Investigated models are \\textbf{Claude}-3.5-Sonnet, Llama-3.1-8B-\\textbf{Base}, and Llama-3.1-8B-\\textbf{Ours}.}   \\label{fig:math}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the proportion of tool usage decisions across different models on the MATH dataset, categorized by 5 difficulty levels. The horizontal axis represents the difficulty levels (Level 1 to Level 5), with tool usage data for three models (Base, Claude, Ours) at each level. The vertical axis indicates the proportion. Each bar consists of four segments: Easy-Not Choosing Tool, Easy-Tool, Hard-Tool, and Hard-Not Choosing Tool. The results indicate that the Ours model shows an increasing trend in tool usage as the difficulty level increases, the Base model exhibits a higher reliance on tools, and the Claude model tends to answer directly for both easy and hard questions."
    },
    "309": {
        "figure1": "2411.00418v1_rm_loop_results",
        "label1": "fig:rm_loop_results",
        "caption1": "\\textbf{Reward modeling improves in performance with iterative evolution.} We demonstrate the performance variation of the model during the iterative process on the HH-RLHF, Ultrafeedback, and Summarize datasets. \\textbf{Baseline} refers to the RM that uses the full dataset of human-annotated data. Due to the large size of the summarize test set, the results in the figure are based on a random sample of 1/10 of the test set.",
        "figure2": "2411.00418v1_rm_data_num",
        "label2": "fig:rm_data_num",
        "caption2": "The percentage of the total data used by the RM in each iteration is shown. (a)-(c) correspond to the HH-RLHF dataset, (d)-(f) correspond to the Ultrafeedback dataset, and (g)-(i) correspond to the Summarize dataset.",
        "text": "\\centering   \\includegraphics[width=1\\linewidth]{rm_loop_results.pdf}   \\caption{\\textbf{Reward modeling improves in performance with iterative evolution.} We demonstrate the performance variation of the model during the iterative process on the HH-RLHF, Ultrafeedback, and Summarize datasets. \\textbf{Baseline} refers to the RM that uses the full dataset of human-annotated data. Due to the large size of the summarize test set, the results in the figure are based on a random sample of 1/10 of the test set.}   \\label{fig:rm_loop_results}\nIn order to conduct a more detailed analysis of what occurs during the model's iterations, we present the changes in the model's accuracy on the validation set, as shown in Figure~\\ref{fig:rm_loop_results}. Additionally, we illustrate the variations in the amount of training data across different iterations, as depicted in Figure~\\ref{fig:rm_data_num}. Our main conclusions are as follows:\nAs shown in Figure~\\ref{fig:rm_loop_results}, in the loop 3 phase, training the model on hard samples further enhances its performance, approaching or even surpassing the results obtained using the full set of human-annotated data. Across multiple iterations, the total amount of training samples and the number of training steps are less than those required for the full dataset, typically representing only about 50\\% of the full data.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the performance variation of Llama 8B, Mistral 7B, and Llama 13B models during the iterative process on the HH-RLHF dataset. The horizontal axis represents training steps, and the vertical axis represents test accuracy in percentage. The graph compares the performance of different loops (Loop 0 to Loop 3) and the baseline model. As the iteration progresses, the test accuracy of the models gradually increases. For the Llama 8B model, the performance of Loop 3 stabilizes after reaching approximately 70% accuracy, approaching the performance of the baseline model. The Mistral 7B model shows a significant performance improvement in the initial stages of Loop 2 and Loop 3, but then tends to level off, eventually also approaching the baseline level. The Llama 13B model experiences a rapid performance improvement in the Loop 1 phase, with smaller incremental improvements in subsequent iterations, and its final performance is slightly higher than the baseline model. The overall trend is that iterative training can effectively improve the performance of the models on the HH-RLHF dataset, and after several iterations, the model performance can approach or even surpass the baseline model trained using the full set of human-annotated data."
    },
    "310": {
        "figure1": "2411.00418v1_rm_data_num",
        "label1": "fig:rm_data_num",
        "caption1": "The percentage of the total data used by the RM in each iteration is shown. (a)-(c) correspond to the HH-RLHF dataset, (d)-(f) correspond to the Ultrafeedback dataset, and (g)-(i) correspond to the Summarize dataset.",
        "figure2": "2411.00418v1_rm_loop_results",
        "label2": "fig:rm_loop_results",
        "caption2": "\\textbf{Reward modeling improves in performance with iterative evolution.} We demonstrate the performance variation of the model during the iterative process on the HH-RLHF, Ultrafeedback, and Summarize datasets. \\textbf{Baseline} refers to the RM that uses the full dataset of human-annotated data. Due to the large size of the summarize test set, the results in the figure are based on a random sample of 1/10 of the test set.",
        "text": "\\centering   \\includegraphics[width=1\\linewidth]{rm_data_num.pdf}   \\caption{The percentage of the total data used by the RM in each iteration is shown. (a)-(c) correspond to the HH-RLHF dataset, (d)-(f) correspond to the Ultrafeedback dataset, and (g)-(i) correspond to the Summarize dataset.}   \\label{fig:rm_data_num}\nIn order to conduct a more detailed analysis of what occurs during the model's iterations, we present the changes in the model's accuracy on the validation set, as shown in Figure~\\ref{fig:rm_loop_results}. Additionally, we illustrate the variations in the amount of training data across different iterations, as depicted in Figure~\\ref{fig:rm_data_num}. Our main conclusions are as follows:\nWe quantify the amount of data filtered out during the iterative process, as shown in Figure~\\ref{fig:rm_data_num}. Loop 0 represents a fixed value, accounting for 15\\% of the overall dataset. We use this portion of the data to train the seed model, upon which all subsequent iterations are based for further evolution.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the percentage of the total data used by the RM in each iteration on the HH-RLHF, Ultrafeedback, and Summarize datasets. Each subplot corresponds to a different dataset and model (Llama 8B, Mistral 7B, Llama 13B). Loop 0 represents human-annotated data, which accounts for a fixed 15% and is used to train the seed model. Subsequent iterations are based on this seed model. From Loop 1 to Loop 3, the percentage of data usage varies across different models and datasets. For example, on the HH-RLHF dataset, the Llama 8B model uses a similar amount of data in Loop 1 and Loop 2, but it increases in Loop 3. On the Ultrafeedback dataset, the Llama 8B model uses significantly more data in Loop 3 than in the previous loops."
    },
    "311": {
        "figure1": "2411.00418v1_comparison_chart",
        "label1": "fig:ppo_results_stackoverflow",
        "caption1": "we use GPT-4 as a judge to evaluate the capabilities of the model trained with PPO. We employ the win rate as the evaluation metric. \\textbf{Left} represents our SER method, \\textbf{SFT} denotes the model fine-tuned with SFT, and \\textbf{Full} refers to the PPO model guided by an RM trained on the full dataset.",
        "text": "\\centering   \\includegraphics[width=1\\linewidth]{comparison_chart.pdf}   \\caption{we use GPT-4 as a judge to evaluate the capabilities of the model trained with PPO. We employ the win rate as the evaluation metric. \\textbf{Left} represents our SER method, \\textbf{SFT} denotes the model fine-tuned with SFT, and \\textbf{Full} refers to the PPO model guided by an RM trained on the full dataset.}   \\label{fig:ppo_results_stackoverflow}\nTo validate the effectiveness of SER, we use the previously mentioned RM to guide PPO training, thereby optimizing the LLM. We conduct experiments on the Anthropic HH RLHF dataset and the Stackoverflow dataset, as shown in Figure~\\ref{fig:ppo_results_stackoverflow}. In the hh-rlhf dataset, all self RLHF models exceed the SFT baseline in terms of win rate, indicating that the self RLHF approach enhances the capabilities of the LLM. Compared to RMs trained with the full human-annotated data, the win rate in PPO experiments demonstrates a consistent trend with the performance of the RMs. For Mistral 7B, the accuracy of the self RLHF RM surpasses that of the RM trained with the full dataset, and in PPO experiments, the win rate also slightly exceeds that of the full model. Additionally, to verify the generalizability of our method, we conduct the same experiment on the StackOverflow dataset. As shown in Figure~\\ref{fig:ppo_results_stackoverflow}(b), the SER models outperform the full models to a certain extent, demonstrating a trend consistent with the accuracy of the RMs. In summary, our main findings are as follows:",
        "summarize_figure": "figure1",
        "summarization": "The picture presents win rates evaluated by GPT-4 for different language models. Panel (a) shows results on the hh-rlhf dataset, comparing our SER method (Left wins) against models fine-tuned with SFT (Right wins) and PPO models guided by RMs trained on the full dataset (Right wins). On hh-rlhf, SER models generally have higher win rates than SFT models (e.g., Mistral 7B vs SFT: SER wins 18%, SFT wins 11%; Llama 8B vs SFT: SER wins 28%, SFT wins 17%). Compared to Full models, results vary: Mistral 7B SER slightly outperforms Full (19% vs 16%), while Llama 8B SER is slightly behind Full (22% vs 24%). Panel (b) shows results on the stackoverflow dataset, comparing SER models (Left wins) against Full models (Right wins). On stackoverflow, SER models consistently demonstrate higher win rates than Full models across various Llama models (7B, 13B, 70B) and Mistral 7B (e.g., Llama 7B vs Full: SER wins 30.5% vs Full wins 27.5%; Mistral 7B vs Full: SER wins 24.5% vs Full wins 20.5%). In both sets of experiments displayed in the picture, ties represent a significant proportion of the outcomes."
    },
    "312": {
        "figure1": "2411.00418v1_ppo_curve",
        "label1": "fig:ppo_curve",
        "caption1": "The learning curve of the model on the HH-RLHF dataset, with the y-axis representing the reward score after scaling. The model reaches convergence after 1200 steps. The shaded area indicates the standard deviation.",
        "text": "\\centering   \\includegraphics[width=1\\linewidth]{ppo_curve.pdf}   \\caption{The learning curve of the model on the HH-RLHF dataset, with the y-axis representing the reward score after scaling. The model reaches convergence after 1200 steps. The shaded area indicates the standard deviation.}   \\label{fig:ppo_curve}\nAs shown in Figure~\\ref{fig:ppo_curve}, we present the reward curves of Mistral 7B and LLaMA 8B on the HH-RLHF dataset. Both models reach convergence at around 1200 steps. We scale the reward scores to the range of \\(-1 \\) to \\(1\\) using the following formula:",
        "summarize_figure": "figure1",
        "summarization": "The figure presents PPO training curves on the HH-RLHF dataset, showing scaled reward on the y-axis and training steps on the x-axis. The left subplot, labeled \"Our Method\", indicates that LLaMA 8B consistently achieves higher scaled reward than Mistral 7B, converging above 0.8, while Mistral 7B converges around 0.65. The right subplot, labeled \"Full Dataset\", shows that Mistral 7B achieves significantly higher scaled reward than LLaMA 8B, converging above 0.8, while LLaMA 8B converges around 0.4. The shaded areas indicate standard deviation. As shown in the figure, the models reach convergence after approximately 1200 steps."
    },
    "313": {
        "figure1": "2411.00418v1_rm_score_delta",
        "label1": "fig:rm_delta",
        "caption1": "The reward score distribution of the model on HH-RLHF, with the y-axis representing probability density and the x-axis representing pairwise score differences. Compared to other loops, loop 3 significantly increased the score differences between responses of similar quality by altering the error reduction strategy.",
        "text": "\\centering   \\includegraphics[width=1\\linewidth]{rm_score_delta.pdf}   \\caption{The reward score distribution of the model on HH-RLHF, with the y-axis representing probability density and the x-axis representing pairwise score differences. Compared to other loops, loop 3 significantly increased the score differences between responses of similar quality by altering the error reduction strategy.}   \\label{fig:rm_delta}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the reward score distribution of the model on the HH-RLHF dataset across different training loops (loop0 to loop3). The x-axis represents pairwise score differences (Score J - Score K), and the y-axis represents probability density. The figure compares the score difference distributions for the llama 8b model across different loops. As the number of loops increases, the distribution of score differences changes. Specifically, loop3 (red) shows a distinct peak at x=0.3 compared to other loops, indicating that loop3 significantly increased the score differences between responses of similar quality. The distributions for loop0, loop1, and loop2 are relatively concentrated around 0. The blue and red dashed lines mark the positions of x=-0.3 and x=0.3, respectively, aiding in observing the range of score differences."
    },
    "314": {
        "figure1": "2411.00430v1_cifar100_b10i10_model_params",
        "label1": "model_params",
        "caption1": "Comparison of parameter increase between the proposed method and the best-performing baseline methods, DynaER and MORE, on the CIFAR100 dataset with 10 tasks.",
        "text": "To more intuitively compare  the growth in the number of parameters of our method with those of the baseline methods with the progress of incremental learning, we conducted comparisons between our proposed method and the best-performing baseline methods, DynaER and MORE. As observed in Figure~\\ref{model_params}, with each incoming task, DynaER experiences a substantial growth of 11.2 million parameters, significantly increasing the resource consumption of the training stage.  Although the MORE method does not exhibit significant parameter growth, it still calculates gradients for all parameters in the model. To protect important parameters, it applies masks to zero out the gradients of these parameters. This results in a significant computational cost due to the need to compute gradients for the entire model. Furthermore, these masks are task-specific, with each mask requiring approximately 70,000 parameters. In contrast, our method exhibits a minimal increase of only 15,000 parameters with the arrival of each task, which is negligible compared to the total number of backbone parameters. Our method not only significantly reduce the number of used parameters, but also consistently outperforms all other methods in terms of performance across three datasets.\n\\centering     \\includegraphics[width=0.4\\textwidth, height=0.3\\textheight, keepaspectratio]{fig/cifar100_b10i10_model_params.png}     \\caption{Comparison of parameter increase between the proposed method and the best-performing baseline methods, DynaER and MORE, on the CIFAR100 dataset with 10 tasks.}     \\label{model_params}",
        "summarize_figure": "figure1",
        "summarization": "The image illustrates the change in trainable parameters for three methods, DynaER, MORE, and Ours, as the number of classes increases during incremental learning. The DynaER method shows a substantial linear increase in trainable parameters with the number of classes, starting from approximately 11 million and reaching about 110 million with 100 classes. In contrast, the MORE method's trainable parameters remain relatively stable, staying around 25 million without significant growth as classes increase. The Ours method maintains a very low number of trainable parameters, showing minimal increase with the number of classes, significantly lower than both DynaER and MORE methods. This highlights the notable advantage of the Ours method in terms of parameter growth."
    },
    "315": {
        "figure1": "2411.00431v1_pareto",
        "label1": "figure:pareto",
        "caption1": "Pareto front showcasing the complexity and performance curve for the Łukasiewicz implication",
        "text": "Constraining the implementation of fuzzy implications results in worse performance compared to the unconstrained version. This finding suggests that the flexibility to explore a wide range of expression structures is more suitable for optimal performance. By constraining the model to use specific logical rules, such as placing the S-implication at the root node, the model's ability to discover effective patterns is limited. This insight challenges the hypothesis that strict logical constraints always lead to better performance, highlighting the importance of allowing the model to explore diverse expression configurations. However, the expression still generates interesting insights into the implementation of fuzzy logic into the DSR package. Expression \\ref{eq:constrained} shows the best performing expression generated with the given constraints. OBD stands for oldBalanceDest, NBD stands for newBalanceDest and  TF stands for type\\_TRANSFER. The T-norm, T-conorm and S-implication all correspond to the Łukasiewicz implications. Fuzzy logic offers intuitive rule-based expressions that are easier to understand compared to traditional black-box models. However, there is a trade-off between the complexity of these expressions and their predictive performance. More complex expressions can capture nuanced patterns in the data but may become less explainable. The Pareto front, shown in Figure \\ref{figure:pareto}, helps visualize this trade-off, allowing for the selection of expressions that balance performance and complexity. This balance is crucial for ensuring that the fraud detection system is both effective and explainable.\n\\centering     \\includegraphics[width=1\\linewidth]{pareto.png}     \\caption{Pareto front showcasing the complexity and performance curve for the Łukasiewicz implication}     \\label{figure:pareto}",
        "summarize_figure": "figure1",
        "summarization": "The figure shows the Pareto front, which illustrates the trade-off between complexity and performance for the Lukasiewicz implication. The left plot shows that the reward value increases with complexity, rising from approximately 0.02 at a complexity of 2.5 to about 0.19 at a complexity of 20. The right plot shows that the error value decreases with complexity, reducing from around 65 at a complexity of 2.5 to about 2 at a complexity of 20. This Pareto front helps visualize the balance between performance and complexity, allowing for the selection of expressions that balance performance and interpretability."
    },
    "316": {
        "figure1": "2411.00431v1_par_comparison",
        "label1": "fig:pareto_comparison",
        "caption1": "Pareto front showcasing the complexity versus reward for the Łukasiewicz implication versus the Product implication.",
        "text": "The simpler structure of the Product formula means fewer nested operations, making it more transparent how each feature affects the outcome. The use of basic fuzzy operations without excessive nesting helps in understanding the logical flow from input features to the fraud likelihood score. This can also be see in Figure \\ref{fig:pareto_comparison}, where the complexity of formulas is generally lower for the Product implication compared to the Łukasiewicz implication.\n\\centering     \\includegraphics[width=1\\linewidth]{par_comparison.png}     \\caption{Pareto front showcasing the complexity versus reward for the Łukasiewicz implication versus the Product implication.}     \\label{fig:pareto_comparison}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the Pareto front of the Łukasiewicz implication and the Product implication, where the horizontal axis represents complexity and the vertical axis represents reward. For Product implication, as complexity increases, the reward value increases rapidly, reaches a plateau with slight fluctuations, and reaches the highest value at high complexity. In contrast, the complexity of the Łukasiewicz implication is generally higher than that of the Product implication."
    },
    "317": {
        "figure1": "2411.00432v1_figure3",
        "label1": "fig:curriculm_learning",
        "caption1": "An analysis of the easy samples and hard samples used in our curriculum learning strategy. (a) Point clouds with a higher proportion of points with low curvature values, resulting in a distribution with high skewness, were classified as easy samples. (b) Conversely, point clouds with a higher proportion of points with high curvature values were classified as hard samples.}  \\vspace{-10pt",
        "text": "\\centering     \\includegraphics[width=\\linewidth]{figure/figure3.pdf}     \\vspace{-15pt}     \\caption{An analysis of the easy samples and hard samples used in our curriculum learning strategy. (a) Point clouds with a higher proportion of points with low curvature values, resulting in a distribution with high skewness, were classified as easy samples. (b) Conversely, point clouds with a higher proportion of points with high curvature values were classified as hard samples.}     \\label{fig:curriculm_learning}",
        "summarize_figure": "figure1",
        "summarization": "The picture illustrates the analysis of easy and hard samples in a curriculum learning strategy. For easy samples, the point cloud has a higher proportion of points with low curvature values, resulting in a curvature value distribution with high skewness. In the picture, the curvature value distribution shows that both the mean (red dashed line) and the threshold (black dashed line) are skewed to the left of the distribution, indicating that most points have low curvature values. The point cloud color gradually changes from blue to red, with blue representing low curvature and red representing high curvature."
    },
    "318": {
        "figure1": "2411.00461v1_Schematic",
        "label1": "fig:{Schematic",
        "caption1": "Encoder feature space of four test engines from FD001 visualized by t-SNE. (a) is the feature space for regression training only, and the features with the same RUL label between different engines are far away from each other. (b) is the feature space trained by multi-granularity supervised comparisons with close feature distances between different engines with the same RUL label.} ",
        "text": "\\centering   \\includegraphics[width=1\\linewidth]{fig/Schematic.pdf}   \\caption{Encoder feature space of four test engines from FD001 visualized by t-SNE. (a) is the feature space for regression training only, and the features with the same RUL label between different engines are far away from each other. (b) is the feature space trained by multi-granularity supervised comparisons with close feature distances between different engines with the same RUL label.}   \\label{fig:{Schematic}}\nHowever, most of the current deep learning-based RUL prediction approaches use only the mean square error (MSE) as the loss function, a metric that focuses only on the magnitude of the error without considering the underlying structural features, as shown in Fig \\ref{fig:{Schematic}}a. Contrast learning is an unsupervised learning method that uses contrast loss (e.g., infoNCE \\cite{oord2018representation} and triplet loss \\cite{weinberger2009distance}) to pull similar embeddings together and push dissimilar embeddings apart in feature space. This helps to improve the performance of downstream tasks \\cite{chen2020simple}. For labeled samples, supervised contrastive learning can be used. It extends a single positive sample in minibatch into multiple positive samples by the idea of class label-based aggregation, which results in tighter embedding of different classes \\cite{khosla2020supervised}.\nHowever, for RUL prediction tasks with dense labels, a huge batch size is required to balance positive and negative samples in supervised contrastive learning using RUL labels directly, which is usually limited by hardware. To solve this problem, this paper proposes a multi-granularity supervised contrastive (MGSC) framework that incorporates two contrastive strategies, coarse-grained and fine-grained, to balance the samples while avoiding large batch sizes through a large classification scale label of health status (HS). The intuition of the framework is that the features used as input to the RUL regression layer should be aligned by RUL labels. Based on this, a simple scalable network structure was designed for validation in \\ref{struc}. Moreover, to integrate the MGSC and regression tasks, a multi-phase training strategy is proposed. The proposed framework exceeds the accuracy of the baselines on the CMAPSS dataset, as shown in Fig \\ref{fig:{Schematic}}b.",
        "summarize_figure": "figure1",
        "summarization": "The encoder feature space of four test engines from FD001 is visualized by t-SNE. In the picture, the feature space trained by regression only shows that features with the same remaining useful life (RUL) label between different engines are far from each other, with a root mean square error (RMSE) of 14.78 and a score of 405.30."
    },
    "319": {
        "figure1": "2411.00461v1_HS",
        "label1": "fig:{HS",
        "caption1": "Supervised contrastive labels of HS and RUL.} ",
        "text": "In this paper, the normal operation phase of an aero-engine is defined as the period in which the remaining useful life (RUL) exceeds 125 units. During this phase, the engine experiences minimal performance degradation, and the RUL label is maintained at a constant value of 125. Subsequently, as the engine enters the degradation phase, the RUL label gradually decreases linearly from 125 to 0 with the operational time, as shown in Fig \\ref{fig:{HS}}.\n\\centering   \\includegraphics[width=0.65\\linewidth]{fig/HS.pdf}   \\caption{Supervised contrastive labels of HS and RUL.}   \\label{fig:{HS}}\nFor the HS labels, we have defined different HSs based on the RUL values, as shown in Fig \\ref{fig:{HS}}. HS0 is assigned when the RUL is equal to 125. HS1 is assigned when the RUL is between 100 and 124. HS2 is assigned in the range of 75 to 99. HS3 is assigned in the range of 50 to 74. HS4 is assigned in the range of 25 to 49. Finally, HS5 is assigned when the RUL is between 0 and 24.",
        "summarize_figure": "figure1",
        "summarization": "The image illustrates the supervised contrastive labels for Remaining Useful Life (RUL) and Health State (HS). The RUL decreases linearly from 125 to 0, representing the entire process from normal operation to complete failure of the equipment. The HS is discretized into six states (HS0 to HS5), corresponding to different RUL ranges: HS0 corresponds to RUL of 125, HS1 corresponds to RUL between 100 and 124, HS2 corresponds to 75 to 99, HS3 corresponds to 50 to 74, HS4 corresponds to 25 to 49, and HS5 corresponds to 0 to 24. The HS label decreases in a stepwise manner, reflecting the gradual degradation of the equipment's health state."
    },
    "320": {
        "figure1": "2411.00461v1_RUL",
        "label1": "fig:{RUL",
        "caption1": "Predicted results of the CNN-LSTM baseline for (a) FD001, (b) FD002, (c) FD003, and (d) FD004.} ",
        "text": "Fig. \\ref{fig:{RUL}} shows the RUL prediction results for the four sub-datasets using the CNN-LSTM+MGSC model. The comparison reveals a high similarity between the predicted RUL and the actual RUL, especially in the re FD001 and FD003 datasets. This indicates that the proposed model is able to predict the RUL of each engine better without large bias.   \\centering   \\includegraphics[width=0.88\\linewidth]{fig/RUL.pdf}   \\caption{Predicted results of the CNN-LSTM baseline for (a) FD001, (b) FD002, (c) FD003, and (d) FD004.}   \\label{fig:{RUL}}",
        "summarize_figure": "figure1",
        "summarization": "English:The image presents the Remaining Useful Life (RUL) prediction results of the CNN-LSTM baseline model for four datasets: FD001, FD002, FD003, and FD004. Each subplot displays the predicted RUL (blue line) against the actual RUL (red dashed line) for multiple test engines. The comparison shown in the image reveals a high similarity between the predicted and actual RUL values across these datasets, with this similarity being particularly noticeable in the FD001 and FD003 datasets."
    },
    "321": {
        "figure1": "2411.00461v1_Schematic2",
        "label1": "fig:{Schematic2",
        "caption1": "CNN-LSTM Encoder feature space of four test engines from FD002 visualized by t-SNE. (a) Without MGSC. (b) With MGSC.} ",
        "figure2": "2411.00461v1_Schematic4",
        "label2": "fig:{Schematic4",
        "caption2": "CNN-LSTM Encoder feature space of four test engines from FD004 visualized by t-SNE. (a) Without MGSC. (b) With MGSC.} ",
        "figure3": "2411.00461v1_Schematic3",
        "label3": "fig:{Schematic3",
        "caption3": "CNN-LSTM Encoder feature space of four test engines from FD003 visualized by t-SNE. (a) Without MGSC. (b) With MGSC.} ",
        "figure4": "2411.00461v1_Schematic4",
        "label4": "fig:{Schematic4",
        "caption4": "CNN-LSTM Encoder feature space of four test engines from FD004 visualized by t-SNE. (a) Without MGSC. (b) With MGSC.} ",
        "text": "Fig. \\ref{fig:{Schematic2}}-Fig. \\ref{fig:{Schematic4}} shows the encoder feature space visualization results for the FD002-FD004 dataset with or without the MGSC strategy. The t-SNE results show that the encoder of the used MGSC strategy constitutes an ideal manifold structure in the feature space. For samples of different engines with the same or similar RUL labels, their feature embeddings are close to each other and aligned. This is consistent with our intuition that aligning feature embeddings with the same RUL labels in the feature space before the regression layer will aid in RUL prediction. Furthermore, due to the combined effect of the MSE loss function and the fine-grained contrast loss, there is a clear degradation trend in the feature space, which is good for visualizing and explaining the health state of the aero-engine.\n\\centering   \\includegraphics[width=1\\linewidth]{fig/Schematic2.pdf}   \\caption{CNN-LSTM Encoder feature space of four test engines from FD002 visualized by t-SNE. (a) Without MGSC. (b) With MGSC.}   \\label{fig:{Schematic2}}   \\centering   \\includegraphics[width=1\\linewidth]{fig/Schematic3.pdf}   \\caption{CNN-LSTM Encoder feature space of four test engines from FD003 visualized by t-SNE. (a) Without MGSC. (b) With MGSC.}   \\label{fig:{Schematic3}}   \\centering   \\includegraphics[width=1\\linewidth]{fig/Schematic4.pdf}   \\caption{CNN-LSTM Encoder feature space of four test engines from FD004 visualized by t-SNE. (a) Without MGSC. (b) With MGSC.}   \\label{fig:{Schematic4}}",
        "summarize_figure": "figure1",
        "summarization": "The figure shows the t-SNE visualization of the CNN-LSTM encoder feature space for four test engines (#11, #44, #90, #133) from the FD002 dataset. Subfigure (a) displays the feature space distribution without the MGSC strategy, where points from different engines are somewhat clustered but generally mixed without a clear structure or trend. Subfigure (b) illustrates the feature space distribution with the MGSC strategy, where points are color-mapped according to RUL (Remaining Useful Life), transitioning from yellow (high RUL) to deep purple (low RUL). Compared to subfigure (a), subfigure (b) shows that the feature points form a more ideal manifold structure. Points with similar RUL values are close and aligned, and a clear, continuous degradation trend from high RUL to low RUL is evident, as indicated by the red arrow. This indicates that the MGSC strategy effectively organizes the feature space, making it more conducive to visualizing and predicting engine health status."
    },
    "322": {
        "figure1": "2411.00461v1_Schematic3",
        "label1": "fig:{Schematic3",
        "caption1": "CNN-LSTM Encoder feature space of four test engines from FD003 visualized by t-SNE. (a) Without MGSC. (b) With MGSC.} ",
        "figure2": "2411.00461v1_Schematic2",
        "label2": "fig:{Schematic2",
        "caption2": "CNN-LSTM Encoder feature space of four test engines from FD002 visualized by t-SNE. (a) Without MGSC. (b) With MGSC.} ",
        "figure3": "2411.00461v1_Schematic4",
        "label3": "fig:{Schematic4",
        "caption3": "CNN-LSTM Encoder feature space of four test engines from FD004 visualized by t-SNE. (a) Without MGSC. (b) With MGSC.} ",
        "text": "\\centering   \\includegraphics[width=1\\linewidth]{fig/Schematic2.pdf}   \\caption{CNN-LSTM Encoder feature space of four test engines from FD002 visualized by t-SNE. (a) Without MGSC. (b) With MGSC.}   \\label{fig:{Schematic2}}   \\centering   \\includegraphics[width=1\\linewidth]{fig/Schematic3.pdf}   \\caption{CNN-LSTM Encoder feature space of four test engines from FD003 visualized by t-SNE. (a) Without MGSC. (b) With MGSC.}   \\label{fig:{Schematic3}}   \\centering   \\includegraphics[width=1\\linewidth]{fig/Schematic4.pdf}   \\caption{CNN-LSTM Encoder feature space of four test engines from FD004 visualized by t-SNE. (a) Without MGSC. (b) With MGSC.}   \\label{fig:{Schematic4}}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the CNN-LSTM encoder feature space of four test engines (Engine number 11, Engine number 44, Engine number 90, Engine number 133) from the FD002 dataset visualized by t-SNE without MGSC. Each point in the figure represents a feature vector, with the color indicating the RUL mapping, from yellow to blue representing high to low RUL. It can be observed that the feature points of different engines exhibit some aggregation in space but are mixed together, without a clear separation trend."
    },
    "323": {
        "figure1": "2411.00461v1_Schematic4",
        "label1": "fig:{Schematic4",
        "caption1": "CNN-LSTM Encoder feature space of four test engines from FD004 visualized by t-SNE. (a) Without MGSC. (b) With MGSC.} ",
        "figure2": "2411.00461v1_Schematic2",
        "label2": "fig:{Schematic2",
        "caption2": "CNN-LSTM Encoder feature space of four test engines from FD002 visualized by t-SNE. (a) Without MGSC. (b) With MGSC.} ",
        "figure3": "2411.00461v1_Schematic2",
        "label3": "fig:{Schematic2",
        "caption3": "CNN-LSTM Encoder feature space of four test engines from FD002 visualized by t-SNE. (a) Without MGSC. (b) With MGSC.} ",
        "figure4": "2411.00461v1_Schematic3",
        "label4": "fig:{Schematic3",
        "caption4": "CNN-LSTM Encoder feature space of four test engines from FD003 visualized by t-SNE. (a) Without MGSC. (b) With MGSC.} ",
        "text": "Fig. \\ref{fig:{Schematic2}}-Fig. \\ref{fig:{Schematic4}} shows the encoder feature space visualization results for the FD002-FD004 dataset with or without the MGSC strategy. The t-SNE results show that the encoder of the used MGSC strategy constitutes an ideal manifold structure in the feature space. For samples of different engines with the same or similar RUL labels, their feature embeddings are close to each other and aligned. This is consistent with our intuition that aligning feature embeddings with the same RUL labels in the feature space before the regression layer will aid in RUL prediction. Furthermore, due to the combined effect of the MSE loss function and the fine-grained contrast loss, there is a clear degradation trend in the feature space, which is good for visualizing and explaining the health state of the aero-engine.\n\\centering   \\includegraphics[width=1\\linewidth]{fig/Schematic2.pdf}   \\caption{CNN-LSTM Encoder feature space of four test engines from FD002 visualized by t-SNE. (a) Without MGSC. (b) With MGSC.}   \\label{fig:{Schematic2}}   \\centering   \\includegraphics[width=1\\linewidth]{fig/Schematic3.pdf}   \\caption{CNN-LSTM Encoder feature space of four test engines from FD003 visualized by t-SNE. (a) Without MGSC. (b) With MGSC.}   \\label{fig:{Schematic3}}   \\centering   \\includegraphics[width=1\\linewidth]{fig/Schematic4.pdf}   \\caption{CNN-LSTM Encoder feature space of four test engines from FD004 visualized by t-SNE. (a) Without MGSC. (b) With MGSC.}   \\label{fig:{Schematic4}}",
        "summarize_figure": "figure1",
        "summarization": "English Summary:The picture displays the t-SNE visualization of the CNN-LSTM encoder feature space for four test engines (#11, #44, #90, #133) from the FD002 dataset. In subplot (a) without the MGSC strategy, the data points for different engines and varying RUL (Remaining Useful Life, color ranging from yellow for longer RUL to blue for shorter RUL) are scattered in the feature space without a distinct structure. In contrast, subplot (b) with the applied MGSC strategy shows that data points are clustered more cohesively according to RUL color. Points with similar RUL values are closer, forming a clear degradation trend trajectory indicated by the red arrow, demonstrating that this strategy helps construct a manifold structure in the feature space beneficial for RUL prediction."
    },
    "324": {
        "figure1": "2411.00491v1_confmat-corrections",
        "label1": "fig:confmat-corrections",
        "caption1": "Confusion Matrix for Corrected Relations and their Initially Predicted Labels.} \\vspace{-10pt",
        "text": "\\centering     \\includegraphics[width=\\columnwidth]{figures/confmat-corrections.pdf}     \\caption{Confusion Matrix for Corrected Relations and their Initially Predicted Labels.}     \\vspace{-10pt}     \\label{fig:confmat-corrections}\nFigure \\ref{fig:confmat-corrections} provides the confusion matrix for manually corrected relations in the test set (corrected relations on the y-axis) and their originally predicted relation as outputted by the conversion process (x-axis). The figure indicates that, apart from errors being overall rare, some relation predictions are completely reliable (e.g.~Hypophora are trivial to predict given RST's \\textsc{question-answer} annotations). The most frequent error type is inferring a \\textsc{cause} relation where annotators felt a less marked \\textsc{Expansion} was warranted. Generally speaking, correction into an \\textsc{Expansion} category was the most common type, likely because implicit \\textsc{Expansion} connectives, such as `and', `in fact', or `specifically' are among the easiest to insert between sentence pairs.",
        "summarize_figure": "figure1",
        "summarization": "The figure displays a confusion matrix comparing manually corrected relation labels on the y-axis with their initial predicted labels on the x-axis. The diagonal entries represent instances where the initial prediction matched the corrected label, indicating correct predictions. The matrix shows that many relation types are frequently predicted correctly, such as Hypophora, temporal.asynchronous, and temporal.synchronous, evidenced by large values on the main diagonal. However, off-diagonal entries highlight common misclassification patterns. Notably, several Expansion subtypes, including expansion.disjunction and expansion.equivalence, are often incorrectly predicted as expansion.conjunction. The matrix also reveals cases where contingency.cause is predicted for relations that are corrected to Expansion types such as expansion.instantiation or expansion.level-of-detail. While a significant number of predictions are accurate, the matrix clearly illustrates specific confusions between related relation types and within the broader Expansion category."
    },
    "325": {
        "figure1": "2411.00499v1_Fig5",
        "label1": "fig:fig5",
        "caption1": "Comparative analysis of the proposed model (baseline) against models with CAM module and Swin transformer block across various performance metrics: (a) accuracy, (b) precision, (c) recall, (d) F1 score, (e) IoU and (f) FAR",
        "text": "\\centering     \\includegraphics{Fig5.png}   \\caption{Comparative analysis of the proposed model (baseline) against models with CAM module and Swin transformer block across various performance metrics: (a) accuracy, (b) precision, (c) recall, (d) F1 score, (e) IoU and (f) FAR}   \\label{fig:fig5}",
        "summarize_figure": "figure1",
        "summarization": "This is a comparative analysis of the proposed model (baseline) against models with CAM module and Swin transformer block across various performance metrics. The image shows the curves of accuracy, precision, recall, F1 score, IoU, and FAR as a function of threshold. As can be seen, in terms of accuracy, the proposed model (Group1) reaches its peak at a threshold of about 0.4, outperforming the other two models. In terms of precision, the proposed model also shows a leading edge, increasing with the increase of the threshold. In terms of recall, the proposed model decreases as the threshold increases. In terms of F1 score, the proposed model outperforms the other models at different thresholds. In terms of IoU, the proposed model also performs the best, being higher than the other models at all thresholds. In terms of FAR, the proposed model is significantly lower than the other models, indicating that it has a lower false acceptance rate."
    },
    "326": {
        "figure1": "2411.00499v1_Fig6",
        "label1": "fig:fig6",
        "caption1": "Comparison of the total parameters and inference time",
        "text": "\\centering     \\includegraphics{Fig6.png}   \\caption{Comparison of the total parameters and inference time}   \\label{fig:fig6}",
        "summarize_figure": "figure1",
        "summarization": "English Summary:The image compares the proposed method, CAM, and Swin transformer models in terms of total learnable parameters and inference time. For learnable parameters, the proposed method and CAM both have 1.18 M, while Swin transformer is significantly higher at 7.98 M. In terms of inference time on the GPU, the proposed method is the shortest at 6.59 ms, followed by CAM at 10.17 ms, and Swin transformer is the longest at 11.2 ms. Regarding inference time on the CPU, the proposed method is still the shortest at 123.04 ms, followed by CAM at 176.23 ms, and Swin transformer is the longest at 367 ms."
    },
    "327": {
        "figure1": "2411.00499v1_Fig7",
        "label1": "fig:fig7",
        "caption1": "Demonstration of actual segmentation results",
        "text": "\\centering     \\includegraphics{Fig7.png}   \\caption{Demonstration of actual segmentation results}   \\label{fig:fig7}",
        "summarize_figure": "figure1",
        "summarization": "The figure demonstrates actual segmentation results. From left to right, each row shows different segmentation scenarios, including (a) to (j), totaling 10 scenarios. Each scenario includes four subfigures, corresponding to the segmentation results of Ground truth, Group 1 Proposed, Group 2 CAM, and Group 3 Swin transformer, respectively. The color bar indicates that the color from blue to red represents the predicted value from 0 to 1. By comparing the segmentation results of different methods with the ground truth, the performance of various segmentation methods can be visually evaluated."
    },
    "328": {
        "figure1": "2411.00499v1_Fig8",
        "label1": "fig:fig8",
        "caption1": "Variation of the metric IoU with azimuth",
        "text": "\\centering     \\includegraphics{Fig8.png}   \\caption{Variation of the metric IoU with azimuth}   \\label{fig:fig8}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the variation of the metric IoU with azimuth. There are three groups of data, corresponding to Group1 (proposed method), Group2 (CAM), and Group3 (Swin Transformer). The IoU value of Group1 is higher than the other two groups at all azimuth angles. The IoU value of Group1 increases as the azimuth increases from -45 degrees to 0 degrees, reaching a peak of approximately 0.908 at 0 degrees, and then decreases as the azimuth increases to 45 degrees. The IoU value of Group2 reaches a peak at 0 degrees, but is generally lower than Group1. The IoU value of Group3 is relatively stable with only slight variations, and it remains consistently lower than Group1 and Group2."
    },
    "329": {
        "figure1": "2411.00499v1_Fig9",
        "label1": "fig:fig9",
        "caption1": "Variation of the metric IoU with distance",
        "text": "\\centering     \\includegraphics{Fig9.png}   \\caption{Variation of the metric IoU with distance}   \\label{fig:fig9}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the variation of the metric IoU with distance for three groups: Group1 (proposed), Group2 (CAM), and Group3 (Swin Transformer). As shown in the figure, the IoU values of all groups decrease as the distance increases. At short distances (0-4 meters), Group1 has the highest IoU value, outperforming Group2 and Group3. As the distance increases, the rate of decrease in IoU for Group1 slows down and stabilizes after 6 meters, remaining higher than Group2 and Group3. The IoU values for Group2 and Group3 also stabilize after 6 meters, but the values are lower than Group1. Overall, Group1 demonstrates relatively better performance at various distances."
    },
    "330": {
        "figure1": "2411.00499v1_Fig10",
        "label1": "fig:fig10",
        "caption1": "Comparison of the IoU metric in different scenes",
        "text": "\\centering     \\includegraphics{Fig10.png}   \\caption{Comparison of the IoU metric in different scenes}   \\label{fig:fig10}",
        "summarize_figure": "figure1",
        "summarization": "The figure compares the Intersection over Union (IoU) metric of three methods (Proposed, CAM, and Swin transformer) in different scenes, including laboratory, corridors, open spaces, and mining tunnels. Across all scenes, the Proposed method consistently shows higher IoU values than CAM and Swin transformer. Specifically, the Proposed method performs best in open spaces and mining tunnels, achieving an IoU of 0.92. In the laboratory scene, the IoU values of all three methods are relatively low, with Proposed at 0.86, CAM at 0.83, and Swin transformer at 0.82. In the corridors scene, the IoU value for the Proposed method is 0.89, while CAM and Swin transformer both have a value of 0.85."
    },
    "331": {
        "figure1": "2411.00499v1_Fig11",
        "label1": "fig:fig11",
        "caption1": "Comparison of the IoU metric in different scenes",
        "text": "\\centering     \\includegraphics{Fig11.png}   \\caption{Comparison of the IoU metric in different scenes}   \\label{fig:fig11}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the performance comparison of RD tensor (Proposed), RA tensor, and Raw ADC data across six metrics: Accuracy, Precision, Recall, F1 score, IoU, and FAR, under varying thresholds. Regarding Accuracy (a), RD tensor and RA tensor exhibit similar performance, surpassing Raw ADC data. In terms of Precision (b), RD tensor slightly outperforms RA tensor, with both significantly exceeding Raw ADC data. For Recall (c), RD tensor marginally outperforms RA tensor at lower thresholds, but its performance declines as the threshold increases, while RA tensor's decline is more pronounced, and Raw ADC data performs the worst. Regarding the F1 score (d), RD tensor and RA tensor perform similarly and outperform Raw ADC data. In terms of IoU (e), RD tensor and RA tensor are superior to Raw ADC data. For FAR (f), RD tensor and RA tensor significantly outperform Raw ADC data, with RD tensor performing slightly better than RA tensor. Overall, RD tensor exhibits excellent performance across multiple metrics, making it a competitive algorithm."
    },
    "332": {
        "figure1": "2411.00527v1_metric_distance_nocircle",
        "label1": "fig:metrics_distance",
        "caption1": "We plot the distribution of the mean error across all objects with respect to different object-to-sensor distances. Solid (---) and dashed (- -) horizontal lines indicate the median and the mean of the distribution, respectively. The results are discussed in \\autoref{sec:discussion_depth}.",
        "figure2": "2411.00527v1_radar_signal_analysis",
        "label2": "fig:radar_intensity",
        "caption2": "We present the received signal magnitude (mean absolute phasor value) in relation to the mean depth deviation of metric \\protect{}\\mone{}\\protect{} and further investigate both quantities with respect to varying object material, geometry (median surface incidence angle), and size (relative surface area). We highlight measurements where large objects appear outside the radar's field of view (FOV) in grey regions, as they exhibit comparatively higher depth deviations compared to the ground truth reconstructions that possibly extend beyond this FOV. The results are discussed in \\autoref{sec:discussion_radar_signal}.",
        "text": "In \\autoref{table:metrics}, we list the mean $\\mu$ and standard deviation $\\sigma$ of each metric type with respect to 12 selected objects from the MAROON dataset.  These objects were positioned at an object-to-sensor distance of 30~cm. We provide object images and a comprehensive evaluation of all 45 objects in the supplementary material. For completeness, we give a brief overview of the overall statistics by investigating the number of best and worst results across all objects for $\\mu$, respectively: \t\\item RF ToF: performs worst in \\mone{} \t\\item NIR ToF: performs worst in \\mtwo{}, \\mthree{}, \\mfour{} \t\\item Active Stereo: performs best in \\mone{}, \\mtwo{}, \\mthree{}, \\mfour{} \t\\item Passive Stereo: performs neither best or worst To give an intuition on relative depth deviations between sensors, we present the median, mean, and standard deviation, denoted as $\\widetilde{\\mu}/\\mu~(\\pm \\sigma)$, calculated across the differences in metric values for all pairwise sensor combinations. The results for each metric type are: \t\\item \\mone{}:~$0.23~\\text{cm}/0.48~\\text{cm}~(\\pm 0.61~\\text{cm})$ \t\\item \\mtwo{}:~$0.19~\\text{cm}/1.06~\\text{cm}~(\\pm 3.53~\\text{cm})$ \t\\item \\mthree{}:~$0.34~\\text{cm}/1.58~\\text{cm}~(\\pm 4.36~\\text{cm})$ \t\\item \\mfour{}:~$0.31~\\text{cm}/1.78~\\text{cm}~(\\pm 5.14~\\text{cm})$ Additionally, we illustrate the distribution of the depth deviation across all objects for varying placement distances of 30~cm, 40~cm, and 50~cm. The results are displayed as boxplots in \\autoref{fig:metrics_distance}.\n\\centering \t\\includegraphics[width=\\linewidth]{images/metric_distance_nocircle.pdf} \t\\caption{We plot the distribution of the mean error across all objects with respect to different object-to-sensor distances. Solid (---) and dashed (- -) horizontal lines indicate the median and the mean of the distribution, respectively. The results are discussed in \\autoref{sec:discussion_depth}.} \t\\label{fig:metrics_distance} \t\\centering \t\\includegraphics[width=\\linewidth]{images/radar_signal_analysis.pdf} \t\\caption{We present the received signal magnitude (mean absolute phasor value) in relation to the mean depth deviation of metric \\protect{}\\mone{}\\protect{} and further investigate both quantities with respect to varying object material, geometry (median surface incidence angle), and size (relative surface area). We highlight measurements where large objects appear outside the radar's field of view (FOV) in grey regions, as they exhibit comparatively higher depth deviations compared to the ground truth reconstructions that possibly extend beyond this FOV. \tThe results are discussed in \\autoref{sec:discussion_radar_signal}.} \t\\label{fig:radar_intensity}\nConsidering the results over varying object-to-sensor distances in \\autoref{fig:metrics_distance}, we observe that the depth deviation of the passive stereo and the NIR ToF sensor is significantly more distance-specific compared to active stereo and RF ToF. For passive stereo, this may be due to a decrease in effective spatial resolution, where $\\delta_z$ directly depends on $\\delta_{x,y}$ (see supplementary). As the distance between the object and the sensor increases, $\\delta_{x,y}$ decreases, resulting in a loss of high-frequency color details while the object appears smaller in the image. Compared to passive stereo, the active stereo sensor has a comparably higher effective resolution, assuming that the resolution of both sensors differs in accordance with $\\delta_{xyz}$ in \\autoref{table:sensors}. The unique active NIR pattern may also be less sensitive to decreases in spatial resolution, maintaining the quality of correspondence matches. The trend for the NIR ToF sensor aligns with findings from Bamji et al.~\\shortcite{kinect_bamji_2018} for 30--50~cm distances. However, we argue that absolute errors for a target with 20\\% reflectivity do not fully represent all objects in our experiments. We suggest that, in addition to the expected decrease in spatial resolution, a greater depth deviation with increasing distance arises from the signal-to-noise ratio with respect to environmental light, which typically decreases over distance due to the inverse-square law.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the distribution of mean error for various depth sensors (RF ToF, NIR ToF, Active Stereo, and Passive Stereo) at different object-to-sensor distances (30 cm, 40 cm, and 50 cm). Each subplot represents a different metric type (Metric Type Cg, Metric Type Cs, Metric Type P, Metric Type Pe), with boxplots showing the error distribution for each distance and sensor type. Solid and dashed lines within the boxplots indicate the median and mean, respectively. It can be observed that as the distance increases, the error distribution range becomes larger for some sensors (such as NIR ToF), indicating an increase in error with distance. In contrast, the Active Stereo sensor exhibits a relatively stable error distribution across different distances."
    },
    "333": {
        "figure1": "2411.00527v1_signal_thresholds",
        "label1": "fig:signal_threshold",
        "caption1": "Visualization of the 2D confidence map from the \\textsl{\\textsf{{S1 Hand Open}}} capture at various thresholds. The filtered confidence map is subsequently used to extract valid depth information.",
        "text": "\\centering \t\\includegraphics[width=0.8\\linewidth]{images/signal_thresholds.pdf} \t\\caption{Visualization of the 2D confidence map from the \\textsl{\\textsf{{S1 Hand Open}}} capture at various thresholds. The filtered confidence map is subsequently used to extract valid depth information.} \t\\label{fig:signal_threshold}\nFor our experiments and within the dataset, we chose an empirical threshold of $-14$~dB over all objects, which\\;---\\;to the best of our knowledge\\;---\\;has proven to yield the best balance of noise pruning while retaining relevant object measurements. In \\autoref{fig:signal_threshold}, we show how the signal-to-noise ratio of the radar confidence map, i.e., the absolute value of $c_{\\text{BP}}$, behaves over different thresholds for the \\textsl{\\textsf{{S1 Hand Open}}} capture.",
        "summarize_figure": "figure1",
        "summarization": "The image visualizes 2D confidence maps of the S1 Hand Open capture at various thresholds. From left to right, the noise level increases, corresponding to thresholds of -7 dB, -14 dB (the authors' chosen threshold), -21 dB, and no threshold. As the threshold decreases (noise level increases), the number of noise points retained in the confidence map gradually increases, and the outline of the hand becomes increasingly blurred. At -14 dB, the shape features of the hand are well preserved while removing most of the noise. This filtered confidence map is then used to extract valid depth information."
    },
    "334": {
        "figure1": "2411.00527v1_radar_signal_analysis2",
        "label1": "fig:radar_intensity_2",
        "caption1": "We present the signal magnitude (\\textit{left}) and depth deviation with respect to metric type~\\mone{} (\\textit{right}) in relation to object geometry (median surface incidence angle) and object size (relative surface area). Large objects outside the radar's  field of view (FOV) exhibit higher depth deviations with respect to the ground truth reconstructions that possibly extend beyond this FOV.",
        "text": "\\centering \t\\includegraphics[width=0.9\\linewidth]{images/radar_signal_analysis2.pdf} \t\\caption{We present the signal magnitude (\\textit{left}) and depth deviation with respect to metric type~\\mone{} (\\textit{right}) in relation to object geometry (median surface incidence angle) and object size (relative surface area). Large objects outside the radar's  field of view (FOV) exhibit higher depth deviations with respect to the ground truth reconstructions that possibly extend beyond this FOV.} \t\\label{fig:radar_intensity_2}\nIn the main paper, we presented both, the signal magnitude and the depth deviation concerning object material, geometry, and size. To demonstrate that the signal magnitude is not only influenced by either object geometry or by object size\\;---\\;which would be possible due to the high diversity of captured objects that prevents us from isolating one variable while keeping the others constant\\;---\\;we visualize the signal magnitude in \\autoref{fig:radar_intensity_2} (\\textit{left}), in relation to object geometry (median surface incidence angle) and size (relative surface area). A general trend on the $x$-axis shows an increase in the signal magnitude from left to right, particularly for objects with a surface incidence angle greater than $10^\\circ$. The majority of objects below this $10^\\circ$ angle on the $y$-axis exhibit significantly higher signal magnitudes, regardless of their relative surface area, hence indicating the influence of object geometry. On the \\textit{right} side of \\autoref{fig:radar_intensity_2}, we visualize the mean depth deviation in relation to object geometry and size. While a similar trend is observed for object geometry on the $y$-axis\\;---\\;where objects of lower surface incidence angle exhibit smaller errors\\;---\\;no overall trend appears on the $x$-axis, suggesting depth deviation is more influenced by object geometry instead of size.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the relationship between signal magnitude and object geometry (median surface incidence angle) and size (relative surface area). The x-axis represents relative surface area, and the y-axis represents median surface incidence angle. The color gradient indicates the magnitude of the mean phasor, with darker colors indicating higher magnitudes. A general trend shows an increase in signal magnitude from left to right, particularly for objects with a surface incidence angle greater than 10 degrees. Objects below 10 degrees on the y-axis exhibit significantly higher signal magnitudes, indicating the influence of object geometry on the signal magnitude."
    },
    "335": {
        "figure1": "2411.00533v2_F1On10Clusters",
        "label1": "FIG:lalebs_per_cluster",
        "caption1": "F1 Score on WikiGold when the number of clusters is fixed at 10.",
        "text": "Notably, as it is shown in Figure \\ref{FIG:lalebs_per_cluster}, when the number of clusters is fixed, simply increasing the number of labels constructed for each cluster is unlikely to bring performance improvement and may instead lead to a decrease in F1 score stability.\n\\centering \t\\includegraphics[width=.9\\columnwidth]{F1On10Clusters.pdf} \t\\caption{F1 Score on WikiGold when the number of clusters is fixed at 10.} \t\\label{FIG:lalebs_per_cluster}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the F1 score performance on WikiGold when the number of clusters is fixed at 10. The x-axis represents the number of labels per cluster, ranging from 1 to 10, and the y-axis represents the mean F1 score. As the number of labels per cluster increases, the mean F1 score does not show a significant improvement and exhibits some fluctuation. The F1 scores are relatively higher when the number of labels is 3, 4, and 5, but they decrease with further increases in the number of labels. The error margin also indicates that the stability of the F1 score decreases as the number of labels increases."
    },
    "336": {
        "figure1": "2411.00543v2_figure5_limited_view",
        "label1": "fig:limited_views",
        "caption1": "\\textbf{Experiment on ModelNet10-SO(3) with few-shot training views.} Results with solid lines of I-PDF~\\cite{murphy2021implicit}, I2S~\\cite{klee2023image}, and RotLaplace~\\cite{yin2023laplace} denote to a ResNet-50 backbone, while dotted lines indicate a ResNet-101 backbone. Our method outperforms all metrics and reduces training views. Baseline results \\cite{yin2023laplace, klee2023image} were obtained using the source code provided by the authors. ",
        "text": "\\centering     \\scalebox{0.36}{     \\includegraphics{figures/figure5_limited_view.pdf}     }     \\vspace{-0.2cm}     \\caption{\\textbf{Experiment on ModelNet10-SO(3) with few-shot training views.} Results with solid lines of I-PDF~\\cite{murphy2021implicit}, I2S~\\cite{klee2023image}, and RotLaplace~\\cite{yin2023laplace} denote to a ResNet-50 backbone, while dotted lines indicate a ResNet-101 backbone. Our method outperforms all metrics and reduces training views. Baseline results \\cite{yin2023laplace, klee2023image} were obtained using the source code provided by the authors.      }     \\label{fig:limited_views}\nFigure~\\ref{fig:limited_views} illustrates the pose estimation results on ModelNet10-SO(3) with few-shot training views. Notably, as the number of training views from a single CAD model decreases, our model consistently achieves the highest accuracy and lowest error. This performance surpasses the baselines of direct rotation regression~\\cite{zhou2019continuity, bregier2021deep}, parametric distribution parameters regression~\\cite{yin2023laplace}, and non-parametric distribution estimation~\\cite{murphy2021implicit, klee2023image}. This few-shot training experiment verifies that our SO(3)-equivariant model %, trained with Wigner-D representation,  contributes to superior data efficiency and generalization to unseen rotations.",
        "summarize_figure": "figure1",
        "summarization": "English Summary:The image presents pose estimation experiment results on ModelNet10-SO(3) using a limited number of training views. The left and middle plots show Acc@15 degrees and Acc@30 degrees accuracy respectively, while the right plot displays the median rotation error, all as functions of the number of training views ranging from 3 to 100. The results indicate that accuracy generally increases and error decreases with more training views for most methods. Compared to baselines including Zhou et al. (2019), Brégier (2021), I-PDF, I2S, and RotLaplace, our method consistently achieves the highest Acc@15 and Acc@30 degrees accuracy and the lowest median rotation error across all tested training view counts. This performance superiority is particularly notable in few-shot scenarios with fewer training views. Furthermore, employing a ResNet-101 backbone (dotted lines) generally leads to better performance compared to using ResNet-50 (solid lines), which is observed for both our method and RotLaplace."
    },
    "337": {
        "figure1": "2411.00543v2_supp_fig1_limited_view_lower_thres",
        "label1": "fig:limited_views_lower_thres",
        "caption1": "\\textbf{Results with finer thresholds on ModelNet10-SO(3) few-shot training views.} Results with solid lines denote a ResNet-50 backbone, while dotted lines indicate a ResNet-101 backbone. Our method outperforms all metrics and reduces training views even at finer thresholds. For comparison, the I2S (ResNet-50) model~\\cite{klee2023image} is shown with a blue line, and the RotLaplace (ResNet-50 and ResNet-101) models~\\cite{yin2023laplace} are depicted with purple solid and dashed lines, respectively. ",
        "text": "Figure~\\ref{fig:limited_views_lower_thres} shows the results with finer thresholds, Acc@3°, Acc@5°, and Acc@10°, on the ModelNet10-SO(3) few-shot training views, which are additional results to Figure~\\ref{fig:limited_views}. The graphs illustrate that our method outperforms all other methods across all metrics (Acc@3°, Acc@5°, and Acc@10°) and requires fewer training views to achieve high accuracy, even at finer thresholds.  This shows that our model is capable of more precise pose estimation with less number of training data, proving the data efficiency of our SO(3)-equivariant harmonics pose estimator. Baseline results \\cite{yin2023laplace, klee2023image} were obtained using the source code provided by the authors.\n\\centering     \\scalebox{0.36}{     \\includegraphics{figures/supp_fig1_limited_view_lower_thres.pdf}     }     \\caption{\\textbf{Results with finer thresholds on ModelNet10-SO(3) few-shot training views.} Results with solid lines denote a ResNet-50 backbone, while dotted lines indicate a ResNet-101 backbone. Our method outperforms all metrics and reduces training views even at finer thresholds. For comparison, the I2S (ResNet-50) model~\\cite{klee2023image} is shown with a blue line, and the RotLaplace (ResNet-50 and ResNet-101) models~\\cite{yin2023laplace} are depicted with purple solid and dashed lines, respectively.     }     \\label{fig:limited_views_lower_thres}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the performance of different models on the ModelNet10-SO(3) dataset at 3 degrees accuracy (Acc3 degree) with varying numbers of training views. The horizontal axis represents the number of training views, and the vertical axis represents the accuracy. It compares the performance of I2S (ResNet-50), RotLaplace (ResNet-50), RotLaplace (ResNet-101), and the proposed models (ResNet-50 and ResNet-101). As shown in the figure, the proposed models (ResNet-50 and ResNet-101) outperform other models under all training view numbers. Specifically, when the number of training views is small (e.g., 3 to 20), the accuracy improvement of the proposed model is particularly significant. In addition, models using the ResNet-101 backbone (dashed lines) generally perform better than models using the ResNet-50 backbone (solid lines)."
    },
    "338": {
        "figure1": "2411.00593v2_adapt_eval_loss",
        "label1": "fig:model_transfer",
        "caption1": "Evaluation loss after initializing OLMo-7B with token translator $\\rmP$ learned from OLMo-1B.  Along the x-axis, $\\text{S2T2-}\\alpha$ represent S2T2 with the $\\alpha$-entropy regularizer that controls the sparsity of $\\rmP$. \\textbf{New Tok.} is OLMo-7B with the new tokenizer and truncated $\\rmE, \\rmL$; \\textbf{Orig Tok.} is OLMo-7B with the original tokenizer. The red dashed line is the loss when you \\textbf{randomly guess} the next token.",
        "text": "\\centering \t\\includegraphics[scale=0.4]{figures/adapt_eval_loss.pdf} \t\\caption{Evaluation loss after initializing OLMo-7B with token translator $\\rmP$ learned from OLMo-1B.  Along the x-axis, $\\text{S2T2-}\\alpha$ represent S2T2 with the $\\alpha$-entropy regularizer that controls the sparsity of $\\rmP$. \\textbf{New Tok.} is OLMo-7B with the new tokenizer and truncated $\\rmE, \\rmL$; \\textbf{Orig Tok.} is OLMo-7B with the original tokenizer. The red dashed line is the loss when you \\textbf{randomly guess} the next token.}  \\label{fig:model_transfer}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the evaluation loss after initializing OLMo-7B with a token translator P learned from OLMo-1B. The x-axis represents different model configurations, including S2T2 models (with varying alpha-entropy regularizers controlling the sparsity of P), OLMo-7B with the new tokenizer (New Tok.), and OLMo-7B with the original tokenizer (Orig. Tok.). The y-axis indicates the evaluation cross-entropy loss. A red dashed line marks the loss of randomly guessing the next token, at a value of 6.24. The loss values for S2T2-0.01 and S2T2-0.1 models are both 5.94, and the S2T2-1.0 model has a loss of 6.09. The \"New Tok.\" model shows a loss of 6.44, while the \"Orig. Tok.\" model shows a loss of 6.46."
    },
    "339": {
        "figure1": "2411.00608v1_front",
        "label1": "fig:perfcomp",
        "caption1": "Performance comparison of \\name with baselines on embedded devices on MOT16. Circle size indicates the memory usage, while the number above the circle represents processing rate in fps.} \\vspace{-4 mm",
        "text": "Table~\\ref{tbl:all_com} shows a comparative analysis of existing frameworks on both high-end GPUs and embedded devices (Jetson AGX). Figure~\\ref{fig:perfcomp} illustrates \\name's balanced performance in accuracy, processing rate, energy and memory usage comparing with baseline frameworks. Figure~\\ref{fig: mota_imp} highlights the gradual accuracy improvement on embedded devices over the years, emphasizing the need for further exploration in this area.  We summarize our main contributions below.",
        "summarize_figure": "figure1",
        "summarization": "The image presents a performance comparison of different frameworks on embedded devices, measuring energy consumption (in kJ), MOTA accuracy, processing rate (in frames per second), and memory usage. The x-axis shows MOTA percentage, and the y-axis shows energy consumption. Circle size indicates memory usage, and numbers denote processing rate in fps. Five frameworks are compared: JDE(Embed), RTMOVT, Byte(Embd), HopTrack(Full), and HopTrack(Swift). JDE(Embed) and RTMOVT exhibit lower MOTA values, while HopTrack(Full), Byte(Embd), and HopTrack(Swift) achieve higher MOTA. Specifically, HopTrack(Full) achieves the highest MOTA accuracy, whereas HopTrack(Swift) demonstrates the lowest energy consumption and highest processing rate, along with relatively lower MOTA and the smallest memory footprint. Byte(Embd) shows moderate MOTA but high energy consumption. The image visually represents the trade-offs between these performance metrics for the evaluated frameworks on embedded devices."
    },
    "340": {
        "figure1": "2411.00623v1_Inference",
        "label1": "flops_time",
        "caption1": "Figure (a) demonstrates the approximated average FLOPs (Tera) during training and inference on each batch of data points. Figure (b) demonstrates the actual average running time for different schemes to perform inference on a task, using one NVIDIA A100 GPU.",
        "text": "\\centering \t  \\subfloat[FLOPs]{        \\includegraphics[width=0.5\\linewidth]        {figures/FLOPs.png}} \t  \\subfloat[Inference Time]{         \\includegraphics[width=0.49\\linewidth]{figures/Inference.png}} \t\\caption{Figure (a) demonstrates the approximated average FLOPs (Tera) during training and inference on each batch of data points. Figure (b) demonstrates the actual average running time for different schemes to perform inference on a task, using one NVIDIA A100 GPU.} To better compare DualLoRA with other baselines in terms of computation, we report number of floating point operations (FLOPs) during the training and inference phases in Figure.\\ref{flops_time}.(a) and the average inference time on a task in Figure.\\ref{flops_time}.(b). According to the results in the figure, InfLoRA has the lowest FLOPs during inference, while DualLoRA has the lowest FLOPs during training because InfLoRA requires a double forward pass as mentioned earlier. During inference, InfLoRA and DualLoRA have similar inference time across different datasets, which are less than 50\\% inference time compared to the prompt-base CL schemes. Details on computing FLOPs are provided in Appendix 3.",
        "summarize_figure": "figure1",
        "summarization": "The image presents the approximated average FLOPs in Tera during training (left bar) and inference (right bar) for various schemes across different datasets. Prompt-based methods including L2P, DualPrompt, and CodaPrompt exhibit very high training and inference FLOPs on all datasets. In contrast, InfLoRA shows high training FLOPs comparable to prompt-based methods but significantly lower inference FLOPs, being the lowest among all schemes. DualLoRA and DualLoRA+ demonstrate the lowest training FLOPs and also have inference FLOPs much lower than prompt-based methods, although slightly higher than InfLoRA's inference FLOPs."
    },
    "341": {
        "figure1": "2411.00627v1_results4_new",
        "label1": "fig:rst4",
        "caption1": "The percentage of correctly classified examples in the test set versus \\emph{(a)} the removal percentage for each model in each side of a polygon and \\emph{(b)} the number of vertices at 10\\% removal percentage. ",
        "text": "The performance of models on the training set (when the removal percentage is 0) is depicted in Fig. \\ref{fig:rst4}. VGG16 and SqueezeNet V1.1 perform relatively well with an accuracy of around 90\\%. AlexNet and ResNet50 also have a good performance and achieve an accuracy of higher than 70\\%. Inception V3, ShuffleNet V2 and DenseNet121 obtain an accuracy of more than 60\\%. However, the accuracy of EfficientNet B0 and MobileNetV3 are between 40\\% and 50\\%, respectively, due to their lower complexities. It suggests that they cannot do the task reliably.\nA closer inspection of model performance reveals that accuracy decreases by removal percentage as a general trend (Fig.~\\ref{fig:rst4}(a)), but the performance of individual CNNs over vertices is inconsistent (Fig.~\\ref{fig:rst4}(b)). The accuracy of classification remains much higher than chance level (10\\%) when the removal percentage is not larger than 20\\% to 30\\%, although it decreases quickly when the percentage of removal is larger. This aligns with the findings of Amanatiadis \\emph{et al.} \\cite{amanatiadis2018}. However, through the small gradual manipulations in our experiment, we observe a drastic decrease in accuracy already at the first stimulus manipulation. This is not in accordance with contour completion elicited by the closure effect.",
        "summarize_figure": "figure1",
        "summarization": "The figure shows the percentage of correctly classified samples in the test set for different models after removing different proportions on each side of a polygon. The horizontal axis represents the removal percentage, and the vertical axis represents the accuracy. It can be seen from the figure that as the removal percentage increases, the accuracy of all models generally shows a decreasing trend. When the removal percentage is less than 20% to 30%, the classification accuracy is much higher than the random level of 10%, but when the removal percentage increases, the accuracy decreases rapidly. Among them, the VGG16 model performs best when the removal percentage is low, while the accuracy of EfficientNet B0 and MobileNetV3 models is relatively low."
    },
    "342": {
        "figure1": "2411.00646v1_Figure_5_logitlens_Recall_avg3datasets",
        "label1": "fig:5",
        "caption1": "Averaged recall of decoded LogitLens words. Results are averaged over 2 datasets, each of which consists of randomly chosen 400 images.",
        "text": "In addition, we found that the first several tokens received far more attention than other tokens. In particular, starting from the middle layers, their attention scores become substantially stronger than those of other tokens as the layer deepens. To explain this observation more intuitively, we plot the norm-based attention scores of the last text token to several chosen non-trivial tokens against the layer number. As shown in Fig.~\\ref{fig:5} Below, we found that across all layers, the first five tokens are allocated significantly more attention than the others.\nFig.~\\ref{fig:5} shows the averaged recall score of the decoded LogitLens words using annotated captions as ground truth. We find that the recall coheres well with the phase diagram of multimodal contextualization, especially Phase III and IV.\n\\centering     \\includegraphics[width=0.8\\linewidth]{Figures/Figure_5_logitlens_Recall_avg3datasets.pdf}     \\caption{Averaged recall of decoded LogitLens words. Results are averaged over 2 datasets, each of which consists of randomly chosen 400 images.}     \\label{fig:5}",
        "summarize_figure": "figure1",
        "summarization": "The picture illustrates the averaged recall of decoded LogitLens words, with results averaged over two datasets, each consisting of 400 randomly chosen images. As shown in the picture, the averaged recall increases with the number of Transformer layers and then becomes stable. The recall rate is low and close to 0 in Phase I and Phase II. From Phase II to Phase III, the recall increases significantly, reaching a peak of approximately 0.25 in Phase III. After entering Phase IV, the recall decreases slightly but remains around 0.2."
    },
    "343": {
        "figure1": "2411.00660v1_function",
        "label1": "fig:scale_func",
        "caption1": "The plotted curve of $f(x)=x^{0.095}-x^{0.076}$. When $0 < x \\ll 10^{15} $, $0 < f(x) \\ll 10$.}  \\end{center} \\vskip -0.2in",
        "text": "By constructing the function $f(x)=x^{0.095}-x^{0.076}$ and plotting its curve (Fig. \\ref{fig:scale_func}), it becomes evident that when $0 < x \\ll 10^{15} $, $0 < f(x) \\ll 10$. Therefore, it can be inferred that the bases on both sides of the equation are nearly equal, with the difference in exponents attributed to observational errors.",
        "summarize_figure": "figure1",
        "summarization": "The picture displays the curve of the function f(x) = x to the power of 0.095 minus x to the power of 0.076. The x-axis uses a logarithmic scale ranging from 10 to the power of 0 up to 10 to the power of 15. The y-axis also uses a logarithmic scale, spanning from 10 to the power of negative 8 to 10 to the power of 1. The curve starts near 0 at x=1 and increases monotonically as x increases. For x values much less than 10 to the power of 15, the value of f(x) remains between 0 and 10. The growth of the curve is initially steeper at smaller x values and becomes gradually less steep as x increases."
    },
    "344": {
        "figure1": "2411.00662v1_fig17",
        "label1": "fig16",
        "caption1": "End-to-End model latency.",
        "text": "We test the Baseline against the $O_{1}/O_{2}/O_{3}$ optimizations, and the convergence curves are completely consistent, not affecting the numerical results. It can be seen that MoNTA selected the $O_{1}$ strategy.It can be observed that the performance of $O_{1}$ and $O_{2}$ is comparable and better than the Baseline, with the proportion of AllToAll communication reduced from 22\\% to 10\\%. The normalized end-to-end latency of the model is shown in Fig.~\\ref{fig16}. $O_{1}$ and $O_{2}$ have similar performance, with an overall latency reduction of about 13\\%. In contrast, the performance of $O_{3}$ declined compared to $O_{1}$ and $O_{2}$.",
        "summarize_figure": "figure1",
        "summarization": "This is a bar chart about the normalized end-to-end latency of the model. The image shows the latency of four different configurations, Baseline (AllToAll plus PXN), MoNTA, O2 (AllToAll plus AllGather Pipeline), and O3 (AllToAll plus AllGather Pipeline and AllGather plus D2D Pipeline). The latency of MoNTA and O2 are similar and approximately 13% lower than the Baseline. The latency of O3 is slightly higher than MoNTA and O2, but still lower than the Baseline."
    },
    "345": {
        "figure1": "2411.00664v2_vq_memory_eval_impl",
        "label1": "fig:vq_memory_eval_impl",
        "caption1": "Memory usage of baseline and our proposed approach with or without a custom kernel to perform index selection, sum reduction and retrieval at the same time. The FSQ uses \\mbox{$G=16$} groups and \\mbox{$\\sL=[8,5,5,5]$} levels.",
        "text": "The problem %with this implementation  is that the index selection step would create an intermediate tensor, \\texttt{a}, of shape \\mbox{\\texttt{[T, B*L*G]}}, which is dominated by \\texttt{B} for large biasing catalogues. For instance, for a typical setting in our experiments we could have \\texttt{B=10000}, \\texttt{L=4}, \\texttt{G=16}, and \\texttt{T=33} for a 2sec audio (200 frames with 6-fold downsampling). Additionally, the TopK operation is performed only after we have the entire score tensor, \\texttt{s}. That said, the memory consumption in that case is still smaller than the baseline approach that needs $O(|\\sB| D)$ space, since \\mbox{$D > G \\cdot |\\sL|$} in a typical setting. To achieve full memory reduction, however, one would need to implement a kernel that performs index selection, sum reduction and TopK retrieval at the same time as in Algorithm~\\ref{alg:fsq_cross_attention}. Fig.~\\ref{fig:vq_memory_eval_impl} compares the memory usage with or without this custom kernel, using the same assumptions as in Section~\\ref{subsec:compute_memory_eval}.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the memory usage of the Baseline, FSQ_G16_L8555_No_Kernel, and FSQ_G16_L8555_Kernel methods with varying numbers of biasing phrases. The x-axis represents the number of biasing phrases, ranging from 10k to 1m, and the y-axis represents memory usage in MB. As the number of biasing phrases increases, the memory usage of the Baseline method increases significantly, reaching 982.7MB at 1m biasing phrases. The memory usage of the FSQ_G16_L8555_No_Kernel method also increases with the number of biasing phrases, but the increase is less than that of the Baseline method. The memory usage of the FSQ_G16_L8555_Kernel method is much lower than both the Baseline and FSQ_G16_L8555_No_Kernel methods across all numbers of biasing phrases, indicating that using a custom kernel can effectively reduce memory usage."
    },
    "346": {
        "figure1": "2411.00664v2_tab2_success_v3",
        "label1": "fig:retrieval_success_rates",
        "caption1": "Retrieval success rates for Top1, Top5, and Top10, for various FSQ settings. The baseline numbers (w/o quantization) are given in the legend.",
        "figure2": "2411.00664v2_tab2_retrieved_v3",
        "label2": "fig:retrieval_mean_phrases",
        "caption2": "Average number of phrases retrieved per utterance for Top1, Top5, and Top10 retrieval, for various FSQ settings. The baseline (w/o quantization) numbers are given in the legend. The error bars represent one standard deviation from the averages.",
        "text": "We evaluate the retrieval capabilities of the models on a random subset of 1.4k examples drawn from the test set, all of which contain some contextual phrase (contact, app) anonymized at the user level, stored in the user's biasing list. For this experiment we limit the maximum length of biasing catalogues to $\\max|\\sB|=5$k. We report the success rates for various FSQ configurations in Fig.~\\ref{fig:retrieval_success_rates}, while in Fig.~\\ref{fig:retrieval_mean_phrases} we report the number of retrieved phrases per utterance (i.e., the cardinality of the set $\\sS_K$ per our previous notation).",
        "summarize_figure": "figure1",
        "summarization": "English Summary:The image illustrates the success rates of Top1, Top5, and Top10 retrieval under different FSQ (product quantization) settings. The image includes two sets of bar charts, corresponding to experimental results when L equals [8, 5, 5, 5, 5] and L equals [7, 5, 5, 5, 5], respectively. The horizontal axis represents the number of groups G, ranging from 2 to 32. The vertical axis represents the success rate, expressed as a percentage. For each value of G, there are three groups of bars, representing the success rates of Top1, Top5, and Top10, respectively. The legend provides the baseline numbers without quantization (w/o VQ), with baseline success rates of 91.8% for Top1, 98.3% for Top5, and 99.3% for Top10. As can be seen from the picture, as the number of groups G increases, the success rates of Top1, Top5, and Top10 generally improve, and the retrieval success rate approaches the baseline when quantized."
    },
    "347": {
        "figure1": "2411.00664v2_tab2_retrieved_v3",
        "label1": "fig:retrieval_mean_phrases",
        "caption1": "Average number of phrases retrieved per utterance for Top1, Top5, and Top10 retrieval, for various FSQ settings. The baseline (w/o quantization) numbers are given in the legend. The error bars represent one standard deviation from the averages.",
        "figure2": "2411.00664v2_tab2_success_v3",
        "label2": "fig:retrieval_success_rates",
        "caption2": "Retrieval success rates for Top1, Top5, and Top10, for various FSQ settings. The baseline numbers (w/o quantization) are given in the legend.",
        "text": "We evaluate the retrieval capabilities of the models on a random subset of 1.4k examples drawn from the test set, all of which contain some contextual phrase (contact, app) anonymized at the user level, stored in the user's biasing list. For this experiment we limit the maximum length of biasing catalogues to $\\max|\\sB|=5$k. We report the success rates for various FSQ configurations in Fig.~\\ref{fig:retrieval_success_rates}, while in Fig.~\\ref{fig:retrieval_mean_phrases} we report the number of retrieved phrases per utterance (i.e., the cardinality of the set $\\sS_K$ per our previous notation).",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the retrieval success rates for Top1, Top5, and Top10 under various FSQ settings. The horizontal axis represents the number of groups G, and the vertical axis represents the success rate (in percentage). There are two sets of bar graphs, corresponding to configurations L = [8, 5, 5, 5, 5] and L = [7, 5, 5, 5, 5], respectively. Each set of bar graphs shows the comparison of success rates for Top1, Top5, and Top10 with and without VQ (vector quantization) under different numbers of groups G. The baseline data without quantization (w/o VQ) is provided in the legend. It can be observed that the retrieval success rate generally shows an upward trend as the number of groups G increases."
    },
    "348": {
        "figure1": "2411.00664v2_tab2_collision_v5r",
        "label1": "fig:retrieval_collisions",
        "caption1": "Collision vs. Top1 retrieval success rate, for various FSQ settings.",
        "text": "Since we are introducing a VQ-based approach, it is important to examine potential collisions. A collision happens when different biasing phrases get the same quantization indices, meaning that the model can no longer distinguish them. Here we estimate the collision rate as      \\text{collision} =  \\frac{ \\#\\text{unique phrases} - \\#\\text{unique indices }}  {\\#\\text{unique phrases}} for all the phrases across all the biasing catalogues in our set. Note that in any case we include exactly $K$ phrases in the TopK retrieved list (even when multiple biasing phrases collide). As we can see in Fig.~\\ref{fig:retrieval_collisions}, the collision rate is negatively correlated with the retrieval success rate, but is consistently low for \\mbox{$G\\geq8$}. After inspecting some collisions in those cases, most of them occur between pairs of phrases with different word ordering (e.g.,~\\emph{Vector Quant} vs.~\\emph{Quant Vector}), different punctuation (e.g.,~\\emph{Vector Quant} vs.~\\emph{Vector Quant.}), or slightly different spelling (e.g.,~\\emph{Vector} vs.~\\emph{Vecctor}). The very high collision rates and, hence, the low success rates for \\mbox{$G=1$} are not surprising. The subset under examination contains about 700k unique biasing phrases, whereas the maximum capacity of the quantizer for $G=1$ is equal to \\mbox{$\\prod_{i=1}^{|\\sL|}l_i$}, which is only 1,000 for \\mbox{$\\sL = [8,5,5,5]$} and 4,375 for \\mbox{$\\sL = [7,5,5,5,5]$}.",
        "summarize_figure": "figure1",
        "summarization": "This is a graph illustrating the relationship between the quantization index collision rate and the Top1 retrieval success rate.In the figure, the horizontal axis represents the number of groups G, the left vertical axis represents the Top1 success rate (percentage), and the right vertical axis represents the collision rate (percentage). There are two curves in the graph, representing the trends of Top1 success rate and collision rate with G under different FSQ settings. The first curve (L=[8,5,5,5]) shows that as G increases, the Top1 success rate rises from about 0.6% when G=1 to about 91.2% when G=32, while the collision rate decreases from about 99.99% when G=1 to about 0.58% when G=32. The second curve (L=[7,5,5,5,5]) shows a similar trend, with the Top1 success rate rising from about 0.9% when G=1 to about 91.4% when G=32, and the collision rate decreasing from about 99.87% when G=1 to about 0.57% when G=32. The overall trend indicates that as the number of groups increases, the collision rate of the quantization index is negatively correlated with the retrieval success rate, and when G is greater than or equal to 8, the collision rate stabilizes at a low level."
    },
    "349": {
        "figure1": "2411.00664v2_vq_runtime_eval",
        "label1": "fig:vq_runtime_eval",
        "caption1": "Runtime analysis of baseline and proposed approaches across various biasing list sizes. The error bars represent one standard deviation from the averages, computed from all the queries in the same bin.",
        "text": "Fig.~\\ref{fig:vq_runtime_eval} shows the runtime evaluation of the baseline and proposed approaches for dot-product estimation.  The runtime was measured with an Intel Xeon Gold 5128 processor clocked at 2.3GHz running on a single thread. In this analysis, we measured the time for computing cross-attention up to the point of (and including) the dot-product between queries and keys during inference. Any computation that can be prepared offline, such as the linear transformation of the keys in Eq.~\\eqref{eqn:qkv_linearities}, is not accounted for. For the baseline approach, this includes the query preparation and the dot-product between queries and keys in Eq.~\\eqref{eqn:qkv_linearities} -- \\eqref{eqn:cross_attention}. For the proposed approach, this includes the query preparation, the index selection, and the sum reduction as shown in Algorithm~\\ref{alg:fsq_cross_attention}.\nThe results showcase that the proposed algorithm is faster than the baseline approach and the gains are bigger as the number of biasing phrases increases, which is expected since the time complexity is proportional to $|\\sB|$ (Section~\\ref{subsec:compute_memory_theory}). When the biasing list has over 10k entries, all FSQ configurations achieve at least 20\\% reduction in runtime. For the largest biasing lists, the runtime of our proposed algorithm with the fastest FSQ setting (among the ones examined) is roughly half of the baseline. Among different FSQ configurations, smaller number of groups, $G$, and fewer levels, $|\\sL|$, are contributing factors. This is consistent with the complexity analysis in Table~\\ref{tbl:complexity}, where we saw that runtime complexity is proportional to both those parameters. Also note that for all the models in Fig.~\\ref{fig:vq_runtime_eval}, \\mbox{$|\\sL|\\cdot G < D = 256$}.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the runtime analysis of the baseline and proposed FSQ approaches across various numbers of biasing phrases. The x-axis represents the number of biasing phrases, ranging from 0 to over 20k, and the y-axis represents the runtime in milliseconds. Five methods are compared: Baseline, FSQ G16 L75555, FSQ G16 L8555, FSQ G8 L75555, and FSQ G8 L8555. The figure shows that as the number of biasing phrases increases, the runtime of all methods increases. When the number of biasing phrases exceeds 10k, all FSQ configurations achieve at least a 20% reduction in runtime. For the largest biasing lists, the runtime of the proposed algorithm with the fastest FSQ setting is roughly half that of the baseline method. Among different FSQ configurations, a smaller number of groups G and fewer levels L contribute to reducing runtime."
    },
    "350": {
        "figure1": "2411.00664v2_vq_memory_eval",
        "label1": "fig:vq_memory_eval",
        "caption1": "Memory usage of baseline and proposed approaches across various biasing list sizes. The FSQ configuration has \\mbox{$G=16$} groups and \\mbox{$\\sL = [8,5,5,5]$} levels.",
        "text": "Fig.~\\ref{fig:vq_memory_eval} shows the memory usage analysis of the baseline and proposed approaches. For this analysis, we picked a query with about one second of audio and roughly 10k biasing phrases. Then, we artificially created larger biasing lists by repetition. The purpose of doing so is to compare memory usage in extreme hypothetical scenarios with huge biasing lists. Similar to the runtime analysis,  we measured the memory usage for computing cross-attention up to the point of dot-product between queries and keys. In this analysis, we used full precision for floating point numbers. For indices, since we only need \\mbox{3 bits} for each level (to represent up to \\mbox{$\\max l_i=8$} values), we can use a 16-bit integer to represent all the elements $e_i^g$ in $\\ve^g$, for every group \\mbox{$g\\in\\{1,2,\\ldots,G\\}$} (Eq.~\\eqref{eqn:fsq_bound_round}). Given that this particular FSQ configuration has 16 groups, each biasing phrase only needs 32 bytes, which is substantially smaller than the space needed for the baseline approach.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the memory usage of the baseline and FSQ G16 L8555 methods across varying numbers of biasing phrases. The horizontal axis represents the number of biasing phrases, ranging from 10k to 1m. The vertical axis indicates memory usage in MB. The memory usage of the FSQ G16 L8555 method is significantly lower than that of the baseline method for all numbers of biasing phrases. For instance, with 1m biasing phrases, the baseline method's memory usage is approximately 982.7 MB, whereas the FSQ G16 L8555 method uses only 30.8 MB."
    },
    "351": {
        "figure1": "2411.00680v1_correlation_3dplot",
        "label1": "fig:experiments_correlation_symmetry_performance",
        "caption1": "% The relationship between the 1st-order symmetry (\\autoref{def:eval_zero_mean}, $x$-axis), the 2nd-order symmetry (\\autoref{def:eval_isotropic_position}, $y$-axis), and task performance (color). Each point represents either pre-trained or post-processed word embeddings (GloVe, word2Vec, and fastText). The Zipfian measure well captures the downstream task performance (\\textbf{right}), while the uniform isotropic measure cannot (\\textbf{left}). ",
        "text": "To what extent does our symmetry score (an intrinsic evaluation of embedding spaces) correlate with downstream task performance (an extrinsic evaluation of those)? As baselines, we use versions of our symmetry score that do \\emph{not} account for word frequency, calculated in a uniform manner. We also compare with popular symmetry scores in NLP, the average of cosine similarity (\\textbf{Ave. Cos.})~\\cite{ethayarajh2019-how-contextual} and the recently proposed \\textbf{IsoScore}~\\cite{Rudman2022-sb}. Note that all these baselines implicitly assume uniform word frequency. Additional experimental settings can be found in \\autoref{sec:experimental_settings}. The right side of \\autoref{fig:experiments_correlation_symmetry_performance} demonstrates the superiority of the Zipfian approach. Moving from the bottom-left to the top-right of the figure---i.e.\\ as both the 1st ($x$-axis) and 2nd moments ($y$-axis) of the symmetry score increase---it is clearly visible that the downstream task performance increases (the color becomes more red). In contrast, in the left-hand plot, which assumes uniform word frequency, there is no observed relationship between the symmetry score ($x$ and $y$-axis) and the downstream task performance (color). Surprisingly, when the most popular Ave.\\ Cos.\\ metric shows almost no correlation ($0.04$) with downstream task performance (STS-B), Zipfian symmetry metric has a strong positive correlation ($0.83$) with it.     \\centering     \\includegraphics[width=1.0\\columnwidth, keepaspectratio=true]{fig/correlation_3dplot.pdf}     \\caption{%         The relationship between the 1st-order symmetry (\\autoref{def:eval_zero_mean}, $x$-axis), the 2nd-order symmetry (\\autoref{def:eval_isotropic_position}, $y$-axis), and task performance (color).         Each point represents either pre-trained or post-processed word embeddings (GloVe, word2Vec, and fastText).         The Zipfian measure well captures the downstream task performance (\\textbf{right}), while the uniform isotropic measure cannot (\\textbf{left}).     }     \\label{fig:experiments_correlation_symmetry_performance}     Spearman's $\\rho \\times 100$ (each cell) between the symmetry scores (each column) and downstream STS-B performance (each row),     on pre-trained and post-processed embeddings (GloVe, word2Vec, and fastText).     The scores based on the Zipfian prior show a significantly higher correlation with task performance compared to those based on the uniform prior including Ave.\\ Cos.\\ and IsoScore. } & \\text{Ave.\\ Cos.} & \\text{IsoScore} & \\multicolumn{2}{c}{\\cellcolor{black!5}Uniform} & \\multicolumn{2}{c}{\\cellcolor{blue!5}Zipfian} \\\\  & ~\\cite{ethayarajh2019-how-contextual}& ~\\cite{Rudman2022-sb} & {\\cellcolor{black!5}}\\text{1st moment} & {\\cellcolor{black!5}}\\text{2nd moment} & {\\cellcolor{blue!5}}\\text{1st moment} & {\\cellcolor{blue!5}}\\text{2nd moment} \\\\",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the relationship between symmetry scores and downstream task performance. The horizontal axis represents the 1st-order symmetry, the vertical axis represents the 2nd-order symmetry, and the color indicates task performance. The left side of the figure calculates the symmetry score based on a uniform distribution, while the right side is based on a Zipfian distribution. It can be seen that the Zipfian-based symmetry measure better reflects the performance of downstream tasks. In the right figure, as the 1st and 2nd order symmetry scores increase, the performance of downstream tasks also increases, and the color becomes redder. In contrast, in the left figure, there is no clear relationship between the symmetry score and downstream task performance."
    },
    "352": {
        "figure1": "2411.00680v1_norm_glove",
        "label1": "fig:norm_whitening",
        "caption1": " Relationships between the information content $-\\log{p(w)}$ and the vector norms $\\lVert\\boldsymbol w\\rVert_2$ for top 500 frequent words $w$. The figure in the center represents the pre-trained GloVe model. By using \\textbf{Zipfian} whitening, the information content gets encoded in the norm (center to right). Conversely, with \\textbf{uniform} whitening, this phenomenon does not occur (center to left). ",
        "text": "\\centering\\includegraphics[width=1.0\\columnwidth, keepaspectratio=true]{fig/norm_glove.pdf}         \\caption{         Relationships between the information content $-\\log{p(w)}$ and the vector norms $\\lVert\\boldsymbol w\\rVert_2$ for top 500 frequent words $w$.         The figure in the center represents the pre-trained GloVe model.         By using \\textbf{Zipfian} whitening, the information content gets encoded in the norm (center to right).         Conversely, with \\textbf{uniform} whitening, this phenomenon does not occur (center to left).         }     \\label{fig:norm_whitening} In \\autoref{fig:norm_whitening}, we experimentally confirmed that the norms of informative words become larger with Zipfian whitening (shown from center to the right in \\autoref{fig:norm_whitening}), bringing them closer to the ideal Zipfian prior model%     Given these results, some readers may be interested in the experimental outcomes for a baseline where uniform whitening is applied, followed by rescaling norms based on information content through Zipfian whitening. For these results, please refer to \\autoref{sec:experiments_mix_uniform_and_zipfian}. }.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the relationship between information content (-log(p(w))) and vector norms (||w||2) for the top 500 frequent words w in the pre-trained GloVe model. It can be observed that there is no clear correlation between the information content and vector norms in the pre-trained GloVe model. After processing with Zipfian whitening, the information content is encoded into the vector norms, meaning the vector norms increase as the information content increases. However, this phenomenon does not exist after processing with uniform whitening, and there is no clear correlation between the information content and vector norms."
    },
    "353": {
        "figure1": "2411.00696v1_hypermarameter",
        "label1": "fig:ablation_number_of_prototypes",
        "caption1": "Ablation study on the number of prototypes. } \\vspace{-0.5cm",
        "text": "We analyze the effects of two key hyperparameters: loss weights $\\lambda_1$ and $\\lambda_2$, shown in Table~\\ref{tab:ablation_hyperparameters}, and the number of prototypes in Fig.~\\ref{fig:ablation_number_of_prototypes}. For the loss weights, $\\lambda_1 = 0.1$ and $\\lambda_2 = 0.5$ consistently yield the best performance across all settings. However, excessively large values for $\\lambda_1$ and $\\lambda_2$ can cause the model to prioritize cross-modal alignment and reconstruction tasks over predictive performance. Regarding the number of prototypes, we find that using 16 prototypes achieves the best results, though our model remains robust, with 8 prototypes yielding similar outcomes.\n\\centering     \\includegraphics[width=0.9\\linewidth]{figs/hypermarameter.pdf}     \\caption{Ablation study on the number of prototypes. }     \\vspace{-0.5cm}     \\label{fig:ablation_number_of_prototypes}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the impact of the number of prototypes on model performance, with the first graph (48-IHM) and the second graph (24-PHE) showing results on different datasets. In the 48-IHM dataset, the AUROC value increases significantly as the number of prototypes increases from 4 to 8, then decreases slightly between 8 and 32, and finally decreases slightly between 32 and 64, but remains relatively stable overall. The AUPR value does not change much with different numbers of prototypes and remains relatively stable at around 0.52. In the 24-PHE dataset, the AUROC value shows a trend of first increasing and then decreasing as the number of prototypes increases, peaking at 16 prototypes and then decreasing. The AUPR value also shows a similar trend, reaching its highest value at 16 prototypes."
    },
    "354": {
        "figure1": "2411.00699v1_FSS3_weekly",
        "label1": "fig:FSS3Weekly",
        "caption1": "TA FSS: Detail view of weekly effects which allows for adjustments by dragging the white handle points.",
        "text": "The user can redraw the yearly effects and reset them.  The user can adjust the weekly effects by dragging the white handle points and reset them.  A slider allows limiting the number of past fluctuations shown in \\emph{Edit Yearly} and \\emph{Edit Weekly}. Figure \\ref{fig:FSS3Weekly} shows a screenshot after enabling \\emph{Edit Weekly}. Only the past 38 weeks of fluctuations, as purple points, are shown. Moving the slider limits that amount. Fluctuations are ordered per day by date (old to new, left to right), and their y-coordinates indicate the respective residuals (model prediction without weekly effect) on that day. After switching back to the main graph, there is a slight delay of 800ms until the main graph is smoothly updated to allow for the visual perception of the change created through the adjustments of these components. The last \\ac{TA}-specific function is:\n\\includegraphics[width=1.0\\textwidth]{figures/FSS3_weekly.png} \t\\caption{TA FSS: Detail view of weekly effects which allows for adjustments by dragging the white handle points.} \t\\label{fig:FSS3Weekly}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates a detailed view of weekly effects, adjustable by dragging white handle points. The horizontal axis represents Monday through Sunday, and the vertical axis indicates residuals (model predictions without weekly effects). Purple dots represent fluctuations over the past 38 weeks, ordered by date from left to right, representing data from old to new. A gray curve connects the average residual values for each day of the week, with Monday's average residual around 0, Tuesday to Thursday around -5, Friday around 0, Saturday around 7, and Sunday around 10."
    },
    "355": {
        "figure1": "2411.00699v1_study_3_nonzero_volume_rmae",
        "label1": "fig:study3_volumermae_nonzero",
        "caption1": "$AV$ and $rMAE$ per FSS design conditional on positive $AV$. White squares mark the mean values.",
        "text": "The average \\ac{AV} per treatment is shown in Table \\ref{tbl:study3_unconditional}. Both transparent conditions (\\ac{T} and \\ac{TA}) have a significantly lower \\ac{AV} than the Opaque (\\ac{O}). This finding may be driven by the lower \\ac{AF} (see Equation \\ref{eq:adjfreq}) in \\ac{T} and \\ac{TA}. Participants in \\ac{O} leave only 13.6\\% of forecasts untouched whereas those in \\ac{TA} do not adjust 38\\% of forecasts at all. Drilling down on the \\ac{AV} conditional on non-zero adjustments, Figure \\ref{fig:study3_volumermae_nonzero} depicts the distributions of \\ac{AV}s. As expected, the conditional \\ac{AV}s are larger than the unconditional \\ac{AV}s. Now, only the \\ac{AV} of \\ac{T} is significantly smaller than that of \\ac{O}.  Strikingly, \\ac{TA}'s conditional \\ac{AV} has the largest mean and standard deviation.  These results confirm Hypothesis \\ref{hyp:1a} with the strong caveat that \\emph{if} participants adjust forecasts, their \\ac{AV} in \\ac{TA} is larger and highly variant.\n\\includegraphics[width=1.0\\textwidth]{figures/study_3_nonzero_volume_rmae.png} \t\\caption{$AV$ and $rMAE$ per FSS design conditional on positive $AV$. White squares mark the mean values.} \t\\label{fig:study3_volumermae_nonzero}\nExamining the \\ac{rMAE} scores conditional on positive \\ac{AV}, Figure \\ref{fig:study3_volumermae_nonzero} makes clear that participants in \\ac{TA} have the greatest variance in their forecasting accuracy with many outliers \\emph{if} they adjust forecasts. A one-way ANOVA, however, finds no significant differences in group means (p=0.54) after logarithmizing the initially log-normally distributed \\ac{rMAE} values.  Thereby, I reject Hypothesis \\ref{hyp:1b} but point out that the Transparent design shows the lowest error rates and standard deviations whereas the Transparently Adjustable design has the highest respective values.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the distributions of Volume and rMAE for different FSS designs (O, T, and TA) conditional on positive AV. For Volume, the TA design exhibits the highest mean adjustment (μ = 0.51) and the largest standard deviation (σ = 0.75), followed by the O design (μ = 0.44, σ = 0.48), while the T design has the lowest mean adjustment (μ = 0.32, σ = 0.36). In terms of rMAE, the TA design demonstrates the highest mean rMAE (μ = 1.14, σ = 0.56), followed by the O design (μ = 1.08, σ = 0.39), with the T design showing the lowest mean rMAE (μ = 1.06, σ = 0.25). The white squares in the figure mark the mean values."
    },
    "356": {
        "figure1": "2411.00699v1_Study3_Comments",
        "label1": "fig:study3_comments",
        "caption1": "Counts of voluntary free-text comments.",
        "text": "At the end of the \\ac{HIT}, a textbox asks participants whether they particularly liked or disliked anything. About half of the participants in each condition left a comment. Comments in \\ac{O} are pretty uniform, thanking for the unique study, stating that it was fun or interesting, and easy to control. Participants in \\ac{T} and \\ac{TA} give more diverse comments. The number of comments stating enjoyment declines from \\ac{O} to \\ac{T} to \\ac{TA}, while more comments suggest difficulties and confusion.  Comments about understanding \\emph{and} not understanding both increase in the transparent conditions. A large number of comments in \\ac{T} and \\ac{TA} states that they see the value in the forecasting tool but wished for more time, experience, or explanations to get used to it.  Figure \\ref{fig:study3_comments} provides the number of comments for a wordstem or meaning (i.e. ``confusing'' counts comments that include ``confused'').\n\\includegraphics[width=1.0\\textwidth]{figures/Study3_Comments.png} \t\\caption{Counts of voluntary free-text comments.} \t\\label{fig:study3_comments}",
        "summarize_figure": "figure1",
        "summarization": "The figure shows the counts of voluntary free-text comments from participants under different conditions (Opaque, Transparent, and TranspAdjust). The comments are categorized as \"enjoyable or fun\", \"interesting\", \"easy\", \"difficult\", \"confusing\", \"understand\", and \"not understand\". In the \"enjoyable or fun\" category, the Opaque condition has the highest number of comments, while the TranspAdjust condition has the fewest. For the \"interesting\" and \"easy\" categories, the number of comments is relatively close across the three conditions. In the \"difficult\" and \"confusing\" categories, the TranspAdjust condition has a higher number of comments than the other two conditions. In the \"understand\" and \"not understand\" categories, the TranspAdjust condition also has a relatively higher number of comments."
    },
    "357": {
        "figure1": "2411.00699v1_study_3_volume_per_product",
        "label1": "fig:study3_perproduct_volume",
        "caption1": "$Adjustment Volume$ and proportion of $good$ adjustments, marked by each bar's lower part, per product and FSS design. Values under the product index indicate Prophet's $rMAE$ relative to a simple exponential smoothing model.",
        "figure2": "2411.00699v1_study_3_error_per_product",
        "label2": "fig:study3_perproduct_rmae",
        "caption2": "$rMAE$ per product time series and FSS design. Values under the product index indicate Prophet's $rMAE$ relative to a simple exponential smoothing model.",
        "text": "Having established the general behavioral effects of transparency, I will now explore how they relate to individual product time seris. Figure \\ref{fig:study3_perproduct_volume} shows the unconditional \\ac{AV} per \\ac{FSS} design and product time series and Figure \\ref{fig:study3_perproduct_rmae} gives the respective \\ac{rMAE}s.  The values below the product indices in both figures give Prophet's \\ac{rMAE} relative to a \\ac{SES} model. Notably, average improvements through adjustments are conditional on Prophet performing worse than \\ac{SES}. Conversely, large disimprovements through adjustments only occur where Prophet is significantly more accurate than \\ac{SES}.\n\\includegraphics[width=1.0\\textwidth]{figures/study_3_volume_per_product.png} \t\\caption{$Adjustment Volume$ and proportion of $good$ adjustments, marked by each bar's lower part, per product and FSS design. Values under the product index indicate Prophet's $rMAE$ relative to a simple exponential smoothing model.} \t\\label{fig:study3_perproduct_volume}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the adjustment volume and proportion of good adjustments under different products and FSS designs, with the proportion of good adjustments marked in the lower part of each bar. The values below each product indicate Prophet's relative mean absolute error (rMAE) relative to a simple exponential smoothing (SES) model. It can be observed that for different products, the adjustment volume and the proportion of good adjustments vary under the three FSS designs: Opaque, Transparent, and TranspAdjust. For example, Product_03 has a significantly higher adjustment volume under the Opaque design than the other two designs. Product_05 has the highest adjustment volume under the TranspAdjust design, while Product_06 has a relatively high adjustment volume under the Transparent design. Product_10 shows higher adjustment volumes under the Transparent and TranspAdjust designs but relatively lower adjustment volumes under the Opaque design."
    },
    "358": {
        "figure1": "2411.00699v1_study_3_error_per_product",
        "label1": "fig:study3_perproduct_rmae",
        "caption1": "$rMAE$ per product time series and FSS design. Values under the product index indicate Prophet's $rMAE$ relative to a simple exponential smoothing model.",
        "figure2": "2411.00699v1_study_3_volume_per_product",
        "label2": "fig:study3_perproduct_volume",
        "caption2": "$Adjustment Volume$ and proportion of $good$ adjustments, marked by each bar's lower part, per product and FSS design. Values under the product index indicate Prophet's $rMAE$ relative to a simple exponential smoothing model.",
        "text": "Having established the general behavioral effects of transparency, I will now explore how they relate to individual product time seris. Figure \\ref{fig:study3_perproduct_volume} shows the unconditional \\ac{AV} per \\ac{FSS} design and product time series and Figure \\ref{fig:study3_perproduct_rmae} gives the respective \\ac{rMAE}s.  The values below the product indices in both figures give Prophet's \\ac{rMAE} relative to a \\ac{SES} model. Notably, average improvements through adjustments are conditional on Prophet performing worse than \\ac{SES}. Conversely, large disimprovements through adjustments only occur where Prophet is significantly more accurate than \\ac{SES}.\n\\includegraphics[width=1.0\\textwidth]{figures/study_3_error_per_product.png} \t\\caption{$rMAE$ per product time series and FSS design. Values under the product index indicate Prophet's $rMAE$ relative to a simple exponential smoothing model.} \t\\label{fig:study3_perproduct_rmae}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the adjustment volume and proportion of good adjustments for each product and FSS design, with the lower part of each bar marking the proportion of good adjustments. The values under the product index indicate Prophet's rMAE relative to a simple exponential smoothing model. It can be observed that there are significant differences in adjustment volume among different products. For example, Product_03 has a noticeably higher adjustment volume compared to other products, while Product_06 has a relatively lower adjustment volume. Additionally, different FSS designs (Opaque, Transparent, and TranspAdjust) also affect the adjustment volume. For some products (such as Product_05), the Transparent design seems to lead to a higher adjustment volume."
    },
    "359": {
        "figure1": "2411.00711v1_linear_decodability_vfinal",
        "label1": "fig:linear_dec",
        "caption1": "Results of Linear Decodability (LD): Panel (a) compares LD of the \\texttt{Male} bias attribute from a frozen baseline network (blue) and our method (orange), both pretrained on different target attributes. Panel (b) shows LD of multiple bias attributes from networks pretrained on \\texttt{Blond Hair}, comparing the baseline (blue) and our method (orange).",
        "text": "Previous works~\\cite{seo2022unsupervised,capitani2024clusterfix} have used the CelebA dataset to analyze the feature semantics over the target and bias attributes. The studies revealed that images from certain groups, defined by a combination of target and bias attribute values, are clustered in the feature space, even without using the bias information during training. Expanding on their analysis, we conduct the following experiments to investigate the layer where the clustering could be more pronounced. We begin by training a ResNet18 model for 50 epochs to classify a target attribute (e.g., \\texttt{Blond Hair}). Next, we assess feature decodability at different network depths to evaluate how well the bias attribute \\texttt{Male} can be decoded, keeping the network parameters frozen. A decoder, consisting of a single linear layer and softmax, is trained using activations from a specific layer and an unbiased validation set labeled with the bias attribute.  Figure~\\ref{fig:linear_dec} -- (a, blue bars) shows the decodability of \\texttt{Male} across layers. We observe that bias decodability generally decreases with network depth, indicating that shallower layers are more effective at detecting bias. In more complex scenarios, where the bias involves combinations of attributes, a similar trend is observed (Figure~\\ref{fig:linear_dec} -- (b, blue bars)). These findings align with previous studies that identified bias detection in shallow layers~\\cite{tiwari2024using,tiwari2023overcoming,dagaev2023too} and were theoretically validated~\\cite{hermann2020shapes}.\n\\centering     \\includegraphics[width=1\\linewidth]{linear_decodability_vfinal.png}     \\caption{Results of Linear Decodability (LD): Panel (a) compares LD of the \\texttt{Male} bias attribute from a frozen baseline network (blue) and our method (orange), both pretrained on different target attributes. Panel (b) shows LD of multiple bias attributes from networks pretrained on \\texttt{Blond Hair}, comparing the baseline (blue) and our method (orange).}     \\label{fig:linear_dec}",
        "summarize_figure": "figure1",
        "summarization": "The figure presents the results of Linear Decodability (LD), comparing the decodability of bias attributes between a frozen baseline network (blue) and another method (orange) pretrained on different target attributes. Target attributes include Blond Hair, Bangs, and Female with Heavy Makeup, while bias attributes include Male. Specifically, the figure shows the ability to decode bias attributes in different network layers (L1 to L4) for the baseline and other methods under different target attributes. It is observed that for target attributes such as Blond Hair and Bangs, the other method consistently shows higher decodability across all network layers compared to the baseline method. Similar results are observed for other target attributes such as Female with Heavy Makeup and Pale Skin, where the other method generally exhibits higher decodability of bias attributes across different network layers."
    },
    "360": {
        "figure1": "2411.00715v1_loc_comp_new",
        "label1": "fig:comp_across_models",
        "caption1": "\\textbf{Localisation Performance of $\\mat W(\\vec x)\\vec x$.} We compute the contribution maps according to the dynamic linear summaries $\\mat W(\\vec x)$ of the pre-trained models (`Standard'), their B-cosified versions, and the original pre-trained \\bcos models and evaluate their localisation performance on the Grid Pointing Game as in \\cite{boehle2024bcos}. We find  localisation to significantly improve for B-cosified models, achieving results on par with the models of \\cite{boehle2024bcos}.",
        "figure2": "2411.00715v1_IMN-baseline-dn121+rn50",
        "label2": "fig:comp2posthoc",
        "caption2": "\\textbf{Comparison to Post-hoc Methods.} For two of the models in \\cref{fig:comp_across_models} (ResNet-50-v1, DenseNet-121) we compare the localisation performance of the dynamic matrices $\\mat W(\\vec x)\\vec x$ to post-hoc explanations for the pre-trained models. Similar to the original \\bcos models \\cite{boehle2024bcos}, the model-inherent explanations perform favourably.",
        "figure3": "2411.00715v1_IMN-baseline-dn121+rn50",
        "label3": "fig:comp2posthoc",
        "caption3": "\\textbf{Comparison to Post-hoc Methods.} For two of the models in \\cref{fig:comp_across_models} (ResNet-50-v1, DenseNet-121) we compare the localisation performance of the dynamic matrices $\\mat W(\\vec x)\\vec x$ to post-hoc explanations for the pre-trained models. Similar to the original \\bcos models \\cite{boehle2024bcos}, the model-inherent explanations perform favourably.",
        "text": "To evaluate the interpretability of our B-cosified models, we report the GridPG localization scores in \\cref{fig:comp_across_models}, and compare with conventional and B-cos models; following \\cite{boehle2024bcos}, we report the results of 3x3 image grids for convolutional models, and of 2x2 grids for the ViTs. For a fair comparison, for all models, we evaluate the localization of the dynamic linear summary of the model% $\\mat W(\\vec x) \\vec x$ (see \\cref{sec:standardvsbcos:background}). We find that across architectures, B-cosified models significantly outperform conventional DNNs in terms of localization (32.7pp-71.0pp) and perform on par with B-cos models. Since post-hoc attribution methods (e.g. \\cite{selvaraju2017grad,shrikumar2017learning,sundararajan2017axiomatic})  are often used to interpret conventional DNNs, similar to \\cite{boehle2022bcos}, in \\cref{fig:comp2posthoc}, we compare the localization of the model inherent explanations from two of our B-cosified models with post-hoc explanations applied to the corresponding conventional models. Similar to the results reported by \\cite{boehle2022bcos}, we find our B-cosified models to strongly outperform all post-hoc methods, including GradCAM \\cite{selvaraju2017grad}, with a near perfect localization score, showing that B-cosification is effective in yielding highly interpretable yet model-faithful explanations.\n\\centering     \\includegraphics[width=\\linewidth]{figs/loc_comp_new.pdf}     \\caption{\\textbf{Localisation Performance of $\\mat W(\\vec x)\\vec x$.} We compute the contribution maps according to the dynamic linear summaries $\\mat W(\\vec x)$ of the pre-trained models (`Standard'), their B-cosified versions, and the original pre-trained \\bcos models and evaluate their localisation performance on the Grid Pointing Game as in \\cite{boehle2024bcos}. We find  localisation to significantly improve for B-cosified models, achieving results on par with the models of \\cite{boehle2024bcos}.}     \\label{fig:comp_across_models}\n\\centering     \\includegraphics[width=\\linewidth]{figs/IMN-baseline-dn121+rn50.pdf}     \\caption{\\textbf{Comparison to Post-hoc Methods.} For two of the models in \\cref{fig:comp_across_models} (ResNet-50-v1, DenseNet-121) we compare the localisation performance of the dynamic matrices $\\mat W(\\vec x)\\vec x$ to post-hoc explanations for the pre-trained models. Similar to the original \\bcos models \\cite{boehle2024bcos}, the model-inherent explanations perform favourably.}     \\label{fig:comp2posthoc}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the localization performance of pre-trained models (Standard Model W(x)x), their B-cosified versions, and original pre-trained B-cos models on the Grid Pointing Game. The vertical axis represents the localization rate (%), and the horizontal axis represents different model architectures, including ResNet-18, ResNet-50-v1, ResNet-50-v2, DenseNet-121, and various ViT models (ViT-Ti, ViT-S, ViT-B, ViT-L, ViT subscript c-Ti, ViT subscript c-S, ViT subscript c-B, ViT subscript c-L). The results indicate that B-cosified models significantly outperform conventional pre-trained models in terms of localization performance, with improvements ranging from 32.7 to 71.0 percentage points, and perform comparably to the original B-cos models."
    },
    "361": {
        "figure1": "2411.00715v1_clip_results2",
        "label1": "fig:clip_zeroshot_main",
        "caption1": " \\textbf{Classification performance on the CLIP Benchmark} \\cite{clipbench2024} of various CLIP models for the zero-shot setting (\\textit{left}) and linear probing (\\textit{right}). Specifically, we compare two B-cosified CLIPs---trained on ImageNet (IMN) and CC3M respectively---to the Text2Concept approach by \\cite{moayeri2023text} and the original pre-trained CLIP model. We find B-cosified versions of CLIP to consistently outperform Text2Concept on natural and specialised data. ",
        "text": "\\centering   \\includegraphics[width=\\linewidth]{figs/clip_results2.pdf}   \\caption{   \\textbf{Classification performance on the CLIP Benchmark} \\cite{clipbench2024} of various CLIP models for the zero-shot setting (\\textit{left}) and linear probing (\\textit{right}). Specifically, we compare two B-cosified CLIPs---trained on ImageNet (IMN) and CC3M respectively---to the Text2Concept approach by \\cite{moayeri2023text} and the original pre-trained CLIP model. We find B-cosified versions of CLIP to consistently outperform Text2Concept on natural and specialised data.   }   \\label{fig:clip_zeroshot_main}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the classification performance of different CLIP models on the CLIP Benchmark in a zero-shot setting. The x-axis represents different data types: Natural Data, Specialized Data, and Structured Data. The y-axis represents the accuracy in percentage. The legend compares the performance of Text2Concept, B-cosified models trained on ImageNet (IMN) and CC3M, and the Standard CLIP model. The B-cosified models outperform Text2Concept on Natural and Specialized Data. For Structured Data, the accuracy of all models is relatively low. The Standard CLIP model exhibits the highest zero-shot classification accuracy across all datasets."
    },
    "362": {
        "figure1": "2411.00726v1_lambda",
        "label1": "parameter",
        "caption1": "Results of different loss function weights $\\lambda$. The numbers in square brackets indicate the 95\\% confidence interval",
        "text": "\\centering     \\includegraphics[width=0.9\\textwidth]{lambda.png}     \\caption{Results of different loss function weights $\\lambda$. The numbers in square brackets indicate the 95\\% confidence interval}     \\label{parameter}\nWe also investigated the influence of different sets of loss weights. As shown in Fig. \\ref{parameter}, loss function weight $\\lambda$ get the best performance at 0.6, with a kappa score of 84.44\\%. Excessively high or low weights would significantly affect the performance and generalizability of the network, ultimately hindering its ability to achieve optimal results.  More supervision information on CFP is beneficial, but lack of supervision information on IFP image will also lead to poor effect.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the impact of different loss function weights lambda on model performance, measured by the Kappa coefficient in percentage. The value of lambda ranges from 0.1 to 0.9. The Kappa coefficient reaches its peak of 84.44% when lambda is 0.6, with a 95% confidence interval of [80.47%, 88.41%]. The curve indicates an increasing trend in the Kappa coefficient as lambda increases from 0.1 to 0.6. Beyond 0.6, the Kappa coefficient declines rapidly. The Kappa coefficients and their 95% confidence intervals for other lambda values are also provided. For instance, when lambda is 0.1, the Kappa coefficient is 83.62% with a confidence interval of [79.65%, 87.22%]; when lambda is 0.9, the Kappa coefficient drops to 82.05% with a confidence interval of [77.89%, 85.86%]. It shows that excessively high or low weights would significantly affect the performance and generalizability of the network."
    },
    "363": {
        "figure1": "2411.00743v1_sae_comparison_plot_final",
        "label1": "fig:tailA",
        "caption1": "\\small Proportion of tokens with SAE features vs. Token frequency in Physics arXiv data. SSAE trained with dense retrieval captures more tail tokens (concepts) in its features.} \\vspace{-0.2in",
        "figure2": "2411.00743v1_sae_comparison_plot_toxicity_adjusted_camera_ready",
        "label2": "fig:tailtoxicA",
        "caption2": "\\small Proportion of tokens with SAE features vs. Token frequency in Toxicity data. SSAE trained with dense retrieval captures more tail tokens (concepts) in its features.",
        "text": "\\centering     \\includegraphics[width=0.5\\textwidth]{figures/sae_comparison_plot_final.png}     \\caption{\\small Proportion of tokens with SAE features vs. Token frequency in Physics arXiv data. SSAE trained with dense retrieval captures more tail tokens (concepts) in its features.}     \\vspace{-0.2in}     \\label{fig:tailA}\nTo probe tail concept learning we use convergent validity \\citep{campbell1959convergent} with the Logit Lens \\citep{bloom2024understandingfeatureslogitlens}. Figure \\ref{fig:tailA}, uses the unembedding matrix as a logit lens to analyze the top-10 token logits associated with each SSAE feature. For each frequency bucket in the Physics arXiv test data, we calculate the percentage of tokens that appear among the top-10 logits for at least one feature. This measures the extent to which SSAE features represent tokens across different frequency ranges. We compare two SSAEs at test $L_0$ of 100: one finetuned on full OWT dataset, another using Dense retrieval. The Dense retrieval finetuned SSAE captures a significantly higher proportion of tail tokens in its features compared to the OWT finetuned SSAE. Moreover, these captured tail tokens often correspond to physics-specific concepts, suggesting that SSAEs are indeed learning to represent rare, domain-relevant concepts. Similar results are obtained for toxicity data in \\autoref{fig:tailtoxicA}.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the proportion of tokens with SAE features versus token frequency in the Physics arXiv dataset. The horizontal axis represents token frequency in the corpus on a logarithmic scale, ranging from 10 to the power of 6 to 10 to the power of 1. The vertical axis indicates the proportion of tokens with SAE features. The figure compares two models: Reference OWT and Dense retrieval, with hyperparameters bin size set to 10 and top-K set to 10. The SSAE trained with dense retrieval captures more tail tokens (concepts) in its features, showing a higher proportion for lower-frequency tokens compared to the Reference OWT model, indicating a better ability to capture less frequent concepts in the corpus."
    },
    "364": {
        "figure1": "2411.00743v1_token_rank_vs_error_plot",
        "label1": "fig:etokenrankerror",
        "caption1": "\\small Reconstruction error vs. token rank for TERM-trained and ERM-trained GSAEs. TERM exhibits lower error variance and maximum error for tail tokens.",
        "figure2": "2411.00743v1_error_distribution_plot",
        "label2": "fig:errorhistogram",
        "caption2": "\\small Reconstruction error distribution of TERM-trained and ERM-trained GSAE. TERM-trained GSAE minimizes the maximum error at the cost of average error.",
        "text": "\\centering     \\includegraphics[width=0.5\\textwidth]{figures/token_rank_vs_error_plot.pdf}         \\caption{\\small Reconstruction error vs. token rank for TERM-trained and ERM-trained GSAEs. TERM exhibits lower error variance and maximum error for tail tokens.}     \\label{fig:etokenrankerror}\nFigure \\ref{fig:errorhistogram} shows the distribution of reconstruction error for the TERM-trained GSAE. TERM minimizes max error at the cost of slightly higher average error. Figure \\ref{fig:etokenrankerror} plots the reconstruction error for tokens ranked by frequency, showing that TERM reduces reconstruction error and error variance for tail tokens compared to ERM.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the relationship between token rank and reconstruction error, with the x-axis representing token rank (log scale) and the y-axis representing average reconstruction error (log scale). It compares GSAE models trained with ERM and Tilted ERM. The plot shows reconstruction errors for individual tokens, bucket means, and bucket means plus or minus one standard deviation for both GSAE models. It can be observed that the GSAE trained with Tilted ERM exhibits lower error variance and maximum error for tail tokens."
    },
    "365": {
        "figure1": "2411.00743v1_gsae_normalized_diversity_scores_distribution",
        "label1": "fig:gsaedist",
        "caption1": "\\small Feature diversity score distributions for TERM-trained and ERM-trained GSAEs. TERM leads to both higher and lower diversity features. Lower diversity features specialize in tail concepts, while higher diversity features capture a broader range of concepts.",
        "text": "\\centering     \\includegraphics[width=0.5\\textwidth]{figures/gsae_normalized_diversity_scores_distribution.png}     \\caption{\\small Feature diversity score distributions for TERM-trained and ERM-trained GSAEs. TERM leads to both higher and lower diversity features. Lower diversity features specialize in tail concepts, while higher diversity features capture a broader range of concepts.}     \\label{fig:gsaedist}\nFigure \\ref{fig:gsaedist} presents diversity score distributions for TERM- and ERM-trained GSAE feature explanations (examples in \\autoref{sec:tinystories-features}), capturing the variety of examples explainable by each feature using Claude 3.5 Sonnet (see Section \\ref{sec:feature-diversity}). TERM-trained GSAEs exhibit both higher and lower diversity features compared to ERM, with lower diversity features specializing in tail concepts and higher diversity features capturing a broader range of concepts, both frequent and rare.\nWhen comparing the feature diversity score distribution of TERM-trained GSAE with ERM-trained GSAE in \\autoref{fig:gsaedist}, we observe that TERM-trained GSAE induces some features to specialize in tail concepts, while others generalize to represent a broader range of concepts relative to the ERM-trained GSAE.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the normalized distribution of feature diversity scores for GSAE methods trained with TERM and ERM. It shows that TERM-trained GSAE results in a bimodal distribution of feature diversity scores compared to ERM-trained GSAE. Specifically, TERM training leads to both lower and higher diversity features. The feature diversity scores of ERM-trained GSAE are relatively concentrated, with the peak located between the two peaks of TERM training. This suggests that TERM training enables some features to specialize in tail concepts, while others generalize to represent a broader range of concepts."
    },
    "366": {
        "figure1": "2411.00743v1_merged.drawio",
        "label1": "fig:tokenentropy",
        "caption1": "\\small TERM feature activation patterns. (Left) TERM token activation entropy is lower, suggesting more specialized features. (Right) TERM max feature activations per token are higher. These characteristics, from minimizing max risk, contribute to TERM's enhanced tail concept detection.",
        "figure2": "2411.00743v1_token_coverage_comparison",
        "label2": "fig:tokensthreshold",
        "caption2": "\\small Proportion of tokens detected vs. activation threshold for TERM-trained and ERM-trained GSAEs. TERM-trained features exhibit stronger activations.",
        "text": "\\centering     \\includegraphics[width=0.48\\textwidth]{figures/merged.drawio.png}         \\caption{\\small TERM feature activation patterns. (Left) TERM token activation entropy is lower, suggesting more specialized features. (Right) TERM max feature activations per token are higher. These characteristics, from minimizing max risk, contribute to TERM's enhanced tail concept detection.}     \\label{fig:tokenentropy}\nFigures  \\ref{fig:tokenentropy} and \\ref{fig:tokensthreshold} show that TERM-trained GSAE features exhibit stronger activations and lower entropy compared to ERM-trained GSAE on the data.  This, combined with their high recall, suggests a strategy for rare concept detection: tag features strongly associated with rare concepts during pretraining, and at test time, strong activation of these tagged features triggers further investigation. This is more effective than using error nodes with ERM-trained SAEs for rare concept detection, as error nodes do not disambiguate types of rare features.",
        "summarize_figure": "figure1",
        "summarization": "This image presents the distribution of feature activation patterns for GSAE ERM and GSAE Tilted ERM (TERM). The left panel shows the distribution of positive token activation entropy, where TERM's activation entropy distribution peaks much lower than GSAE ERM, indicating more specialized or concentrated activation features for TERM. The right panel displays the distribution of maximum feature activation per token, showing that TERM's maximum activation value distribution peaks significantly higher than GSAE ERM, suggesting stronger feature activation intensity for TERM. These differences in activation entropy and maximum activation values are a reflection of the impact of the TERM training method on feature learning."
    },
    "367": {
        "figure1": "2411.00743v1_tilt_tail_new_2_crop",
        "label1": "fig:tailB",
        "caption1": "\\small Cumulative proportion of tokens with SAE features vs. cumulative percentage of tokens in Physics arXiv data, normalized per model so that the cumulative proportion of tokens with features is 1 over the entire dataset. SSAE trained with dense retrieval and larger tilt captures more tail tokens (concepts) in its features.",
        "figure2": "2411.00743v1_sae_normalized_cumulative_distribution_plot_log_matplotlib",
        "label2": "fig:tailtoxicB",
        "caption2": "\\small Cumulative proportion of tokens with SAE features vs. cumulative percentage of tokens in Toxicity data, normalized per model so that the cumulative proportion of tokens with features is 1 over the entire dataset. SSAE trained with dense retrieval and larger tilt captures more tail tokens (concepts) in its features. Note that the curves at tilt 500 and tilt $10^9$ overlap.",
        "text": "\\centering     \\includegraphics[width=0.5\\textwidth]{figures/tilt_tail_new_2_crop.png}     \\caption{\\small Cumulative proportion of tokens with SAE features vs. cumulative percentage of tokens in Physics arXiv data, normalized per model so that the cumulative proportion of tokens with features is 1 over the entire dataset. SSAE trained with dense retrieval and larger tilt captures more tail tokens (concepts) in its features.}     \\label{fig:tailB}\nFigure \\ref{fig:tailB} plots the cumulative proportion of tokens with SSAE features (identified using the logit lens approach) versus the cumulative percentage of tokens in the Physics arXiv data for different SSAEs. We normalize the curves per model at a validation $L_0$ of 100, so that the cumulative proportion of tokens with features is 1 over the entire dataset. Results show that SSAEs trained with Dense retrieval and tilt capture a greater proportion of tail tokens compared to Dense retrieval alone, with this effect increasing with tilt. Figure \\ref{fig:tailtoxicB} shows a similar trend for the Toxicity dataset.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the cumulative proportion of tokens with SAE features versus the cumulative percentage of tokens in the Physics arXiv dataset. The curves are normalized so that the cumulative proportion of tokens with features is 1 over the entire dataset for each model. The blue curve represents the model trained with dense retrieval, the red curve represents the model trained with dense retrieval with a tilt of 500, and the green curve represents the model trained with dense retrieval with a tilt of 10 to the power of 9. The figure shows that SSAE models trained with dense retrieval and tilt capture more tail tokens (concepts) than models trained with dense retrieval alone, and this effect increases with the tilt value. Specifically, when the cumulative percentage of tokens is less than 1%, the green curve, corresponding to the model with a tilt of 10 to the power of 9, is significantly higher than the other two curves, indicating that it captures more tail tokens faster."
    },
    "368": {
        "figure1": "2411.00743v1_tilt_log_log_camera_ready",
        "label1": "fig:activation_rank",
        "caption1": "\\small \\textbf{Feature activation count vs. feature rank} for SSAEs trained on the Physics arXiv dataset using different strategies: full OWT, Dense retrieval, and Dense retrieval with tilt. Tilt encourages the learning of more broadly activating features, indicating increased concept coverage and recall.",
        "text": "\\centering     \\includegraphics[width=0.48\\textwidth]{figures/tilt_log_log_camera_ready.png}     \\caption{\\small \\textbf{Feature activation count vs. feature rank} for SSAEs trained on the Physics arXiv dataset using different strategies: full OWT, Dense retrieval, and Dense retrieval with tilt. Tilt encourages the learning of more broadly activating features, indicating increased concept coverage and recall.}     \\label{fig:activation_rank}\nFigure \\ref{fig:activation_rank} plots feature activation count vs. feature rank, showing that TERM with large tilt encourages learning more broadly activating features with increased concept recall.  This represents a fundamentally different mechanism for feature learning compared to standard ERM, promoting more compositional features that capture tail concepts.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the relationship between feature activation count and feature rank for SSAEs trained on the Physics arXiv dataset using different strategies, with both axes on a logarithmic scale. Four strategies are compared: full OWT (Baseline), Dense retrieval, and two Dense retrieval strategies with tilt (tilt=500 and tilt=10 to the power of 9). The results indicate that Dense retrieval with tilt learns more broadly activating features. Specifically, the feature activation counts are similar for all four strategies at lower feature ranks. However, as the feature rank increases, the feature activation counts for the two Dense retrieval strategies with tilt are significantly higher than the other two strategies, especially when tilt=10 to the power of 9. This suggests that the tilt strategy tends to learn more broadly activating features, thereby increasing concept coverage and recall."
    },
    "369": {
        "figure1": "2411.00743v1_f1_score_distributions_large",
        "label1": "fig:interpretability",
        "caption1": "\\small \\textbf{Automated interpretability}: F1 score distributions for predicting feature activation on Physics arXiv, using only FM-generated explanations. An LM is given examples activating a feature and asked to generate an explanation, which is then used to predict activations on new examples. Dense retrieval with tilt produces more predictive explanations than both the OWT baseline and Dense retrieval alone. ",
        "text": "\\centering     \\includegraphics[width=0.48\\textwidth]{figures/f1_score_distributions_large.png}     \\caption{\\small \\textbf{Automated interpretability}:      F1 score distributions for predicting feature activation on Physics arXiv, using only FM-generated explanations. An LM is given examples activating a feature and asked to generate an explanation, which is then used to predict activations on new examples. Dense retrieval with tilt produces more predictive explanations than both the OWT baseline and Dense retrieval alone.     }     \\label{fig:interpretability}\nFig \\ref{fig:interpretability} shows that TERM-trained SSAEs achieve higher F1 scores than the OWT baseline and ERM-trained SSAEs, indicating their explanations are more effective in predicting activation on new examples.  Interestingly, despite superior downstream perplexity vs. $L_0$, ERM-trained SSAEs did not yield more interpretable explanations than the baseline. This aligns with findings in \\citet{o2024disentangling}, where interpretability decreased with increasing SAE width attributed to less interpretable fine-grained features.  As TERM encourages coarser, more compositional features, its explanations are more readily interpretable.",
        "summarize_figure": "figure1",
        "summarization": "The figure shows the F1 score distributions for predicting feature activation on the Physics arXiv dataset using FM-generated explanations. It compares four methods: the OWT baseline, dense retrieval, and two tilted dense retrieval methods with different parameters (Tilt=500 and Tilt=10^9). The results indicate that tilted dense retrieval produces more predictive explanations than both the OWT baseline and dense retrieval alone. The mean F1 scores for tilted dense retrieval are 0.557 and 0.563, respectively, which are higher than the OWT baseline's 0.537 and dense retrieval's 0.525."
    },
    "370": {
        "figure1": "2411.00743v1_sft_sae_comparison_absolute",
        "label1": "fig:physicsood",
        "caption1": "\\small Pareto curves for SSAE trained with various data selection strategies as the sparsity coefficient is varied on Physics instruction test data. We plot absolute perplexity with the spliced in SSAE. We find that both BM25 retrieval and training on the validation data generalize poorly when tested out of domain. All curves are averaged over three SAE training run seeds.",
        "text": "\\centering     \\includegraphics[width=0.48\\textwidth]{figures/sft_sae_comparison_absolute.pdf}         \\caption{\\small Pareto curves for SSAE trained with various data selection strategies as the sparsity coefficient is varied on Physics instruction test data. We plot absolute perplexity with the spliced in SSAE. We find that both BM25 retrieval and training on the validation data generalize poorly when tested out of domain. All curves are averaged over three SAE training run seeds.}     \\label{fig:physicsood}",
        "summarize_figure": "figure1",
        "summarization": "This is a Pareto curve for SSAE trained with various data selection strategies on physics instruction test data. The figure shows the absolute perplexity of the spliced-in SSAE as the sparsity coefficient varies. The horizontal axis is SAE val L0, and the vertical axis is Perplexity with SAE. The figure shows curves for five data selection strategies: All Data (SFT), Validation (SFT), BM25, Dense retrieval (SFT), and Dense Tracin (SFT). The BM25 strategy has the highest perplexity. The Validation (SFT) strategy has high perplexity when the SAE val L0 value is small, but the perplexity decreases rapidly as the SAE val L0 value increases. The curves for All Data (SFT), Dense retrieval (SFT), and Dense Tracin (SFT) are relatively close and have low perplexity, indicating that these strategies perform well."
    },
    "371": {
        "figure1": "2411.00743v1_sae_comparison_plot_toxicity_adjusted_camera_ready",
        "label1": "fig:tailtoxicA",
        "caption1": "\\small Proportion of tokens with SAE features vs. Token frequency in Toxicity data. SSAE trained with dense retrieval captures more tail tokens (concepts) in its features.",
        "figure2": "2411.00743v1_sae_comparison_plot_final",
        "label2": "fig:tailA",
        "caption2": "\\small Proportion of tokens with SAE features vs. Token frequency in Physics arXiv data. SSAE trained with dense retrieval captures more tail tokens (concepts) in its features.} \\vspace{-0.2in",
        "text": "To probe tail concept learning we use convergent validity \\citep{campbell1959convergent} with the Logit Lens \\citep{bloom2024understandingfeatureslogitlens}. Figure \\ref{fig:tailA}, uses the unembedding matrix as a logit lens to analyze the top-10 token logits associated with each SSAE feature. For each frequency bucket in the Physics arXiv test data, we calculate the percentage of tokens that appear among the top-10 logits for at least one feature. This measures the extent to which SSAE features represent tokens across different frequency ranges. We compare two SSAEs at test $L_0$ of 100: one finetuned on full OWT dataset, another using Dense retrieval. The Dense retrieval finetuned SSAE captures a significantly higher proportion of tail tokens in its features compared to the OWT finetuned SSAE. Moreover, these captured tail tokens often correspond to physics-specific concepts, suggesting that SSAEs are indeed learning to represent rare, domain-relevant concepts. Similar results are obtained for toxicity data in \\autoref{fig:tailtoxicA}.\n\\centering     \\includegraphics[width=0.48\\textwidth]{figures/sae_comparison_plot_toxicity_adjusted_camera_ready.png}     \\caption{\\small Proportion of tokens with SAE features vs. Token frequency in Toxicity data. SSAE trained with dense retrieval captures more tail tokens (concepts) in its features.}     \\label{fig:tailtoxicA}",
        "summarize_figure": "figure1",
        "summarization": "The picture illustrates the relationship between the proportion of tokens with SAE features and token frequency in the Physics arXiv dataset. It compares two models: Reference OWT and Dense retrieval. The x-axis represents token frequency on a log scale, and the y-axis shows the proportion of tokens with SAE features. The Dense retrieval model captures a higher proportion of tokens with SAE features for lower frequency tokens (tail tokens) compared to the Reference OWT model. This indicates that the SAE model trained with Dense retrieval is better at capturing tail tokens (concepts). The hyperparameters are set as follows: Bin Size is 10, and Top-K is 10."
    },
    "372": {
        "figure1": "2411.00743v1_sae_comparison_plot_neurips_large_physics",
        "label1": "fig:tilteval",
        "caption1": "\\small Pareto curves for SSAEs finetuned on the \\textbf{Physics arXiv dataset} using different strategies: full OpenWebText (OWT), Dense retrieval, and Dense retrieval with Tilted Empirical Risk Minimization (TERM, tilt=500 and TERM, tilt=$10^9$). TERM-finetuned SSAEs achieve competitive performance with Dense retrieval alone within the $L_0$ range of 85-100.  Outside this range, our current training methodology results in higher reconstruction errors. All curves are averaged over three SAE training run seeds.",
        "figure2": "2411.00743v1_sae_comparison_plot_neurips_toxicity",
        "label2": "fig:tiltoxicteval",
        "caption2": "\\small Pareto curves for SSAEs finetuned on the \\textbf{Toxicity dataset} using different strategies: full OpenWebText (OWT), Dense retrieval, and Dense retrieval with Tilted Empirical Risk Minimization (TERM, tilt=500). TERM-finetuned SSAEs achieve competitive performance with Dense retrieval alone within the $L_0$ range of 100-140. All curves are averaged over three SAE training run seeds. ",
        "text": "Figures \\ref{fig:tilteval} and \\ref{fig:tiltoxicteval} show that TERM-finetuned SSAEs achieve comparable downstream perplexity to ERM-finetuned SSAEs within the typical $L_0$ regime used. However at very large or low $L_0$, training with Adam can lead to higher average risk or many inactive features. Adaptive penalty schemes offer a promising solution to this challenge (see \\autoref{sec:tiltpareto}).\nFigure \\ref{fig:tilteval} evaluates SSAEs trained with Tilted ERM on the Physics arXiv dataset, displaying Pareto curves where the x-axis represents $L_0$ and the y-axis shows downstream perplexity with patched-in SSAE. TERM-finetuned SSAEs achieve competitive performance with Dense retrieval alone within the $L_0$ range of 85-100.\n\\centering     \\includegraphics[width=0.5\\textwidth]{figures/sae_comparison_plot_neurips_large_physics.pdf}     \\caption{\\small Pareto curves for SSAEs finetuned on the \\textbf{Physics arXiv dataset} using different strategies: full OpenWebText (OWT), Dense retrieval, and Dense retrieval with Tilted Empirical Risk Minimization (TERM, tilt=500 and TERM, tilt=$10^9$). TERM-finetuned SSAEs achieve competitive performance with Dense retrieval alone within the $L_0$ range of 85-100.  Outside this range, our current training methodology results in higher reconstruction errors. All curves are averaged over three SAE training run seeds.}     \\label{fig:tilteval}",
        "summarize_figure": "figure1",
        "summarization": "The picture shows Pareto curves for SSAEs finetuned on the Physics arXiv dataset, comparing different strategies: full OpenWebText (OWT), Dense Retrieval, and Dense Retrieval with Tilted Empirical Risk Minimization (TERM, tilt=500 and tilt=10^9). The x-axis is SAE Validation L0, and the y-axis is Perplexity with SAE. The results indicate that within the L0 range of 85 to 100, TERM-finetuned SSAEs achieve competitive performance compared to Dense Retrieval alone. Outside this range, the current training methodology results in higher reconstruction errors."
    },
    "373": {
        "figure1": "2411.00743v1_sae_comparison_plot_neurips_toxicity",
        "label1": "fig:tiltoxicteval",
        "caption1": "\\small Pareto curves for SSAEs finetuned on the \\textbf{Toxicity dataset} using different strategies: full OpenWebText (OWT), Dense retrieval, and Dense retrieval with Tilted Empirical Risk Minimization (TERM, tilt=500). TERM-finetuned SSAEs achieve competitive performance with Dense retrieval alone within the $L_0$ range of 100-140. All curves are averaged over three SAE training run seeds. ",
        "figure2": "2411.00743v1_sae_comparison_plot_neurips_large_physics",
        "label2": "fig:tilteval",
        "caption2": "\\small Pareto curves for SSAEs finetuned on the \\textbf{Physics arXiv dataset} using different strategies: full OpenWebText (OWT), Dense retrieval, and Dense retrieval with Tilted Empirical Risk Minimization (TERM, tilt=500 and TERM, tilt=$10^9$). TERM-finetuned SSAEs achieve competitive performance with Dense retrieval alone within the $L_0$ range of 85-100.  Outside this range, our current training methodology results in higher reconstruction errors. All curves are averaged over three SAE training run seeds.",
        "text": "Figures \\ref{fig:tilteval} and \\ref{fig:tiltoxicteval} show that TERM-finetuned SSAEs achieve comparable downstream perplexity to ERM-finetuned SSAEs within the typical $L_0$ regime used. However at very large or low $L_0$, training with Adam can lead to higher average risk or many inactive features. Adaptive penalty schemes offer a promising solution to this challenge (see \\autoref{sec:tiltpareto}).\n\\centering     \\includegraphics[width=0.5\\textwidth]{figures/sae_comparison_plot_neurips_toxicity.pdf}     \\caption{\\small Pareto curves for SSAEs finetuned on the \\textbf{Toxicity dataset} using different strategies: full OpenWebText (OWT), Dense retrieval, and Dense retrieval with Tilted Empirical Risk Minimization (TERM, tilt=500). TERM-finetuned SSAEs achieve competitive performance with Dense retrieval alone within the $L_0$ range of 100-140. All curves are averaged over three SAE training run seeds. }          \\label{fig:tiltoxicteval}",
        "summarize_figure": "figure1",
        "summarization": "Here is the summary of the figure:The figure illustrates a comparison of different SAE strategies, with the horizontal axis representing SAE Validation L0 and the vertical axis representing Perplexity with SAE. The graph includes four curves, representing Pareto curves of SSAEs fine-tuned on the Physics arXiv dataset using strategies such as full OpenWebText (OWT), Dense Retrieval, Dense Retrieval with Tilted Empirical Risk Minimization (TERM, tilt=500), and Dense Retrieval with Tilted Empirical Risk Minimization (TERM, tilt=10 to the power of 9). The results indicate that TERM-fine-tuned SSAEs achieve competitive performance compared to Dense Retrieval alone within the L0 range of 85 to 100. However, outside this range, the current training methodology results in higher reconstruction errors. All curves are averaged over three SAE training run seeds."
    },
    "374": {
        "figure1": "2411.00743v1_sae_normalized_cumulative_distribution_plot_log_matplotlib",
        "label1": "fig:tailtoxicB",
        "caption1": "\\small Cumulative proportion of tokens with SAE features vs. cumulative percentage of tokens in Toxicity data, normalized per model so that the cumulative proportion of tokens with features is 1 over the entire dataset. SSAE trained with dense retrieval and larger tilt captures more tail tokens (concepts) in its features. Note that the curves at tilt 500 and tilt $10^9$ overlap.",
        "figure2": "2411.00743v1_tilt_tail_new_2_crop",
        "label2": "fig:tailB",
        "caption2": "\\small Cumulative proportion of tokens with SAE features vs. cumulative percentage of tokens in Physics arXiv data, normalized per model so that the cumulative proportion of tokens with features is 1 over the entire dataset. SSAE trained with dense retrieval and larger tilt captures more tail tokens (concepts) in its features.",
        "text": "Figure \\ref{fig:tailB} plots the cumulative proportion of tokens with SSAE features (identified using the logit lens approach) versus the cumulative percentage of tokens in the Physics arXiv data for different SSAEs. We normalize the curves per model at a validation $L_0$ of 100, so that the cumulative proportion of tokens with features is 1 over the entire dataset. Results show that SSAEs trained with Dense retrieval and tilt capture a greater proportion of tail tokens compared to Dense retrieval alone, with this effect increasing with tilt. Figure \\ref{fig:tailtoxicB} shows a similar trend for the Toxicity dataset.\n\\centering     \\includegraphics[width=0.5\\textwidth]{figures/sae_normalized_cumulative_distribution_plot_log_matplotlib.png}     \\looseness=-1     \\caption{\\small Cumulative proportion of tokens with SAE features vs. cumulative percentage of tokens in Toxicity data, normalized per model so that the cumulative proportion of tokens with features is 1 over the entire dataset. SSAE trained with dense retrieval and larger tilt captures more tail tokens (concepts) in its features. Note that the curves at tilt 500 and tilt $10^9$ overlap.}     \\label{fig:tailtoxicB}",
        "summarize_figure": "figure1",
        "summarization": "This picture shows the relationship between the normalized cumulative proportion of tokens with SAE features and the cumulative percentage of tokens (least to most frequent) for different models on the Physics arXiv dataset. The curves in the picture indicate that as the cumulative percentage of tokens increases from least frequent to most frequent, the normalized cumulative proportion of tokens with features also increases. For low-frequency tokens (e.g., cumulative percentage less than 1%), models using Dense retrieval with Tilt (red and green lines) capture a larger proportion of tokens with features compared to the model using only Dense retrieval (blue line). Specifically, the model with a Tilt value of 10 to the power of 9 (green line) generally shows a higher cumulative proportion in the low-frequency token region, suggesting that a larger Tilt value allows the model to capture features for more low-frequency tokens. As the cumulative percentage of tokens approaches 100%, the normalized cumulative proportion for all models approaches 1, indicating that they eventually cover all tokens in the dataset."
    },
    "375": {
        "figure1": "2411.00743v1_neurips_feature_activation_count_difference_histogram_active_tilt10_9",
        "label1": "fig:relativehistogram2",
        "caption1": "\\small Distribution of log-ratio feature activation count differences between specialized SAEs and the OWT baseline on the Physics arXiv test set, normalized per SAE model. Blue represents the ERM-trained SSAE with Dense retrieval, orange represents the TERM-trained SSAE with tilt=500. The ERM-trained SSAE exhibits more probability mass on the right, indicating an emphasis on representing common concepts, while the TERM-trained SSAE's shift towards the left suggests a greater focus on representing domain-specific tail concepts.",
        "figure2": "2411.00743v1_neurips_feature_activation_count_difference_histogram_active_tilt500_camera_ready",
        "label2": "fig:relativehistogram3",
        "caption2": "\\small Distribution of log-ratio feature activation count differences on the Physics arXiv test set, normalized per SAE model. Blue represents the ERM-trained SSAE with Dense retrieval, orange represents the TERM-trained SSAE with tilt=$10^9$. The intensified leftward shift of probability mass with higher tilt demonstrates that TERM increasingly prioritizes representing tail concepts compared to standard ERM-trained SSAE, which focuses more on frequent concepts.",
        "figure3": "2411.00743v1_neurips_feature_activation_count_difference_histogram_active_tilt500_camera_ready",
        "label3": "fig:relativehistogram3",
        "caption3": "\\small Distribution of log-ratio feature activation count differences on the Physics arXiv test set, normalized per SAE model. Blue represents the ERM-trained SSAE with Dense retrieval, orange represents the TERM-trained SSAE with tilt=$10^9$. The intensified leftward shift of probability mass with higher tilt demonstrates that TERM increasingly prioritizes representing tail concepts compared to standard ERM-trained SSAE, which focuses more on frequent concepts.",
        "text": "Figures \\ref{fig:relativehistogram2} and \\ref{fig:relativehistogram3} analyze the distribution of differences in feature activation counts between SSAEs (both ERM and TERM-trained) and the OWT baseline on the Physics arXiv test set.  The peak at 0 indicates that SSAEs retain some similarity to the baseline in their activation patterns.  The ERM-trained SSAE exhibits greater probability mass on the right, indicating a focus on frequent concepts, while the TERM-trained SSAEs shift probability mass leftward as tilt increases, suggesting a stronger emphasis on representing domain-specific tail concepts.\nFigures \\ref{fig:relativehistogram2} and \\ref{fig:relativehistogram3} analyze the distribution of differences in feature activation counts between the same features in specialized SAEs (both ERM and TERM-trained) and the OWT baseline on the Physics arXiv test set. The difference is quantified as the log ratio of feature activation counts: $\\log_2 (\\frac{M+1}{B+1})$, where $M$ represents the SSAE and $B$ the OWT baseline. Positive values indicate features activating on more data points in the specialized SAEs relative to the baseline SAE.\n\\caption{\\small Distribution of log-ratio feature activation count differences between specialized SAEs and the OWT baseline on the Physics arXiv test set, normalized per SAE model. Blue represents the ERM-trained SSAE with Dense retrieval, orange represents the TERM-trained SSAE with tilt=500. The ERM-trained SSAE exhibits more probability mass on the right, indicating an emphasis on representing common concepts, while the TERM-trained SSAE's shift towards the left suggests a greater focus on representing domain-specific tail concepts.}     \\label{fig:relativehistogram2}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the distribution of feature activation count differences between specialized SAEs (both ERM and TERM-trained) and the OWT baseline on the Physics arXiv test set. The x-axis represents the log ratio of feature activation counts, calculated as log2((Model + 1) / (Baseline + 1)), and the y-axis represents the probability density (log scale). The blue line represents the ERM-trained SSAE with dense retrieval, and the green line represents the TERM-trained SSAE with tilt=500. The ERM-trained SSAE exhibits greater probability mass on the right, indicating a focus on representing common concepts, while the TERM-trained SSAE's shift towards the left suggests a stronger emphasis on representing domain-specific tail concepts."
    },
    "376": {
        "figure1": "2411.00743v1_neurips_feature_activation_count_difference_histogram_active_tilt500_camera_ready",
        "label1": "fig:relativehistogram3",
        "caption1": "\\small Distribution of log-ratio feature activation count differences on the Physics arXiv test set, normalized per SAE model. Blue represents the ERM-trained SSAE with Dense retrieval, orange represents the TERM-trained SSAE with tilt=$10^9$. The intensified leftward shift of probability mass with higher tilt demonstrates that TERM increasingly prioritizes representing tail concepts compared to standard ERM-trained SSAE, which focuses more on frequent concepts.",
        "figure2": "2411.00743v1_neurips_feature_activation_count_difference_histogram_active_tilt10_9",
        "label2": "fig:relativehistogram2",
        "caption2": "\\small Distribution of log-ratio feature activation count differences between specialized SAEs and the OWT baseline on the Physics arXiv test set, normalized per SAE model. Blue represents the ERM-trained SSAE with Dense retrieval, orange represents the TERM-trained SSAE with tilt=500. The ERM-trained SSAE exhibits more probability mass on the right, indicating an emphasis on representing common concepts, while the TERM-trained SSAE's shift towards the left suggests a greater focus on representing domain-specific tail concepts.",
        "figure3": "2411.00743v1_neurips_feature_activation_count_difference_histogram_active_tilt10_9",
        "label3": "fig:relativehistogram2",
        "caption3": "\\small Distribution of log-ratio feature activation count differences between specialized SAEs and the OWT baseline on the Physics arXiv test set, normalized per SAE model. Blue represents the ERM-trained SSAE with Dense retrieval, orange represents the TERM-trained SSAE with tilt=500. The ERM-trained SSAE exhibits more probability mass on the right, indicating an emphasis on representing common concepts, while the TERM-trained SSAE's shift towards the left suggests a greater focus on representing domain-specific tail concepts.",
        "text": "Figures \\ref{fig:relativehistogram2} and \\ref{fig:relativehistogram3} analyze the distribution of differences in feature activation counts between SSAEs (both ERM and TERM-trained) and the OWT baseline on the Physics arXiv test set.  The peak at 0 indicates that SSAEs retain some similarity to the baseline in their activation patterns.  The ERM-trained SSAE exhibits greater probability mass on the right, indicating a focus on frequent concepts, while the TERM-trained SSAEs shift probability mass leftward as tilt increases, suggesting a stronger emphasis on representing domain-specific tail concepts.\nFigures \\ref{fig:relativehistogram2} and \\ref{fig:relativehistogram3} analyze the distribution of differences in feature activation counts between the same features in specialized SAEs (both ERM and TERM-trained) and the OWT baseline on the Physics arXiv test set. The difference is quantified as the log ratio of feature activation counts: $\\log_2 (\\frac{M+1}{B+1})$, where $M$ represents the SSAE and $B$ the OWT baseline. Positive values indicate features activating on more data points in the specialized SAEs relative to the baseline SAE.\n\\caption{\\small Distribution of log-ratio feature activation count differences on the Physics arXiv test set, normalized per SAE model. Blue represents the ERM-trained SSAE with Dense retrieval, orange represents the TERM-trained SSAE with tilt=$10^9$. The intensified leftward shift of probability mass with higher tilt demonstrates that TERM increasingly prioritizes representing tail concepts compared to standard ERM-trained SSAE, which focuses more on frequent concepts.}     \\label{fig:relativehistogram3}",
        "summarize_figure": "figure1",
        "summarization": "The figure shows the distribution of feature activation count differences between specialized sparse autoencoders (SAEs) and the OWT baseline on the Physics arXiv test set. The x-axis represents the log ratio of feature activation counts, calculated as log2((M + 1) / (B + 1)), where M represents the SAE and B represents the OWT baseline. The y-axis represents the probability density on a log scale. The figure compares two curves: the blue curve represents the ERM-trained SAE with dense retrieval, and the green curve represents the TERM-trained SAE with tilt=500. It can be observed that the blue curve (ERM-trained SAE) has more probability mass concentrated on the right, indicating a greater emphasis on representing common concepts. The green curve (TERM-trained SAE), on the other hand, shifts towards the left, suggesting a stronger focus on representing domain-specific tail concepts."
    },
    "377": {
        "figure1": "2411.00743v1_compare_gsae_models_umap",
        "label1": "fig:umapplot",
        "caption1": "\\small UMAP visualization of token activations and decoder features for a TERM-trained and ERM-trained GSAE. Decoder directions for TERM-trained GSAE appear more spread out, suggesting the SAE has wider coverage than the ERM-trained GSAE. ",
        "figure2": "2411.00743v1_decoder_coverage_analysis",
        "label2": "fig:decodercoverage",
        "caption2": "\\small Distribution of cosine similarities between decoder directions of TERM-trained and ERM-trained GSAEs. TERM-trained GSAE shows lower similarity between decoder feature directions implying greater coverage. ",
        "figure3": "2411.00743v1_pca_coverage_analysis",
        "label3": "fig:pcacoverage",
        "caption3": "\\small \\looseness=-1 Number of PCA components required to explain variance in decoder feature directions of TERM-trained and ERM-trained GSAEs. TERM-trained GSAE shows greater variance in decoder feature directions implying greater coverage.",
        "text": "We analyze decoder feature vector coverage using three approaches. Figure \\ref{fig:umapplot} presents a UMAP visualization of token activations and decoder features for both GSAEs, revealing a greater dispersion of decoder directions for the TERM-trained GSAE, indicating broader coverage. Figure \\ref{fig:decodercoverage} quantifies the distribution of cosine similarities between decoder directions, with the TERM-trained GSAE showing lower overall similarity, suggesting greater coverage. Figure \\ref{fig:pcacoverage} shows that the TERM-trained GSAE requires more PCA components to explain variance in decoder feature directions (40) compared to the ERM-trained GSAE (21). Taken together, this shows that TERM-trained SAEs cover a wider range of features than ERM-trained SAEs.\n\\centering     \\includegraphics[width=0.48\\textwidth]{figures/compare_gsae_models_umap.pdf}         \\caption{\\small UMAP visualization of token activations and decoder features for a TERM-trained and ERM-trained GSAE. Decoder directions for TERM-trained GSAE appear more spread out, suggesting the SAE has wider coverage than the ERM-trained GSAE. }     \\label{fig:umapplot}",
        "summarize_figure": "figure1",
        "summarization": "The figure presents a UMAP visualization of token activations and decoder features for TERM-trained and ERM-trained GSAEs. As shown in the figure, the decoder directions for the TERM-trained GSAE appear more spread out, suggesting that the SAE has wider coverage than the ERM-trained GSAE."
    },
    "378": {
        "figure1": "2411.00743v1_decoder_coverage_analysis",
        "label1": "fig:decodercoverage",
        "caption1": "\\small Distribution of cosine similarities between decoder directions of TERM-trained and ERM-trained GSAEs. TERM-trained GSAE shows lower similarity between decoder feature directions implying greater coverage. ",
        "figure2": "2411.00743v1_compare_gsae_models_umap",
        "label2": "fig:umapplot",
        "caption2": "\\small UMAP visualization of token activations and decoder features for a TERM-trained and ERM-trained GSAE. Decoder directions for TERM-trained GSAE appear more spread out, suggesting the SAE has wider coverage than the ERM-trained GSAE. ",
        "figure3": "2411.00743v1_pca_coverage_analysis",
        "label3": "fig:pcacoverage",
        "caption3": "\\small \\looseness=-1 Number of PCA components required to explain variance in decoder feature directions of TERM-trained and ERM-trained GSAEs. TERM-trained GSAE shows greater variance in decoder feature directions implying greater coverage.",
        "text": "We analyze decoder feature vector coverage using three approaches. Figure \\ref{fig:umapplot} presents a UMAP visualization of token activations and decoder features for both GSAEs, revealing a greater dispersion of decoder directions for the TERM-trained GSAE, indicating broader coverage. Figure \\ref{fig:decodercoverage} quantifies the distribution of cosine similarities between decoder directions, with the TERM-trained GSAE showing lower overall similarity, suggesting greater coverage. Figure \\ref{fig:pcacoverage} shows that the TERM-trained GSAE requires more PCA components to explain variance in decoder feature directions (40) compared to the ERM-trained GSAE (21). Taken together, this shows that TERM-trained SAEs cover a wider range of features than ERM-trained SAEs.\n\\centering     \\includegraphics[width=0.48\\textwidth]{figures/decoder_coverage_analysis.pdf}         \\caption{\\small Distribution of cosine similarities between decoder directions of TERM-trained and ERM-trained GSAEs. TERM-trained GSAE shows lower similarity between decoder feature directions implying greater coverage. }     \\label{fig:decodercoverage}",
        "summarize_figure": "figure1",
        "summarization": "This is an analysis of decoder feature vector coverage.The image shows a UMAP visualization of token activations and decoder features for TERM-trained and ERM-trained GSAEs. The decoder directions for the TERM-trained GSAE appear more spread out, suggesting the SAE has wider coverage than the ERM-trained GSAE. In the image, the decoder directions of the TERM-trained GSAE exhibit greater dispersion, indicating broader coverage."
    },
    "379": {
        "figure1": "2411.00743v1_pca_coverage_analysis",
        "label1": "fig:pcacoverage",
        "caption1": "\\small \\looseness=-1 Number of PCA components required to explain variance in decoder feature directions of TERM-trained and ERM-trained GSAEs. TERM-trained GSAE shows greater variance in decoder feature directions implying greater coverage.",
        "figure2": "2411.00743v1_compare_gsae_models_umap",
        "label2": "fig:umapplot",
        "caption2": "\\small UMAP visualization of token activations and decoder features for a TERM-trained and ERM-trained GSAE. Decoder directions for TERM-trained GSAE appear more spread out, suggesting the SAE has wider coverage than the ERM-trained GSAE. ",
        "figure3": "2411.00743v1_decoder_coverage_analysis",
        "label3": "fig:decodercoverage",
        "caption3": "\\small Distribution of cosine similarities between decoder directions of TERM-trained and ERM-trained GSAEs. TERM-trained GSAE shows lower similarity between decoder feature directions implying greater coverage. ",
        "text": "We analyze decoder feature vector coverage using three approaches. Figure \\ref{fig:umapplot} presents a UMAP visualization of token activations and decoder features for both GSAEs, revealing a greater dispersion of decoder directions for the TERM-trained GSAE, indicating broader coverage. Figure \\ref{fig:decodercoverage} quantifies the distribution of cosine similarities between decoder directions, with the TERM-trained GSAE showing lower overall similarity, suggesting greater coverage. Figure \\ref{fig:pcacoverage} shows that the TERM-trained GSAE requires more PCA components to explain variance in decoder feature directions (40) compared to the ERM-trained GSAE (21). Taken together, this shows that TERM-trained SAEs cover a wider range of features than ERM-trained SAEs.\n\\centering     \\includegraphics[width=0.48\\textwidth]{figures/pca_coverage_analysis.pdf}         \\caption{\\small \\looseness=-1 Number of PCA components required to explain variance in decoder feature directions of TERM-trained and ERM-trained GSAEs. TERM-trained GSAE shows greater variance in decoder feature directions implying greater coverage.}     \\label{fig:pcacoverage}",
        "summarize_figure": "figure1",
        "summarization": "The image shows a UMAP visualization of token activations and decoder features for TERM-trained and ERM-trained GSAEs. In the image, blue dots represent token activations, red stars represent decoder directions for ERM-trained GSAE, and green stars represent decoder directions for TERM-trained GSAE. Decoder directions for TERM-trained GSAE appear more spread out, suggesting the SAE has wider coverage than the ERM-trained GSAE."
    },
    "380": {
        "figure1": "2411.00743v1_error_distribution_plot",
        "label1": "fig:errorhistogram",
        "caption1": "\\small Reconstruction error distribution of TERM-trained and ERM-trained GSAE. TERM-trained GSAE minimizes the maximum error at the cost of average error.",
        "figure2": "2411.00743v1_token_rank_vs_error_plot",
        "label2": "fig:etokenrankerror",
        "caption2": "\\small Reconstruction error vs. token rank for TERM-trained and ERM-trained GSAEs. TERM exhibits lower error variance and maximum error for tail tokens.",
        "text": "Figure \\ref{fig:errorhistogram} shows the distribution of reconstruction error for the TERM-trained GSAE. TERM minimizes max error at the cost of slightly higher average error. Figure \\ref{fig:etokenrankerror} plots the reconstruction error for tokens ranked by frequency, showing that TERM reduces reconstruction error and error variance for tail tokens compared to ERM.\n\\centering     \\includegraphics[width=0.48\\textwidth]{figures/error_distribution_plot.pdf}         \\caption{\\small Reconstruction error distribution of TERM-trained and ERM-trained GSAE. TERM-trained GSAE minimizes the maximum error at the cost of average error.}     \\label{fig:errorhistogram}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the reconstruction error distribution of GSAEs trained with TERM and ERM. The red distribution represents GSAE trained with ERM, while the green distribution represents GSAE trained with Tilted ERM (TERM). The reconstruction error distribution of TERM-trained GSAE is more concentrated with a higher peak but shifted to the right, compared to ERM-trained GSAE. This indicates that TERM-trained GSAE minimizes the maximum error at the cost of a slightly higher average error. Specifically, the distribution of ERM-trained GSAE is more dispersed, while the distribution of TERM-trained GSAE is more concentrated around higher error values."
    },
    "381": {
        "figure1": "2411.00743v1_token_coverage_comparison",
        "label1": "fig:tokensthreshold",
        "caption1": "\\small Proportion of tokens detected vs. activation threshold for TERM-trained and ERM-trained GSAEs. TERM-trained features exhibit stronger activations.",
        "figure2": "2411.00743v1_merged.drawio",
        "label2": "fig:tokenentropy",
        "caption2": "\\small TERM feature activation patterns. (Left) TERM token activation entropy is lower, suggesting more specialized features. (Right) TERM max feature activations per token are higher. These characteristics, from minimizing max risk, contribute to TERM's enhanced tail concept detection.",
        "text": "Figures  \\ref{fig:tokenentropy} and \\ref{fig:tokensthreshold} show that TERM-trained GSAE features exhibit stronger activations and lower entropy compared to ERM-trained GSAE on the data.  This, combined with their high recall, suggests a strategy for rare concept detection: tag features strongly associated with rare concepts during pretraining, and at test time, strong activation of these tagged features triggers further investigation. This is more effective than using error nodes with ERM-trained SAEs for rare concept detection, as error nodes do not disambiguate types of rare features.\n\\centering     \\includegraphics[width=0.48\\textwidth]{figures/token_coverage_comparison.png}         \\caption{\\small Proportion of tokens detected vs. activation threshold for TERM-trained and ERM-trained GSAEs. TERM-trained features exhibit stronger activations.}     \\label{fig:tokensthreshold}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the distribution of entropy of positive token activations in GSAEs trained with TERM (GSAE Tilted ERM) and ERM (GSAE ERM). The figure indicates that TERM-trained GSAEs exhibit lower token activation entropy compared to ERM-trained GSAEs. The peak of activation entropy density for TERM-trained GSAEs is around 2, while the peak for ERM-trained GSAEs is between 4 and 5. This suggests that TERM-trained GSAEs possess more specialized features."
    },
    "382": {
        "figure1": "2411.00744v1_efficiency_figure",
        "label1": "fig:Efficiency",
        "caption1": "Efficiency Comparison",
        "text": "\\centering   \\includegraphics[width=\\linewidth]{figures/efficiency_figure.pdf}   \\caption{Efficiency Comparison}   \\label{fig:Efficiency} As shown in Table~\\ref{tab:rouge_comparison}, we compare \\textit{CORAG} with several baselines across different datasets, primarily using WikiPassageQA and MARCO. The evaluations are conducted on three different chunk sizes, utilizing ROUGE-1, ROUGE-2, and ROUGE-L metrics to assess the improvements in responses generated by the LLM due to our retrieval method. \\textit{CORAG} demonstrate a substantial improvement of approximately 25\\% compared to mainstream RAG approaches such as \\textit{NaiveRAG} and \\textit{RAPTOR}. As expected,  \\textit{CORAG} does not exceed the upper bound,  which represents an extreme scenario where all possible combination orders are exhaustively enumerated, which is clearly inefficient and impractical. In summary, \\textit{CORAG} outperforms baselines,  enhancing retrieval relevancy while pruning the search space  effectively. As shown in Figure~\\ref{fig:Efficiency}, since \\textit{CORAG} is based on the tree search algorithm, the agent assists in predicting the optimal reranker and parameters for a given query. Therefore, it is crucial to evaluate the impact of different chunk sizes and datasets on retrieval optimization task efficiency. We tested efficiency using various datasets and chunk sizes, observing that \\textit{NaiveRAG}, which uses a traditional retrieval approach, achieved shorter retrieval times but lower ROUGE scores. \\textit{CORAG upper} performs well in terms of ROUGE, but its efficiency is significantly reduced due to exploring the entire search space. Similarly, RAPTOR, which leverages an external LLM for summarization, exhibited poor efficiency. In contrast, our \\textit{CORAG} approach strikes a balance between efficiency and  retrieval relevance, achieving an effective trade-off.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the ROUGE scores (left axis) and retrieval times (right axis) of different retrieval methods (Raptor, NaiveRAG, CORAG w/o Agent, CORAG+Agent, CORAG upper) on the WikiPassageQA dataset with varying chunk sizes (256, 512, and 1024). NaiveRAG exhibits the shortest retrieval time but also the lowest ROUGE score. CORAG upper achieves a higher ROUGE score but with a significant increase in retrieval time. CORAG+Agent consistently outperforms Raptor, NaiveRAG, and CORAG w/o Agent across different chunk sizes, while maintaining a reasonable retrieval time, demonstrating a good balance between retrieval relevance and efficiency."
    },
    "383": {
        "figure1": "2411.00744v1_breakdown_abl",
        "label1": "fig:breakdown",
        "caption1": "Performance Breakdown",
        "text": "\\centering   \\includegraphics[width=\\linewidth]{figures/breakdown_abl.pdf}   \\caption{Performance Breakdown}   \\label{fig:breakdown}",
        "summarize_figure": "figure1",
        "summarization": "The image displays the latency breakdown across different chunk sizes (256, 512, 1024) for two datasets, WikiPassageQA and MARCO. On the WikiPassageQA dataset, the total latency decreases as chunk size increases, with reranking being the dominant source of latency; embedding and retrieve latency decrease with increasing chunk size, while prompt contributes minimally. On the MARCO dataset, total latency decreases from chunk size 256 to 512 and slightly increases from 512 to 1024; reranking is also the primary latency component, but its absolute value and relative proportion are lower than in WikiPassageQA; embedding and retrieve latency also decrease or remain low with increasing chunk size, and prompt latency is negligible. Compared between the two datasets, the total latency on MARCO is significantly lower than on WikiPassageQA, and the proportion attributed to reranking is also lower."
    },
    "384": {
        "figure1": "2411.00744v1_C_figure_separate_wiki_marco",
        "label1": "fig:C_figure",
        "caption1": "ROUGE Comparison between different C",
        "figure2": "2411.00744v1_lambda_figure_separate_wiki_marco",
        "label2": "fig:lambda_figure",
        "caption2": "ROUGE Comparison between different lambda",
        "text": "\\centering   \\includegraphics[width=\\linewidth]{figures/C_figure_separate_wiki_marco.pdf}   \\caption{ROUGE Comparison between different C}   \\label{fig:C_figure}   \\centering   \\includegraphics[width=\\linewidth]{figures/lambda_figure_separate_wiki_marco.pdf}   \\caption{ROUGE Comparison between different lambda}   \\label{fig:lambda_figure}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the comparison of ROUGE-1 (R1) and ROUGE-L (RL) scores under different C parameter values on the WikiPassageQA dataset. The horizontal axis represents the C parameter, with values of 0, 1, 2, and 3. The vertical axis represents the ROUGE score. It can be seen that for the R1 metric, the score is the highest when C=2, and the RL metric is also relatively high when C=2. The R1 score is significantly higher than the RL score."
    },
    "385": {
        "figure1": "2411.00744v1_lambda_figure_separate_wiki_marco",
        "label1": "fig:lambda_figure",
        "caption1": "ROUGE Comparison between different lambda",
        "figure2": "2411.00744v1_C_figure_separate_wiki_marco",
        "label2": "fig:C_figure",
        "caption2": "ROUGE Comparison between different C",
        "text": "\\centering   \\includegraphics[width=\\linewidth]{figures/C_figure_separate_wiki_marco.pdf}   \\caption{ROUGE Comparison between different C}   \\label{fig:C_figure}   \\centering   \\includegraphics[width=\\linewidth]{figures/lambda_figure_separate_wiki_marco.pdf}   \\caption{ROUGE Comparison between different lambda}   \\label{fig:lambda_figure}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the ROUGE-1 (R1) and ROUGE-L (RL) scores with different C values on the WikiPassageQA dataset. The C values are 0, 1, 2, and 3, respectively. The R1 score reaches its peak when C=2, followed by a slight decrease. The RL score also shows a trend of increasing and then decreasing as the C value increases, reaching its highest point at C=2. The R1 score is consistently higher than the RL score."
    },
    "386": {
        "figure1": "2411.00750v1_performance",
        "label1": "fig:performance",
        "caption1": " \\textbf{Iterative performance in the self-improvement.} Experiments are conducted on GSM8K with varying sampling numbers $k$. Solid markers show the performance of vanilla self-improve,  with the solid line fitting these points. The performance plateaus after a few iterations. Hollow markers represent the performance after supplementing tail data, with a dashed line trend. It balances the distribution and alleviates performance bottlenecks.",
        "text": "\\centering     \\includegraphics[width=\\textwidth]{figure/performance.pdf}     \\caption{     \\textbf{Iterative performance in the self-improvement.} Experiments are conducted on GSM8K with varying sampling numbers $k$.      Solid markers show the performance of vanilla self-improve,  with the solid line fitting these points. The performance plateaus after a few iterations.     Hollow markers represent the performance after supplementing tail data, with a dashed line trend. It balances the distribution and alleviates performance bottlenecks.}     \\label{fig:performance}",
        "summarize_figure": "figure1",
        "summarization": "The image illustrates the iterative self-improvement performance of the Llama2-7B model on the GSM8K dataset. The x-axis represents the number of iterations, and the y-axis represents the performance. The image shows the performance changes of two self-improvement methods (Vanilla Self-Improve and Balanced Self-Improve) under different sampling numbers k (k=1, 2, 4, 8). The performance of the Vanilla Self-Improve method tends to stabilize after several iterations, while the Balanced Self-Improve method (supplemented with tail data) generally exhibits higher performance under different k values, indicating that this method can balance the data distribution, thereby alleviating performance bottlenecks. Especially when k=1, the performance of the Vanilla Self-Improve method decreases with the increase of the number of iterations."
    },
    "387": {
        "figure1": "2411.00771v1_explosion",
        "label1": "fig: explosion",
        "caption1": "Illustration of the motivation and effectiveness of our Elongation Filter. We take the reconstruction of \\textit{Rubble} \\citep{turki2022mega} scene as an example. On the left, we highlight the collection of Gaussian primitives with high gradient or extreme elongation. There is a significant overlap between two collections. By restricting densification of these sand-like points, we prevent out-of-memory (OOM) errors caused by an explosion in Gaussian count, enabling a steady count evolution analogous to CityGaussian \\citep{liu2024citygaussian} in parallel tuning, as depicted on the right.",
        "text": "As discussed in \\cref{sec: intro}, the critical obstacle to scaling up 2DGS is the excessive proliferation of certain primitives during the parallel tuning stage. Typically, a 2D Gaussian can collapse to a very small point when projected from a distance, especially those exhibiting extreme elongation \\citep{huang20242d}. With high opacity, the movement of these minuscule points can cause significant pixel changes in complex scenes, leading to pronounced position gradients. As evidenced in the left portion of \\cref{fig: explosion}, these tiny, sand-like projected points contribute substantially to points with high gradients. And they belong to those with extreme elongation. Moreover, some points project smaller than one pixel, resulting in their covariance being replaced by a fixed value through the antialiased low-pass filter. Consequently, these points cannot properly adjust their scaling and rotation with valid gradients. In block-wise parallel tuning, the views assigned to each block are much less than the total. These distant views are therefore frequently observed, causing the gradients of degenerated points to accumulate rapidly. These points consequently trigger exponential increases in Gaussian count and ultimately lead to out-of-memory errors, as demonstrated in the right portion of \\cref{fig: explosion}.\nIn light of this observation, we implement a straightforward yet effective Elongation Filter to address this problem. Before densification, we assess the elongation rate of each surfel, defined as $\\eta_n = \\min(s_{n,u}, s_{n,v}) / \\max(s_{n,u}, s_{n,v})$. Surfels with $\\eta_n$ below a certain threshold are excluded from the cloning and splitting process. As shown in the right portion of \\cref{fig: explosion}, this filter mitigates out-of-memory errors and facilitates a more steady Gaussian count evolution. Furthermore, experimental results in \\cref{tab: ablation} demonstrate that it does not compromise performance at the pretraining stage.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the effectiveness of the Elongation Filter. It compares the Gaussian count of CityGaussian,ours with Elongation Filter and without Elongation Filter over Finetune Step. The method without the Elongation Filter encounters an out-of-memory (OOM) error early in training, while the method with the Elongation Filter stabilizes the Gaussian count, avoiding the OOM error and achieving a similar count evolution to CityGaussian."
    },
    "388": {
        "figure1": "2411.00773v1_fig5",
        "label1": "fig:vis_tl",
        "caption1": "Continual learning results of GNN~\\citep{xu2018gnn} and NLM~\\citep{dong2019nlm} in the VAP task. The mean results from three random runs are displayed in solid lines and the variance is reported as the semi-transparent regions. We also show the results of the models trained from scratch using 100\\% data in dashed lines. ",
        "text": "\\centering \t\\includegraphics[width=0.9\\columnwidth]{imgs/fig5.pdf} \t\\caption{Continual learning results of GNN~\\citep{xu2018gnn} and NLM~\\citep{dong2019nlm} in the VAP task. The mean results from three random runs are displayed in solid lines and the variance is reported as the semi-transparent regions. We also show the results of the models trained from scratch using 100\\% data in dashed lines. \t} \t\\label{fig:vis_tl}\nSimilar to the SPF task, we investigate how much data the methods need to continually learn abstract rules in the VAP task. The models pre-trained in \\textit{easy} mode are used as the initial weights, which are continually trained with different sets of data from the \\textit{hard} mode. The data are sampled for 3 times and the mean and variance of the results are reported in \\fref{fig:vis_tl}, where we also report the results from the models trained with 100\\% data from scratch as dashed lines (``upper bound''). We observe that the two methods could struggle to reach their ``upper bound'' if fixed training agents are used. For the random agent setting, NLM~\\citep{dong2019nlm} could progressively learn new rules and reach its ``upper bound\" with around 50\\% data while GNN fails even with 100\\% data.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the continual learning results of GNN and NLM models with fixed training agents in the VAP task. The horizontal axis represents the data amount, and the vertical axis represents the weighted accuracy (wAcc.). The figure shows that as the data amount increases, the weighted accuracy of both models shows a trend of first increasing and then decreasing. The weighted accuracy of the NLM model peaks at a data amount of 0.1, at approximately 0.45, and then begins to decrease, slightly rebounding at a data amount of 1, at approximately 0.43. The weighted accuracy of the GNN model is relatively lower, peaking at a data amount of 0.1, at approximately 0.36, and also shows a decreasing trend, slightly rebounding at a data amount of 1, at approximately 0.37. The results of GNN and NLM models trained from scratch using 100% data are also shown, at 0.398 and 0.464 respectively. It can be seen that the continually learned GNN and NLM models both struggle to reach the performance limit of models trained from scratch. The shaded areas represent the variance."
    },
    "389": {
        "figure1": "2411.00776v1_ar_compare",
        "label1": "fig:ar_compare",
        "caption1": " \\textbf{Comparison among different language modeling compatible autoregressive (AR) image generators.} The proposed RAR demonstrates significant  improvements over previous AR methods. RAR-B, with only \\textit{261M} parameters, achieves an FID score of 1.95, outperforming both LlamaGen-XXL (1.4B parameters) and Open-MAGVIT2-XL (1.5B parameters). ",
        "text": "\\centering     \\includegraphics[width=1.0\\linewidth]{figure/ar_compare.pdf}     \\caption{     \\textbf{Comparison among different language modeling compatible autoregressive (AR) image generators.}     The proposed RAR demonstrates significant  improvements over previous AR methods. RAR-B, with only \\textit{261M} parameters, achieves an FID score of 1.95, outperforming both LlamaGen-XXL (1.4B parameters) and Open-MAGVIT2-XL (1.5B parameters).      }     \\label{fig:ar_compare}\nTo this end, we present a simple, effective, and scalable autoregressive model training paradiam named \\textbf{R}andomized \\textbf{A}uto\\textbf{R}egressive modeling (\\textbf{RAR}). RAR retains the original autoregressive model architecture and formulation, ensuring full compatibility with language modeling. At the same time, it significantly improves the generation quality of autoregressive models at no additional cost. On the ImageNet-256 benchmark~\\cite{deng2009imagenet}, RAR achieves an FID score of $1.48$, substantially outperforming previous state-of-the-art autoregressive image generators, as illustrated in~\\figref{fig:ar_compare}. By addressing the limitations of unidirectional context modeling, RAR represents a critical step towards autoregressive visual generation and opens up new possibilities for further advancements in the field.",
        "summarize_figure": "figure1",
        "summarization": "The figure compares different language modeling compatible autoregressive (AR) image generators. The horizontal axis represents the model size in millions (M), and the vertical axis represents the FID score, where a lower FID score is better. The RAR (ours) model exhibits lower FID scores across different model sizes, significantly outperforming other AR methods such as LlamaGen, Open-MAGVIT2, and TamingTrans. Specifically, the RAR model achieves an FID score below 2.0 when the model size is around 200M, and the FID score decreases slightly with increasing model size, then stabilizes. In contrast, LlamaGen and Open-MAGVIT2 have higher FID scores, with smaller decreases as the model size increases. TamingTrans has the highest FID score at approximately 5.2. The FID score of VIM is approximately 3.0, and the model size is approximately 1600M."
    },
    "390": {
        "figure1": "2411.00864v1_word2vec_example",
        "label1": "fig:word2vec",
        "caption1": " 3 dimensional representation of the words words \"gun\", \"firearm\", and \"knife\". Similarity between 'gun' and 'knife': 0.5895; Similarity between 'gun' and 'firearm': 0.7925; Similarity between 'knife' and 'firearm': 0.5644. ",
        "text": "As an example, consider the words ``gun'', ``firearm'' and ``knife''. Using the fastText embedding technique \\cite{DBLP:journals/corr/JoulinGBM16}, we can plot the three words and calculate the cosine similarity between them (See Figure ~\\ref{fig:word2vec}). It is expected that the first two words will be more similar than with the third one, since they share more closely semantic meanings.\n\\includegraphics[width=0.9\\textwidth]{word2vec_example.png}   \\caption   {     3 dimensional representation of the words words \"gun\", \"firearm\", and \"knife\". Similarity between 'gun' and 'knife': 0.5895; Similarity between 'gun' and 'firearm': 0.7925; Similarity between 'knife' and 'firearm': 0.5644.   }   \\label{fig:word2vec}",
        "summarize_figure": "figure1",
        "summarization": "The figure is a 3D PCA projection of vectors for the words \"gun\", \"knife\", and \"firearm\" after applying the fastText embedding technique. It shows the vector representation of these three words in a 3D space. According to the description, the similarity between 'gun' and 'knife' is 0.5895, the similarity between 'gun' and 'firearm' is 0.7925, and the similarity between 'knife' and 'firearm' is 0.5644. It can be seen that 'gun' and 'firearm' have the highest similarity, followed by 'gun' and 'knife', and 'knife' and 'firearm' have the lowest similarity."
    },
    "391": {
        "figure1": "2411.00869v1_fig5",
        "label1": "fig:5",
        "caption1": "Confusion Matrix of Federated Model on Independent Test-set ",
        "text": "\\centering     \\hspace*{-0.5cm}     \\caption{Confusion Matrix of Federated Model on Independent Test-set }      \\label{fig:5}",
        "summarize_figure": "figure1",
        "summarization": "The image is the confusion matrix of the federated model on an independent test set. The image shows the model's prediction results across five different categories: No DR, Mild DR, Moderate DR, Severe DR, and Proliferative DR. The numbers on the diagonal represent the number of correctly classified samples, which are 1185 for No DR, 1083 for Mild DR, 1098 for Moderate DR, 1228 for Severe DR, and 1231 for Proliferative DR. The numbers off the diagonal indicate the number of times the model misclassified one category as another. For example, 128 Mild DR samples were misclassified as No DR."
    },
    "392": {
        "figure1": "2411.00869v1_fig6",
        "label1": "fig:6",
        "caption1": "Generalizibility Matrix of Each Model at Each Test-Set ",
        "text": "There may exist discrepancies in imaging equipment, differences in patient population, and fundus image preparation in real-world scenarios. Therefore, it is crucial for DR diagnosis models to generalize to data of different image preparation procedures and different patient populations. In real life, images at a particular medical institution may be of lower-quality due to poor imaging equipment. As a result, a generalizibility evaluation was simulated in this work by purposefully reducing the image quality of all fundus images at H3's dataset. This institution also consisted of the least amount of images to simulate the data-inadequecy of medical institutions in under-resourced regions. After the federated-learning training process, each local model was tested on its own local test-set as well as the test-set of the other two medical instituions. The federated model was also tested on the test-sets of all three medical institutions. Figure 6 displays the results of this experiment.      \\centering     \\hspace*{-0.5cm}     \\caption{Generalizibility Matrix of Each Model at Each Test-Set }      \\label{fig:6}",
        "summarize_figure": "figure1",
        "summarization": "The figure presents a generalizability matrix evaluating the performance of different models on different test sets. The matrix compares the H1 model, H2 model, H3 model, and federated model on the H1 test set, H2 test set, and H3 test set. The H1 model performs best on the H1 test set, with a score of 0.9246. The H2 model performs best on the H2 test set, with a score of 0.9124. The H3 model's performance on the H3 test set is lower than the other models, but the federated model demonstrates the highest generalizability across all test sets, reaching scores of 0.9708, 0.9634, and 0.9105 on the H1 test set, H2 test set, and H3 test set, respectively."
    },
    "393": {
        "figure1": "2411.00871v1_oversmoothing",
        "label1": "fig:oversmoothing_fig",
        "caption1": "Node representations of graph encoder with 1,2,4,5 layers. As the number of layers increases, node representations collapse. % example of limitation of details. }  % \\vspace{-17pt",
        "text": "\\centering     \\includegraphics[width=1.0\\textwidth]{Figures/oversmoothing.pdf}     \\caption{Node representations of graph encoder with 1,2,4,5 layers. As the number of layers increases, node representations collapse.     % example of limitation of details.     }      \\label{fig:oversmoothing_fig}     % \\vspace{-17pt}",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the node representations of a graph encoder with varying numbers of layers (1, 2, 4, and 5 layers). It demonstrates that as the number of layers increases, the node representations tend to cluster together, eventually collapsing. With 1 layer, the node distribution is relatively dispersed. As the number of layers increases to 2, the nodes start to cluster. By 4 and 5 layers, the nodes are almost entirely clustered together, making them difficult to distinguish."
    },
    "394": {
        "figure1": "2411.00873v1_clean_nois_mem",
        "label1": "fig:mem_diff",
        "caption1": "Ratios of memorizing clean (\\textbf{Left}, larger is better) and noisy samples (\\textbf{Right}, smaller is better) on different routing methods. Dashed lines and solid lines indicate Deterministic Routing and Clean Routing (ours), respectively.",
        "text": "As shown in the lower part of Table 3, adopting Random and Noisy Routing substantially decreases both metrics, indicating that favoring clean samples in the routing is indeed beneficial to achieve both generalization and robustness ability. We then compare our Clean Routing with the Deterministic Routing. We observe that Deterministic Routing exhibits inferior performance than our Clean Routing. Moreover, Figure \\ref{fig:mem_diff} illustrates the differences in accuracy on clean and noisy samples between Clean Routing and Deterministic Routing.  It reveals that Deterministic Routing leads to increased memorization of noisy samples compared to our stochastic Clean Routing, which is attributed to lower performance. These results emphasize the significance of stochastic routing in Clean Routing, which can differentiate samples in a more fine-grained manner.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the ratios of memorizing correct samples (left, larger is better) and noisy samples (right, smaller is better) under different routing methods. Dashed lines represent Deterministic Routing, and solid lines represent Clean Routing. As shown in the figure, Clean Routing learns correct samples better and suppresses the memorization of noisy samples to some extent compared to Deterministic Routing. In the early training stages, the memorization ratios of noisy samples are similar for both routing methods. However, as training progresses, Deterministic Routing exhibits a significant increase in the memorization ratio of noisy samples, while Clean Routing shows a slower increase in this ratio. Clean Routing demonstrates a finer-grained approach in differentiating samples."
    },
    "395": {
        "figure1": "2411.00873v1_peft_analysis_bank_loss",
        "label1": "fig:peft_analysis_bank",
        "caption1": "Comparison between PEFT methods and full fine-tuning on BANKING77 with symmetric noise (60\\%). Dashed lines represent the training accuracy and loss of clean samples on uncorrupted datasets (i.e. only clean samples).",
        "text": "We further conducted the same analysis on a different dataset to validate the generality of our observations. Specifically, we examined PEFT methods on the BANKING77 \\cite{banking77} dataset, which focuses on an intent detection task. The results are presented in Figure \\ref{fig:peft_analysis_bank}. Similar to our analysis of the SST-5 dataset, we observed a similar trend: (i) PEFT methods exhibit greater robustness than full fine-tuning, and (ii) its limited memorization on noisy label attributes to robustness, although they also inhibit memorization even on clean samples. These findings further support our observations and analysis regarding the robustness of PEFT methods to noisy labels.",
        "summarize_figure": "figure1",
        "summarization": "The picture illustrates the comparison results between different Parameter-Efficient Fine-Tuning (PEFT) methods and Full Fine-tuning on the BANKING77 dataset with symmetric noise (60%). The upper part shows the change of test accuracy with training epochs, and the lower part shows the change of training loss with training iterations. In the figure, the solid line represents the training results of noisy samples, and the dashed line represents the training results of clean samples only on the uncorrupted dataset. It can be seen that for different PEFT methods (BitFit, Prompt, LoRA) and Full Fine-tuning, the training loss of noisy samples is higher than that of clean samples, but the PEFT method shows better robustness than Full Fine-tuning on the whole."
    },
    "396": {
        "figure1": "2411.00876v1_DB_vs_AUC",
        "label1": "fig:davies_bouldin_comparison",
        "caption1": "Average AUC vs D-B index for the \\texttt{sOSR} approach and the datasets from the benchmark at different values of $\\beta$.",
        "text": "Although our proposal effectively learns to detect UC, it still undergoes similar issues to a regular classifier and is not enough to solve the issue. As discussed in Section \\ref{Proposed Approach}, the reliability of the \\textit{entropy} measure depends on the ability of the clustering model to find representative centers. This statement is buttressed by Figure \\ref{fig:davies_bouldin_comparison}, which depicts the Davies-Bouldin (D-B) index of the \\texttt{sOSR} approach for all datasets at each level of $\\beta$. The higher the D-B index is, the lower the quality of the clustering will be. The core limitation of the STREAMKmeans algorithm, together with the fixed value of $\\gamma_H$ used in the experiments, make the overall framework not to be suitable to properly characterize all data streams in the benchmark, as evinced by the correlation between the D-B indices and the AUC scores of the \\texttt{sOSR} framework in Tables \\ref{tab:performance_comparison} and \\ref{tab:performance_comparison2}. An incorrect threshold also damages the performance of the closed-set classifier due to many KC instances being identified as UC. Future work will explore the use of automated configuration techniques and other incremental clustering algorithms, potentially better suited for complex streaming data distributions.",
        "summarize_figure": "figure1",
        "summarization": "The image illustrates the relationship between Average AUC and Average D-B index for evaluating the sOSR method's performance. It includes results for the isoGauss, hyperCube, and insects datasets. The isoGauss dataset exhibits high Average AUC and low Average D-B index, indicating good clustering quality. The hyperCube dataset has medium Average AUC and Average D-B index. The insects dataset shows low Average AUC and high Average D-B index, suggesting poor clustering quality. Overall, there is a negative correlation between Average AUC and Average D-B index, with a Pearson correlation coefficient of -0.92. A higher D-B index indicates lower clustering quality."
    },
    "397": {
        "figure1": "2411.00888v1_ablation_v6",
        "label1": "ablation",
        "caption1": "skip}{0pt} \\setlength{\\belowcaptionskip}{0pt} \\setlength{\\abovedisplayskip}{0pt} \\setlength\\belowdisplayskip{0pt} \\centering \\includegraphics[width=0.98\\linewidth]{ablation_v6.pdf} \\caption{Results of our methods and its variants in ANI vs. HC classification.",
        "text": "\\centering \t\\includegraphics[width=0.98\\linewidth]{ablation_v6.pdf} \t\\caption{Results of our methods   and its variants in ANI vs. HC classification.} \t\\label{ablation}\nWe compare TGAN and TGAE with 5 variants:  \\textbf{Naive} trained on the target cohort without the pretext model, without the learnable attention mask, with results %of these methods for ANI vs. HC classification  reported in Fig.~\\ref{ablation}. From Fig.~\\ref{ablation}, TGAN and TGAE are superior to the Naive, % (without pretrained encoder).  validating the necessity of utilizing auxiliary fMRI  for model pretraining  when dealing with small-scale target data.  Besides, our methods %(\\ie, TGAN and TGAE) generally outperform frozen counterparts (\\ie, TGAN-F and TGAE-F), %whose encoders are not further fine-tuned on target data,  indicating that fine-tuning can enhance the adaptability of the pretrained encoder to target data.  Additionally, our methods generally outperform TGANw/tA and TGAEw/tA in most cases,  suggesting the effectiveness of the attention mask in learning discriminative graph features.",
        "summarize_figure": "figure1",
        "summarization": "The figure illustrates the results of different methods and their variants in the ANI vs. HC classification. For the AUC metric, TGAN (Ours) and TGAE (Ours) achieve the best performance, reaching approximately 67.9% and 69.1% respectively, significantly outperforming the Naive method (approximately 64%) and other variants such as TGAN-F, TGAE-F, TGANw/tA, and TGAEw/tA. In terms of ACC, TGAE (Ours) slightly outperforms other methods, reaching 62.8%. TGAN and TGAE also demonstrate relatively good performance in SEN, SPE, and BAC metrics. Overall, TGAN and TGAE show competitive performance across all metrics, indicating the effectiveness of these two methods in the ANI vs. HC classification task."
    },
    "398": {
        "figure1": "2411.00893v1_Dirichlet",
        "label1": "fig:Dirichlet Model",
        "caption1": "Visualization of $\\qr$ in \\eqref{eq:circ model}. ",
        "text": "Let $\\hp$ denote the Discrete Fourier Transform (DFT) of $\\fr$. We show that the DFT coefficients $\\{\\hp[l]\\}_{l\\in\\id{M}}$ and its continuous counterpart $\\{\\bp[l]\\}_{l\\in\\id{M}}$ are linearly dependent, since &=\\inner{\\fr(t)}{\\sum\\nolimits_{|m|\\leqslant L}  e^{-\\jmath \\frac{2m\\pi t}{\\tau}} \\delta \\sqb{l-m}} = \\inner{\\fr(t)}{e^{-\\jmath \\frac{2l\\pi t}{\\tau}}} \\eqr{eq:Fourier series} \\tau \\bp[l]. Similarly, we have  $ $ where $\\bgr[m]= \\frac{1}{\\tau}\\int_{0}^{\\tau} {\\gr(t)}  \\e^{-\\jmath \\frac{2m\\pi t}{\\tau}} dt$. Moreover, notice that, = \\sum\\nolimits_{k = 0}^{K - 1} {\\gammar\\sqb{k}\\fr \\left( {t - \\taur\\sqb{k}} \\right)}  \\\\ &= \\sum\\nolimits_{m \\in \\mathbb{Z}} (\\bp[m] \\sum\\nolimits_{k = 0}^{K - 1} \\gammar\\sqb{k}  \\e^{-\\jmath \\frac{2m\\pi \\taur[k]}{\\tau}}) \\e^{\\jmath \\frac{2m\\pi t}{\\tau}}. Hence, $\\bgr[m]$ can be expressed in terms of $\\bp[m]$ and $\\{\\gammar[k], \\taur[k]\\}_{k\\in \\id{K}}$ as Combining \\eqref{eq:convolution} and \\eqref{eq:bh}, we eventually derive the interplay between $\\hgr$ and $\\hp$ as &=\\sum\\nolimits_{u =0}^{N-1} \\fr [u] \\rob{\\frac{1}{N}  \\sum\\nolimits_{l =0}^{N-1} \\bsr [l] \\zm{N}{(n-u)\\cdot l} }  =\\sum\\nolimits_{u =0}^{N-1} \\fr [u]  \\qr[\\rob{n-u}_{{\\rm mod}\\;N}]  where ${\\rm mod}$ denotes modulo operation and $\\qr$ is  parametrized by $\\{\\taur[k], \\gammar[k]\\}_{k\\in\\id{K}}$ (see \\fig{fig:Dirichlet Model}) as &= \\sum\\limits_{k=0}^{K-1}  \\frac{{{\\Gamma _{\\mathbf{r}}}\\left[ k \\right]} \\big(1 - (u_k)^N \\big)  }{ N(1 - u_k \\zm{N}{n}) }  = \\frac{\\PMs (\\zm{N}{n})}{\\QMs (\\zm{N}{n})}, \\; \\;  In the above, $\\QMs$ is related to $\\H$ in \\eqref{eq:h filter} as $ $",
        "summarize_figure": "figure1",
        "summarization": "Here is a summary of the data in the figure:The figure shows the real and imaginary parts of the Dirichlet kernel, as well as samples of the real and imaginary parts. The horizontal axis is the sample index, ranging from 0 to 8. The vertical axis is the amplitude, in a.u. The real part of the Dirichlet kernel is represented by a red curve, and the imaginary part of the Dirichlet kernel is represented by a blue curve. Red circles represent real samples, and blue squares represent imaginary samples. At the sample index of 3, there is a black triangle, indicating the spike to be estimated. The real part reaches its peak at sample index 3, while the imaginary part has a value close to zero at this point."
    },
    "399": {
        "figure1": "2411.00894v1_LP",
        "label1": "fig:littlewood",
        "caption1": "Illustration of the Fourier transform magnitude of a $\\Delta_j$ operator in one dimension.",
        "text": "The Littlewood-Paley filter associated with scale $j$, denoted $\\Delta_j$, is defined by 1 \\qquad \\text{if}\\quad 2^{j-1}\\leqslant\\xi\\leqslant 2^{j} \\\\ 0 \\qquad \\text{if}\\quad \\xi\\leqslant 2^{j-2} \\qquad \\text{or} \\qquad 2^{j+1}\\leqslant \\xi. Figure~\\ref{fig:littlewood} sketches the magnitude of the Fourier transform of such operator.",
        "summarize_figure": "figure1",
        "summarization": "Here is a summary of the chart:The picture illustrates the Fourier transform magnitude of a one-dimensional Delta subscript j operator. The horizontal axis ranges from 0 to 1, and the vertical axis ranges from 0 to 1. It can be seen that the curve rises sharply at approximately 0.3 on the horizontal axis, remains at a level close to 1 between 0.4 and 0.6, then rapidly decreases after 0.7, and approaches 0 near 1. This shape resembles a band-pass filter, with a high amplitude response within a specific frequency range."
    }
}